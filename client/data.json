{
    "status": "success",
    "course_id": 84647,
    "scraped_at": "2025-12-09 17:21:26",
    "post_count": 560,
    "posts": [
        {
            "guid": 7435036,
            "author": "Sultan Daniels",
            "project_title": "Exam Review Sessions",
            "post_body": "On Wednesday, December 10, instead of discussion sections, there will be exam review sessions. Each review session will focus on one or two homework problems for a specific topic. This gives an opportunity to ask questions to the GSI while solving the problem(s).  Here are the details for the review sessions:\n\nKevin 11am-12pm Zoom (Optimizers)\n\nMeeting ID: 968 5129 5691\n\nPasscode: 158789\n\nProblems\n\nHW3 Q1\n\nHW3 Q2 (muP)\n\nReview of Muon\n\nSultan 3-4pm 136 Social Sciences (Transformers/Attention)\n\nHW9 Q4: Transformer Decoding Optimization\n\nHW9 Q2: Argmax Attention\n\nJoey 4-5pm Hearst Field Annex B1 (Post-training)\n\nReview of RLHF and DPO Motivation\n\nHW 13 Q2 (DPO)\n\nHW 12 Old Exam Problem (if time permits)\n\nLance 5-6pm 242 Hearst Gym (Diffusion)\n\nHW13 Q4: Diffusion Models",
            "content_xml": "<document version=\"2.0\"><paragraph>On <bold>Wednesday, December 10,</bold> instead of discussion sections, there will be exam review sessions. Each review session will focus on one or two homework problems for a specific topic. This gives an opportunity to ask questions to the GSI while solving the problem(s).  Here are the details for the review sessions:</paragraph><list style=\"bullet\"><list-item><paragraph>Kevin 11am-12pm <link href=\"https://berkeley.zoom.us/j/96851295691?pwd=ZjjNALlB0CKY9NftLEM902diJ3fDOb.1\">Zoom</link> <bold>(Optimizers)</bold></paragraph><list style=\"bullet\"><list-item><paragraph>Meeting ID: 968 5129 5691</paragraph></list-item><list-item><paragraph>Passcode: 158789</paragraph></list-item><list-item><paragraph>Problems</paragraph></list-item></list></list-item><list-item><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph>HW3 Q1</paragraph></list-item><list-item><paragraph>HW3 Q2 (muP)</paragraph></list-item><list-item><paragraph>Review of Muon</paragraph></list-item></list></list-item></list></list-item><list-item><paragraph>Sultan 3-4pm 136 Social Sciences <bold>(Transformers/Attention)</bold></paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>HW9 Q4: Transformer Decoding Optimization</paragraph></list-item><list-item><paragraph>HW9 Q2: Argmax Attention</paragraph></list-item></list></list-item><list-item><paragraph>Joey 4-5pm Hearst Field Annex B1 <bold>(Post-training)</bold></paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Review of RLHF and DPO Motivation</paragraph></list-item><list-item><paragraph>HW 13 Q2 (DPO)</paragraph></list-item><list-item><paragraph>HW 12 Old Exam Problem (if time permits)</paragraph></list-item></list></list-item><list-item><paragraph>Lance 5-6pm 242 Hearst Gym <bold>(Diffusion)</bold></paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>HW13 Q4: Diffusion Models</paragraph></list-item></list></list-item></list></document>",
            "links": [
                "https://berkeley.zoom.us/j/96851295691?pwd=ZjjNALlB0CKY9NftLEM902diJ3fDOb.1"
            ],
            "attachments": [],
            "created_at": "2025-12-09T08:55:57.534306+11:00",
            "category": "Exam"
        },
        {
            "guid": 7434972,
            "author": "Gireeja Ranade",
            "project_title": "Check your poster number! --- poster session logistics",
            "post_body": "Looking forward to the poster session tomorrow! Please arrive at the Woz at 11 am tomorrow with your poster. We will provide poster boards and easels. The session will go will 1 pm.\n\nEach poster/project has been assigned a number. All of the easels will also be numbered. Please put up your posters (or slide printouts) at the appropriate spot. Poster numbers are in the spreadsheet below.\n\n\n\nhttps://docs.google.com/spreadsheets/d/1IENZsyY4pVBS_SDWD4ZbQoQazJH0af3RsD0VC7FiI68/edit?usp=sharing\n\n\n\nBest,\n\n182 Staff",
            "content_xml": "<document version=\"2.0\"><paragraph>Looking forward to the poster session tomorrow! Please arrive at the Woz at 11 am tomorrow with your poster. We will provide poster boards and easels. The session will go will 1 pm.</paragraph><paragraph>Each poster/project has been assigned a number. All of the easels will also be numbered. Please put up your posters (or slide printouts) at the appropriate spot. Poster numbers are in the spreadsheet below.</paragraph><paragraph/><paragraph><link href=\"https://docs.google.com/spreadsheets/d/1IENZsyY4pVBS_SDWD4ZbQoQazJH0af3RsD0VC7FiI68/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1IENZsyY4pVBS_SDWD4ZbQoQazJH0af3RsD0VC7FiI68/edit?usp=sharing</link></paragraph><paragraph/><paragraph>Best,</paragraph><paragraph>182 Staff</paragraph></document>",
            "links": [
                "https://docs.google.com/spreadsheets/d/1IENZsyY4pVBS_SDWD4ZbQoQazJH0af3RsD0VC7FiI68/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-09T08:48:50.297059+11:00",
            "category": "Project"
        },
        {
            "guid": 7433631,
            "author": "Hong Joey",
            "project_title": "Reviews Assigned for Final Reports",
            "post_body": "Hi all,\n\nReview assignments have been made for all the final reports via CMT (go to the 'Reviewer' console of the conference link). Reviews will be due EOD Thursday (December 11). \n\nEach of you should be assigned exactly one review to do. The paper you review is likely different from the one you were assigned when reviewing drafts, but the questions you answer in the review will mostly be the same. To help with completing the review, we recommend visiting the poster for your assigned report at the poster session tomorrow. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi all,<break/><break/>Review assignments have been made for all the final reports via CMT (go to the 'Reviewer' console of the <link href=\"https://cmt3.research.microsoft.com/DLFP2025\">conference link</link>). Reviews will be <bold>due EOD Thursday (December 11)</bold>. <break/><break/>Each of you should be assigned exactly one review to do. The paper you review is likely different from the one you were assigned when reviewing drafts, but the questions you answer in the review will mostly be the same. To help with completing the review, we recommend visiting the poster for your assigned report at the poster session tomorrow. </paragraph><paragraph/></document>",
            "links": [
                "https://cmt3.research.microsoft.com/DLFP2025"
            ],
            "attachments": [],
            "created_at": "2025-12-09T06:17:09.726336+11:00",
            "category": "Project"
        },
        {
            "guid": 7412341,
            "author": "Anant Sahai",
            "project_title": "Lecture 27",
            "post_body": "This thread is to discuss the final lecture in the course. We finished our treatment of diffusion-model training, DDPM style stochastic sampling using what was trained, and DDIM style deterministic sampling using the same exact network that was trained in diffusion style. This should give you what you need to be able to understand diffusion-style models on your own as well as do the final homework 13.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/WphKDgwZ6aCiqLeisXGUGlwq\" filename=\"Lecture 27.pdf\"/><paragraph>This thread is to discuss the final lecture in the course. We finished our treatment of diffusion-model training, DDPM style stochastic sampling using what was trained, and DDIM style deterministic sampling using the same exact network that was trained in diffusion style. This should give you what you need to be able to understand diffusion-style models on your own as well as do the final homework 13.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T16:45:19.25015+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7412221,
            "author": "Alex Proshkin",
            "project_title": "Compute Portal Storage Update 12/4",
            "post_body": "Hi everyone,\n\nWe know some of you have been running into issues with the compute portal. To be fully transparent, the resources we received came with very minimal infrastructure, so we\u2019ve had to build almost everything from scratch. We appreciate your patience as we continue improving it.\n\nGoing forward, we don\u2019t think it\u2019s fair to impose strict storage limits on individuals, but we do ask that you be mindful of space usage. Each of the three instances only has 150 GB for the entire class, so if you\u2019re working with large files, please try to clean up and delete them when you\u2019re done otherwise the portal will again give 500 internal server error. If storage fills up again, we may need to manually clear space by removing the largest user directories.\n\nThanks for your understanding, and have fun with the projects!\n\nBest,\nAlex",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>We know some of you have been running into issues with the compute portal. To be fully transparent, the resources we received came with very minimal infrastructure, so we\u2019ve had to build almost everything from scratch. We appreciate your patience as we continue improving it.</paragraph><paragraph>Going forward, we don\u2019t think it\u2019s fair to impose strict storage limits on individuals, but we do ask that you be mindful of space usage. Each of the three instances only has <bold>150 GB for the entire class</bold>, so if you\u2019re working with large files, please try to clean up and delete them when you\u2019re done otherwise the portal will again give 500 internal server error. If storage fills up again, we may need to manually clear space by removing the largest user directories.</paragraph><paragraph>Thanks for your understanding, and have fun with the projects!</paragraph><paragraph>Best,<break/>Alex</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T16:21:12.097369+11:00",
            "category": "Project"
        },
        {
            "guid": 7405185,
            "author": "Sultan Daniels",
            "project_title": "HW 13: Diffusion Models (Optional)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/PzPtFcDaZKUCvI104USWaQaz\" width=\"658\" height=\"709.8853046594982\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T17:22:20.513792+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7405181,
            "author": "Sultan Daniels",
            "project_title": "HW 13: Honey, Where's My Reward Model?",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/hpOzuFhF0hrXYhAyYmscTzPf\" width=\"658\" height=\"286.5092250922509\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/lKUE3mhKLHgVr0BoiBTfxsQO\" width=\"658\" height=\"908.4886363636363\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/dR68NTC4j0kzbfvkN36iRCax\" width=\"643\" height=\"867.7495327102804\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/SIiueIwquedzJUT2kr0axf1I\" width=\"643\" height=\"181.3012259194396\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T17:21:35.8455+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7405171,
            "author": "Sultan Daniels",
            "project_title": "HW 13: DDPM/DDIM Fun: From a Gaussian",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/HMtv0DIIDxF6NsDFFIxN79MW\" width=\"658\" height=\"744.6753246753246\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/uO4mhDM9TGbijsnoQvZ0Haeb\" width=\"643\" height=\"587.5254901960784\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T17:19:45.368522+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7405023,
            "author": "Anant Sahai",
            "project_title": "Lecture 26 Thread",
            "post_body": "This lecture hit RLHF, DPO, and then started diffusion. We'll continue with diffusion in the next lecture. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Ykyum7otxP4qvEJ6jX3DokzX\" filename=\"Lecture 26.pdf\"/><paragraph>This lecture hit RLHF, DPO, and then started diffusion. We'll continue with diffusion in the next lecture. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T16:48:02.895965+11:00",
            "category": "Admin"
        },
        {
            "guid": 7403736,
            "author": "Lance Mathias",
            "project_title": "Discussion 13 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/rNoLQqkW07FC3a0drChDQNtp\" filename=\"dis13_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/RTl8Dome01whe72wPeSuBW2a\" filename=\"dis13_solution.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T13:33:38.03238+11:00",
            "category": "Sections"
        },
        {
            "guid": 7395242,
            "author": "Gireeja Ranade",
            "project_title": "Extra Credit Update",
            "post_body": "We were overwhelmed by the response to the Extra Credit Post --- we thought there would only be a few students interested.\n\nIn light of this, we are changing the nature of the extra credit to try a new experiment  --- so please ask clarification questions if anything isn't clear.\n\n\nThe new rules for extra credit are as follows. For each participation category there will be Blue Teams and Red Teams.\n\nRed Team: The job of the Red Team is to establish a baseline. What is the best website that can be purely AI generated (with human guidance)? So be smart about how you prompt and thoughtful about the design, but code should be mostly AI written. Any analysis, summarization, grouping etc. should also be done by an AI. Meta level orchestrations is fine by a person. For instance, you can read a subset of the posts for validation purposes, but you should not read all the posts. It should be as though a human never looked at any of the posts or submissions to understand what they are doing. Ideally though there should be a functioning website that displays all content in a readable fashion. If you end up in a situation where it not possible to prompt engineer your way to fucntionality, put in the minimum human effort required to get this functioning and then document a short document/paragraph explaining what was done by an agent and where basic human intervention was necessary. All Red Team submissions will be awarded 4-5 points per student team member. Please feel free to ask clarification questions -- the goal of this is more to do a fun experiment than to be some ridiculous hoop that must be jumped through, we're very curious how far this can get! \n\n\nBlue Team: Your job is to do better than the Red Teams. You may use as much AI as you wish to help generate your website but we ask that you go beyond. Blue Teams will be awarded 5-10 points credit per student (based on staff judgement) if they can do better than the baseline. \n\n\nNote that these points are added to the total points in the course.\n\n\nPlease add yourself to the spreadsheet below so it will be easier to track the teams: https://docs.google.com/spreadsheets/d/14QaSXdvKzj1UGRjPJ6-udmzQSB_UyyS5fl4-VvT6sLs/edit?gid=0#gid=0\n\nNote: all the constraints in the earlier post still apply. Max team size is still 4 etc. \n\nhttps://edstem.org/us/courses/84647/discussion/7384785\n\nAlso note: You may only sign up for ONE team.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>We were overwhelmed by the response to the Extra Credit Post --- we thought there would only be a few students interested.<break/><break/>In light of this, we are changing the nature of the extra credit to try a new experiment  --- so please ask clarification questions if anything isn't clear.</paragraph><paragraph><break/>The new rules for extra credit are as follows. For each participation category there will be Blue Teams and Red Teams.<break/><break/>Red Team: The job of the Red Team is to establish a baseline. What is the best website that can be purely AI generated (with human guidance)? So be smart about how you prompt and thoughtful about the design, but code should be mostly AI written. Any analysis, summarization, grouping etc. should also be done by an AI. Meta level orchestrations is fine by a person. For instance, you can read a subset of the posts for validation purposes, but you should not read all the posts. It should be as though a human never looked at any of the posts or submissions to understand what they are doing. Ideally though there should be a functioning website that displays all content in a readable fashion. If you end up in a situation where it not possible to prompt engineer your way to fucntionality, put in the minimum human effort required to get this functioning and then document a short document/paragraph explaining what was done by an agent and where basic human intervention was necessary. All Red Team submissions will be awarded 4-5 points per student team member. Please feel free to ask clarification questions -- the goal of this is more to do a fun experiment than to be some ridiculous hoop that must be jumped through, we're very curious how far this can get! <break/></paragraph><paragraph>Blue Team: Your job is to do better than the Red Teams. You may use as much AI as you wish to help generate your website but we ask that you go beyond. Blue Teams will be awarded 5-10 points credit per student (based on staff judgement) if they can do better than the baseline. <break/></paragraph><paragraph>Note that these points are added to the total points in the course.<break/></paragraph><paragraph>Please add yourself to the spreadsheet below so it will be easier to track the teams: <link href=\"https://docs.google.com/spreadsheets/d/14QaSXdvKzj1UGRjPJ6-udmzQSB_UyyS5fl4-VvT6sLs/edit?gid=0#gid=0\">https://docs.google.com/spreadsheets/d/14QaSXdvKzj1UGRjPJ6-udmzQSB_UyyS5fl4-VvT6sLs/edit?gid=0#gid=0</link></paragraph><paragraph><bold>Note: all the constraints in the earlier post still apply. Max team size is still 4 etc.</bold> </paragraph><paragraph><link href=\"https://edstem.org/us/courses/84647/discussion/7384785\">https://edstem.org/us/courses/84647/discussion/7384785</link></paragraph><paragraph><bold>Also note: You may only sign up for ONE team.</bold></paragraph><paragraph/></document>",
            "links": [
                "https://docs.google.com/spreadsheets/d/14QaSXdvKzj1UGRjPJ6-udmzQSB_UyyS5fl4-VvT6sLs/edit?gid=0#gid=0",
                "https://edstem.org/us/courses/84647/discussion/7384785"
            ],
            "attachments": [],
            "created_at": "2025-12-03T12:11:43.35436+11:00",
            "category": "Admin"
        },
        {
            "guid": 7394497,
            "author": "Anant Sahai",
            "project_title": "Participation Form",
            "post_body": "Please copy the following Google Doc into your own drive:\n\nhttps://docs.google.com/document/d/1NkNZbLGbU-XeDaV6IqsEPuabpOqTri0YTqpkZ54-xhE/edit?usp=sharing\n\nAnd submit a PDF to Gradescope after you fill it out.  Submit to only one of the participation assignments --- the one that corresponds to the GSI/Tutor you feel would recognize you the best. For those counting on online (rather than in-person discussion), that is likely to be Kevin. \n\nThe form is due on Wednesday Dec 10th.\n\nThe time required to do this should be minimal. Take a selfie of sufficient quality that it will help course staff remember you clearly. Place that on the second page. Fill out the rest. \n\nFor those who did basic participation by coming to discussion and participating throughout the term, you basically just have to point us to your four Special Participation posts at the end. \n\nIf you need basic participation credit from having regularly interacted on Ed online instead, then also use the fourth and fifth page to insert screenshots of your posts/followups.  You have to fit within those two pages.\n\nThe purpose of doing this is to allow you yourself to put yourself in the best light possible vis-a-vis your class participation. Remember, participation is worth 15% of your grade. (It's impossible to get an A of any kind without participation. There is no curve and we use fixed bins.)",
            "content_xml": "<document version=\"2.0\"><paragraph>Please copy the following Google Doc into your own drive:</paragraph><paragraph><link href=\"https://docs.google.com/document/d/1NkNZbLGbU-XeDaV6IqsEPuabpOqTri0YTqpkZ54-xhE/edit?usp=sharing\">https://docs.google.com/document/d/1NkNZbLGbU-XeDaV6IqsEPuabpOqTri0YTqpkZ54-xhE/edit?usp=sharing</link></paragraph><paragraph>And submit a PDF to Gradescope after you fill it out.  Submit to only one of the participation assignments --- the one that corresponds to the GSI/Tutor you feel would recognize you the best. For those counting on online (rather than in-person discussion), that is likely to be Kevin. </paragraph><paragraph>The form is due on Wednesday Dec 10th.</paragraph><paragraph>The time required to do this should be minimal. Take a selfie of sufficient quality that it will help course staff remember you clearly. Place that on the second page. Fill out the rest. </paragraph><paragraph>For those who did basic participation by coming to discussion and participating throughout the term, you basically just have to point us to your four Special Participation posts at the end. </paragraph><paragraph>If you need basic participation credit from having regularly interacted on Ed online instead, then also use the fourth and fifth page to insert screenshots of your posts/followups.  You have to fit within those two pages.</paragraph><paragraph>The purpose of doing this is to allow you yourself to put yourself in the best light possible vis-a-vis your class participation. Remember, participation is worth 15% of your grade. (It's impossible to get an A of any kind without participation. There is no curve and we use fixed bins.)</paragraph></document>",
            "links": [
                "https://docs.google.com/document/d/1NkNZbLGbU-XeDaV6IqsEPuabpOqTri0YTqpkZ54-xhE/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-03T10:43:25.838509+11:00",
            "category": "Admin"
        },
        {
            "guid": 7385279,
            "author": "Hong Joey",
            "project_title": "Project Report Final Version",
            "post_body": "Please submit final reports to:  https://cmt3.research.microsoft.com/DLFP2025\n\nNote that this is technically a different \"conference\" than the one you guys used for drafts. The process for making a submission should be the same as last time though. \n\nHowever, this means that you all should have received another reviewer invitation for this conference. Please accept the reviewer invite as soon as possible, so we can (hopefully) do reviewer assignments more smoothly. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Please submit final reports to:  <link href=\"https://cmt3.research.microsoft.com/DLFP2025\">https://cmt3.research.microsoft.com/DLFP2025</link></paragraph><paragraph>Note that this is technically a different \"conference\" than the one you guys used for drafts. The process for making a submission should be the same as last time though. <break/><break/>However, this means that you all should have received another reviewer invitation for this conference. Please accept the reviewer invite as soon as possible, so we can (hopefully) do reviewer assignments more smoothly. </paragraph><paragraph/></document>",
            "links": [
                "https://cmt3.research.microsoft.com/DLFP2025"
            ],
            "attachments": [],
            "created_at": "2025-12-02T08:08:13.388091+11:00",
            "category": "Project"
        },
        {
            "guid": 7384785,
            "author": "Gireeja Ranade",
            "project_title": "Extra Credit Opportunity --- curate special participation posts",
            "post_body": "Please see the new updated extra credit post here: https://edstem.org/us/courses/84647/discussion/7395242\n\nDear students, \n\nWe've enjoyed reading your extra credits posts and engagement throughout the semester and found it very interesting ourselves. So much so that we would like to record this for posterity and create a forum that allows you to get visibility for this. This means we can follow up on some of your ideas more easily as well (with due credit of course). As a first step in this direction, we would like to create a searchable website that will document all participation for each of the extra credit participation categories. This would allow us, and other students, to easily navigate through the different ideas and explore what works for them and what does not. Furthermore, our goal will be to link student websites/linked-in pages, so all of you can gain visibility on the work you have done. This website will be hosted on the eecs182.org.\n\nWe are offering 5-10 points of extra credit per student (more points for better quality submissions, more points for the categories we care about more) for students who help make such a website. We understand this will be a significant lift, but hope you can leverage deep learning skills to make this happen. Ideally, this will be presented as a directory that we can simply drop into the eecs182 website. Multiple students are allowed to collaborate on this as below:\n\n\n\nWe care the most about this for E since this allows us to easily build tools in the future.\n\nSpecial Participation E: 3-4 students\n\nA nice version of this would categorize the submissions into different types of submissions (e.g. generating new questions, helping understand existing content, creating new content etc. --- come up with your own categories.) If you think some submissions are particularly impressive, highlight them. \n\n\n\nSecond we care about documenting student interactions with LLMs.\n\nSpecial Participation A: 3-4 students\n\nSpecial Participation B: 3-4 students\n\nFor each of these two cases, we would like to have a summary of insights on how each of the different LLMs behave and common issues. What insights were gained from \n\n\n\nThird we care about the Muon and MuP updates\n\nSpecial Participation D: 2 students\n\n\n\nLast (fourth) priority\n\nSince it would be nice to have this all coordinated in one website, if someone wants to take on a coordination role: 1-2 students. \n\nFor each kind of special participation we should be able to read what every student submitted as text and as attachment. Every student should get credited for the work they have put in. Include links to student websites/github repos if those are included in the post (All students --- feel free to go and edit your posts to include this information so that they will show up in the common collection).\n\nBonus points if the website is searchable by keyword/student name etc. Other design choices we leave up to you. Feel free to ask questions below.  \n\n\n\nPlease do not claim a spot to complete this unless you are truly planning to, since it does not allow others to take this on. Use this thread to self-organize so you can make teams to do this. \n\nThanks,\n\n182 Staff\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Please see the new updated extra credit post here: https://edstem.org/us/courses/84647/discussion/7395242</paragraph><paragraph>Dear students, </paragraph><paragraph>We've enjoyed reading your extra credits posts and engagement throughout the semester and found it very interesting ourselves. So much so that we would like to record this for posterity and create a forum that allows you to get visibility for this. This means we can follow up on some of your ideas more easily as well (with due credit of course). As a first step in this direction, we would like to create <bold>a searchable website that will document all participation for each of the extra credit participation categories</bold>. This would allow us, and other students, to easily navigate through the different ideas and explore what works for them and what does not. Furthermore, our goal will be to link student websites/linked-in pages, so all of you <bold>can gain visibility</bold> on the work you have done. This website will be hosted on the eecs182.org.</paragraph><paragraph>We are offering <bold>5-10 points of extra credit</bold> per student (more points for better quality submissions, more points for the categories we care about more) for students who help make such a website. We understand this will be a significant lift, but hope you can leverage deep learning skills to make this happen. Ideally, this will be presented as a directory that we can simply drop into the eecs182 website. Multiple students are allowed to collaborate on this as below:</paragraph><paragraph/><list style=\"unordered\"><list-item><paragraph>We care the most about this for E since this allows us to easily build tools in the future.</paragraph></list-item></list><blockquote>Special Participation E: 3-4 students</blockquote><blockquote>A nice version of this would categorize the submissions into different types of submissions (e.g. generating new questions, helping understand existing content, creating new content etc. --- come up with your own categories.) If you think some submissions are particularly impressive, highlight them. </blockquote><paragraph/><list style=\"unordered\"><list-item><paragraph>Second we care about documenting student interactions with LLMs.</paragraph></list-item></list><blockquote>Special Participation A: 3-4 students</blockquote><blockquote>Special Participation B: 3-4 students</blockquote><blockquote>For each of these two cases, we would like to have a summary of insights on how each of the different LLMs behave and common issues. What insights were gained from </blockquote><paragraph/><list style=\"unordered\"><list-item><paragraph>Third we care about the Muon and MuP updates</paragraph></list-item></list><blockquote>Special Participation D: 2 students</blockquote><paragraph/><list style=\"unordered\"><list-item><paragraph>Last (fourth) priority</paragraph></list-item></list><blockquote>Since it would be nice to have this all coordinated in one website, if someone wants to take on a coordination role: 1-2 students. </blockquote><paragraph>For each kind of special participation we should be able to read what every student submitted as text and as attachment. Every student should get credited for the work they have put in. Include links to student websites/github repos if those are included in the post <bold>(All students --- feel free to go and edit your posts to include this information so that they will show up in the common collection).</bold></paragraph><paragraph>Bonus points if the website is searchable by keyword/student name etc. Other design choices we leave up to you. Feel free to ask questions below.  </paragraph><paragraph/><paragraph>Please do not claim a spot to complete this unless you are truly planning to, since it does not allow others to take this on. Use this thread to self-organize so you can make teams to do this. </paragraph><paragraph>Thanks,</paragraph><paragraph>182 Staff</paragraph><list style=\"bullet\"/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T07:11:44.121633+11:00",
            "category": "Admin"
        },
        {
            "guid": 7377005,
            "author": "Sultan Daniels",
            "project_title": "HW 11 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/XUuDptfaGtS38bf2sNFh518T\" filename=\"hw11codesolution.zip\"/><file url=\"https://static.us.edusercontent.com/files/RpfafJeboMOxOEyRkkgdWs59\" filename=\"hw11_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/RQYfSEcrRexANTSkquAcQ7GD\" filename=\"hw11_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T14:32:34.89119+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7374436,
            "author": "Hong Joey",
            "project_title": "HW 10 Solutions",
            "post_body": "Attached are solutions to HW 10. Use this thread if you have any questions.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are solutions to HW 10. Use this thread if you have any questions.</paragraph><file url=\"https://static.us.edusercontent.com/files/h3tv3GuAOcTo8dMk0fLy0JWt\" filename=\"hw10_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/zP1JkEM3aCeysBQPNDCinv6a\" filename=\"q_hand_transformer_sol.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/0IhHtEm7lV8PnA1uZf84qz4J\" filename=\"q_summarize_sol.ipynb\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T16:00:53.002127+11:00",
            "category": "Admin"
        },
        {
            "guid": 7366224,
            "author": "Anant Sahai",
            "project_title": "Lecture 25: Test-time compute, sampling, and RL post-training",
            "post_body": "Use this thread to discuss the lecture. \n\nI misspoke during lecture at the end when I was trying to explain the min term. This is a reward that is being maximized and so the min here is stopping large values from becoming too large (not the other way around). The paper discussed clipping in both directions (both are done in different papers) but suggests that it is important to stop accidentally having very large weights to a single generation/token due to a mismatch in the ratio. I misspoke in lecture but the expressions written are correct. Sorry.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/1chm7oisBBtHpGT4MEgh7z8Q\" filename=\"Lecture 25.pdf\"/><paragraph>Use this thread to discuss the lecture. </paragraph><paragraph>I misspoke during lecture at the end when I was trying to explain the min term. This is a reward that is being maximized and so the min here is stopping large values from becoming too large (not the other way around). The paper discussed clipping in both directions (both are done in different papers) but suggests that it is important to stop accidentally having very large weights to a single generation/token due to a mismatch in the ratio. I misspoke in lecture but the expressions written are correct. Sorry.</paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-26T11:36:33.911819+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7351756,
            "author": "Anant Sahai",
            "project_title": "Lecture 24: VAEs and starting test-time compute and post-training",
            "post_body": "This is so students can ask questions about the lecture. I forgot to mention in lecture that Chapters 14 and 17 in Prince are relevant.\n\nRemember, after starting Generative models, we are going to do a little interlude to go through more details of test-time compute and post-training for LLMs. The reason we're doing this is two-fold: \n\nA) The VAE discussion, along with the homework, has unlocked some of the technical tools that we are going to need. Since we have enough now, we can return to post-training since it continues our earlier theme of fine-tuning.\n\nB) Some of the things that we are going to be hitting during post-training will then help us in understanding diffusion-based generative models --- which is what we will do afterwards.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/yF94T6o6P3n9frsyn6cLcPlz\" filename=\"Lecture 24.pdf\"/><paragraph>This is so students can ask questions about the lecture. I forgot to mention in lecture that Chapters 14 and 17 in Prince are relevant.</paragraph><paragraph>Remember, after starting Generative models, we are going to do a little interlude to go through more details of test-time compute and post-training for LLMs. The reason we're doing this is two-fold: </paragraph><paragraph>A) The VAE discussion, along with the homework, has unlocked some of the technical tools that we are going to need. Since we have enough now, we can return to post-training since it continues our earlier theme of fine-tuning.</paragraph><paragraph>B) Some of the things that we are going to be hitting during post-training will then help us in understanding diffusion-based generative models --- which is what we will do afterwards.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-23T05:39:42.344447+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7350819,
            "author": "Anant Sahai",
            "project_title": "IMPORTANT: Students must accept the reviewer invitation email...",
            "post_body": "We have quite a few people who haven't accepted their reviewer invitations --- sent to the email address that we can see using CalCentral. (Those who accepted can stop reading this message...)\n\nRemember how grading works for this class and for the project.  Reviewing performance (the quality of your peer reviews) multiplicatively hits the project grade for individuals. This has a consequence --- not doing reviews will get someone a zero on the project. \n\nRemember, there is no curve and failing the project means you automatically fail the class according to our grading policies.\n\nSo, you really really need to accept your reviewer invitations and then do a diligent job on the reviews. Otherwise, it will not end well for you in this course. \n\nWe don't typically send out email notifications for Ed Posts. But given the timelines involved and the risk to students, we made an exception in this case. ",
            "content_xml": "<document version=\"2.0\"><paragraph>We have quite a few people who haven't accepted their reviewer invitations --- sent to the email address that we can see using CalCentral. (Those who accepted can stop reading this message...)</paragraph><paragraph>Remember how grading works for this class and for the project.  Reviewing performance (the quality of your peer reviews) <italic>multiplicatively</italic> hits the project grade for individuals. This has a consequence --- not doing reviews will get someone a zero on the project. </paragraph><paragraph>Remember, there is no curve and failing the project means you automatically fail the class according to our grading policies.</paragraph><paragraph>So, you really really need to accept your reviewer invitations and then do a diligent job on the reviews. Otherwise, it will not end well for you in this course. </paragraph><paragraph>We don't typically send out email notifications for Ed Posts. But given the timelines involved and the risk to students, we made an exception in this case. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-22T17:19:12.010355+11:00",
            "category": "Project"
        },
        {
            "guid": 7348556,
            "author": "Hong Joey",
            "project_title": "Peer Review for Project Drafts",
            "post_body": "Review assignments are out for the project drafts and will be due by midnight next Tuesday 11/21. You can access them via the Reviewer console on Microsoft CMT (will look something like the below image): \n\nEveryone will have either 1 or 2 reports to review. The questions you need to answer in the review are tailored for this class, so it won't be the same questions as for a typical conference submission. Every paper will have between 2-4 reviews, with a majority having at least 3.\n\nIf you encounter any problems with submitting reviewers, feel free to ask questions on this thread.\n\nUPDATE: To keep workload fair among students, we have updated assignments so that every student will do exactly 1 review (for those who initially had 2, I removed one of yours at random).  ",
            "content_xml": "<document version=\"2.0\"><paragraph>Review assignments are out for the project drafts and will be <bold>due by midnight next Tuesday 11/21</bold>. You can access them via the Reviewer console on Microsoft CMT (will look something like the below image): </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/rsd2FHeheehLW749gCKZgqtE\" width=\"658\" height=\"171.24923453766075\"/></figure><paragraph>Everyone will have either 1 or 2 reports to review. The questions you need to answer in the review are tailored for this class, so it won't be the same questions as for a typical conference submission. Every paper will have between 2-4 reviews, with a majority having at least 3.<break/><break/>If you encounter any problems with submitting reviewers, feel free to ask questions on this thread.</paragraph><paragraph>UPDATE: To keep workload fair among students, we have updated assignments so that every student will do exactly 1 review (for those who initially had 2, I removed one of yours at random).  </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-22T07:25:02.363701+11:00",
            "category": "Project"
        },
        {
            "guid": 7345482,
            "author": "Sultan Daniels",
            "project_title": "Discussion 12 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/WQBMr9mVA0kmsWNeosE7eXwf\" filename=\"dis12_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/nGI5mNPZPBokPEODXd24vAbW\" filename=\"dis12_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-21T13:14:58.005565+11:00",
            "category": "Sections"
        },
        {
            "guid": 7330082,
            "author": "Anant Sahai",
            "project_title": "Project Report",
            "post_body": "For submitting the Draft:\n\nhttps://cmt3.research.microsoft.com/DLFPD2025\n\nWe are going to run this like a conference. So use that to submit. You should get an email after we add you to it. Be sure to include all members of your team. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Ga4T0HD2Xv8rEwktQc4pMadX\" filename=\"Project Report Requirements.pdf\"/><paragraph>For submitting the Draft:</paragraph><paragraph><link href=\"https://cmt3.research.microsoft.com/DLFPD2025\">https://cmt3.research.microsoft.com/DLFPD2025</link></paragraph><paragraph>We are going to run this like a conference. So use that to submit. You should get an email after we add you to it. Be sure to include all members of your team. </paragraph><paragraph/></document>",
            "links": [
                "https://cmt3.research.microsoft.com/DLFPD2025"
            ],
            "attachments": [],
            "created_at": "2025-11-19T05:23:44.405993+11:00",
            "category": "Project"
        },
        {
            "guid": 7328421,
            "author": "Alex Proshkin",
            "project_title": "Update: Compute Portal Time Slots Removed",
            "post_body": "Hi everyone,\n\nWe've noticed that the compute resources aren't being used very often, so we're removing the assigned time slots. You can now feel free to use the compute portal at any time that works for you.\n\nBest,\nAlex",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>We've noticed that the compute resources aren't being used very often, so we're removing the assigned time slots. You can now feel free to use the compute portal at any time that works for you.</paragraph><paragraph>Best,<break/>Alex</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T18:13:57.038203+11:00",
            "category": "Admin"
        },
        {
            "guid": 7313350,
            "author": "Anant Sahai",
            "project_title": "Information about Special Topics Class next semester...",
            "post_body": "Dear students,\n\nI had promised many of you some details for the Special Topics Course that meets in Spring 2026 (currently TuTh 9:30am-11am in 521 Cory).\n\nFor undergraduates, the department created a course with CCN 34668 (formal name EE194-16)\n\nFor graduate students, the department created a course with CCN 34123 (formal name EE290-16) \n\nSign up and add yourself to the waitlist if it is full. If the waitlist gets full, we'll ask the department to increase the waitlist. The department needs to see what the demand is to decide whether to get a bigger room, etc... \n\nI will be co-teaching it with Prof. Jiantao Jiao who is at NVidia full time while maintaining an affiliation here. (Prof. Ranade is involved in our planning but she's teaching 127/227A next semester so can't do this on top of that.) Our goal, which is still a work in progress, is to get people access to both massive amounts of compute (multi-GPU and multi-machine --- not just more Thinking Machines credit which we already have secured.) as well as a set of material that will expose you to how these kinds of things are done at scale. Our dream is to make sure that everyone taking it can get some real experience with industry-level pre-training/post-training/fine-tuning infrastructure and tooling.  We can't commit to being able to do this, but it's what we are working towards. This is an experiment because it is our collective gut feeling that we need something like this for our students. \n\nThe other part of the vision is to give students a way to take what they've learned in 182/282A with their projects and go beyond. I did this before in Spring 2024 and multiple student groups got paper submissions, etc. after taking their projects from the class, merging/reshuffling teams, and pushing beyond. Our dream perspective on this is to give you a way of combining/expanding your ideas and being able to investigate them at much larger scale...\n\nIf there ends up being a mismatch in the number of students we can support in the class and the level of interest, we will use a combination of the quality of student projects in 182/282A combined with performance in other relevant classes to figure out who gets in. But nobody can get a shot if they don't sign up. \n\n \n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Dear students,</paragraph><paragraph>I had promised many of you some details for the Special Topics Course that meets in Spring 2026 (currently TuTh 9:30am-11am in 521 Cory).</paragraph><paragraph>For undergraduates, the department created a course with CCN 34668 (formal name EE194-16)</paragraph><paragraph>For graduate students, the department created a course with CCN 34123 (formal name EE290-16) </paragraph><paragraph>Sign up and add yourself to the waitlist if it is full. If the waitlist gets full, we'll ask the department to increase the waitlist. The department needs to see what the demand is to decide whether to get a bigger room, etc... </paragraph><paragraph>I will be co-teaching it with Prof. Jiantao Jiao who is at NVidia full time while maintaining an affiliation here. (Prof. Ranade is involved in our planning but she's teaching 127/227A next semester so can't do this on top of that.) Our goal, which is still a work in progress, is to get people access to both massive amounts of compute (multi-GPU and multi-machine --- not just more Thinking Machines credit which we already have secured.) as well as a set of material that will expose you to how these kinds of things are done at scale. Our dream is to make sure that everyone taking it can get some real experience with industry-level pre-training/post-training/fine-tuning infrastructure and tooling.  We can't commit to being able to do this, but it's what we are working towards. This is an experiment because it is our collective gut feeling that we need something like this for our students. </paragraph><paragraph>The other part of the vision is to give students a way to take what they've learned in 182/282A with their projects and go beyond. I did this before in Spring 2024 and multiple student groups got paper submissions, etc. after taking their projects from the class, merging/reshuffling teams, and pushing beyond. Our dream perspective on this is to give you a way of combining/expanding your ideas and being able to investigate them at much larger scale...</paragraph><paragraph>If there ends up being a mismatch in the number of students we can support in the class and the level of interest, we will use a combination of the quality of student projects in 182/282A combined with performance in other relevant classes to figure out who gets in. But nobody can get a shot if they don't sign up. </paragraph><paragraph> </paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-15T18:35:44.612982+11:00",
            "category": "Admin"
        },
        {
            "guid": 7269940,
            "author": "Alex Proshkin",
            "project_title": "New: JupyterHub Compute Portal Now Available",
            "post_body": "Hi everyone,\n\nThe CS182 JupyterHub Compute Portal is now live! \ud83c\udf89\nThis will be your place to run course assignments with dedicated compute.\n\nHow to access:\nYou can find it from the course website\u2019s navigation bar, or go directly here:\nhttps://berkeley-cs182.github.io/jupyterhub/\n\nWhat you get:\n\nJupyterHub servers with CPU and GPU options\n\nLogin with your @berkeley.edu account\n\nCompute options:\n\nCPU (default): Use anytime for regular work\n\nGPU (limited): Access to full NVIDIA A100 40GB. Use only during assigned time when needed for deep learning workloads\n\nGroup assignments:\nEach student is assigned to Group A, B, or C. Check the portal to see where you're assigned. Slots will be continuously added, so check in regularly.\n\n!!! IMPORTANT !!!\n\nOnly one team should run in a group server at a time\n\nIf you want to swap times with another team, coordinate in the thread below\n\nPlease shut down your server when you're done so others can use it \ud83d\udc4d\n\nIf you run into issues or have questions, reply in the thread below or email me (pro@berkeley.edu).\n\nThanks and happy training!\nAlex",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>The CS182 JupyterHub Compute Portal is now live! \ud83c\udf89<break/>This will be your place to run course assignments with dedicated compute.</paragraph><paragraph><bold>How to access:</bold><break/>You can find it from the course website\u2019s navigation bar, or go directly here:<break/><link href=\"https://berkeley-cs182.github.io/jupyterhub/\">https://berkeley-cs182.github.io/jupyterhub/</link></paragraph><paragraph><bold>What you get:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>JupyterHub servers with CPU and GPU options</paragraph></list-item><list-item><paragraph>Login with your @berkeley.edu account</paragraph></list-item></list><paragraph><bold>Compute options:</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>CPU (default):</bold> Use anytime for regular work</paragraph></list-item><list-item><paragraph><bold>GPU (limited):</bold> Access to full NVIDIA A100 40GB. Use only during assigned time when needed for deep learning workloads</paragraph></list-item></list><paragraph><bold>Group assignments:</bold><break/>Each student is assigned to Group A, B, or C. Check the portal to see where you're assigned. Slots will be continuously added, so check in regularly.</paragraph><paragraph><bold>!!! IMPORTANT !!!</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Only one team should run in a group server at a time</paragraph></list-item><list-item><paragraph>If you want to swap times with another team, coordinate in the thread below</paragraph></list-item><list-item><paragraph>Please shut down your server when you're done so others can use it \ud83d\udc4d</paragraph></list-item></list><paragraph>If you run into issues or have questions, reply in the thread below or email me (<link href=\"null\">pro@berkeley.edu</link>).</paragraph><paragraph>Thanks and happy training!<break/>Alex</paragraph></document>",
            "links": [
                "https://berkeley-cs182.github.io/jupyterhub/",
                "null"
            ],
            "attachments": [],
            "created_at": "2025-11-07T10:49:58.265283+11:00",
            "category": "Admin"
        },
        {
            "guid": 7268035,
            "author": "Gireeja Ranade",
            "project_title": "Tinker access for final projects --- Opt-in deadline Nov 7 at 10 am.",
            "post_body": "Dear students, \n\n\n\nWe have secured access to Tinker, a new API from Thinking Machines that provides fine-tuning support for many popular open-weight models. You can read more here:\n\n\n\nhttps://thinkingmachines.ai/tinker/\n\nWe assume most of you would like access to this for your project/to try out things otherwise. This requires us to create an account for you on their API using your name/email. If you would prefer that we not share your name and email with Thinking Machines to get you access, please opt-out using the form below. If you want to guarantee you get access, please also indicate this below. \n\n\n\nhttps://forms.gle/vUuQwYeFHgtqX74z8\n\nGiven a tight timeline, there is a 24 hour deadline to opt-in/out-out. Form closes at 10 am Nov 7.\n\nThanks,\n\n182 Staff. \n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Dear students, </paragraph><paragraph/><paragraph>We have secured access to Tinker, a new API from Thinking Machines that provides fine-tuning support for many popular open-weight models. You can read more here:</paragraph><paragraph/><paragraph><link href=\"https://thinkingmachines.ai/tinker/\">https://thinkingmachines.ai/tinker/</link></paragraph><paragraph>We assume most of you would like access to this for your project/to try out things otherwise. This requires us to create an account for you on their API using your name/email. If you would prefer that we not share your name and email with Thinking Machines to get you access, please opt-out using the form below. If you want to guarantee you get access, please also indicate this below. </paragraph><paragraph/><paragraph><link href=\"https://forms.gle/vUuQwYeFHgtqX74z8\">https://forms.gle/vUuQwYeFHgtqX74z8</link></paragraph><paragraph>Given a tight timeline, there is a 24 hour deadline to opt-in/out-out. Form closes at 10 am Nov 7.</paragraph><paragraph>Thanks,</paragraph><paragraph>182 Staff. <break/></paragraph><paragraph/></document>",
            "links": [
                "https://thinkingmachines.ai/tinker/",
                "https://forms.gle/vUuQwYeFHgtqX74z8"
            ],
            "attachments": [],
            "created_at": "2025-11-07T06:04:53.599999+11:00",
            "category": "Admin"
        },
        {
            "guid": 7197077,
            "author": "Anant Sahai",
            "project_title": "Appointment slots for the project",
            "post_body": "https://docs.google.com/spreadsheets/d/1hOqLiCQuekARSADubqDPQmFXNSsR01bsBQzjN4OUIpU/edit?gid=0#gid=0\n\nThe spreadsheet above has links to individual appointment calendars for the course staff along with what project topics we will be supporting. \n\nPlease set up an appointment for next week. And then, once your project proposals are in, for the following week to get feedback.\n\nRemember, these are required meetings for each group. At least one person from your group must attend the meeting.\n\n The appointments next week should help you find a problem and/or refine it.",
            "content_xml": "<document version=\"2.0\"><paragraph><link href=\"https://docs.google.com/spreadsheets/d/1hOqLiCQuekARSADubqDPQmFXNSsR01bsBQzjN4OUIpU/edit?gid=0#gid=0\">https://docs.google.com/spreadsheets/d/1hOqLiCQuekARSADubqDPQmFXNSsR01bsBQzjN4OUIpU/edit?gid=0#gid=0</link></paragraph><paragraph>The spreadsheet above has links to individual appointment calendars for the course staff along with what project topics we will be supporting. </paragraph><paragraph>Please set up an appointment for next week. And then, once your project proposals are in, for the following week to get feedback.</paragraph><paragraph>Remember, these are required meetings for each group. At least one person from your group must attend the meeting.</paragraph><paragraph> The appointments next week should help you find a problem and/or refine it.</paragraph></document>",
            "links": [
                "https://docs.google.com/spreadsheets/d/1hOqLiCQuekARSADubqDPQmFXNSsR01bsBQzjN4OUIpU/edit?gid=0#gid=0"
            ],
            "attachments": [],
            "created_at": "2025-10-25T10:44:16.231052+11:00",
            "category": "Project"
        },
        {
            "guid": 7132226,
            "author": "Anant Sahai",
            "project_title": "Project Thread 3: Interpretability",
            "post_body": "This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations. Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. \n\nThis particular thread is likely to involve a lot more learning on your own to get started since we aren't going to be covering many of the core techniques in class. But all project options 1,2, and 3 involve reading papers/blogs/etc. on your own as well as figuring out tools and codebases on your own because being able to do that is the core way to show that you've actually internalized the foundations that we are teaching you here. No static set of course materials is ever going to suffice in a field like this --- it's all about being able to learn on your own. That's what you want to demonstrate with a project like this in your portfolio.",
            "content_xml": "<document version=\"2.0\"><paragraph>This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. <bold>This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations.</bold> Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. </paragraph><paragraph>This particular thread is likely to involve a lot more learning on your own to get started since we aren't going to be covering many of the core techniques in class. But all project options 1,2, and 3 involve reading papers/blogs/etc. on your own as well as figuring out tools and codebases on your own <italic>because</italic> being able to do that is the core way to show that you've actually internalized the foundations that we are teaching you here. No static set of course materials is ever going to suffice in a field like this --- it's all about being able to learn on your own. That's what you want to demonstrate with a project like this in your portfolio.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T10:01:48.797649+11:00",
            "category": "Project"
        },
        {
            "guid": 7132205,
            "author": "Anant Sahai",
            "project_title": "Project Thread 2: ICL via toy problems",
            "post_body": "This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations. Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. ",
            "content_xml": "<document version=\"2.0\"><paragraph>This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. <bold>This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations.</bold> Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T09:59:43.452413+11:00",
            "category": "Project"
        },
        {
            "guid": 7132201,
            "author": "Anant Sahai",
            "project_title": "Project Thread 1: Optimization and hyperparameter transfer",
            "post_body": "This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations. Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. ",
            "content_xml": "<document version=\"2.0\"><paragraph>This thread exists to help students form teams in this thread as well as deconflict what exactly you want to do. The main document introducing projects has some examples of what can be proposed. <bold>This particular project thread is particularly amenable for having the projects have a bit of life after the semester ends to pull things together into a paper that expounds on this story with a set of systematic explorations.</bold> Consequently, there is a lot of value to be had by deconflicting early and making sure that you get fuller coverage. Everyone whose project contributes to the paper will be a co-author. In previous semesters, I have taught a follow-on special topics course the subsequent semester to help students further develop their projects if they want to. I hope to do that again in the Spring but obviously, that is not required in any way to pursue this option. It's perfectly fine to have a project that ends once the course is done. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T09:59:04.76962+11:00",
            "category": "Project"
        },
        {
            "guid": 7132188,
            "author": "Anant Sahai",
            "project_title": "Project Thread 0: Projects applying Deep Learning to another domain.",
            "post_body": "This is to help people coordinate, find teams, etc. Remember, any project of this type requires a graduate student (with a faculty adviser --- grad students from coursework-only programs or who are doing capstone projects with anyone other than regular faculty are not eligible for this option) from another department (not EECS) to be on the team to demonstrate disciplinary expertise in the domain being proposed and to take responsibility for that side. Others on the team can be from any department. Undergraduates from other departments cannot lead such projects unless they can clearly demonstrate faculty PI commitment to providing disciplinary guidance. (That means we need a letter from the faculty member in another department saying that they're going to support you vis-a-vis this line of investigation on the disciplinary side and do what it takes.)",
            "content_xml": "<document version=\"2.0\"><paragraph>This is to help people coordinate, find teams, etc. Remember, any project of this type requires a graduate student (with a faculty adviser --- grad students from coursework-only programs or who are doing capstone projects with anyone other than regular faculty are not eligible for this option) from another department (not EECS) to be on the team to demonstrate disciplinary expertise in the domain being proposed and to take responsibility for that side. Others on the team can be from any department. Undergraduates from other departments cannot lead such projects unless they can clearly demonstrate faculty PI commitment to providing disciplinary guidance. (That means we need a letter from the faculty member in another department saying that they're going to support you vis-a-vis this line of investigation on the disciplinary side and do what it takes.)</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T09:56:58.48185+11:00",
            "category": "Project"
        },
        {
            "guid": 7132154,
            "author": "Anant Sahai",
            "project_title": "Main Project Info Thread",
            "post_body": "Deadlines: (Same for each of the topic choices above)\n\nRequired pre-proposal meeting with course staff: Week of Oct 27th (sign up details TBD)\n\nProject Proposal (details below): Monday Nov 3rd (This includes confirmed teaming...) \n\nRequired meeting with course staff for feedback: Week of Nov 3rd. (sign up details TBD)\n\nDraft submission of your final project report for peer review: Thu Nov 20th\n\nDetailed individual peer feedback due: Tue Nov 25th.\n\nFinal project report due: Fri Dec 5th\n\nPoster Session: During the class slot on Tue Dec 9th 11-1PM in The Woz (4th floor Soda)\n\nFinal Peer review due: Thu Dec 11th. (So you can incorporate comments from the poster session as well)\n\nFinal project report due that addresses all review comments: Sun Dec 14th\n\nProject proposal requirements (Due Nov 3rd --- submit on Gradescope)\n\nTeam: List of 4 students doing the project together\n\nAbstract: At most 6 sentences. \n\nIntroduction and Background: This must include a literature review of at least 8 papers related to your project. There should be at least one sentence summaries of the key point of each of the papers.\n\nKey questions: This should be a short section that must include actual questions you are trying to answer. What is the problem you are trying to solve or the phenomenon you are trying to understand/replicate? This should be backed up by the earlier section.\n\nHypothesis: What is your hypothesis? This could be simple e.g. I expect a phenomenon described in XYZ reference to be replicated in a different setting. But it is important to be expecting something --- it's fine if something different ends up happening. \n\nMethods: How will you test your hypothesis? What are the experiments you will run? What is the expected compute requirement?\n\nWe expect proposals to be written using Latex, submitted as PDFs, with one-inch margins, and 12 point font. The purpose of the proposal format is to help structure your thoughts. The required short pre-proposal meetings are intended to help guide you towards a good proposal --- you should be attending those with a draft proposal (or proposal options) in your hand. And the required post-proposal meetings are intended to get you feedback and help you make your efforts better.\n\nThis is not a class in which a project can be completed overnight or even in a week. You need to start early and expect to have all four team members working hard. Deep Learning being what it is, there will be setbacks, confusion, and lots of iterative refinement on top of having to do a lot of reading and learning on your own.\n\nPoster Session:\n\nThe idea of the poster session is to allow you to share your work with your peers and for them to ask you questions. This is required because being able to present work concisely in poster form is a critically important skill. If there is demand for wanting to give a very short (2min) lightning talks too, we can schedule that for the class slot on Thu Dec 11th.  \n\nOther logistics:\n\nGroups must be 4 people in size. Exceptions can be granted for a group that ends up at 3 or 5 because someone dropped or picking up students orphaned. \n\nWe have secured some compute that we can make available to student groups --- but there isn't that much of it. Details on how to get this will come later.\n\nProject grades are shared with your team, except multiplicatively modulated by the quality of your individual peer-review. Do a bad job as a peer-reviewer, and you will lose points. Don't do all required peer review, and you'll get a zero on the project. In addition, you can gain participation credit for solid helpful interaction with other groups within your project theme in the respective Ed threads.\n\nWe will separately announce which TA office hours are best for which project type.\n\nThis thread is entirely for administrative or logistics-related questions. There will be specific threads for the four project options that can be used to help set up teaming, etc. \n\nGuidance and advice:\n\nYou are encouraged to leverage generative AI tools to help you with writing/modifying code as well as to leverage \"Deep Research\" type tools to help you seed your literature search as well as refine/sharpen your ideas. Cite the use of these tools and include an appendix (doesn't count against any page limits or guidance) where you give a narrative description of how you engaged with generative AI tools during your project. We live in 2025. :-)\n\nIt is also fine to use generative AI tools to help with your writing, figure making, etc. Just fully describe this use in your appendix. However, note that any hallucinations that end up in your final report will be severely penalized --- you are expected to understand and double-check everything that you are writing. The point of permitting the use of these tools is to improve overall quality and to help you do the best project you can in the given time.\n\nWe will be releasing peer-review guidance and rubrics later. One of the things you will be expected to do along with each submission is to also submit a peer-review form for your own project. This can be thought of as a checklist that is designed to help your submission be in better shape and to avoid subjecting the reviewers to bad projects. On this front, we anticipate a substantial extra-credit opportunity for those who figure out (and share with your classmates) how to prompt a publicly available LLM to help groups review their own work to get rounds of criticism and feedback before having to subject your human peers to your work. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/iJWlTq0JScegTBCZq7SreCB7\" filename=\"Final Project Topics Fall 2025 (1).pdf\"/><paragraph>Deadlines: (Same for each of the topic choices above)</paragraph><list style=\"unordered\"><list-item><paragraph>Required pre-proposal meeting with course staff: Week of Oct 27th (sign up details TBD)</paragraph></list-item><list-item><paragraph>Project Proposal (details below): Monday Nov 3rd (This includes confirmed teaming...) </paragraph></list-item><list-item><paragraph>Required meeting with course staff for feedback: Week of Nov 3rd. (sign up details TBD)</paragraph></list-item><list-item><paragraph>Draft submission of your final project report for peer review: Thu Nov 20th</paragraph></list-item><list-item><paragraph>Detailed individual peer feedback due: Tue Nov 25th.</paragraph></list-item><list-item><paragraph>Final project report due: Fri Dec 5th</paragraph></list-item><list-item><paragraph><bold>Poster Session: During the class slot on Tue Dec 9th 11-1PM in The Woz (4th floor Soda)</bold></paragraph></list-item><list-item><paragraph>Final Peer review due: Thu Dec 11th. (So you can incorporate comments from the poster session as well)</paragraph></list-item><list-item><paragraph>Final project report due that addresses all review comments: Sun Dec 14th</paragraph></list-item></list><heading level=\"3\"><bold>Project proposal requirements (Due Nov 3rd --- submit on Gradescope)</bold></heading><list style=\"unordered\"><list-item><paragraph>Team: List of 4 students doing the project together</paragraph></list-item><list-item><paragraph>Abstract: At most 6 sentences. </paragraph></list-item><list-item><paragraph>Introduction and Background: This must include a literature review of at least 8 papers related to your project. There should be at least one sentence summaries of the key point of each of the papers.</paragraph></list-item><list-item><paragraph>Key questions: This should be a short section that must include actual questions you are trying to answer. What is the problem you are trying to solve or the phenomenon you are trying to understand/replicate? This should be backed up by the earlier section.</paragraph></list-item><list-item><paragraph>Hypothesis: What is your hypothesis? This could be simple e.g. I expect a phenomenon described in XYZ reference to be replicated in a different setting. But it is important to be expecting something --- it's fine if something different ends up happening. </paragraph></list-item><list-item><paragraph>Methods: How will you test your hypothesis? What are the experiments you will run? What is the expected compute requirement?</paragraph></list-item></list><paragraph>We expect proposals to be written using Latex, submitted as PDFs, with one-inch margins, and 12 point font. The purpose of the proposal format is to help structure your thoughts. The required short pre-proposal meetings are intended to help guide you towards a good proposal --- you should be attending those with a draft proposal (or proposal options) in your hand. And the required post-proposal meetings are intended to get you feedback and help you make your efforts better.</paragraph><paragraph>This is not a class in which a project can be completed overnight or even in a week. You need to start early and expect to have all four team members working hard. Deep Learning being what it is, there will be setbacks, confusion, and lots of iterative refinement on top of having to do a lot of reading and learning on your own.</paragraph><paragraph><bold>Poster Session:</bold></paragraph><paragraph>The idea of the poster session is to allow you to share your work with your peers and for them to ask you questions. This is required because being able to present work concisely in poster form is a critically important skill. If there is demand for wanting to give a very short (2min) lightning talks too, we can schedule that for the class slot on Thu Dec 11th.  </paragraph><paragraph><bold>Other logistics:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Groups must be 4 people in size. Exceptions can be granted for a group that ends up at 3 or 5 because someone dropped or picking up students orphaned. </paragraph></list-item><list-item><paragraph>We have secured some compute that we can make available to student groups --- but there isn't that much of it. Details on how to get this will come later.</paragraph></list-item><list-item><paragraph>Project grades are shared with your team, except multiplicatively modulated by the quality of your individual peer-review. Do a bad job as a peer-reviewer, and you will lose points. Don't do all required peer review, and you'll get a zero on the project. In addition, you can gain participation credit for solid helpful interaction with other groups within your project theme in the respective Ed threads.</paragraph></list-item><list-item><paragraph>We will separately announce which TA office hours are best for which project type.</paragraph></list-item></list><paragraph>This thread is entirely for administrative or logistics-related questions. There will be specific threads for the four project options that can be used to help set up teaming, etc. </paragraph><paragraph><bold>Guidance and advice:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>You are encouraged to leverage generative AI tools to help you with writing/modifying code as well as to leverage \"Deep Research\" type tools to help you seed your literature search as well as refine/sharpen your ideas. Cite the use of these tools and include an appendix (doesn't count against any page limits or guidance) where you give a narrative description of how you engaged with generative AI tools during your project. We live in 2025. :-)</paragraph></list-item><list-item><paragraph>It is also fine to use generative AI tools to help with your writing, figure making, etc. Just fully describe this use in your appendix. However, note that any hallucinations that end up in your final report will be severely penalized --- you are expected to understand and double-check everything that you are writing. The point of permitting the use of these tools is to improve overall quality and to help you do the best project you can in the given time.</paragraph></list-item><list-item><paragraph>We will be releasing peer-review guidance and rubrics later. One of the things you will be expected to do along with each submission is to also submit a peer-review form for your own project. This can be thought of as a checklist that is designed to help your submission be in better shape and to avoid subjecting the reviewers to bad projects. On this front, we anticipate a substantial extra-credit opportunity for those who figure out (and share with your classmates) how to prompt a publicly available LLM to help groups review their own work to get rounds of criticism and feedback before having to subject your human peers to your work. </paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T09:52:10.801402+11:00",
            "category": "Project"
        },
        {
            "guid": 7130547,
            "author": "Kevin Frans",
            "project_title": "Correction to HW3 Q2 Solution",
            "post_body": "Hi, as some of you have discovered on your own, there was a mistake in the solutions to HW3 Q2, specifically, question (d) about implementing muP via per-weight multipliers on the forwards pass. \n\nThe full correct solution is:\n\nclass ScaledMLP(nn.Module):\n    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n        super(ScaledMLP, self).__init__()\n        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n        self.layers = nn.ModuleList()\n        for i in range(len(all_hidden_sizes)-1):\n            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n        ## Rescale weight initializations to account for pre-activation scaling\n        with torch.no_grad():\n            for layer in self.layers:\n                layer.weight.mul_(layer.weight.shape[1])\n        ##\n\n    def forward(self, x):\n        activations = []\n        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            ## TODO\n            p_shape = layer.weight.shape\n            x = x / p_shape[1] \n            ##\n            x = self.sigmoid(x)\n            activations.append(x)\n        x = self.layers[-1](x)\n        activations = activations[1:]\n        return x, [a.detach() for a in activations]\n\ntrain_one_step(mlp=ScaledMLP, optimizer=SimpleAdam)\n\n\nThe previous solution contained x * p_shape[1] instead of x / p_shape[1]. In addition, when we utilize division to scale down the effect of applying an update, we need to also scale up the parameter initialization (so the compute graph at step 0 is identical).\n\nThank you to the various students who brought up this question during office hours -- your confusion was correct!\n\nWe've also updated the solution in #104 to reflect this change. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi, as some of you have discovered on your own, there was a mistake in the solutions to HW3 Q2, specifically, question (d) about implementing muP via per-weight multipliers on the forwards pass. </paragraph><paragraph>The full correct solution is:</paragraph><pre>class ScaledMLP(nn.Module):\n    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n        super(ScaledMLP, self).__init__()\n        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n        self.layers = nn.ModuleList()\n        for i in range(len(all_hidden_sizes)-1):\n            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n        ## Rescale weight initializations to account for pre-activation scaling\n        with torch.no_grad():\n            for layer in self.layers:\n                layer.weight.mul_(layer.weight.shape[1])\n        ##\n\n    def forward(self, x):\n        activations = []\n        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            ## TODO\n            p_shape = layer.weight.shape\n            x = x / p_shape[1] \n            ##\n            x = self.sigmoid(x)\n            activations.append(x)\n        x = self.layers[-1](x)\n        activations = activations[1:]\n        return x, [a.detach() for a in activations]\n\ntrain_one_step(mlp=ScaledMLP, optimizer=SimpleAdam)\n</pre><paragraph>The previous solution contained <code>x * p_shape[1]</code> instead of <code>x / p_shape[1]</code>. In addition, when we utilize division to scale down the effect of applying an update, we need to also scale up the parameter initialization (so the compute graph at step 0 is identical).<break/><break/>Thank you to the various students who brought up this question during office hours -- your confusion was correct!</paragraph><paragraph>We've also updated the solution in #104 to reflect this change. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T06:31:21.604832+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7005612,
            "author": "Sammie Smith",
            "project_title": "Useful Resources",
            "post_body": "Here's a thread to post useful resources! Please comment any below!\n\n\n\nFor example, per my earlier comment:\n\nIf anyone is looking for a nicer way to save the notebook to a pdf, check out this open source ipynb -> html converter I found online. Just download your notebook as an ipynb, upload it as directed to the converter, then print to pdf the resulting html webpage. Super quick & easy.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Here's a thread to post useful resources! Please comment any below!</bold></paragraph><paragraph/><paragraph>For example, per my earlier comment:</paragraph><paragraph>If anyone is looking for a nicer way to save the notebook to a pdf, check out this <link href=\"https://colab.research.google.com/github/Mostafa-MR/Convert_ipynb_to_HTML_in_Colab/blob/main/Convert_ipynb_to_HTML_in_Colab.ipynb\">open source ipynb -&gt; html converter</link> I found online. Just download your notebook as an ipynb, upload it as directed to the converter, then print to pdf the resulting html webpage. Super quick &amp; easy.</paragraph></document>",
            "links": [
                "https://colab.research.google.com/github/Mostafa-MR/Convert_ipynb_to_HTML_in_Colab/blob/main/Convert_ipynb_to_HTML_in_Colab.ipynb"
            ],
            "attachments": [],
            "created_at": "2025-09-24T04:11:50.733358+10:00",
            "category": "Admin"
        },
        {
            "guid": 6961170,
            "author": "Anant Sahai",
            "project_title": "Participation Thread: Two parts...",
            "post_body": "Dear students,\n\nThis thread exists only for administrative questions regarding participation.  Asking administrative questions does not count itself as participation. \n\nAs was described briefly in the 0th lecture, your grade in this course includes an active participation dimension.  This is worth 15% of your grade. (As you can tell from the posted bins, without participation, you cannot earn an A of any kind.) There are two components to positive participation (negative participation is any behavior that diminishes the course experience for others... e.g. trolling in online fora, bad behavior in discussion/lecture, etc.):\n\nOrdinary participation (half of the participation grade)\n\nEither: regular and full participation in the in-person discussion sections each week\n\nOr: active helpful participation on Ed: typically this is helpful participation in the homework problem threads, but can also be similarly helpful participation in discussion or lecture threads. Other possibilities also exist --- but there is no spamming way to do this. \n\nOr: a combination of 1&2 above. \n\nSpecial participation (the other half): Must do at least four for full credit. (We'll make special threads for B, C, and E --- with ones for D waiting until that material has been hit in lecture/discussion/homework so it is properly unlocked)\n\n(A. Can be done at most once) Interactively engage a modern LLM on the non-coding parts of a homework (all the non-coding parts of problems) that gets the LLM to arrive at the correct answers --- or demonstrate that this is basically impossible without dragging it there. Post on Ed an annotated log of the entire interaction where you make observations of its behavior and explain the strategies that you are using. Include an executive summary where you note how often the LLM can one-shot questions, misconceptions/hallucinations, etc...  Note: this must be deconflicted with others: we can't have more than one submissions using ChatGPT 5,  etc. The expectation is that with 250ish students and about 13-14 assignments, we'll have at least 18 different major LLMs represented. (e.g. ChatGPT, gpt-oss, Gemini, gemma, Claude, Grok, Llama, Deepseek, Mistral, Qwen, Kimi, etc. as well as their variants --- with and without \"thinking\", specialized math-oriented finetunes, other non-math-oriented finetunes, etc.)  \nYou can use: https://docs.google.com/spreadsheets/d/1KWk_O2wMFEmRR8QmZQ9paRcIgehc9IeD5u_pUgnAp4k/edit?usp=sharing\nto help deconflict yourselves. But this is entirely voluntary --- however, only the first post for a particular box will get credit.  \n\n(B. Can do at most once) Interactively engage a modern LLM or co-pilot on the coding parts of a homework. Same as above. (including deconfliction)  Here, you need to think about how to record the log of interactions since it is not as easy as for (1). [Feel free to discuss ideas for how to do this in this thread.] \nVoluntary for deconfliction: https://docs.google.com/spreadsheets/d/1vmC2ZKx5TXykmMUchbJ4NBFUDKavzDaXVAfN5ueDlAs/edit?usp=sharing\n\n(C. Can do at most twice) Because the code in our problems was evolved till it worked with specific deep-learning-concept related learning objectives, it is often not good code from the perspective of being exemplary from a software engineering  point of view. For example, it is often not very pythonic, etc. You can, ideally with AI assistance that you document carefully vis-a-vis process, take one of the coding problems/demos and refactor it as well as update the code to follow good documented software engineering and ML Engineering processes. Here, we expect you to give citations to the relevant points of good style and document your changes in a report. The constraint is that the problem code shouldn't lose any of its teaching value --- just be transformed to have good coding practices and style. As always, deconfliction is a must however this can be a group effort by up to three people. \n\n(D. No limits on how many times) As you will soon learn (this week), the traditional approach of doing things like using the Adam (or AdamW) optimizer with a single learning rate for the entire deep network is now on its way to being deprecated in terms of Deep Learning training best practices. Our understanding has now improved with things like muP (maximal update parameterization) and matrix-oriented optimizers like Muon (along with others like SOAP) that seem decisively better. And even in the Adam family, more GPU memory-friendly approaches like Lion (which is a different way of keeping the spirit of signSGD that invokes stochastic dithering to do the job that tracking scale does in Adam when combined with momentum) are ascending. For this option, you have to create additional parts for coding problems that we assign that bring in at least two of these modern approaches. This requires you to make both the additional problem parts (which should include hyperparameter exploration and choice as appropriate) as well as solutions. \nVoluntary deconfliction: https://docs.google.com/spreadsheets/d/1JcL5cToJs0fGcfaBD5k2dzO24jIk6kWhB1jtiYTsc6I/edit?usp=sharing\n\n(E. Can do at most twice) Make AI-enhanced learning tools for individual concepts and lectures. There are now a plethora of new tools out there (e.g. Guided Learning in Gemini, notebookLM by Google, Study Mode in ChatGPT, Learning mode in Claude, etc... Go ahead and explore/use what's out there) and there are also ways that one can prompt and engage with LLMs even without any special modes that can help you learn on your own. For this option, you should be creating (and sharing with your classmates using an Ed post) whatever prompt/artifact/etc. so that they can also engage in this self-learning together with an interaction trace of you with the tool that shows how it can potentially help. This trace should be annotated critically by you pointing out where it is hallucinating, wrong, or misleading --- as well as places where you think it is doing a good job.  One way to think about what you're trying to achieve is as an active AI replacement/substitute for the traditional approach of pre-lecture (or post-lecture) readings that you do to understand the material better before you delve into the homework or your own projects. \n\nBetween A,B,C,D, and E above, you have lots of ways to create something useful and helpful for your classmates (both current and future...) while hopefully also learning a lot yourself in the process. Our goal is to have you engage with the Deep Learning material in a way that recognizes the transformative achievements of Deep Learning in engaging with material. We want your engagement with AI tools to enhance your learning, not inhibit it. \n\n\nEdited to add: There are two steps to submission of this: (1) Post on Ed. This is really important because the whole point is to create something of value both to yourself and to your classmates. (2) Submit to Gradescope --- there are assignments there for A, B, C, and E. For C and E where you can submit twice, there are two distinct assignments so that you can submit first to one and then do a different one for the second assignments. There is nothing there yet for D because that hasn't been unlocked by lecture yet. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Dear students,</paragraph><paragraph>This thread exists only for administrative questions regarding participation.  Asking administrative questions does not count itself as participation. </paragraph><paragraph>As was described briefly in the 0th lecture, your grade in this course includes an active participation dimension.  This is worth 15% of your grade. (As you can tell from the posted bins, without participation, you cannot earn an A of any kind.) There are two components to positive participation (negative participation is any behavior that diminishes the course experience for others... e.g. trolling in online fora, bad behavior in discussion/lecture, etc.):</paragraph><list style=\"number\"><list-item><paragraph>Ordinary participation (half of the participation grade)</paragraph><list style=\"bullet\"><list-item><paragraph>Either: regular and full participation in the in-person discussion sections each week</paragraph></list-item><list-item><paragraph>Or: active helpful participation on Ed: typically this is helpful participation in the homework problem threads, but can also be similarly helpful participation in discussion or lecture threads. Other possibilities also exist --- but there is no spamming way to do this. </paragraph></list-item><list-item><paragraph>Or: a combination of 1&amp;2 above. </paragraph></list-item></list></list-item><list-item><paragraph>Special participation (the other half): Must do at least <bold>four</bold> for full credit. (We'll make special threads for B, C, and E --- with ones for D waiting until that material has been hit in lecture/discussion/homework so it is properly unlocked)</paragraph><list style=\"bullet\"><list-item><paragraph>(A. Can be done at most once) Interactively engage a modern LLM on the non-coding parts of a homework (all the non-coding parts of problems) that gets the LLM to arrive at the correct answers --- or demonstrate that this is basically impossible without dragging it there. Post on Ed an annotated log of the entire interaction where you make observations of its behavior and explain the strategies that you are using. Include an executive summary where you note how often the LLM can one-shot questions, misconceptions/hallucinations, etc...  Note: this must be deconflicted with others: we can't have more than one submissions using ChatGPT 5,  etc. The expectation is that with 250ish students and about 13-14 assignments, we'll have at least 18 different major LLMs represented. (e.g. ChatGPT, gpt-oss, Gemini, gemma, Claude, Grok, Llama, Deepseek, Mistral, Qwen, Kimi, etc. as well as their variants --- with and without \"thinking\", specialized math-oriented finetunes, other non-math-oriented finetunes, etc.)  <break/>You can use: <link href=\"https://docs.google.com/spreadsheets/d/1KWk_O2wMFEmRR8QmZQ9paRcIgehc9IeD5u_pUgnAp4k/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1KWk_O2wMFEmRR8QmZQ9paRcIgehc9IeD5u_pUgnAp4k/edit?usp=sharing</link><break/>to help deconflict yourselves. But this is entirely voluntary --- however, only the first post for a particular box will get credit.  </paragraph></list-item><list-item><paragraph>(B. Can do at most once) Interactively engage a modern LLM or co-pilot on the coding parts of a homework. Same as above. (including deconfliction)  Here, you need to think about how to record the log of interactions since it is not as easy as for (1). [Feel free to discuss ideas for how to do this in this thread.] <break/>Voluntary for deconfliction: <link href=\"https://docs.google.com/spreadsheets/d/1vmC2ZKx5TXykmMUchbJ4NBFUDKavzDaXVAfN5ueDlAs/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1vmC2ZKx5TXykmMUchbJ4NBFUDKavzDaXVAfN5ueDlAs/edit?usp=sharing</link></paragraph></list-item><list-item><paragraph>(C. Can do at most twice) Because the code in our problems was evolved till it worked with specific deep-learning-concept related learning objectives, it is often not good code from the perspective of being exemplary from a software engineering  point of view. For example, it is often not very pythonic, etc. You can, ideally with AI assistance that you document carefully vis-a-vis process, take one of the coding problems/demos and refactor it as well as update the code to follow good documented software engineering and ML Engineering processes. Here, we expect you to give citations to the relevant points of good style and document your changes in a report. The constraint is that the problem code shouldn't lose any of its teaching value --- just be transformed to have good coding practices and style. As always, deconfliction is a must however this can be a group effort by up to three people. </paragraph></list-item><list-item><paragraph>(D. No limits on how many times) As you will soon learn (this week), the traditional approach of doing things like using the Adam (or AdamW) optimizer with a single learning rate for the entire deep network is now on its way to being deprecated in terms of Deep Learning training best practices. Our understanding has now improved with things like muP (maximal update parameterization) and matrix-oriented optimizers like Muon (along with others like SOAP) that seem decisively better. And even in the Adam family, more GPU memory-friendly approaches like Lion (which is a different way of keeping the spirit of signSGD that invokes stochastic dithering to do the job that tracking scale does in Adam when combined with momentum) are ascending. For this option, you have to create additional parts for coding problems that we assign that bring in at least two of these modern approaches. This requires you to make both the additional problem parts (which should include hyperparameter exploration and choice as appropriate) as well as solutions. <break/>Voluntary deconfliction: <link href=\"https://docs.google.com/spreadsheets/d/1JcL5cToJs0fGcfaBD5k2dzO24jIk6kWhB1jtiYTsc6I/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1JcL5cToJs0fGcfaBD5k2dzO24jIk6kWhB1jtiYTsc6I/edit?usp=sharing</link></paragraph></list-item><list-item><paragraph>(E. Can do at most twice) Make AI-enhanced learning tools for individual concepts and lectures. There are now a plethora of new tools out there (e.g. Guided Learning in Gemini, notebookLM by Google, Study Mode in ChatGPT, Learning mode in Claude, etc... Go ahead and explore/use what's out there) and there are also ways that one can prompt and engage with LLMs even without any special modes that can help you learn on your own. For this option, you should be creating (and sharing with your classmates using an Ed post) whatever prompt/artifact/etc. so that they can also engage in this self-learning together with an interaction trace of you with the tool that shows how it can potentially help. This trace should be annotated critically by you pointing out where it is hallucinating, wrong, or misleading --- as well as places where you think it is doing a good job.  One way to think about what you're trying to achieve is as an active AI replacement/substitute for the traditional approach of pre-lecture (or post-lecture) readings that you do to understand the material better before you delve into the homework or your own projects. </paragraph></list-item></list></list-item></list><paragraph>Between A,B,C,D, and E above, you have lots of ways to create something useful and helpful for your classmates (both current and future...) while hopefully also learning a lot yourself in the process. Our goal is to have you engage with the Deep Learning material in a way that recognizes the transformative achievements of Deep Learning in engaging with material. We want your engagement with AI tools to enhance your learning, not inhibit it. </paragraph><paragraph><break/>Edited to add: There are two steps to submission of this: (1) Post on Ed. This is really important because the whole point is to create something of value both to yourself and to your classmates. (2) Submit to Gradescope --- there are assignments there for A, B, C, and E. For C and E where you can submit twice, there are two distinct assignments so that you can submit first to one and then do a different one for the second assignments. There is nothing there yet for D because that hasn't been unlocked by lecture yet. </paragraph></document>",
            "links": [
                "https://docs.google.com/spreadsheets/d/1KWk_O2wMFEmRR8QmZQ9paRcIgehc9IeD5u_pUgnAp4k/edit?usp=sharing",
                "https://docs.google.com/spreadsheets/d/1vmC2ZKx5TXykmMUchbJ4NBFUDKavzDaXVAfN5ueDlAs/edit?usp=sharing",
                "https://docs.google.com/spreadsheets/d/1JcL5cToJs0fGcfaBD5k2dzO24jIk6kWhB1jtiYTsc6I/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-09-15T09:06:25.131409+10:00",
            "category": "Admin"
        },
        {
            "guid": 6932241,
            "author": "Anant Sahai",
            "project_title": "Prior knowledge reference thread",
            "post_body": "Hello students, \n\nThis thread exists to create pointers to concrete resources that can help students review material that they might be rusty on. Feel free to add your own helpful pointers here. Of course, this kind of reference material can just jog your memory --- the internalization of these concepts and skill in their use happened when you took the courses, did the homeworks, etc. \n\n\n\nOptimization and key linear-algebraic math: (EECS 127/227A stuff)\n\nhttps://inst.eecs.berkeley.edu/~ee127/sp24/assets/notes/eecs127_reader.pdf\n\nAnd from one of Prof. Sahai's past TAs in 189/289A:\n\nhttp://gwthomas.github.io/docs/math4ml.pdf\n\nThis also has some probability in it.\n\n\n\nMachine Learning: (189/289A stuff)\n\nJonathan Shewchuk's notes: http://www.cs.berkeley.edu/~jrs/papers/machlearn.pdf\n\nProbability (EECS 126 stuff)\n\nhttps://www.springer.com/us/book/9783030499945\n\nAs well as notes from:\n\nhttps://inst.eecs.berkeley.edu/~ee126/sp25/\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hello students, </paragraph><paragraph>This thread exists to create pointers to concrete resources that can help students review material that they might be rusty on. Feel free to add your own helpful pointers here. Of course, this kind of reference material can just jog your memory --- the internalization of these concepts and skill in their use happened when you took the courses, did the homeworks, etc. <break/></paragraph><heading level=\"2\"><break/>Optimization and key linear-algebraic math: (EECS 127/227A stuff)</heading><paragraph><link href=\"https://inst.eecs.berkeley.edu/~ee127/sp24/assets/notes/eecs127_reader.pdf\">https://inst.eecs.berkeley.edu/~ee127/sp24/assets/notes/eecs127_reader.pdf</link></paragraph><paragraph>And from one of Prof. Sahai's past TAs in 189/289A:</paragraph><paragraph><link href=\"http://gwthomas.github.io/docs/math4ml.pdf\">http://gwthomas.github.io/docs/math4ml.pdf</link></paragraph><paragraph>This also has some probability in it.</paragraph><paragraph/><heading level=\"2\">Machine Learning: (189/289A stuff)</heading><paragraph>Jonathan Shewchuk's notes: <link href=\"http://www.cs.berkeley.edu/~jrs/papers/machlearn.pdf\">http://www.cs.berkeley.edu/~jrs/papers/machlearn.pdf</link></paragraph><heading level=\"2\">Probability (EECS 126 stuff)</heading><paragraph><link href=\"https://www.springer.com/us/book/9783030499945\">https://www.springer.com/us/book/9783030499945</link></paragraph><paragraph>As well as notes from:</paragraph><paragraph><link href=\"https://inst.eecs.berkeley.edu/~ee126/sp25/\">https://inst.eecs.berkeley.edu/~ee126/sp25/</link></paragraph><paragraph/></document>",
            "links": [
                "https://inst.eecs.berkeley.edu/~ee127/sp24/assets/notes/eecs127_reader.pdf",
                "http://gwthomas.github.io/docs/math4ml.pdf",
                "http://www.cs.berkeley.edu/~jrs/papers/machlearn.pdf",
                "https://www.springer.com/us/book/9783030499945",
                "https://inst.eecs.berkeley.edu/~ee126/sp25/"
            ],
            "attachments": [],
            "created_at": "2025-09-09T09:52:10.156099+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6899118,
            "author": "Anant Sahai",
            "project_title": "Study group facilitation thread",
            "post_body": "As was mentioned in lecture, we strongly recommend the use of small study groups to keep yourself accountable (come to or watch lecture together; share and compare notes; come to discussion together; do homework without distractions; etc.) and to have people that you can study with (talk about the homework problems, figure out what you don't understand, etc.). Study groups also sometimes act as default nucleation sites for forming project groups later. \n\nThis thread is designed to help you self-organize with other students in the course. You don't have to do this of course. This is just there as a resource to help people connect. Experience suggests that a study group with someone you don't already know works better if you: \n\nEstablish honest and straightforward expectations regarding what you're aiming to achieve in this course. We understand that some students are aiming for a B while others are looking to completely maximize their understanding. It's good for study group members to know this about each other to avoid future conflicts due to differing implicit assumptions.\n\nGet the logistics of meetings times outside of class worked out fast. In-person work together tends to work better, but even if not in-person, common times are required. \n\nEstablish honest and clear understandings of each other's background with regard to this material. Heterogenous background groups can sometimes work out, but it is good to avoid conflicts due to different implicit assumptions because you hadn't talked about this earlier. \n\nNot all this has to happen in this thread of course! We just think you should do it before finalizing your group. ",
            "content_xml": "<document version=\"2.0\"><paragraph>As was mentioned in lecture, we strongly recommend the use of small study groups to keep yourself accountable (come to or watch lecture together; share and compare notes; come to discussion together; do homework without distractions; etc.) and to have people that you can study with (talk about the homework problems, figure out what you don't understand, etc.). Study groups also sometimes act as default nucleation sites for forming project groups later. </paragraph><paragraph>This thread is designed to help you self-organize with other students in the course. You don't have to do this of course. This is just there as a resource to help people connect. Experience suggests that a study group with someone you don't already know works better if you: </paragraph><list style=\"bullet\"><list-item><paragraph>Establish honest and straightforward expectations regarding what you're aiming to achieve in this course. We understand that some students are aiming for a B while others are looking to completely maximize their understanding. It's good for study group members to know this about each other to avoid future conflicts due to differing implicit assumptions.</paragraph></list-item><list-item><paragraph>Get the logistics of meetings times outside of class worked out fast. In-person work together tends to work better, but even if not in-person, common times are required. </paragraph></list-item><list-item><paragraph>Establish honest and clear understandings of each other's background with regard to this material. Heterogenous background groups can sometimes work out, but it is good to avoid conflicts due to different implicit assumptions because you hadn't talked about this earlier. </paragraph></list-item></list><paragraph>Not all this has to happen in this thread of course! We just think you should do it before finalizing your group. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-02T03:27:51.139026+10:00",
            "category": "Admin"
        },
        {
            "guid": 7428058,
            "author": "Jin Ying",
            "project_title": "Special Participation E: Learning RNN with Sonnet 4",
            "post_body": "When we were learning RNNs two months ago, I wasn't able to understand the lectures very well, and the topic was also not in the Prince's textbook. So I asked Claude to test my understanding of the topic by asking me one question at a time; this was my first time trying this and it helped me a lot in understanding concepts. So later in the semester, I tried this method also on many other topics, as well as for other classes, and had since improved the method a lot better over the semester. Rather than passively absorbing explanations, I had to actively reason through each concept\u2014why we need weight sharing, how the hidden state acts as memory, why vanishing gradients occur, and how LSTMs solve this with additive updates. Claude would push back on incomplete answers (\"But let me push you further...\") and use concrete examples to guide my thinking toward insights I couldn't discover myself.",
            "content_xml": "<document version=\"2.0\"><paragraph>When we were learning RNNs two months ago, I wasn't able to understand the lectures very well, and the topic was also not in the Prince's textbook. So I asked Claude to test my understanding of the topic by asking me one question at a time; this was my first time trying this and it helped me a lot in understanding concepts. So later in the semester, I tried this method also on many other topics, as well as for other classes, and had since improved the method a lot better over the semester. Rather than passively absorbing explanations, I had to actively reason through each concept\u2014why we need weight sharing, how the hidden state acts as memory, why vanishing gradients occur, and how LSTMs solve this with additive updates. Claude would push back on incomplete answers (\"But let me push you further...\") and use concrete examples to guide my thinking toward insights I couldn't discover myself.</paragraph><file url=\"https://static.us.edusercontent.com/files/Jk4BzVh4dlZuWAu8TOp1C6Rs\" filename=\"RNN with Claude.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T10:29:02.162471+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427959,
            "author": "Edward Zhang",
            "project_title": "Special Participation B: DeepSeek on HW02 Problem 3",
            "post_body": "Having worked through this deep learning initialization task with DeepSeek, I was impressed by how effectively we implemented and compared different weight initialization schemes. What stood out most was how we not only coded the He initialization correctly on the first try, but also built comprehensive gradient tracking to visualize why it works better. The implementation captured exactly why He initialization prevents vanishing gradients in ReLU networks - something that often takes students much trial and error to grasp. What surprised me was how quickly we moved from theory to practical validation, with the gradient norm plots clearly showing why zero initialization fails and why random initialization struggles in deeper networks. This experience showed me how proper tooling and clear explanations can make complex concepts like variance preservation in deep networks immediately understandable and verifiable through experimentation.",
            "content_xml": "<document version=\"2.0\"><paragraph>Having worked through this deep learning initialization task with DeepSeek, I was impressed by how effectively we implemented and compared different weight initialization schemes. What stood out most was how we not only coded the He initialization correctly on the first try, but also built comprehensive gradient tracking to visualize why it works better. The implementation captured exactly why He initialization prevents vanishing gradients in ReLU networks - something that often takes students much trial and error to grasp. What surprised me was how quickly we moved from theory to practical validation, with the gradient norm plots clearly showing why zero initialization fails and why random initialization struggles in deeper networks. This experience showed me how proper tooling and clear explanations can make complex concepts like variance preservation in deep networks immediately understandable and verifiable through experimentation.</paragraph><file url=\"https://static.us.edusercontent.com/files/3xJQKp2b7TeEg9L6q76uT3Nw\" filename=\"hw2_coding_report.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T10:15:33.82273+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427939,
            "author": "Edward Zhang",
            "project_title": "Special Participation A: DeepSeek on HW07",
            "post_body": "In this chat, I utilized DeepSeek 3.2 in its default reasoning mode to work through the machine learning homework problems step by step. Starting with the full context of the assignment, I prompted the model to solve each sub\u2011question sequentially, occasionally providing clarifications or corrections when needed\u2014though such interventions were minimal, as the model consistently delivered accurate, well\u2011reasoned solutions. What stood out was the model\u2019s deliberate avoidance of simplifying numerical results, leaving expressions in exact form to prevent calculation errors and encourage manual verification. I also noticed occasional inconsistencies in variable naming across steps, but the logical flow remained clear and correct.\n\nCompared to other models I have used, DeepSeek favored concise, direct answers over lengthy exposition, which made the solutions easier to follow but sometimes omitted broader contextual commentary. This trade\u2011off between brevity and nuance reflects a distinct design philosophy. Finally, when I once mistakenly referenced the wrong part of a problem, the model did not simply guess; instead, it recognized the inconsistency, inferred the likely intended question, and answered accordingly. This demonstrated a welcome resistance to hallucination and an ability to maintain coherence even when the prompt was imperfect. Overall, the interaction proved efficient and reliable for technical problem\u2011solving.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/I3cif0PfdLXUqCgWNj0sdcSS\" filename=\"eecs182_HW07.pdf\"/><paragraph>In this chat, I utilized DeepSeek 3.2 in its default reasoning mode to work through the machine learning homework problems step by step. Starting with the full context of the assignment, I prompted the model to solve each sub\u2011question sequentially, occasionally providing clarifications or corrections when needed\u2014though such interventions were minimal, as the model consistently delivered accurate, well\u2011reasoned solutions. What stood out was the model\u2019s deliberate avoidance of simplifying numerical results, leaving expressions in exact form to prevent calculation errors and encourage manual verification. I also noticed occasional inconsistencies in variable naming across steps, but the logical flow remained clear and correct.</paragraph><paragraph>Compared to other models I have used, DeepSeek favored concise, direct answers over lengthy exposition, which made the solutions easier to follow but sometimes omitted broader contextual commentary. This trade\u2011off between brevity and nuance reflects a distinct design philosophy. Finally, when I once mistakenly referenced the wrong part of a problem, the model did not simply guess; instead, it recognized the inconsistency, inferred the likely intended question, and answered accordingly. This demonstrated a welcome resistance to hallucination and an ability to maintain coherence even when the prompt was imperfect. Overall, the interaction proved efficient and reliable for technical problem\u2011solving.</paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T10:12:14.47487+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427934,
            "author": "Jin Ying",
            "project_title": "Special Participation B: Opus 4.5 on HW2 (and also compare with GPT 5.1 Thinking)",
            "post_body": "I asked Opus 4.5 to solve questions 3, 4, and 6 in HW2 with justificiation. Opus 4.5 basically one-shotted all the questions with minor errors. This was actually not surprising to me, as the coding parts in this homework are all very basic; they are simply asking us to translate basic concepts into code, and I think models before Opus 4.5 can already handle this very well -- I also asked for help from GPT 5 and Sonnet 4.5 when doing this homework at the beginning of the semester, when Opus 4.5 was not out, and if i asked an individual question, they could generate the answer and explain the code very easily. What surprised me here is when given all the questions and code Opus 4.5 solved it in a short amount of time (Claude did not record the working time, but it takes no longer than 3 mins). Since the solutions it gave are all correct, I wanted to analyze how Opus 4.5 do it (instead of focusing on our interactions, since it does not need much instructions here to solve the questions) and compare with it with GPT 5.1 thinking, which also did not exist when we are doing this homework. My initial guess is Claude would be superior in the sense of conciseness. I tried the same prompt and questions with GPT 5.1 thinking, it takes way longer than Opus 4.5; in my first try, it errored in 13m 36s when just starting generating the answers. The second time took also about 13m and finally generated the correct answers. Claude Opus 4.5 is ~4x faster while maintaining solution correctness and providing solid conceptual explanations. Both models handle these straightforward translation-to-code problems easily, suggesting the real differentiator is efficiency rather than capability for basic homework questions. I include further analysis and comparsion in the doc. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I asked Opus 4.5 to solve questions 3, 4, and 6 in HW2 with justificiation. Opus 4.5 basically one-shotted all the questions with minor errors. This was actually not surprising to me, as the coding parts in this homework are all very basic; they are simply asking us to translate basic concepts into code, and I think models before Opus 4.5 can already handle this very well -- I also asked for help from GPT 5 and Sonnet 4.5 when doing this homework at the beginning of the semester, when Opus 4.5 was not out, and if i asked an individual question, they could generate the answer and explain the code very easily. What surprised me here is when given all the questions and code Opus 4.5 solved it in a short amount of time (Claude did not record the working time, but it takes no longer than 3 mins). Since the solutions it gave are all correct, I wanted to analyze how Opus 4.5 do it (instead of focusing on our interactions, since it does not need much instructions here to solve the questions) and compare with it with GPT 5.1 thinking, which also did not exist when we are doing this homework. My initial guess is Claude would be superior in the sense of conciseness. I tried the same prompt and questions with GPT 5.1 thinking, it takes way longer than Opus 4.5; in my first try, it errored in 13m 36s when just starting generating the answers. The second time took also about 13m and finally generated the correct answers. Claude Opus 4.5 is ~4x faster while maintaining solution correctness and providing solid conceptual explanations. Both models handle these straightforward translation-to-code problems easily, suggesting the real differentiator is efficiency rather than capability for basic homework questions. I include further analysis and comparsion in the doc. </paragraph><file url=\"https://static.us.edusercontent.com/files/DK1ATWepmmognNyttMQ5frA8\" filename=\"HW2 with Opus 4 and GPT Thinking 5.1.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T10:11:48.245631+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427874,
            "author": "Hiya Shah",
            "project_title": "Special Participation A: Llama 4 Maverick on HW 11",
            "post_body": "I guided Llama 4 Maverick to solve the non-coding questions for Homework 11, which was largely about model finetuning, LoRA, and Fermi Estimation and memory calculation for large-scale DL models. I provided the model with the following prompt,\n\n\"You will be solving various questions from this Deep Learning homework pdf. Please go over one question at a time when I specify the question. First, output the exact problem statement from the PDF, then give a full explanation of how you reached your answer, along with the answer.\"\n\nOverall, the model performed well on the proof-based questions. However, it performed relatively poorly on complex calculations (especially in the Fermi estimation question) and on multistep formula derivations with numerical value plugins. I was surprised to find that it did not hallucinate the problem question, giving it back to me accurately word for word.\n\nProblem 1: In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training. For part i, it correctly noted to increase the rank, but it did not mention alternatives such as changing the initialization and changing the learning rate. For the rest of the parts, it generally gave the correct answer. However, it would not mention the practical alternatives such as initializing A with random initialization and initializing B with 0.  \n\nProblem 2: For this problem, I asked the model to solve each question part by part. For part i, it got the correct answer, but I noticed that it did not solve the questions in the same manner a student would. By this, I mean that the model would first give the answer and then walk through the solutions instead of step by step arriving at the answer. It would also not highlight what is the most important takeaway from the problem/what to do know generally as the solutions pdf does (ex. it is important to know that the calculation of logits only depends on the input token). In this problem, it did not reference the figures in its answer either. For the SVD problem, I wish it had enough context in for example long term memory to reference where in previous lectures I could understand this information.\n\nProblem 3: As this was a coding problem, I did not prompt Llama 4 to solve this part.\n\nProblem 4: As this was a coding problem, I did not prompt Llama 4 to solve this part.\n\nProblem 5: In this problem, Llama 4 was tasked with Fermi estimation, which is a technique for estimating quantities through rough approximations and educated guesses. As the model was tasked with approximation, I found that it performed quite poorly compared to the approximations given by the solutions. It missed nearly every single calculation and formula derivation, and it had trouble analyzing multimodal context in the form of the figures and table. I believe the labels on the graph were slightly small, so they were not readable by the model. However, the model successfully skipped the problems which did not require a solution, meaning it did analyze the given context or at least attempt to.\n\nProblem 6: In this problem, I asked Llama 4 to solve each part step by step. I believe by this time, the model was experiencing context rot as it frequently provided short responses that I had to reprompt with screenshots of the problem part (even though I had provided a pdf earlier). It generally gave correct answers, but they were not as verbose as the solutions. The answer to part a was only halfway correct. For part d, it did not consider the variations on the basic MAML approach.\n\nOverall, the model performed ~62.5% accuracy, and it mainly missed questions later in the pdf or questions requiring significant calculation and approximation.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I guided <link href=\"https://app.chathub.gg/chat/cloud-llama4?utm_source=models\">Llama 4</link> Maverick to solve the non-coding questions for Homework 11, which was largely about model finetuning, LoRA, and Fermi Estimation and memory calculation for large-scale DL models. I provided the model with the following prompt,</paragraph><paragraph>\"You will be solving various questions from this Deep Learning homework pdf. Please go over one question at a time when I specify the question. First, output the exact problem statement from the PDF, then give a full explanation of how you reached your answer, along with the answer.\"</paragraph><paragraph>Overall, the model performed well on the proof-based questions. However, it performed relatively poorly on complex calculations (especially in the Fermi estimation question) and on multistep formula derivations with numerical value plugins. I was surprised to find that it did not hallucinate the problem question, giving it back to me accurately word for word.</paragraph><paragraph><italic>Problem 1:</italic> In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training. For part i, it correctly noted to increase the rank, but it did not mention alternatives such as changing the initialization and changing the learning rate. For the rest of the parts, it generally gave the correct answer. However, it would not mention the practical alternatives such as initializing A with random initialization and initializing B with 0.  </paragraph><paragraph><italic>Problem 2:</italic> For this problem, I asked the model to solve each question part by part. For part i, it got the correct answer, but I noticed that it did not solve the questions in the same manner a student would. By this, I mean that the model would first give the answer and then walk through the solutions instead of step by step arriving at the answer. It would also not highlight what is the most important takeaway from the problem/what to do know generally as the solutions pdf does (ex. it is important to know that the calculation of logits only depends on the input token). In this problem, it did not reference the figures in its answer either. For the SVD problem, I wish it had enough context in for example long term memory to reference where in previous lectures I could understand this information.</paragraph><paragraph><italic>Problem 3:</italic> As this was a coding problem, I did not prompt Llama 4 to solve this part.</paragraph><paragraph><italic>Problem 4:</italic> As this was a coding problem, I did not prompt Llama 4 to solve this part.</paragraph><paragraph><italic>Problem 5</italic>: In this problem, Llama 4 was tasked with Fermi estimation, which is a technique for estimating quantities through rough approximations and educated guesses. As the model was tasked with approximation, I found that it performed quite poorly compared to the approximations given by the solutions. It missed nearly every single calculation and formula derivation, and it had trouble analyzing multimodal context in the form of the figures and table. I believe the labels on the graph were slightly small, so they were not readable by the model. However, the model successfully skipped the problems which did not require a solution, meaning it did analyze the given context or at least attempt to.</paragraph><paragraph><italic>Problem 6:</italic> In this problem, I asked Llama 4 to solve each part step by step. I believe by this time, the model was experiencing context rot as it frequently provided short responses that I had to reprompt with screenshots of the problem part (even though I had provided a pdf earlier). It generally gave correct answers, but they were not as verbose as the solutions. The answer to part a was only halfway correct. For part d, it did not consider the variations on the basic MAML approach.</paragraph><paragraph>Overall, the model performed ~62.5% accuracy, and it mainly missed questions later in the pdf or questions requiring significant calculation and approximation.</paragraph><file url=\"https://static.us.edusercontent.com/files/UVlpPTLBEEeap7wlrjOQfZHj\" filename=\"special_participation_A_cs182_llama_4.pdf\"/><paragraph/></document>",
            "links": [
                "https://app.chathub.gg/chat/cloud-llama4?utm_source=models"
            ],
            "attachments": [],
            "created_at": "2025-12-08T10:03:33.356926+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427837,
            "author": "Diana Kohr",
            "project_title": "Special Participation A: Gemini 2.5 Flash on Homework 1",
            "post_body": "I used Gemini 2.5 Flash to answer HW 1 written problems. \n\nConversation: https://gemini.google.com/share/11b5f1b89778\n\nAnnotated: https://drive.google.com/file/d/1vGRWvLGliMGdQhvNDdYq0SKYC575tPdd/view?usp=sharing\n\nSummary: Gemini was able to one-shot a majority of the problems. The only issues were minor misinterpretations of the problem statement (notably, the interpretation of the error factor in 1b). Gemini did a good job of justifying each step in problems that required many sequential equivalences. One issue: Gemini tended to forget that it was solving problems from the provided document, sometimes coming up with its own problem to solve. At the end, I tried to see if Gemini would be able to add clarifications to its previous responses given the answer key, but it just confirmed that its answers were correct, and didn't yield any improvements. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini 2.5 Flash to answer HW 1 written problems. </paragraph><paragraph>Conversation: <link href=\"https://gemini.google.com/share/11b5f1b89778\">https://gemini.google.com/share/11b5f1b89778</link></paragraph><paragraph>Annotated: https://drive.google.com/file/d/1vGRWvLGliMGdQhvNDdYq0SKYC575tPdd/view?usp=sharing</paragraph><paragraph>Summary: Gemini was able to one-shot a majority of the problems. The only issues were minor misinterpretations of the problem statement (notably, the interpretation of the error factor in 1b). Gemini did a good job of justifying each step in problems that required many sequential equivalences. One issue: Gemini tended to forget that it was solving problems from the provided document, sometimes coming up with its own problem to solve. At the end, I tried to see if Gemini would be able to add clarifications to its previous responses given the answer key, but it just confirmed that its answers were correct, and didn't yield any improvements. </paragraph><paragraph/></document>",
            "links": [
                "https://gemini.google.com/share/11b5f1b89778"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:58:55.944225+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427798,
            "author": "Vijay Kethanaboyina",
            "project_title": "Special Participation E: Visualizing LLM Decoding",
            "post_body": "For this participation assignment, I created an interactive LLM Decoding Visualizer that demonstrates how Large Language Models (like GPT-3.5) generate text one token at a time. It allows you to experiment with inference-time parameters in real time to see how they affect the model output.\n\nTry it out for yourself here: https://www.vkethana.com/visualize_decode/\n\nA transcript of my conversation with the model can be found here: https://drive.google.com/file/d/1Ie75FGR4iAvZ4KIRmQxlilKTIGN9h1RR/view?usp=sharing. I used Cursor as my IDE for this assignment, and the actual model generations came from Gemini 3 Pro. The model performed well, and aside from a few edge cases it was able to one-shot all prompts I gave it.\n\nClass Concepts Covered:\n\nInference-Time Sampling: Recall that there was a discussion worksheet that covered the various LM sampling techniques. This tool builds off that content.\n\nGreedy Decoding: You can see what happens when the model is forced to always pick the single most likely token (Temperature = 0). There is a special button in the tool allowing you to enable greedy decoding.\n\nTemperature & Top-P: It visualizes how \"temperature\" flattens the probability distribution (making the model more creative/random) or sharpens it (making it more deterministic). The tool also gives you control over the value of p used for top-p sampling.\n\nTo use the interactive features, you'll need to provide your own OpenAI API key (due to inference costs, I am unable to host the model directly on my personal website). If you'd prefer not to use your API key, you're welcome to watch a video demo of the tool here: https://drive.google.com/file/d/1gGQwGiIx12-OZ_mrhktIW8nObmyv7dYb/view",
            "content_xml": "<document version=\"2.0\"><paragraph>For this participation assignment, I created an interactive LLM Decoding Visualizer that demonstrates how Large Language Models (like GPT-3.5) generate text one token at a time. It allows you to experiment with inference-time parameters in real time to see how they affect the model output.</paragraph><paragraph>Try it out for yourself here: <link href=\"https://www.vkethana.com/visualize_decode/\">https://www.vkethana.com/visualize_decode/</link></paragraph><paragraph>A transcript of my conversation with the model can be found here: <link href=\"https://drive.google.com/file/d/1Ie75FGR4iAvZ4KIRmQxlilKTIGN9h1RR/view?usp=sharing\">https://drive.google.com/file/d/1Ie75FGR4iAvZ4KIRmQxlilKTIGN9h1RR/view?usp=sharing</link>. I used Cursor as my IDE for this assignment, and the actual model generations came from Gemini 3 Pro. The model performed well, and aside from a few edge cases it was able to one-shot all prompts I gave it.</paragraph><paragraph>Class Concepts Covered:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Inference-Time Sampling</bold>: Recall that there was a discussion worksheet that covered the various LM sampling techniques. This tool builds off that content.</paragraph></list-item><list-item><paragraph><bold>Greedy Decoding:</bold> You can see what happens when the model is forced to always pick the single most likely token (Temperature = 0). There is a special button in the tool allowing you to enable greedy decoding.</paragraph></list-item><list-item><paragraph><bold>Temperature &amp; Top-P:</bold> It visualizes how \"temperature\" flattens the probability distribution (making the model more creative/random) or sharpens it (making it more deterministic). The tool also gives you control over the value of <italic>p</italic> used for top-p sampling.</paragraph></list-item></list><paragraph>To use the interactive features, you'll need to provide your own OpenAI API key (due to inference costs, I am unable to host the model directly on my personal website). If you'd prefer not to use your API key, you're welcome to watch a video demo of the tool here: <link href=\"https://drive.google.com/file/d/1gGQwGiIx12-OZ_mrhktIW8nObmyv7dYb/view\">https://drive.google.com/file/d/1gGQwGiIx12-OZ_mrhktIW8nObmyv7dYb/view</link></paragraph></document>",
            "links": [
                "https://www.vkethana.com/visualize_decode/",
                "https://drive.google.com/file/d/1Ie75FGR4iAvZ4KIRmQxlilKTIGN9h1RR/view?usp=sharing",
                "https://drive.google.com/file/d/1gGQwGiIx12-OZ_mrhktIW8nObmyv7dYb/view"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:55:08.583548+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427672,
            "author": "Keshab Agarwal",
            "project_title": "Special Participation A: Claude Opus 4.5 with Extended Thinking on HW10",
            "post_body": "I experimented with Claude Opus 4.5 using Extended Thinking on HW10, and the experience was, not surprisingly, great. I provided it with screenshots of each problem, along with the full FaceNet paper PDF from arXiv for reference. Claude handled the input flawlessly: it read every question accurately, interpreted the diagrams and text without misidentification, and produced step-by-step reasoning that aligned with the expected computations.\n\nSomething impressive was its ability to parse the research paper correctly and ground its answers in the actual content rather than hallucinating details or making unsupported claims. While a few of the explanations could have been more detailed or expanded with additional intuition, the overall responses were coherent, well structured, and factually correct. In the end, it solved every problem in a single attempt.\n\nHere is the chat:\n\nhttps://claude.ai/share/4655874a-b723-458e-bcbb-481006722865\n\nHere is my annotated pdf:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I experimented with Claude Opus 4.5 using Extended Thinking on HW10, and the experience was, not surprisingly, great. I provided it with screenshots of each problem, along with the full FaceNet paper PDF from arXiv for reference. Claude handled the input flawlessly: it read every question accurately, interpreted the diagrams and text without misidentification, and produced step-by-step reasoning that aligned with the expected computations.</paragraph><paragraph>Something impressive was its ability to parse the research paper correctly and ground its answers in the actual content rather than hallucinating details or making unsupported claims. While a few of the explanations could have been more detailed or expanded with additional intuition, the overall responses were coherent, well structured, and factually correct. In the end, it solved every problem in a single attempt.</paragraph><paragraph>Here is the chat:<break/><break/><link href=\"https://claude.ai/share/4655874a-b723-458e-bcbb-481006722865\">https://claude.ai/share/4655874a-b723-458e-bcbb-481006722865</link></paragraph><paragraph>Here is my annotated pdf:</paragraph><file url=\"https://static.us.edusercontent.com/files/SP5g8OOJlUUSu68kka56AiMx\" filename=\"HW10-ClaudeOpus4.5-ExtendedThinking-Annotated.pdf\"/><paragraph/></document>",
            "links": [
                "https://claude.ai/share/4655874a-b723-458e-bcbb-481006722865"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:40:03.075613+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427600,
            "author": "Tiger Zhang",
            "project_title": "Special Participation B: Kimi 1.5 on HW9 Coding",
            "post_body": "Executive Summary:\n\nI used Kimi 1.5 (specifically the 1.5 version instead of Kimi 2 because Kimi 2 doesn\u2019t seem to have image-understanding capabilities) to do the coding question on homework 9, visualizing BERT.\n\nThis is a special coding question: it doesn\u2019t involve writing any code, but instead involves interpreting the visualizations of GPT2 and BERT.\n\nIn accordance with the spirit of participation B, I evaluated whether or not Kimi 1.5 is able to perform the task in this question (i.e. visualizing and then interpreting the visualization to answer the questions), and the results were disappointing.\n\nResults:\n\nMy first attempt is to let Kimi choose exactly which combinations of layer/head it wants to visualize, and then I would be it\u2019s \u201ctool\u201d to get the visualizations, and then it would draw conclusions from there. However, even after efforts to tune the prompt for this task, Kimi failed to perform this task.\n\nMy next steps were to evaluate how well Kimi can take hints to perform the task of understanding the images. It seemed like Kimi didn\u2019t have the ability to process too many images at the same time and would get very confused for having too many images, and did not know which images to focus on to ask me for the right images. Therefore, I chose good images for Kimi for each of the questions.\n\nAfter the aforementioned change, Kimi\u2019s performance started increasing slowly. Still, towards the beginning, I had to provide significant hints for it to arrive at the correct answer. Kimi\u2019s better performance on later questions may be attributed to it's in-context learning capabilities, as it became more familiar with the task; alternatively, it could also be attributed to the fact that later questions are more \u201cguessable\u201d even if one doesn\u2019t understand the visualizations.\n\nTo see my conversation with Kimi, please look at the following log, where I also annotated in detail what is happening:",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold>:</paragraph><paragraph>I used Kimi 1.5 (specifically the 1.5 version instead of Kimi 2 because Kimi 2 doesn\u2019t seem to have image-understanding capabilities) to do the coding question on homework 9, visualizing BERT.</paragraph><paragraph>This is a special coding question: it doesn\u2019t involve writing any code, but instead involves interpreting the visualizations of GPT2 and BERT.</paragraph><paragraph>In accordance with the spirit of participation B, I evaluated whether or not Kimi 1.5 is able to perform the task in this question (i.e. visualizing and then interpreting the visualization to answer the questions), and the results were disappointing.</paragraph><paragraph>Results:</paragraph><paragraph>My first attempt is to let Kimi choose exactly which combinations of layer/head it wants to visualize, and then I would be it\u2019s \u201ctool\u201d to get the visualizations, and then it would draw conclusions from there. However, even after efforts to tune the prompt for this task, Kimi failed to perform this task.</paragraph><paragraph>My next steps were to evaluate how well Kimi can take hints to perform the task of understanding the images. It seemed like Kimi didn\u2019t have the ability to process too many images at the same time and would get very confused for having too many images, and did not know which images to focus on to ask me for the right images. Therefore, I chose good images for Kimi for each of the questions.</paragraph><paragraph>After the aforementioned change, Kimi\u2019s performance started increasing slowly. Still, towards the beginning, I had to provide significant hints for it to arrive at the correct answer. Kimi\u2019s better performance on later questions may be attributed to it's in-context learning capabilities, as it became more familiar with the task; alternatively, it could also be attributed to the fact that later questions are more \u201cguessable\u201d even if one doesn\u2019t understand the visualizations.</paragraph><paragraph>To see my conversation with Kimi, please look at the following log, where I also annotated in detail what is happening:</paragraph><file url=\"https://static.us.edusercontent.com/files/LycjdVOHGJxSGX9X3hKnWd7b\" filename=\"chat_log.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T09:32:33.716334+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427555,
            "author": "Reyna Liu",
            "project_title": "Special Participation E: Muon",
            "post_body": "For this Special Participation E, I made an interactive Muon tutor using a custom prompt for ChatGPT\u2019s Study Mode. The goal was to turn Muon (Momentum Orthogonalized by Newton\u2013Schulz) into a single coherent story that lines up with the lecture content: starting from Shampoo and semi-orthogonal updates, then moving through Newton\u2013Schulz style iterations, odd polynomials, normalization, and finally the full Muon update rule.\n\nThe tutor is designed to complement pre-lecture or post-lecture reading: instead of passively skimming notes, you interact with the model in a guided way, and it is forced (by the prompt) to:\n\nstay on one narrative arc (the \u201capproximate (UV^\\top)\u201d story),\n\nexplain each subtopic with intuition \u2192 light math \u2192 tiny example \u2192 concept check,\n\nnot move on until you answer its question.\n\nPrompt\n\nYou are my tutor for the Muon optimizer (\u201cMomentum Orthogonalized by Newton\u2013Schulz\u201d) and the optimization ideas behind it in modern deep learning.\n\nI want this to be an interactive tutoring session. In every reply:\n\n- Keep your answer focused and reasonably short (about 3\u20136 short paragraphs).\n\n- Do not move on to the next topic until I say so.\n\n- Always end your message by asking me a specific question (for example: \u201cDoes this make sense?\u201d or a small concept-check question), and then wait for my reply.\n\nThe overall story I want to learn is:\n\nHow Muon approximates an \u201cideal\u201d semi-orthogonal update direction (like what Shampoo would give with SVD), but does it cheaply using Newton\u2013Schulz style iterations and polynomials that operate on singular values, plus a momentum-like accumulator.\n\nPlease organize everything as one coherent narrative, not a bag of tricks.\n\nStep 1: Short big-picture intro\n\nFirst response:\n\n1. In at most 2 short paragraphs, explain:\n\n - What problem Muon is trying to solve:\n\n - Shampoo-style methods want to precondition gradients using information like singular values / curvature, but exact SVD-based updates are too expensive.\n\n - We would like an update direction that behaves like \u201cU V\u1d40\u201d from an SVD, i.e., semi-orthogonal and not dominated by large singular values.\n\n - The core idea of Muon in one or two sentences:\n\n - Muon keeps a momentum-like running matrix, then repeatedly applies a simple polynomial transformation (a Newton\u2013Schulz\u2013style iteration) so that its singular values are pushed toward 1, approximating that semi-orthogonal \u201cU V\u1d40\u201d direction without doing a full SVD.\n\n2. Then give a brief roadmap (4\u20137 bullet points) of the main pieces of the story, all explicitly framed as supporting Muon, for example:\n\n - From \u201coptimizer recipe\u201d with a chosen norm to the idea of spectral norm and semi-orthogonal updates.\n\n - Shampoo and \u201cShampoo without accumulation\u201d as the starting conceptual algorithm.\n\n - Why computing the exact SVD-based direction is too expensive, and why an approximate direction is good enough.\n\n - Newton\u2013Schulz\u2013style iterations as a way to adjust singular values toward 1.\n\n - Odd polynomials that operate on singular values while keeping singular vectors fixed, and why that matters.\n\n - Stability issues: why we need to normalize by something like the Frobenius norm so that singular values start in the right range and iterations don\u2019t blow up.\n\n - The final Muon update: momentum buffer, orthogonalization via polynomial iterations, weight update, and how the choice of polynomial coefficients affects behavior.\n\nEnd your first reply by asking me one short question to check my high-level understanding.\n\nAfter I respond, start with the first item in the roadmap (semi-orthogonal / spectral-norm view). Follow the roadmap in order.\n\nStep 2: For each part of the story\n\nWhen we are on a given part of the roadmap, teach it in four layers, but keep each message compact.\n\n1. Intuition tied to Muon\n\n - In 1\u20132 short paragraphs, explain the idea in plain language and explicitly say how it supports Muon\u2019s overall goal:\n\n - Approximating a well-conditioned, semi-orthogonal update direction.\n\n - Making steps more uniform across directions instead of being dominated by the largest singular value.\n\n - Doing this without paying the full SVD cost.\n\n2. Key formal setup\n\n - Introduce only the minimal notation you need, such as:\n\n - A weight matrix and its gradient.\n\n - The idea that you can factor a matrix into singular vectors and singular values.\n\n - A momentum-like buffer that accumulates gradient information over time.\n\n - A simple iteration that applies a polynomial in the matrix (for example, a combination of a linear term and a cubic term) to reshape singular values.\n\n - Describe what each object means verbally (for example, \u201cthis buffer stores a smoothed version of recent gradients\u201d, \u201cthis transformation tries to make all singular values close to 1 so the matrix behaves like a semi-orthogonal matrix\u201d).\n\n - You may show a very small symbolic expression or a short iteration rule, but keep it high-level and do not dump long formulas.\n\n3. Tiny numerical or conceptual example\n\n - Give one very small example to illustrate the effect:\n\n - For instance, a 2\u00d72 matrix whose singular values are very different, and describe qualitatively what happens when you apply one or two polynomial iterations to it (the large singular value goes down, the small one goes up, etc.).\n\n - Or a toy picture of \u201cbefore\u201d vs. \u201cafter\u201d for singular values, and how that makes the update more uniform across directions.\n\n - Keep this example to a few lines.\n\n4. Quick understanding check\n\n - Ask me 1\u20132 short questions related to this part:\n\n - At least one conceptual \u201cwhy\u201d question (for example: \u201cWhy is it good if all singular values are close to 1?\u201d or \u201cWhy do we want a polynomial that keeps singular vectors but changes singular values?\u201d).\n\n - Optionally, a tiny computational or thought experiment question (for example: \u201cWhat might go wrong if the singular values start out too large before applying the iteration?\u201d).\n\n - Then stop and wait for my answer.\n\nAfter, briefly remind me what the next stop in the Muon story is, and then proceed.\n\nStep 3: Muon-centered wrap-up\n\nWhen I say I\u2019m ready to review or summarize:\n\n- Give a compact summary that keeps Muon at the center, explaining how:\n\n - Starting from an \u201coptimizer recipe\u201d with a chosen norm leads to preferring semi-orthogonal directions (like U V\u1d40).\n\n - Shampoo (and its non-accumulating variant) motivates this direction but is too expensive if done with an exact SVD.\n\n - Newton\u2013Schulz\u2013style polynomial iterations let us approximate \u201creplace all singular values by 1\u201d using only matrix multiplications.\n\n - Normalizing by something like the Frobenius norm ensures the singular values start in a safe range so the iterations stay stable.\n\n - Muon wraps this into a practical algorithm: maintain a momentum buffer, apply a few iterations of a chosen polynomial to orthogonalize it, then use that to update the weights.\n\nThroughout the conversation, keep answers reasonably short, interactive, and always connected back to the central Muon story, so that the whole interaction feels like a focused tutoring session.\n\nLink to conversation:\n\nhttps://chatgpt.com/share/6933ad42-7bf4-8007-b500-f1bc59851bee\n\nAssessment\n\nThe good:\n\nConceptually, the model\u2019s output is very close to the actual lecture storyline. It correctly emphasized:\n\nThe role of (UV^\\top) as the \u201csemi-orthogonal ideal\u201d direction.\n\n\n\nThe connection between Shampoo and this ideal (a hypothetical \u201cShampoo without accumulation\u201d step).\n\n\n\nThe idea that Newton\u2013Schulz\u2013like odd polynomials act as scalar functions on singular values, while preserving the singular vectors.\n\n\n\nThe importance of normalizing the buffer so the polynomial iteration doesn\u2019t blow up or collapse.\n\nI didn\u2019t spot any major conceptual contradictions with the lecture. The main approximations were in the toy numerical examples (e.g., saying a singular value might move from 0.01 to \u201csomething like 0.2-0.5\u201d after one step without actually computing it). That\u2019s not \u201cwrong\u201d in a dangerous way, but it\u2019s clearly hand-wavy.\n\nThe bad:\n\nRepetitive concept checks.\nIt kept asking essentially the same question in slightly different forms: \u201cWhy is it good if singular values are close to 1?\u201d \u201cWhy do we want a well-conditioned update?\u201d \u201cWhy is it useful that singular vectors stay the same?\u201d\nThese questions are reasonable the first time; by the third time, it felt redundant and didn\u2019t add new understanding.\n\n\n\nHand-wavy numbers in examples.\nIn a few places it invented specific numeric values like \u201cafter one step this might be around 0.2-0.5\u201d without actually computing the polynomial. This is minor, but it\u2019s a reminder: whenever it gives specific numbers in these toy examples, treat them as qualitative (direction of change) rather than precise quantitative claims.\n\n\n\nBehavior when the user can\u2019t really answer its questions.\nThe prompt forces it to ask me a \u201cconcept check\u201d every turn, but it doesn\u2019t actually check my answers or detect if I\u2019m confused. In our interaction it basically replied \u201cExactly\u201d to everything and then continued. So if a student can\u2019t answer, or answers something half-baked, the model will not push back on its own. The student has to explicitly tell the model to explain. It\u2019s too \u201cpolite\u201dto be a real grader for students.\n\n\n\nOverall, though, the interaction did not hallucinate big structural facts. The misbehavior was more minor.\n\nHow this could help with learning\n\nIn practice, this tool works well as a pre- or post-lecture companion:\n\nBefore the lecture, we can run through Part 1-3 to get the high-level picture of \u201cwhy semi-orthogonal,\u201d \u201chow Shampoo motivates the ideal,\u201d and \u201cwhy approximate is enough,\u201d so we\u2019re not seeing those ideas for the first time in class.\n\nAfter the lecture, we can use Parts 4-7 to re-derive the Newton\u2013Schulz connection, the odd polynomial behavior, the normalization step, and the final Muon update rule, while the model keeps quizzing us.\n\nIt can function as a self-paced interactive reading that is much closer to our actual lecture content than a random internet explanation. The main caveats are:\n\nBe aware it may ask very similar concept checks multiple times.\n\nTreat any made-up numbers in the tiny examples as qualitative intuition, not exact math.\n\nRemember it doesn\u2019t really grade your answers. If you\u2019re lost, you need to tell it explicitly to slow down or re-explain.\n\nUse it to structure understanding, but verify key equations and details against the lecture itself if you care about full rigor.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this Special Participation E, I made an interactive Muon tutor using a custom prompt for ChatGPT\u2019s Study Mode. The goal was to turn Muon (Momentum Orthogonalized by Newton\u2013Schulz) into a single coherent story that lines up with the lecture content: starting from Shampoo and semi-orthogonal updates, then moving through Newton\u2013Schulz style iterations, odd polynomials, normalization, and finally the full Muon update rule.</paragraph><paragraph>The tutor is designed to complement pre-lecture or post-lecture reading: instead of passively skimming notes, you interact with the model in a guided way, and it is forced (by the prompt) to:</paragraph><list style=\"unordered\"><list-item><paragraph>stay on one narrative arc (the \u201capproximate (UV^\\top)\u201d story),</paragraph></list-item><list-item><paragraph>explain each subtopic with intuition \u2192 light math \u2192 tiny example \u2192 concept check,</paragraph></list-item><list-item><paragraph>not move on until you answer its question.</paragraph></list-item></list><paragraph><bold>Prompt</bold></paragraph><paragraph>You are my tutor for the Muon optimizer (\u201cMomentum Orthogonalized by Newton\u2013Schulz\u201d) and the optimization ideas behind it in modern deep learning.</paragraph><paragraph>I want this to be an interactive tutoring session. In every reply:</paragraph><paragraph>- Keep your answer focused and reasonably short (about 3\u20136 short paragraphs).</paragraph><paragraph>- Do not move on to the next topic until I say so.</paragraph><paragraph>- Always end your message by asking me a specific question (for example: \u201cDoes this make sense?\u201d or a small concept-check question), and then wait for my reply.</paragraph><paragraph>The overall story I want to learn is:</paragraph><paragraph>How Muon approximates an \u201cideal\u201d semi-orthogonal update direction (like what Shampoo would give with SVD), but does it cheaply using Newton\u2013Schulz style iterations and polynomials that operate on singular values, plus a momentum-like accumulator.</paragraph><paragraph>Please organize everything as one coherent narrative, not a bag of tricks.</paragraph><paragraph>Step 1: Short big-picture intro</paragraph><paragraph>First response:</paragraph><paragraph>1. In at most 2 short paragraphs, explain:</paragraph><paragraph> - What problem Muon is trying to solve:</paragraph><paragraph> - Shampoo-style methods want to precondition gradients using information like singular values / curvature, but exact SVD-based updates are too expensive.</paragraph><paragraph> - We would like an update direction that behaves like \u201cU V\u1d40\u201d from an SVD, i.e., semi-orthogonal and not dominated by large singular values.</paragraph><paragraph> - The core idea of Muon in one or two sentences:</paragraph><paragraph> - Muon keeps a momentum-like running matrix, then repeatedly applies a simple polynomial transformation (a Newton\u2013Schulz\u2013style iteration) so that its singular values are pushed toward 1, approximating that semi-orthogonal \u201cU V\u1d40\u201d direction without doing a full SVD.</paragraph><paragraph>2. Then give a brief roadmap (4\u20137 bullet points) of the main pieces of the story, all explicitly framed as supporting Muon, for example:</paragraph><paragraph> - From \u201coptimizer recipe\u201d with a chosen norm to the idea of spectral norm and semi-orthogonal updates.</paragraph><paragraph> - Shampoo and \u201cShampoo without accumulation\u201d as the starting conceptual algorithm.</paragraph><paragraph> - Why computing the exact SVD-based direction is too expensive, and why an approximate direction is good enough.</paragraph><paragraph> - Newton\u2013Schulz\u2013style iterations as a way to adjust singular values toward 1.</paragraph><paragraph> - Odd polynomials that operate on singular values while keeping singular vectors fixed, and why that matters.</paragraph><paragraph> - Stability issues: why we need to normalize by something like the Frobenius norm so that singular values start in the right range and iterations don\u2019t blow up.</paragraph><paragraph> - The final Muon update: momentum buffer, orthogonalization via polynomial iterations, weight update, and how the choice of polynomial coefficients affects behavior.</paragraph><paragraph>End your first reply by asking me one short question to check my high-level understanding.</paragraph><paragraph>After I respond, start with the first item in the roadmap (semi-orthogonal / spectral-norm view). Follow the roadmap in order.</paragraph><paragraph>Step 2: For each part of the story</paragraph><paragraph>When we are on a given part of the roadmap, teach it in four layers, but keep each message compact.</paragraph><paragraph>1. Intuition tied to Muon</paragraph><paragraph> - In 1\u20132 short paragraphs, explain the idea in plain language and explicitly say how it supports Muon\u2019s overall goal:</paragraph><paragraph> - Approximating a well-conditioned, semi-orthogonal update direction.</paragraph><paragraph> - Making steps more uniform across directions instead of being dominated by the largest singular value.</paragraph><paragraph> - Doing this without paying the full SVD cost.</paragraph><paragraph>2. Key formal setup</paragraph><paragraph> - Introduce only the minimal notation you need, such as:</paragraph><paragraph> - A weight matrix and its gradient.</paragraph><paragraph> - The idea that you can factor a matrix into singular vectors and singular values.</paragraph><paragraph> - A momentum-like buffer that accumulates gradient information over time.</paragraph><paragraph> - A simple iteration that applies a polynomial in the matrix (for example, a combination of a linear term and a cubic term) to reshape singular values.</paragraph><paragraph> - Describe what each object means verbally (for example, \u201cthis buffer stores a smoothed version of recent gradients\u201d, \u201cthis transformation tries to make all singular values close to 1 so the matrix behaves like a semi-orthogonal matrix\u201d).</paragraph><paragraph> - You may show a very small symbolic expression or a short iteration rule, but keep it high-level and do not dump long formulas.</paragraph><paragraph>3. Tiny numerical or conceptual example</paragraph><paragraph> - Give one very small example to illustrate the effect:</paragraph><paragraph> - For instance, a 2\u00d72 matrix whose singular values are very different, and describe qualitatively what happens when you apply one or two polynomial iterations to it (the large singular value goes down, the small one goes up, etc.).</paragraph><paragraph> - Or a toy picture of \u201cbefore\u201d vs. \u201cafter\u201d for singular values, and how that makes the update more uniform across directions.</paragraph><paragraph> - Keep this example to a few lines.</paragraph><paragraph>4. Quick understanding check</paragraph><paragraph> - Ask me 1\u20132 short questions related to this part:</paragraph><paragraph> - At least one conceptual \u201cwhy\u201d question (for example: \u201cWhy is it good if all singular values are close to 1?\u201d or \u201cWhy do we want a polynomial that keeps singular vectors but changes singular values?\u201d).</paragraph><paragraph> - Optionally, a tiny computational or thought experiment question (for example: \u201cWhat might go wrong if the singular values start out too large before applying the iteration?\u201d).</paragraph><paragraph> - Then stop and wait for my answer.</paragraph><paragraph>After, briefly remind me what the next stop in the Muon story is, and then proceed.</paragraph><paragraph>Step 3: Muon-centered wrap-up</paragraph><paragraph>When I say I\u2019m ready to review or summarize:</paragraph><paragraph>- Give a compact summary that keeps Muon at the center, explaining how:</paragraph><paragraph> - Starting from an \u201coptimizer recipe\u201d with a chosen norm leads to preferring semi-orthogonal directions (like U V\u1d40).</paragraph><paragraph> - Shampoo (and its non-accumulating variant) motivates this direction but is too expensive if done with an exact SVD.</paragraph><paragraph> - Newton\u2013Schulz\u2013style polynomial iterations let us approximate \u201creplace all singular values by 1\u201d using only matrix multiplications.</paragraph><paragraph> - Normalizing by something like the Frobenius norm ensures the singular values start in a safe range so the iterations stay stable.</paragraph><paragraph> - Muon wraps this into a practical algorithm: maintain a momentum buffer, apply a few iterations of a chosen polynomial to orthogonalize it, then use that to update the weights.</paragraph><paragraph>Throughout the conversation, keep answers reasonably short, interactive, and always connected back to the central Muon story, so that the whole interaction feels like a focused tutoring session.</paragraph><paragraph><bold>Link to conversation:</bold></paragraph><paragraph><link href=\"https://chatgpt.com/share/6933ad42-7bf4-8007-b500-f1bc59851bee\"><underline>https://chatgpt.com/share/6933ad42-7bf4-8007-b500-f1bc59851bee</underline></link></paragraph><paragraph><bold>Assessment</bold></paragraph><paragraph><bold>The good:</bold></paragraph><paragraph>Conceptually, the model\u2019s output is very close to the actual lecture storyline. It correctly emphasized:</paragraph><list style=\"unordered\"><list-item><paragraph>The role of (UV^\\top) as the \u201csemi-orthogonal ideal\u201d direction.<break/><break/></paragraph></list-item><list-item><paragraph>The connection between Shampoo and this ideal (a hypothetical \u201cShampoo without accumulation\u201d step).<break/><break/></paragraph></list-item><list-item><paragraph>The idea that Newton\u2013Schulz\u2013like odd polynomials act as scalar functions on singular values, while preserving the singular vectors.<break/><break/></paragraph></list-item><list-item><paragraph>The importance of normalizing the buffer so the polynomial iteration doesn\u2019t blow up or collapse.</paragraph></list-item></list><paragraph>I didn\u2019t spot any major conceptual contradictions with the lecture. The main approximations were in the toy numerical examples (e.g., saying a singular value might move from 0.01 to \u201csomething like 0.2-0.5\u201d after one step without actually computing it). That\u2019s not \u201cwrong\u201d in a dangerous way, but it\u2019s clearly hand-wavy.</paragraph><paragraph><bold>The bad:</bold></paragraph><list style=\"ordered\"><list-item><paragraph>Repetitive concept checks.<break/>It kept asking essentially the same question in slightly different forms: \u201cWhy is it good if singular values are close to 1?\u201d \u201cWhy do we want a well-conditioned update?\u201d \u201cWhy is it useful that singular vectors stay the same?\u201d<break/>These questions are reasonable the first time; by the third time, it felt redundant and didn\u2019t add new understanding.<break/><break/></paragraph></list-item><list-item><paragraph>Hand-wavy numbers in examples.<break/>In a few places it invented specific numeric values like \u201cafter one step this might be around 0.2-0.5\u201d without actually computing the polynomial. This is minor, but it\u2019s a reminder: whenever it gives specific numbers in these toy examples, treat them as qualitative (direction of change) rather than precise quantitative claims.<break/><break/></paragraph></list-item><list-item><paragraph>Behavior when the user can\u2019t really answer its questions.<break/>The prompt forces it to ask me a \u201cconcept check\u201d every turn, but it doesn\u2019t actually check my answers or detect if I\u2019m confused. In our interaction it basically replied \u201cExactly\u201d to everything and then continued. So if a student can\u2019t answer, or answers something half-baked, the model will not push back on its own. The student has to explicitly tell the model to explain. It\u2019s too \u201cpolite\u201dto be a real grader for students.<break/><break/></paragraph></list-item></list><paragraph>Overall, though, the interaction did not hallucinate big structural facts. The misbehavior was more minor.</paragraph><paragraph><bold>How this could help with learning</bold></paragraph><paragraph>In practice, this tool works well as a pre- or post-lecture companion:</paragraph><list style=\"unordered\"><list-item><paragraph>Before the lecture, we can run through Part 1-3 to get the high-level picture of \u201cwhy semi-orthogonal,\u201d \u201chow Shampoo motivates the ideal,\u201d and \u201cwhy approximate is enough,\u201d so we\u2019re not seeing those ideas for the first time in class.</paragraph></list-item><list-item><paragraph>After the lecture, we can use Parts 4-7 to re-derive the Newton\u2013Schulz connection, the odd polynomial behavior, the normalization step, and the final Muon update rule, while the model keeps quizzing us.</paragraph></list-item></list><paragraph>It can function as a self-paced interactive reading that is much closer to our actual lecture content than a random internet explanation. The main caveats are:</paragraph><list style=\"unordered\"><list-item><paragraph>Be aware it may ask very similar concept checks multiple times.</paragraph></list-item><list-item><paragraph>Treat any made-up numbers in the tiny examples as qualitative intuition, not exact math.</paragraph></list-item><list-item><paragraph>Remember it doesn\u2019t really grade your answers. If you\u2019re lost, you need to tell it explicitly to slow down or re-explain.</paragraph></list-item><list-item><paragraph>Use it to structure understanding, but verify key equations and details against the lecture itself if you care about full rigor.<break/><break/></paragraph></list-item></list></document>",
            "links": [
                "https://chatgpt.com/share/6933ad42-7bf4-8007-b500-f1bc59851bee"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:27:48.554248+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427549,
            "author": "Reyna Liu",
            "project_title": "Special Participation E: \u00b5P",
            "post_body": "I designed a prompt that turns ChatGPT\u2019s Study Mode into an interactive tutor for the maximal update parameterization (\u00b5P) lecture.\n\nThe goal is to replace a passive pre-lecture/post-lecture reading with an active dialogue that walks through the \u00b5P story in order:\n\nwhy \u00b5P was introduced,\n\n\n\nhow norms and RMS\u2013RMS\u2013style measures are used to control update size,\n\n\n\nhow the outer-product structure of linear-layer gradients simplifies the math,\n\n\n\nhow sign-based updates and width scaling affect update norms,\n\n\n\nhow CLT-style scaling motivates the parameterization,\n\n\n\nand how all of this connects to feature learning vs. lazy training.\n\n\n\nThe prompt enforces short, interactive replies and forces the model to keep \u00b5P as the central theme, instead of dumping a long wall of text.\n\nPrompt\n\nYou are my tutor for maximal update parameterization (\u00b5P) and the optimization ideas around it in modern deep learning.\n\nI want this to be an interactive tutoring session. In every reply:\n\n- Keep your answer focused and reasonably short (roughly 3\u20136 short paragraphs or equivalent).\n\n- Do not move on to the next topic until I say so.\n\n- Always end your message by asking me a specific question (for example: \u201cDoes this make sense?\u201d or a small concept-check question), and then wait for my reply.\n\nThe overall story I want to learn is:\n\nHow \u00b5P is designed so that neural networks of different widths have comparable update behavior, and how all the surrounding concepts (norms, the simple outer-product structure of gradients in linear layers, sign-based updates, learning-rate scaling, CLT-style scaling analogy, and feature-learning conditions) fit into this one story.\n\nPlease organize the material around \u00b5P as one coherent narrative, rather than separate tricks.\n\nStep 1: Very short big-picture intro\n\nFirst response:\n\n- In at most 2 short paragraphs, explain:\n\n - What problem \u00b5P is trying to solve (e.g., transferring learning rates from small to large models, making training behavior less sensitive to width).\n\n - The core idea in one or two sentences (e.g., keeping effective update magnitudes comparable across widths).\n\n- Then give a brief roadmap (4\u20137 bullet points) of the main pieces of the story, all explicitly framed as supporting \u00b5P, for example:\n\n - Controlling update size via norms.\n\n - RMS\u2013RMS-type norms to measure update size across layers.\n\n - Using the outer-product (rank-1 for batch size 1) structure of linear-layer gradients as a simple tool to analyze norms and update sizes.\n\n - How sign-based updates interact with these norms.\n\n - Learning-rate scaling with width (fan-in / fan-out) and how \u00b5P encodes this.\n\n - CLT-style scaling intuition for width limits.\n\n - Feature-learning conditions in wide networks and how \u00b5P targets them.\n\n- End your first reply by asking me one short question to check my high-level understanding. In your next reply (after I respond), start with the first item in the roadmap (controlling update size via norms). Do not ask me to choose the order; follow the roadmap in order.\n\nStep 2: For each part of the story\n\nWhen we are on a given part of the roadmap (for example \u201cRMS\u2013RMS norms and update size\u201d, \u201couter-product gradient structure as a tool\u201d, \u201clearning-rate scaling with width\u201d, or \u201cfeature-learning conditions\u201d), teach it in four layers, but keep each message compact:\n\n1. Intuition tied to \u00b5P\n\n - In 1\u20132 short paragraphs, explain the idea in plain language and explicitly say how it supports the overall \u00b5P goal (making updates comparable across widths / keeping training behavior width-stable / enabling feature learning at large width).\n\n2. Key formal setup and equations\n\n - Introduce the simplest useful mathematical setup: weight matrix W, input x, gradient \u2207_W L, layer width, fan-in, fan-out, etc.\n\n - State the most relevant equations, such as:\n\n - The gradient of a linear layer for a single example having an outer-product form g x^T (rank-1), used mainly as a convenient way to analyze its norms and scaling.\n\n - A reasonable way to measure update size (e.g., some RMS-style norm).\n\n - A simple example of how learning rate might scale with width or fan-in/fan-out to keep that update size roughly constant.\n\n - When appropriate, express conditions like \u201cactivations h_\\ell stay O(1)\u201d and \u201cupdates \\Delta h_\\ell stay O(1) as width grows\u201d, and link that to constraints on \\|W_\\ell\\| and \\|\\Delta W_\\ell\\|.\n\n - Explain the meaning of each symbol in words.\n\n3. Tiny numerical example\n\n - Give one small concrete example with simple numbers (e.g., a 2\u00d73 layer, or doubling the width) to show what changes and how a scaling rule helps keep updates comparable.\n\n - Keep this to a few lines.\n\n4. Quick understanding check\n\n - Ask me 1\u20132 short questions related to this part:\n\n - At least one conceptual (\u201cwhy\u201d) question.\n\n - Optionally one small calculation (e.g., rank of a gradient matrix, or what happens to update size when width doubles without scaling the learning rate).\n\n - Then stop and wait for my answer, instead of answering your own questions.\n\nWhen I indicate that I am done with the current part and ready to move on, proceed to the next item in the roadmap in order, briefly reminding me which part comes next in the \u00b5P story.\n\nStep 3: \u00b5P-centered wrap-up\n\nWhen I say I\u2019m ready to review or summarize:\n\n- Give a compact summary that keeps \u00b5P at the center, explaining how:\n\n - Norms and RMS\u2013RMS-like measures define what it means for updates to be \u201cthe same size\u201d across widths.\n\n - The simple outer-product structure of gradients makes it easy to relate different norms and see how update size scales with width.\n\n - \u00b5P encodes scaling rules so that small and large models see comparable effective updates.\n\n - Feature-learning conditions on h_\\ell and \\Delta h_\\ell motivate \u00b5P\u2019s design choices.\n\n- Propose a small set of practice questions (both conceptual and short derivations) that someone could solve to check their understanding of \u00b5P and the surrounding ideas.\n\n- Then stop and wait for my answer, instead of answering your own questions.\n\nThroughout the conversation, keep answers reasonably short, interactive, and always connected back to the central \u00b5P story, so that the whole interaction feels like a focused tutoring session.\n\nLink to Conversation\n\nhttps://chatgpt.com/share/6933311c-d9fc-8007-bfb6-35c5f7ee8a1a\n\nInteraction\n\nMotivation for \u00b5P\n\nThe answer matches the lecture well. The lecture explicitly motivates \u00b5P as a way to choose the \u201cright units\u201d and scaling so that the same hyperparameters work at different widths, and we can search on small models and transfer to large ones. The AI also adds a nice refinement: hyperparameters transfer because effective updates have comparable size. That\u2019s clearly aligned with the RMS\u2013RMS induced norm idea from class.\n\nFrobenius norm\n\nThe high-level contrast is good: Frobenius norm grows with number of parameters, while RMS is more \u201cwidth-invariant\u201d. The lecture also emphasizes using an RMS\u2013RMS induced norm instead of a raw spectral/Frobenius norm so we can have a single scalar that behaves well across layers and widths.\n\nBut the numerical language is sloppy: \u201cdouble the width and the Frobenius norm doubles\u201d is not generally true. In the typical i.i.d. setup, the Frobenius norm grows like (number of parameters)^(0.5), not linearly. \n\nAlso, compared to the lecture, the AI never explicitly mentions that we\u2019re using RMS-RMS as an induced norm that upper-bounds the spectral norm and gives layer-specific effective step sizes. That\u2019s an important part of the lecture that\u2019s missing here.\n\nEffective update size\n\nThis is basically what the lecture is implicitly using when it talks about RMS\u2013RMS induced norms. The lecture goes through induced norms / sub-multiplicativity, whereas the AI jumps straight to this \u201cproduct of RMS\u201d heuristic.\n\nHowever, it never connects back to the more formal induced-norm inequality we actually saw in class. The formal induced-norm framing is missing.\n\nOuter-product / low-rank gradient structure\n\nThis is the key observation: the RMS gradient does not depend on width at all. The outer-product formula is exactly what was derived in the lecture.\n\nThe algebra for Frobenius and RMS norms is basically correct, but it is not exactly the argument in the lecture. In class, the rank-1 structure is mainly used to say rank-1 \u21d2 spectral norm = Frobenius norm, which makes it easy to relate the sign matrix S and its norms, and to design a step size for signSGD/Adam that keeps the RMS-RMS fixed.\n\nThe AI instead focuses on RMS(\u2207W) being width-independent, which is a reasonable extrapolation but not literally what is emphasized in the lecture.\n\nSign-based updates\n\nThe AI is vaguely aligned with the idea of signSGD, but it stays very high-level, and doesn\u2019t connect sign-based updates back to the RMS\u2013RMS induced norm recipe. Not hallucinating, but also not actually teaching the signSGD part of the lecture.\n\nLearning-rate scaling with width\n\nIt\u2019s qualitatively right that learning-rate must depend on fan-in, but the \u20181/d_in\u2019 rule is presented as a universal principle, which is not justified from the lecture notes or the original \u00b5P paper.\n\nCLT analogy\n\nThis is very close to what happens in the lecture. It explicitly recalls the CLT, shows how a scaling factor keeps the limit non-trivial, and then asks if we can do something similar for neural nets / hyperparameters. The derivation is correct and is a nice concrete restatement of Xavier-style reasoning.\n\nFeature-learning\n\nThis matches the spirit of the \u201cconditions for feature learning\u201d notes in the lecture. The AI brings in NTK/lazy-training terminology, which is consistent with the broader literature and explains why those O(1) conditions matter.\n\nOverall story\n\nThis is a nice high-level summary, and aligns with several concrete lines in the notes:\n\nHyperparameter transfer from small to large models.\n\nChoosing the right \u201cunits\u201d / parameterization to make that possible.\n\nConditions for feature learning expressed as O(1) constraints.\n\n\n\nOverall assessment\n\nWhat it does well:\n\nKeeps the \u00b5P story coherent.\n\nRepeats several key ideas: hyperparameter transfer, O(1) activations/updates, width-stable dynamics.\n\nThe outer-product derivation of RMS(\u2207W) are correct and and the CLT variance argument is helpful for intuition.\n\nWhat it doesn\u2019t do well:\n\nQuantitative statements about norm scaling (e.g., \u201cFrobenius norm doubles\u201d) are sloppy and should be re-derived.\n\nThe treatment of learning-rate scaling is oversimplified. It presents \u201clearning rate \u221d 1/d_in\u201d as a universal \u00b5P rule without actually deriving it.\n\n\n\n\nHow I would use it:\n\nI\u2019d use this AI tutor as a conceptual organizer. If I forget the big picture of why \u00b5P exists or how CLT and O(1) conditions fit together, this is a good interactive refresher.\n\n\n\nI would not rely on it for precise hyperparameter scaling formulas or optimizer details; those need to be checked against the actual lecture notes and/or the Yang 2022 \u00b5P paper.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I designed a prompt that turns ChatGPT\u2019s Study Mode into an interactive tutor for the maximal update parameterization (\u00b5P) lecture.</paragraph><paragraph>The goal is to replace a passive pre-lecture/post-lecture reading with an active dialogue that walks through the \u00b5P story in order:</paragraph><list style=\"unordered\"><list-item><paragraph>why \u00b5P was introduced,<break/><break/></paragraph></list-item><list-item><paragraph>how norms and RMS\u2013RMS\u2013style measures are used to control update size,<break/><break/></paragraph></list-item><list-item><paragraph>how the outer-product structure of linear-layer gradients simplifies the math,<break/><break/></paragraph></list-item><list-item><paragraph>how sign-based updates and width scaling affect update norms,<break/><break/></paragraph></list-item><list-item><paragraph>how CLT-style scaling motivates the parameterization,<break/><break/></paragraph></list-item><list-item><paragraph>and how all of this connects to feature learning vs. lazy training.<break/><break/></paragraph></list-item></list><paragraph>The prompt enforces short, interactive replies and forces the model to keep \u00b5P as the central theme, instead of dumping a long wall of text.</paragraph><paragraph><bold>Prompt</bold></paragraph><paragraph>You are my tutor for maximal update parameterization (\u00b5P) and the optimization ideas around it in modern deep learning.</paragraph><paragraph>I want this to be an interactive tutoring session. In every reply:</paragraph><paragraph>- Keep your answer focused and reasonably short (roughly 3\u20136 short paragraphs or equivalent).</paragraph><paragraph>- Do not move on to the next topic until I say so.</paragraph><paragraph>- Always end your message by asking me a specific question (for example: \u201cDoes this make sense?\u201d or a small concept-check question), and then wait for my reply.</paragraph><paragraph>The overall story I want to learn is:</paragraph><paragraph>How \u00b5P is designed so that neural networks of different widths have comparable update behavior, and how all the surrounding concepts (norms, the simple outer-product structure of gradients in linear layers, sign-based updates, learning-rate scaling, CLT-style scaling analogy, and feature-learning conditions) fit into this one story.</paragraph><paragraph>Please organize the material around \u00b5P as one coherent narrative, rather than separate tricks.</paragraph><paragraph>Step 1: Very short big-picture intro</paragraph><paragraph>First response:</paragraph><paragraph>- In at most 2 short paragraphs, explain:</paragraph><paragraph> - What problem \u00b5P is trying to solve (e.g., transferring learning rates from small to large models, making training behavior less sensitive to width).</paragraph><paragraph> - The core idea in one or two sentences (e.g., keeping effective update magnitudes comparable across widths).</paragraph><paragraph>- Then give a brief roadmap (4\u20137 bullet points) of the main pieces of the story, all explicitly framed as supporting \u00b5P, for example:</paragraph><paragraph> - Controlling update size via norms.</paragraph><paragraph> - RMS\u2013RMS-type norms to measure update size across layers.</paragraph><paragraph> - Using the outer-product (rank-1 for batch size 1) structure of linear-layer gradients as a simple tool to analyze norms and update sizes.</paragraph><paragraph> - How sign-based updates interact with these norms.</paragraph><paragraph> - Learning-rate scaling with width (fan-in / fan-out) and how \u00b5P encodes this.</paragraph><paragraph> - CLT-style scaling intuition for width limits.</paragraph><paragraph> - Feature-learning conditions in wide networks and how \u00b5P targets them.</paragraph><paragraph>- End your first reply by asking me one short question to check my high-level understanding. In your next reply (after I respond), start with the first item in the roadmap (controlling update size via norms). Do not ask me to choose the order; follow the roadmap in order.</paragraph><paragraph>Step 2: For each part of the story</paragraph><paragraph>When we are on a given part of the roadmap (for example \u201cRMS\u2013RMS norms and update size\u201d, \u201couter-product gradient structure as a tool\u201d, \u201clearning-rate scaling with width\u201d, or \u201cfeature-learning conditions\u201d), teach it in four layers, but keep each message compact:</paragraph><paragraph>1. Intuition tied to \u00b5P</paragraph><paragraph> - In 1\u20132 short paragraphs, explain the idea in plain language and explicitly say how it supports the overall \u00b5P goal (making updates comparable across widths / keeping training behavior width-stable / enabling feature learning at large width).</paragraph><paragraph>2. Key formal setup and equations</paragraph><paragraph> - Introduce the simplest useful mathematical setup: weight matrix W, input x, gradient \u2207_W L, layer width, fan-in, fan-out, etc.</paragraph><paragraph> - State the most relevant equations, such as:</paragraph><paragraph> - The gradient of a linear layer for a single example having an outer-product form g x^T (rank-1), used mainly as a convenient way to analyze its norms and scaling.</paragraph><paragraph> - A reasonable way to measure update size (e.g., some RMS-style norm).</paragraph><paragraph> - A simple example of how learning rate might scale with width or fan-in/fan-out to keep that update size roughly constant.</paragraph><paragraph> - When appropriate, express conditions like \u201cactivations h_\\ell stay O(1)\u201d and \u201cupdates \\Delta h_\\ell stay O(1) as width grows\u201d, and link that to constraints on \\|W_\\ell\\| and \\|\\Delta W_\\ell\\|.</paragraph><paragraph> - Explain the meaning of each symbol in words.</paragraph><paragraph>3. Tiny numerical example</paragraph><paragraph> - Give one small concrete example with simple numbers (e.g., a 2\u00d73 layer, or doubling the width) to show what changes and how a scaling rule helps keep updates comparable.</paragraph><paragraph> - Keep this to a few lines.</paragraph><paragraph>4. Quick understanding check</paragraph><paragraph> - Ask me 1\u20132 short questions related to this part:</paragraph><paragraph> - At least one conceptual (\u201cwhy\u201d) question.</paragraph><paragraph> - Optionally one small calculation (e.g., rank of a gradient matrix, or what happens to update size when width doubles without scaling the learning rate).</paragraph><paragraph> - Then stop and wait for my answer, instead of answering your own questions.</paragraph><paragraph>When I indicate that I am done with the current part and ready to move on, proceed to the next item in the roadmap in order, briefly reminding me which part comes next in the \u00b5P story.</paragraph><paragraph>Step 3: \u00b5P-centered wrap-up</paragraph><paragraph>When I say I\u2019m ready to review or summarize:</paragraph><paragraph>- Give a compact summary that keeps \u00b5P at the center, explaining how:</paragraph><paragraph> - Norms and RMS\u2013RMS-like measures define what it means for updates to be \u201cthe same size\u201d across widths.</paragraph><paragraph> - The simple outer-product structure of gradients makes it easy to relate different norms and see how update size scales with width.</paragraph><paragraph> - \u00b5P encodes scaling rules so that small and large models see comparable effective updates.</paragraph><paragraph> - Feature-learning conditions on h_\\ell and \\Delta h_\\ell motivate \u00b5P\u2019s design choices.</paragraph><paragraph>- Propose a small set of practice questions (both conceptual and short derivations) that someone could solve to check their understanding of \u00b5P and the surrounding ideas.</paragraph><paragraph>- Then stop and wait for my answer, instead of answering your own questions.</paragraph><paragraph>Throughout the conversation, keep answers reasonably short, interactive, and always connected back to the central \u00b5P story, so that the whole interaction feels like a focused tutoring session.</paragraph><paragraph><bold>Link to Conversation</bold></paragraph><paragraph><link href=\"https://chatgpt.com/share/6933311c-d9fc-8007-bfb6-35c5f7ee8a1a\"><underline>https://chatgpt.com/share/6933311c-d9fc-8007-bfb6-35c5f7ee8a1a</underline></link></paragraph><paragraph><bold>Interaction</bold></paragraph><list style=\"ordered\"><list-item><paragraph>Motivation for \u00b5P</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>The answer matches the lecture well. The lecture explicitly motivates \u00b5P as a way to choose the \u201cright units\u201d and scaling so that the same hyperparameters work at different widths, and we can search on small models and transfer to large ones. The AI also adds a nice refinement: hyperparameters transfer because effective updates have comparable size. That\u2019s clearly aligned with the RMS\u2013RMS induced norm idea from class.</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Frobenius norm</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>The high-level contrast is good: Frobenius norm grows with number of parameters, while RMS is more \u201cwidth-invariant\u201d. The lecture also emphasizes using an RMS\u2013RMS induced norm instead of a raw spectral/Frobenius norm so we can have a single scalar that behaves well across layers and widths.</paragraph></list-item><list-item><paragraph>But the numerical language is sloppy: \u201cdouble the width and the Frobenius norm doubles\u201d is not generally true. In the typical i.i.d. setup, the Frobenius norm grows like (number of parameters)^(0.5), not linearly. </paragraph></list-item><list-item><paragraph>Also, compared to the lecture, the AI never explicitly mentions that we\u2019re using RMS-RMS as an induced norm that upper-bounds the spectral norm and gives layer-specific effective step sizes. That\u2019s an important part of the lecture that\u2019s missing here.</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Effective update size</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>This is basically what the lecture is implicitly using when it talks about RMS\u2013RMS induced norms. The lecture goes through induced norms / sub-multiplicativity, whereas the AI jumps straight to this \u201cproduct of RMS\u201d heuristic.</paragraph></list-item><list-item><paragraph>However, it never connects back to the more formal induced-norm inequality we actually saw in class. The formal induced-norm framing is missing.</paragraph></list-item></list><list style=\"ordered\"><list-item><heading level=\"3\">Outer-product / low-rank gradient structure</heading></list-item></list><list style=\"unordered\"><list-item><paragraph>This is the key observation: the RMS gradient does not depend on width at all. The outer-product formula is exactly what was derived in the lecture.</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>The algebra for Frobenius and RMS norms is basically correct, but it is not exactly the argument in the lecture. In class, the rank-1 structure is mainly used to say rank-1 \u21d2 spectral norm = Frobenius norm, which makes it easy to relate the sign matrix S and its norms, and to design a step size for signSGD/Adam that keeps the RMS-RMS fixed.</paragraph></list-item><list-item><paragraph>The AI instead focuses on RMS(\u2207W) being width-independent, which is a reasonable extrapolation but not literally what is emphasized in the lecture.</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Sign-based updates</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>The AI is vaguely aligned with the idea of signSGD, but it stays very high-level, and doesn\u2019t connect sign-based updates back to the RMS\u2013RMS induced norm recipe. Not hallucinating, but also not actually teaching the signSGD part of the lecture.</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Learning-rate scaling with width</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>It\u2019s qualitatively right that learning-rate must depend on fan-in, but the \u20181/d_in\u2019 rule is presented as a universal principle, which is not justified from the lecture notes or the original \u00b5P paper.</paragraph></list-item></list><list style=\"ordered\"><list-item><heading level=\"3\">CLT analogy</heading></list-item></list><list style=\"unordered\"><list-item><paragraph>This is very close to what happens in the lecture. It explicitly recalls the CLT, shows how a scaling factor keeps the limit non-trivial, and then asks if we can do something similar for neural nets / hyperparameters. The derivation is correct and is a nice concrete restatement of Xavier-style reasoning.</paragraph></list-item></list><list style=\"ordered\"><list-item><heading level=\"3\">Feature-learning</heading></list-item></list><list style=\"unordered\"><list-item><paragraph>This matches the spirit of the \u201cconditions for feature learning\u201d notes in the lecture. The AI brings in NTK/lazy-training terminology, which is consistent with the broader literature and explains why those O(1) conditions matter.</paragraph></list-item></list><list style=\"ordered\"><list-item><heading level=\"3\">Overall story</heading></list-item></list><list style=\"unordered\"><list-item><paragraph>This is a nice high-level summary, and aligns with several concrete lines in the notes:</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Hyperparameter transfer from small to large models.</paragraph></list-item><list-item><paragraph>Choosing the right \u201cunits\u201d / parameterization to make that possible.</paragraph></list-item><list-item><paragraph>Conditions for feature learning expressed as O(1) constraints.<break/><break/></paragraph></list-item></list><heading level=\"2\"><bold>Overall assessment</bold></heading><paragraph>What it does well:</paragraph><list style=\"unordered\"><list-item><paragraph>Keeps the \u00b5P story coherent.</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>Repeats several key ideas: hyperparameter transfer, O(1) activations/updates, width-stable dynamics.</paragraph></list-item><list-item><paragraph>The outer-product derivation of RMS(\u2207W) are correct and and the CLT variance argument is helpful for intuition.</paragraph></list-item></list><paragraph>What it doesn\u2019t do well:</paragraph><list style=\"unordered\"><list-item><paragraph>Quantitative statements about norm scaling (e.g., \u201cFrobenius norm doubles\u201d) are sloppy and should be re-derived.</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>The treatment of learning-rate scaling is oversimplified. It presents \u201clearning rate \u221d 1/d_in\u201d as a universal \u00b5P rule without actually deriving it.<break/><break/><break/></paragraph></list-item></list><paragraph><bold>How I would use it:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>I\u2019d use this AI tutor as a conceptual organizer. If I forget the big picture of why \u00b5P exists or how CLT and O(1) conditions fit together, this is a good interactive refresher.<break/><break/></paragraph></list-item><list-item><paragraph>I would not rely on it for precise hyperparameter scaling formulas or optimizer details; those need to be checked against the actual lecture notes and/or the Yang 2022 \u00b5P paper.<break/><break/></paragraph></list-item></list></document>",
            "links": [
                "https://chatgpt.com/share/6933311c-d9fc-8007-bfb6-35c5f7ee8a1a"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:27:00.068242+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427544,
            "author": "Reyna Liu",
            "project_title": "Special Participation B: Claude on hw9",
            "post_body": "Link to conversation:\n\nhttps://claude.ai/share/00a345bb-cd9c-41c1-879c-09aeda6818f9\n\nAlthough the special participation B is about coding, the particular notebook I worked with (Visualizing_BERT.ipynb) was essentially non-coding. All the interesting work was in interpreting attention visualizations and using those interpretations to feed an LLM, not in writing code.\n\nStrategy \n\nI started naively by asking the model to \u201cread\u201d one of the BertViz attention diagrams directly from an image. This immediately exposed a major issue. The model\u2019s vision was extremely unreliable. It confidently described patterns that did not match what I saw in the notebook. For example, it claimed that the word \u201cdog\u201d had the strongest attention to \u201cdog\u201d on the right, while the actual strongest attention was to \u201cThe\u201d. In other words, it was hallucinating a plausible attention map rather than faithfully reporting what was in the picture. Which is understandable, since LLMs\u2019 ability to interpret images is not that good. After this first attempt, I explicitly told it to forget its own image reading and decided I could not treat its visual perception as trustworthy.\n\nFrom that point on, I switched to a different interaction pattern:\n\nI carefully read each visualization myself and converted it into a precise textual description: tokens, layers, heads, what the patterns were, etc.\n\nThe LLM acted as the reasoning engine. Given my textual description, it answered the questions about GPT-2 and BERT attention.\n\nThis setup had an obvious flaw. How well the model did hinged on my ability to accurately describe the visualization. When I forgot to describe the stronger connection between the same words in cell 8, obviously the model could not include that in its answers, since the information was not given. \n\nThere was also a practical bottleneck. Describing all available information was too difficult. There are multiple layers and multiple heads to choose from, plus expanded views of queries and keys. To keep the interaction manageable, I deliberately ignored the vector views and focused only on the attention lines, since most of the problems could be answered from the line patterns alone.\n\nFor the final sub-questions in parts (c) and (d), I made a deliberate choice not to describe the visualizations at all. Those figures would have been extremely cumbersome to encode in text, and the questions themselves were fundamentally conceptual (e.g., \u201cwhat tokens would you expect strong attention between?\u201d). In that setting, I felt that an inaccurate human description might do more harm than good. Instead, I only gave the model the textual setup from the notebook and asked it to reason from its general understanding of attention, rather than from any specific picture.\n\nEvaluation \n\nOnce the model had a clean textual description, its answers were often rich and well-structured. However, I\u2019m not sure how much of this good result should be attributed to very specific descriptions of visualizations. \n\nIn this setting, the main issue was mis-emphasis. For example, in the GPT-2 part, it initially tried to attribute deep linguistic significance to the word \u201cThe\u201d, and I had to push it to reinterpret that as a computational significance of being in position 0, rather than the determiner\u2019s semantics.\n\nThe behavior changed slightly on the last two questions, where I did not describe any visualization at all and simply gave it the textual setup. There, the model leaned purely on its prior knowledge of how attention should behave. It immediately highlighted words that have similar meanings, pairs like \u201chappy\u201d vs. \u201csad\u201d and \u201cI\u201d vs. \u201cI\u201d, and so on, and gave a pretty plausible story about which attention weights would receive large gradients. But it did not incorporate patterns in previous answers in its answer without a direct prompt, such as the special tokens ([CLS] and [SEP]). In other words, when the question was conceptual, its answers were incomplete but not incorrect, even without any grounding in a specific figure. Of course, in these cases, there is no guarantee that its answers match the actual visualization in the notebook. It\u2019s giving a good generic answer, not a picture-specific one.\n\nSynthesis\n\nOverall, this interaction ended up looking less like \u201ccode co-pilot\u201d and more like a division of labor between human perception and LLM reasoning. The LLM was very good at interpreting structured textual summaries of attention patterns, and very bad at extracting those patterns directly from images, which is exactly what you would expect, given how much more mature its language abilities are than its vision. If you let it stare at the picture by itself, it will hallucinate a plausible story. If you constrain it to human descriptions, it becomes a pretty decent analysis assistant that can map those descriptions onto concepts. And for high-level conceptual questions, it can sometimes ignore the visualization entirely and rely on its internal model of how attention should behave, as long as you are comfortable with the fact that those answers are not grounded in the specific figure.\n\nIn that sense, this \u201cnon-coding coding homework\u201d shows that current LLMs are much more reliable as text-based reasoning engines, and that effective human-model cooperation is very important.",
            "content_xml": "<document version=\"2.0\"><paragraph>Link to conversation:</paragraph><paragraph><link href=\"https://claude.ai/share/00a345bb-cd9c-41c1-879c-09aeda6818f9\"><underline>https://claude.ai/share/00a345bb-cd9c-41c1-879c-09aeda6818f9</underline></link></paragraph><paragraph>Although the special participation B is about coding, the particular notebook I worked with (Visualizing_BERT.ipynb) was essentially non-coding. All the interesting work was in interpreting attention visualizations and using those interpretations to feed an LLM, not in writing code.</paragraph><paragraph><bold>Strategy</bold> </paragraph><paragraph>I started naively by asking the model to \u201cread\u201d one of the BertViz attention diagrams directly from an image. This immediately exposed a major issue. The model\u2019s vision was extremely unreliable. It confidently described patterns that did not match what I saw in the notebook. For example, it claimed that the word \u201cdog\u201d had the strongest attention to \u201cdog\u201d on the right, while the actual strongest attention was to \u201cThe\u201d. In other words, it was hallucinating a plausible attention map rather than faithfully reporting what was in the picture. Which is understandable, since LLMs\u2019 ability to interpret images is not that good. After this first attempt, I explicitly told it to forget its own image reading and decided I could not treat its visual perception as trustworthy.</paragraph><paragraph>From that point on, I switched to a different interaction pattern:</paragraph><list style=\"unordered\"><list-item><paragraph>I carefully read each visualization myself and converted it into a precise textual description: tokens, layers, heads, what the patterns were, etc.</paragraph></list-item><list-item><paragraph>The LLM acted as the reasoning engine. Given my textual description, it answered the questions about GPT-2 and BERT attention.</paragraph></list-item></list><paragraph>This setup had an obvious flaw. How well the model did hinged on my ability to accurately describe the visualization. When I forgot to describe the stronger connection between the same words in cell 8, obviously the model could not include that in its answers, since the information was not given. </paragraph><paragraph>There was also a practical bottleneck. Describing all available information was too difficult. There are multiple layers and multiple heads to choose from, plus expanded views of queries and keys. To keep the interaction manageable, I deliberately <bold>ignored the vector views</bold> and focused only on the attention lines, since most of the problems could be answered from the line patterns alone.</paragraph><paragraph>For the final sub-questions in parts (c) and (d), I made a deliberate choice <bold>not</bold> to describe the visualizations at all. Those figures would have been extremely cumbersome to encode in text, and the questions themselves were fundamentally conceptual (e.g., \u201cwhat tokens would you expect strong attention between?\u201d). In that setting, I felt that an inaccurate human description might do more harm than good. Instead, I only gave the model the textual setup from the notebook and asked it to reason from its general understanding of attention, rather than from any specific picture.</paragraph><paragraph><bold>Evaluation</bold> </paragraph><paragraph>Once the model had a clean textual description, its answers were often rich and well-structured. However, I\u2019m not sure how much of this good result should be attributed to very specific descriptions of visualizations. </paragraph><paragraph>In this setting, the main issue was mis-emphasis. For example, in the GPT-2 part, it initially tried to attribute deep linguistic significance to the word \u201cThe\u201d, and I had to push it to reinterpret that as a computational significance of being in position 0, rather than the determiner\u2019s semantics.</paragraph><paragraph>The behavior changed slightly on the last two questions, where I did not describe any visualization at all and simply gave it the textual setup. There, the model leaned purely on its prior knowledge of how attention should behave. It immediately highlighted words that have similar meanings, pairs like \u201chappy\u201d vs. \u201csad\u201d and \u201cI\u201d vs. \u201cI\u201d, and so on, and gave a pretty plausible story about which attention weights would receive large gradients. But it did not incorporate patterns in previous answers in its answer without a direct prompt, such as the special tokens ([CLS] and [SEP]). In other words, when the question was conceptual, its answers were incomplete but not incorrect, even without any grounding in a specific figure. Of course, in these cases, there is no guarantee that its answers match the actual visualization in the notebook. It\u2019s giving a good generic answer, not a picture-specific one.</paragraph><paragraph><bold>Synthesis</bold></paragraph><paragraph>Overall, this interaction ended up looking less like \u201ccode co-pilot\u201d and more like a division of labor between human perception and LLM reasoning. The LLM was very good at interpreting structured textual summaries of attention patterns, and very bad at extracting those patterns directly from images, which is exactly what you would expect, given how much more mature its language abilities are than its vision. If you let it stare at the picture by itself, it will hallucinate a plausible story. If you constrain it to human descriptions, it becomes a pretty decent analysis assistant that can map those descriptions onto concepts. And for high-level conceptual questions, it can sometimes ignore the visualization entirely and rely on its internal model of how attention should behave, as long as you are comfortable with the fact that those answers are not grounded in the specific figure.</paragraph><paragraph>In that sense, this \u201cnon-coding coding homework\u201d shows that current LLMs are much more reliable as text-based reasoning engines, and that effective human-model cooperation is very important.</paragraph></document>",
            "links": [
                "https://claude.ai/share/00a345bb-cd9c-41c1-879c-09aeda6818f9"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:26:14.979546+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427535,
            "author": "Reyna Liu",
            "project_title": "Special Participation A: Qwen on hw11",
            "post_body": "Link to conversation: https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f\n\nThe model was reasonably good at one-shotting the problems, but it\u2019s still far from trustworthy. It was quite good at high-level conceptual explanations and even symbolic derivations, but it made small but fatal mathematical and numerical mistakes, and almost never flagged its own uncertainty. Getting to fully correct answers usually required some active steering.\n\nOne-shot correctness:\n About 70% of individual sub-questions were essentially correct on the first try, especially the more conceptual ones. Throughout the process, it often gave good reasoning behind its answers.\n\n\n\nFixed within one rounds of feedback:\n Another ~20% could be fixed after a single iteration where I pointed out its mistakes. After the incorrect portion was pointed out, the model did a good job refocusing on that specific portion and fixing the issue, whether it was a mathematical error or a conceptual one. However this required the user to know the ground truth answer to be able to tell the model where things went wrong, which was not feasible sometimes.\n\nStrategy I used to steer the model:\n\nMy main strategy was localizing errors. When something was incorrect, I would point out a specific part where things went wrong and ask it to redo that portion or rewrite the answer with that constraint in mind. If the revised answer still contained other errors, I would then point those out as well. This iterative process usually converged to the correct solution, but only because I was checking against the official answers.\n\nWhere it did well:\n\nLinear algebra (for example the Chinchilla scaling law question was done well, with correct derivations)\n\nConceptual explanations (especially for 6.Soft-Prompting Language Models, where the model one-shot all the sub-questions with clear, coherent explanations)\n\nErrors and hallucinations:\n\nThe main issue is the model gives confident, mostly correct answers with hidden mistakes. For example, in 2(a), the model mentioned similarity with a bag-of-words model, while the problem was asking about a token-by-token model. Interestingly, the model knew the correct answer, and did include it in its output(\u201cSo, it's essentially a token-to-token mapping without any awareness of context or sequence structure\u201d). If it had simply not added the bag-of-words remark, the answer would have been fully correct. Another example is 2(b), where it talked about a matrix being block-diagonal when it was horizontally partitioned. These wrong side-comments did not derail the final conclusion, but they weakened the model\u2019s usefulness as a learning tool, because a student might not know which parts to trust.\n\nMathematically, the model made sloppy arithmetic mistakes, messing up the exponent in problem 5(h) twice, and did not fix the second mistake when the first one was pointed out. Without the user explicitly checking the numbers and forcing it to recompute, it would happily present a numerically incorrect result.\n\nIt also made overconfident statements. In 2(e), it made claims about the read subspace without appropriate qualifications. Despite this, it still managed to reach the correct high-level conclusion that \u201cthe read subspace dimension is at most d_head\u201d. When I later prompted it to tone down the overconfident statement, it also dropped the correct statement entirely. This illustrates a common LLM behavior: it is too eager to agree with the user, sometimes over-editing and discarding correct content when asked to correct itself. This makes adjusting the model\u2019s confidence level the most difficult in my opinion.\n\nIt was also occasionally under-confident. Notably in problem 1, the model said \u201cAt initialization, since A=0 and B=0, the gradients may be zero or very small\u201d, when in fact the gradient is exactly zero in this setup. So it hedged (\u201cmay be\u201d) where the correct answer was deterministic.\n\nOverall, the model can one-shot many questions or get very close, but it often introduces mistakes even in derivations that look careful. It\u2019s not reliable as an automated homework solver, and it\u2019s not even a great tutor unless the user actively engages the model, checks its work, and asks thoughtful, targeted follow-up questions to give the model a chance to fix its mistakes. Without active supervision, it\u2019s very easy to walk away with answers that are wrong.",
            "content_xml": "<document version=\"2.0\"><paragraph>Link to conversation: <link href=\"https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f\"><underline>https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f</underline></link></paragraph><paragraph>The model was reasonably good at one-shotting the problems, but it\u2019s still far from trustworthy. It was quite good at high-level conceptual explanations and even symbolic derivations, but it made small but fatal mathematical and numerical mistakes, and almost never flagged its own uncertainty. Getting to fully correct answers usually required some active steering.</paragraph><list style=\"ordered\"><list-item><paragraph>One-shot correctness:<break/> About 70% of individual sub-questions were essentially correct on the first try, especially the more conceptual ones. Throughout the process, it often gave good reasoning behind its answers.<break/><break/></paragraph></list-item><list-item><paragraph>Fixed within one rounds of feedback:<break/> Another ~20% could be fixed after a single iteration where I pointed out its mistakes. After the incorrect portion was pointed out, the model did a good job refocusing on that specific portion and fixing the issue, whether it was a mathematical error or a conceptual one. However this required the user to know the ground truth answer to be able to tell the model where things went wrong, which was not feasible sometimes.</paragraph></list-item></list><paragraph><bold>Strategy I used to steer the model:</bold></paragraph><paragraph>My main strategy was localizing errors. When something was incorrect, I would point out a specific part where things went wrong and ask it to redo that portion or rewrite the answer with that constraint in mind. If the revised answer still contained other errors, I would then point those out as well. This iterative process usually converged to the correct solution, but only because I was checking against the official answers.</paragraph><paragraph><bold>Where it did well:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Linear algebra (for example the Chinchilla scaling law question was done well, with correct derivations)</paragraph></list-item><list-item><paragraph>Conceptual explanations (especially for 6.Soft-Prompting Language Models, where the model one-shot all the sub-questions with clear, coherent explanations)</paragraph></list-item></list><paragraph><bold>Errors and hallucinations:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>The main issue is the model gives confident, mostly correct answers with hidden mistakes. For example, in 2(a), the model mentioned similarity with a bag-of-words model, while the problem was asking about a token-by-token model. Interestingly, the model knew the correct answer, and did include it in its output(\u201cSo, it's essentially a token-to-token mapping without any awareness of context or sequence structure\u201d). If it had simply not added the bag-of-words remark, the answer would have been fully correct. Another example is 2(b), where it talked about a matrix being block-diagonal when it was horizontally partitioned. These wrong side-comments did not derail the final conclusion, but they weakened the model\u2019s usefulness as a learning tool, because a student might not know which parts to trust.</paragraph></list-item><list-item><paragraph>Mathematically, the model made sloppy arithmetic mistakes, messing up the exponent in problem 5(h) twice, and did not fix the second mistake when the first one was pointed out. Without the user explicitly checking the numbers and forcing it to recompute, it would happily present a numerically incorrect result.</paragraph></list-item><list-item><paragraph>It also made overconfident statements. In 2(e), it made claims about the read subspace without appropriate qualifications. Despite this, it still managed to reach the correct high-level conclusion that \u201cthe read subspace dimension is at most d_head\u201d. When I later prompted it to tone down the overconfident statement, it also dropped the correct statement entirely. This illustrates a common LLM behavior: it is too eager to agree with the user, sometimes over-editing and discarding correct content when asked to correct itself. This makes adjusting the model\u2019s confidence level the most difficult in my opinion.</paragraph></list-item><list-item><paragraph>It was also occasionally under-confident. Notably in problem 1, the model said \u201cAt initialization, since A=0 and B=0, the gradients may be zero or very small\u201d, when in fact the gradient is exactly zero in this setup. So it hedged (\u201cmay be\u201d) where the correct answer was deterministic.</paragraph></list-item></list><paragraph>Overall, the model can one-shot many questions or get very close, but it often introduces mistakes even in derivations that <italic>look</italic> careful. It\u2019s not reliable as an automated homework solver, and it\u2019s not even a great tutor unless the user actively engages the model, checks its work, and asks thoughtful, targeted follow-up questions to give the model a chance to fix its mistakes. Without active supervision, it\u2019s very easy to walk away with answers that are wrong.</paragraph></document>",
            "links": [
                "https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:25:31.193131+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427518,
            "author": "Jermaine Lei",
            "project_title": "Special Participation A: ChatGPT 4o on HW 8",
            "post_body": "For this special participation, I used the ChatGPT-4o model to solve the non-coding parts of Homework 8. To start the conversation, I gave the model the full assignment and asked it to act as a \"Deep Learning professor\" who needed to provide \"full solutions, step-by-step.\" I thought this specific instruction would make the model show all of its work.\n\nThe model showed itself to be very accurate and quick. It got the correct answer on the first try for almost all of the problems. This very high one-shot success rate shows that the model has a strong understanding of complex deep learning topics.\n\nHowever, the main problem I ran into was that the model did not follow the format instructions. Even though I asked it to go step-by-step, the model often skipped the detailed logic and just gave the answer. It seemed to prefer being quick and correct rather than being a helpful, step-by-step teacher.\n\nThe most challenging part for the model was in a question about computational complexity. It struggled to properly use the input size variable in its final answer. Specifically, when calculating the time complexity in Big-O notation, the model had a difficult time showing how the total work would scale with n. I had to correct it with follow-up prompts to get it to include this variable correctly.\n\nOverall, the experiment showed that ChatGPT-4o is excellent at finding the right answers to tough academic questions. But it is not a perfect teaching assistant because it often ignores specific instructions on how to explain the answer, and it can struggle with abstract, symbolic math like fully parameterized time complexity.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I used the ChatGPT-4o model to solve the non-coding parts of Homework 8. To start the conversation, I gave the model the full assignment and asked it to act as a \"Deep Learning professor\" who needed to provide \"full solutions, step-by-step.\" I thought this specific instruction would make the model show all of its work.</paragraph><paragraph>The model showed itself to be very accurate and quick. It got the correct answer on the first try for almost all of the problems. This very high one-shot success rate shows that the model has a strong understanding of complex deep learning topics.</paragraph><paragraph>However, the main problem I ran into was that the model did not follow the format instructions. Even though I asked it to go step-by-step, the model often skipped the detailed logic and just gave the answer. It seemed to prefer being quick and correct rather than being a helpful, step-by-step teacher.</paragraph><paragraph>The most challenging part for the model was in a question about computational complexity. It struggled to properly use the input size variable in its final answer. Specifically, when calculating the time complexity in Big-O notation, the model had a difficult time showing how the total work would scale with n. I had to correct it with follow-up prompts to get it to include this variable correctly.</paragraph><paragraph>Overall, the experiment showed that ChatGPT-4o is excellent at finding the right answers to tough academic questions. But it is not a perfect teaching assistant because it often ignores specific instructions on how to explain the answer, and it can struggle with abstract, symbolic math like fully parameterized time complexity.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/LKFY4aHfeS6uzmlWdDin6vpf\" filename=\"SpecialParticipationA_ChatGPT-4o_hw8.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T09:23:32.131726+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427489,
            "author": "Imra Dawoodani",
            "project_title": "Special Participation E: Learning Optimization by spotting mistakes in AI generated answers with a custom Gemini Gem",
            "post_body": "I built a custom Gemini gem that teaches optimization concepts by intentionally giving you flawed explanations and guiding you to catch the errors. I'm sharing two versions of the Gem (structured and conversational) plus annotated transcripts showing what worked, what didn't, and where the AI made mistakes I had to catch. I made V2 after I played around with V1 and wasn't satisfied with the learning outcome it produced.\n\nV1: https://gemini.google.com/gem/1nODd0rYIHnHOMosLrhdSIuoGBRyAieKL?usp=sharing\nV2: https://gemini.google.com/gem/1osRvc6_mQ1ntQd8hjq7QxOmL38ftyJAT?usp=sharing\n\nAnnotated V1 transcripts:\n\nAnnotated V2 transcript:\n\nMy observations:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I built a custom Gemini gem that teaches optimization concepts by intentionally giving you flawed explanations and guiding you to catch the errors. I'm sharing two versions of the Gem (structured and conversational) plus annotated transcripts showing what worked, what didn't, and where the AI made mistakes I had to catch. I made V2 after I played around with V1 and wasn't satisfied with the learning outcome it produced.</paragraph><paragraph>V1: <link href=\"https://gemini.google.com/gem/1nODd0rYIHnHOMosLrhdSIuoGBRyAieKL?usp=sharing\">https://gemini.google.com/gem/1nODd0rYIHnHOMosLrhdSIuoGBRyAieKL?usp=sharing</link><break/>V2: <link href=\"https://gemini.google.com/gem/1osRvc6_mQ1ntQd8hjq7QxOmL38ftyJAT?usp=sharing\">https://gemini.google.com/gem/1osRvc6_mQ1ntQd8hjq7QxOmL38ftyJAT?usp=sharing</link></paragraph><paragraph>Annotated V1 transcripts:</paragraph><file url=\"https://static.us.edusercontent.com/files/fyOdPjYRKh3cZF6KYQDYd08c\" filename=\"gemini.google.com-Google Gemini-fpscreenshot (1).pdf\"/><file url=\"https://static.us.edusercontent.com/files/Qu9hv9enrHjqIf6V4pgueUeL\" filename=\"gemini.google.com-Google Gemini-fpscreenshot.pdf\"/><paragraph>Annotated V2 transcript:</paragraph><file url=\"https://static.us.edusercontent.com/files/ZbC4dXX9mfccdFiA8Eb90zvj\" filename=\"gemini.google.com-Google Gemini-fpscreenshot (2).pdf\"/><paragraph>My observations:</paragraph><file url=\"https://static.us.edusercontent.com/files/XTToFU3qMCqp32fDN5IUZ7kf\" filename=\"special participation E - mistaker.pdf\"/><paragraph/></document>",
            "links": [
                "https://gemini.google.com/gem/1nODd0rYIHnHOMosLrhdSIuoGBRyAieKL?usp=sharing",
                "https://gemini.google.com/gem/1osRvc6_mQ1ntQd8hjq7QxOmL38ftyJAT?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:20:15.432576+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427439,
            "author": "Nazar Ospanov",
            "project_title": "Special Participation B: Kimi on Homework 12",
            "post_body": "For Homework 12, I tested Kimi K2 Thinking on the VAE coding exercises. Kimi followed instructions well and demonstrated strong code-generation ability. In fact, it was able to nearly one-shot the entire implementation, including the reparameterization trick in sample_gaussian, which it completed correctly on the first attempt.\n\nThe only notable difficulty occurred in the negative_elbo_bound implementation. Kimi correctly identified the necessary components like KL divergence via kl_normal and the reconstruction term via log_bernoulli_with_logits, but incorrectly summed the terms instead of averaging over the batch, which leads to a scale mismatch in the loss. After prompting, it was able to fix this on a second attempt.\n\nThus, Kimi K2 Thinking is very strong for structured coding tasks and can implement multi-step neural-network logic with minimal guidance. The main weakness observed was a tendency to choose the mathematically correct form of an expression but the wrong reduction (sum vs. mean), which is easy to miss without supervision. Overall, excellent performance with small but correctable mistakes.\n\nHere is annotated log of the conversation:",
            "content_xml": "<document version=\"2.0\"><paragraph>For Homework 12, I tested <bold>Kimi K2 Thinking</bold> on the VAE coding exercises. Kimi followed instructions well and demonstrated strong code-generation ability. In fact, it was able to nearly one-shot the entire implementation, including the reparameterization trick in sample_gaussian, which it completed correctly on the first attempt.</paragraph><paragraph>The only notable difficulty occurred in the negative_elbo_bound implementation. Kimi correctly identified the necessary components like KL divergence via kl_normal and the reconstruction term via log_bernoulli_with_logits, but incorrectly summed the terms instead of averaging over the batch, which leads to a scale mismatch in the loss. After prompting, it was able to fix this on a second attempt.</paragraph><paragraph>Thus, Kimi K2 Thinking is very strong for structured coding tasks and can implement multi-step neural-network logic with minimal guidance. The main weakness observed was a tendency to choose the mathematically correct <italic>form</italic> of an expression but the wrong <italic>reduction</italic> (sum vs. mean), which is easy to miss without supervision. Overall, excellent performance with small but correctable mistakes.</paragraph><paragraph>Here is annotated log of the conversation:</paragraph><file url=\"https://static.us.edusercontent.com/files/S9hahdhqvFoNPFmKjmhFHdFG\" filename=\"HW12_B_notes.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T09:14:56.572866+11:00",
            "category": "Admin"
        },
        {
            "guid": 7427403,
            "author": "Natalie Wei",
            "project_title": "Special Participation C: Refactoring HW10 Q3",
            "post_body": "Overview\n\nAlena Chao and I refactored the code for HW10 Q3, focusing on improving clarity, consistency, and overall code quality. We updated q_summarize_part_1.ipynb according to PEP 8 (general style), PEP 484 (type hints), PEP 257 (docstrings), and Google's Python style guide (overall best practices). The revised notebook is attached below and more details can be found in our linked report.\n\nOur changes include adding detailed docstrings following PEP 257 to most functions, clearly outlining their purpose and expected inputs and outputs. We also corrected indentation in accordance with PEP 8 and enforcing consistent type hinting across the entire notebook. These updates make the code easier to understand for students working through the assignment.\n\nFurther, we identified several sections of overly long and repetitive code, particularly within the testing functions; we refactored these for cleanliness and modularity. To accomplish this, we created additional helper functions for initializing layers and copying weights. We also added new comments indicating the start of each test case , breaking up the dense original logic and improving readability.\n\nReport\n\nLink ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Overview</bold></paragraph><paragraph>Alena Chao and I refactored the code for HW10 Q3, focusing on improving clarity, consistency, and overall code quality. We updated <italic>q_summarize_part_1.ipynb</italic> according to PEP 8 (general style), PEP 484 (type hints), PEP 257 (docstrings), and Google's Python style guide (overall best practices). The revised notebook is attached below and more details can be found in our linked report.</paragraph><paragraph>Our changes include adding detailed docstrings following PEP 257 to most functions, clearly outlining their purpose and expected inputs and outputs. We also corrected indentation in accordance with PEP 8 and enforcing consistent type hinting across the entire notebook. These updates make the code easier to understand for students working through the assignment.</paragraph><paragraph>Further, we identified several sections of overly long and repetitive code, particularly within the testing functions; we refactored these for cleanliness and modularity. To accomplish this, we created additional helper functions for initializing layers and copying weights. We also added new comments indicating the start of each test case , breaking up the dense original logic and improving readability.</paragraph><paragraph><bold>Report</bold></paragraph><paragraph><link href=\"https://drive.google.com/file/d/1CAagqHaEWGcqg9POI9RTk2coa60Oo-mQ/view?usp=sharing\">Link</link> </paragraph><file url=\"https://static.us.edusercontent.com/files/3mwDgWaIijKdbsg0kKnvVhoi\" filename=\"q_summarize_part_1.ipynb\"/></document>",
            "links": [
                "https://drive.google.com/file/d/1CAagqHaEWGcqg9POI9RTk2coa60Oo-mQ/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T09:10:30.194097+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427400,
            "author": "Nazar Ospanov",
            "project_title": "Special Participation A: Gemini Fast on Homework 3",
            "post_body": "I used Gemini Fast to complete the written questions for Homework 3. As in earlier evaluations, I instructed the model to restate each question, give a step-by-step solution, and identify uncertainties. I prompted each question separately to avoid context-window issues. The full annotated trace is shown in my notes .\n\nOverall, Gemini Fast performed well. It solved most derivations on the first attempt, produced clean LaTeX, and often gave explanations as good as or better than the staff solution.\n\nHowever, I observed two consistent weaknesses:\n\nIncorrect assumptions leading to wrong solutions.\n\nGemini sometimes committed early to an interpretation that wasn\u2019t implied by the problem, and the resulting chain-of-thought led to confident but incorrect answers (e.g., Question 5).\n\nHallucination on research-figure interpretation (Question 3).\n\nWhen asked to analyze specific figures and tables from Tensor Programs V and the Spectral Condition paper, Gemini did not answer the actual sub-questions. Instead, it produced generic summaries of the figures and ignored the prompts, showing that Gemini Fast does not reliably ground its answers in visual research content.\n\nThus, Gemini Fast is excellent for computational and mathematical deep-learning questions, but less reliable for conceptual reasoning that depends on precise assumptions or figure-based interpretation. I would recommend it for derivation-heavy tasks, but not for problems requiring careful reading of research-paper visuals.\n\nHere is the annotated log of the conversation:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used <bold>Gemini Fast</bold> to complete the written questions for Homework 3. As in earlier evaluations, I instructed the model to restate each question, give a step-by-step solution, and identify uncertainties. I prompted each question separately to avoid context-window issues. The full annotated trace is shown in my notes .</paragraph><paragraph>Overall, <bold>Gemini Fast performed well</bold>. It solved most derivations on the first attempt, produced clean LaTeX, and often gave explanations as good as or better than the staff solution.</paragraph><paragraph>However, I observed two consistent weaknesses:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>Incorrect assumptions leading to wrong solutions.</bold></paragraph><paragraph>Gemini sometimes committed early to an interpretation that wasn\u2019t implied by the problem, and the resulting chain-of-thought led to confident but incorrect answers (e.g., Question 5).</paragraph></list-item><list-item><paragraph><bold>Hallucination on research-figure interpretation (Question 3).</bold></paragraph><paragraph>When asked to analyze specific figures and tables from <italic>Tensor Programs V</italic> and the <italic>Spectral Condition</italic> paper, Gemini did <bold>not</bold> answer the actual sub-questions. Instead, it produced generic summaries of the figures and ignored the prompts, showing that Gemini Fast does not reliably ground its answers in visual research content.</paragraph></list-item></list><paragraph>Thus, Gemini Fast is excellent for computational and mathematical deep-learning questions, but less reliable for conceptual reasoning that depends on precise assumptions or figure-based interpretation. I would recommend it for derivation-heavy tasks, but not for problems requiring careful reading of research-paper visuals.</paragraph><paragraph>Here is the annotated log of the conversation:</paragraph><file url=\"https://static.us.edusercontent.com/files/mvnVlZUAQ7cHBUKXV9vMfzZx\" filename=\"HW3_A_notes.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T09:10:21.658737+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427394,
            "author": "Carolyn Liu",
            "project_title": "Special Participation E: HW0 Prerequisite Content Helper",
            "post_body": "I used the ChatGPT 5.1 Thinking model under study mode to help go over pre-requisite content found in Homework 0. I told the model that they are a helper and the goal is to refresh the content found in homework 0 but now actually solve any of the problems. The first thing the model did was ask how comfortable I was with the pre-requisite content, which I appreciated since it tailored the difficulty of the questions from it. Afterwards, they kept on asking questions and explained why some of my answers were incorrect. I believe this is a good prompt to help students with the linear algebra content and basic ML concepts before diving into Homework 0.\n\nI have attached my conversation and also a good starting prompt for this below:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used the ChatGPT 5.1 Thinking model under study mode to help go over pre-requisite content found in Homework 0. I told the model that they are a helper and the goal is to refresh the content found in homework 0 but now actually solve any of the problems. The first thing the model did was ask how comfortable I was with the pre-requisite content, which I appreciated since it tailored the difficulty of the questions from it. Afterwards, they kept on asking questions and explained why some of my answers were incorrect. I believe this is a good prompt to help students with the linear algebra content and basic ML concepts before diving into Homework 0.</paragraph><paragraph>I have attached my conversation and also a good starting prompt for this below:</paragraph><file url=\"https://static.us.edusercontent.com/files/PTDDF0LMrJFrducr1IRuyFDG\" filename=\"Special Participation E_ HW0 Prerequisite Content Helper.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T09:09:50.529973+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427288,
            "author": "Jerry Xiao",
            "project_title": "Special Participation E: AI-Generated Quiz Tool for lectures",
            "post_body": "Hi everyone,\nFor the AI learning-tools option, I built a small quiz-generator assistant that can help generate exam-style question for you to test your understanding on specific field. I use ChatGPT 5.1 to interact with and use the Project functions to help me generate the problem sets.\n\nHere is a short, simple, clean Ed post for your second interaction (the Diffusion quiz generator).\n This version is minimal and directly hits the assignment requirements.\n\n1. Prompt \n\nI want you to become a quiz generator for me to evaluate my learning progress. \nI want you to generate questions on Generative Model like Diffusion. \nPlease follow the style of the second file to generate the problems. \n[Append lecture note and example question here]\n\nThen I asked questions as:\n\n\u201cGenerate Quiz Set 1.\u201d\n\n\u201cGive me solutions.\u201d\n\n\u201cGenerate Set 2.\u201d\n\n\u201cDraw a picture comparing DDPM and DDIM.\u201d\n\n2. What it do good\n\nIt matched the HW format surprisingly well (short structured questions).\n\nThe explanations were clear and aligned with the lecture.\n\nThe DDPM vs DDIM diagram was helpful for intuition.\n\n3. What it do bad\n\nSometimes it added extra comments not in the lecture.\n\nIt was a bit too confident in referencing lecture page numbers.\n\nSketches were descriptive, not actual images (which is fine for learning but not literal HW format).\n\nYou can use this way to quiz yourself before homework/exams, get practice problems instantly, ask for more sets at different difficulty levels.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,<break/>For the AI learning-tools option, I built a small <bold>quiz-generator assistant</bold> that can help generate exam-style question for you to test your understanding on specific field. I use ChatGPT 5.1 to interact with and use the Project functions to help me generate the problem sets.</paragraph><paragraph>Here is a <bold>short, simple, clean Ed post</bold> for your second interaction (the Diffusion quiz generator).<break/> This version is minimal and directly hits the assignment requirements.</paragraph><paragraph>1. Prompt </paragraph><pre>I want you to become a quiz generator for me to evaluate my learning progress. \nI want you to generate questions on Generative Model like Diffusion. \nPlease follow the style of the second file to generate the problems. \n[Append lecture note and example question here]</pre><paragraph>Then I asked questions as:</paragraph><list style=\"unordered\"><list-item><paragraph>\u201cGenerate Quiz Set 1.\u201d</paragraph></list-item><list-item><paragraph>\u201cGive me solutions.\u201d</paragraph></list-item><list-item><paragraph>\u201cGenerate Set 2.\u201d</paragraph></list-item><list-item><paragraph>\u201cDraw a picture comparing DDPM and DDIM.\u201d</paragraph></list-item></list><paragraph>2. What it do good</paragraph><list style=\"unordered\"><list-item><paragraph>It matched the HW format surprisingly well (short structured questions).</paragraph></list-item><list-item><paragraph>The explanations were clear and aligned with the lecture.</paragraph></list-item><list-item><paragraph>The DDPM vs DDIM diagram was helpful for intuition.</paragraph></list-item></list><paragraph>3. What it do bad</paragraph><list style=\"unordered\"><list-item><paragraph>Sometimes it added extra comments not in the lecture.</paragraph></list-item><list-item><paragraph>It was a bit too confident in referencing lecture page numbers.</paragraph></list-item><list-item><paragraph>Sketches were descriptive, not actual images (which is fine for learning but not literal HW format).</paragraph></list-item></list><paragraph>You can use this way to quiz yourself before homework/exams, get practice problems instantly, ask for more sets at different difficulty levels.</paragraph><file url=\"https://static.us.edusercontent.com/files/tesuegU502lGWS0leYQGdy7X\" filename=\"gpt_interaction_2.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T08:57:19.086591+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427284,
            "author": "Joshua Lu",
            "project_title": "Special Participation E: Homework Checker and Mistake Summarizer",
            "post_body": "When going over homework, one of the most important things to do is to list out all the mistakes and make sure not to do that again. For this special participation, I created a pipeline to 1) check the homework and make a list of things that are wrong and 2) make a summary of the mistakes, areas of improvement, areas of strength, and a list of potential reminders on what to be careful of. I used to do this manually when studying for finals in the past, so having an LLM do this would streamline the process.\n\nThe first step is to have the LLM correct the homework. For this, I used Gemini 3 Pro (With Thinking), and I sent the model a pdf of my handwritten homework solutions and a pdf of the official staff solutions. I tried this out with both Homework 9 and Homework 0. Homework 9 contains a lot more conceptual questions while Homework 0 contains a lot more math questions. This will let me test how well the model can grade and analyze each one.\n\nThe model actually did pretty well for most of Homework 9. I purposefully left a few mistakes to see if Gemini can catch it, and surprisingly, it did for all of them. In fact, for one of the questions, I made an arithmetic error that doesn't affect my final answer, but Gemini was still able to catch it, despite my work being handwritten. It's definitely really good at catching these small details. However, for the last question, Question 6, the problem itself required complexity calculations, and my work was completely off, yet Gemini did not note that and assumed I was just misreading the problem. That was when I got suspicious that maybe the model isn't great at more complex, mathematical homeworks.\n\nBefore moving on to the next homework, I also prompted the model to produce a list of my mistakes, areas of improvement, areas of strength, and things to watch out for when taking an exam. I wouldn't say it did amazing on this, but some of the information it gave was still useful. For things to watch out for, that part was not great. All the information it generated was very general and not very helpful, so I tried re-prompting, and it only got a bit better.\n\n\n\nI also tried out this approach with Homework 0, and just as before, it was able to identify my mistakes very well. It found that I solved the first few questions very well, and that I only messed up later on for the last question. However, for the very last question that required a lot of math, it wasn't really able to analyze my work well, and diff it from what is given in the official solution. And just as before, it was able to give a solid checklist of mistakes, things to improve on, and things I should watch out for, but the information is still a bit too general. It could be useful for last-minute prep.\n\n\n\nThere are definitely limitations with using LLM to check homework and analyze hand-written work. It did perform better than I expected, as giving it a pdf, it was able to analyze everything in a single go and identify most things correctly. However, this type of LLM approach should still definitely be used in conjunction with manually going over the homework, but I think it's nevertheless helpful.\n\n\n\nAnnotated Trace: https://drive.google.com/file/d/1CeAMUfZ0uiBXgAkjJxPPa1OtjwzVH0GK/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>When going over homework, one of the most important things to do is to list out all the mistakes and make sure not to do that again. For this special participation, I created a pipeline to 1) check the homework and make a list of things that are wrong and 2) make a summary of the mistakes, areas of improvement, areas of strength, and a list of potential reminders on what to be careful of. I used to do this manually when studying for finals in the past, so having an LLM do this would streamline the process.</paragraph><paragraph>The first step is to have the LLM correct the homework. For this, I used Gemini 3 Pro (With Thinking), and I sent the model a pdf of my handwritten homework solutions and a pdf of the official staff solutions. I tried this out with both Homework 9 and Homework 0. Homework 9 contains a lot more conceptual questions while Homework 0 contains a lot more math questions. This will let me test how well the model can grade and analyze each one.</paragraph><paragraph>The model actually did pretty well for most of Homework 9. I purposefully left a few mistakes to see if Gemini can catch it, and surprisingly, it did for all of them. In fact, for one of the questions, I made an arithmetic error that doesn't affect my final answer, but Gemini was still able to catch it, despite my work being handwritten. It's definitely really good at catching these small details. However, for the last question, Question 6, the problem itself required complexity calculations, and my work was completely off, yet Gemini did not note that and assumed I was just misreading the problem. That was when I got suspicious that maybe the model isn't great at more complex, mathematical homeworks.</paragraph><paragraph>Before moving on to the next homework, I also prompted the model to produce a list of my mistakes, areas of improvement, areas of strength, and things to watch out for when taking an exam. I wouldn't say it did amazing on this, but some of the information it gave was still useful. For things to watch out for, that part was not great. All the information it generated was very general and not very helpful, so I tried re-prompting, and it only got a bit better.</paragraph><paragraph/><paragraph>I also tried out this approach with Homework 0, and just as before, it was able to identify my mistakes very well. It found that I solved the first few questions very well, and that I only messed up later on for the last question. However, for the very last question that required a lot of math, it wasn't really able to analyze my work well, and diff it from what is given in the official solution. And just as before, it was able to give a solid checklist of mistakes, things to improve on, and things I should watch out for, but the information is still a bit too general. It could be useful for last-minute prep.</paragraph><paragraph/><paragraph>There are definitely limitations with using LLM to check homework and analyze hand-written work. It did perform better than I expected, as giving it a pdf, it was able to analyze everything in a single go and identify most things correctly. However, this type of LLM approach should still definitely be used in conjunction with manually going over the homework, but I think it's nevertheless helpful.</paragraph><paragraph/><paragraph>Annotated Trace: https://drive.google.com/file/d/1CeAMUfZ0uiBXgAkjJxPPa1OtjwzVH0GK/view?usp=sharing</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T08:56:47.517833+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427268,
            "author": "Joshua Lu",
            "project_title": "Special Participation E: Multi-Level Discussion Summarizer and Key Notes Extractor with Gemini 3 Pro",
            "post_body": "I always want to make sure that I have all my notes in one place, and one additional source of very useful information is the Discussion. Discussion covers a lot of content, some of them being new content as well, and the information covered in discussion definitely belongs in the notes. For this special participation, I will specifically be working with Discussion 10 (Transformers). The goal of this special participation is to extract the key concepts from the discussion worksheet in note form, but with a twist: The notes should have two section, a main key concepts section and a second deep-dive section. This summary should include details that go beyond what can be found in the discussion and should be a comprehensive review guide.\n\nI used Gemini 3 Pro (With Thinking) for this special participation. My process is the following: to start off, I gave the model a simple prompt with my goal:\n\n\"Summarize the key learning concepts in Discussion 10 on Transformers. Provide the key learning concepts in bullet-point form. Make sure to emphasize what is important to know. This first part should be more general, main ideas.\n\nThen, make a second section where you go deeper into each concept and again provide a list of the key details.\"\n\nThrough this, the model gave a basic summary which was not detailed enough. Then, through an iterative process, I improved the summary of the discussion by asking the model about specific details that comes from the discussion. For Discussion 10 specifically, this included the math for RoPE and the complexity derivations for self-attention and cross-attention. After a few iterations, the model learned about the specific types of details I was looking for and at the end, I asked it to regenerate the summary in a form I can paste into my notes. It's clear from my experiment that the model cannot easily one-shot a good enough summary for me to use, so repeated prompting is necessary to get something useful.\n\nThe good part about this model was that it generally got everything correct. Since the goal is to summarize and review information, it did not hallucinate, and I verified the information looks good. However, one issue of using the chat interface is that the formatting isn't great. Each time I want a new summary, the model has to regenerate it entirely, and the chat log gets very messy.\n\nAfter I got my summary, I then had the model look through the homework and indicate which concepts I should already know based on the discussion, and which concepts I should review in preparation for the homework. This multi-level summary is very useful for me to not just look back later on to review important information from discussion but also help me prepare to do the homework in a setting where I don't need to constantly refer back to notes.\n\n\n\nHere is the trace without annotations: https://gemini.google.com/share/5db1aec72092\n\nHere is the trace with annotations: https://drive.google.com/file/d/18umCOjsk8ATIR5HsZQKgECIUwoOZNbE7/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>I always want to make sure that I have all my notes in one place, and one additional source of very useful information is the Discussion. Discussion covers a lot of content, some of them being new content as well, and the information covered in discussion definitely belongs in the notes. For this special participation, I will specifically be working with Discussion 10 (Transformers). The goal of this special participation is to extract the key concepts from the discussion worksheet in note form, but with a twist: The notes should have two section, a main key concepts section and a second deep-dive section. This summary should include details that go beyond what can be found in the discussion and should be a comprehensive review guide.</paragraph><paragraph>I used Gemini 3 Pro (With Thinking) for this special participation. My process is the following: to start off, I gave the model a simple prompt with my goal:</paragraph><paragraph>\"Summarize the key learning concepts in Discussion 10 on Transformers. Provide the key learning concepts in bullet-point form. Make sure to emphasize what is important to know. This first part should be more general, main ideas.</paragraph><paragraph>Then, make a second section where you go deeper into each concept and again provide a list of the key details.\"</paragraph><paragraph>Through this, the model gave a basic summary which was not detailed enough. Then, through an iterative process, I improved the summary of the discussion by asking the model about specific details that comes from the discussion. For Discussion 10 specifically, this included the math for RoPE and the complexity derivations for self-attention and cross-attention. After a few iterations, the model learned about the specific types of details I was looking for and at the end, I asked it to regenerate the summary in a form I can paste into my notes. It's clear from my experiment that the model cannot easily one-shot a good enough summary for me to use, so repeated prompting is necessary to get something useful.</paragraph><paragraph>The good part about this model was that it generally got everything correct. Since the goal is to summarize and review information, it did not hallucinate, and I verified the information looks good. However, one issue of using the chat interface is that the formatting isn't great. Each time I want a new summary, the model has to regenerate it entirely, and the chat log gets very messy.</paragraph><paragraph>After I got my summary, I then had the model look through the homework and indicate which concepts I should already know based on the discussion, and which concepts I should review in preparation for the homework. This multi-level summary is very useful for me to not just look back later on to review important information from discussion but also help me prepare to do the homework in a setting where I don't need to constantly refer back to notes.</paragraph><paragraph/><paragraph>Here is the trace without annotations: <link href=\"https://gemini.google.com/share/5db1aec72092\">https://gemini.google.com/share/5db1aec72092</link></paragraph><paragraph>Here is the trace with annotations: <link href=\"https://drive.google.com/file/d/18umCOjsk8ATIR5HsZQKgECIUwoOZNbE7/view?usp=sharing\">https://drive.google.com/file/d/18umCOjsk8ATIR5HsZQKgECIUwoOZNbE7/view?usp=sharing</link></paragraph></document>",
            "links": [
                "https://gemini.google.com/share/5db1aec72092",
                "https://drive.google.com/file/d/18umCOjsk8ATIR5HsZQKgECIUwoOZNbE7/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T08:54:18.005087+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427241,
            "author": "Anshul Verma",
            "project_title": "Special Participation B: GPT 5.1 on HW 0",
            "post_body": "Chat History: https://chatgpt.com/share/6935ebc1-4214-800e-b0e8-93e9889d481d\n\nAnnotation: https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing\n\nOverall Performance: The model demonstrated strong proficiency in writing \"Pythonic\" numerical code and producing implementations that were very computationally efficient and correct. However, it lacked intuition regarding training dynamics; while the architecture was correct, the model initially failed to select viable hyperparameters, leading to severe underfitting.\n\nStrategy Used: Rather than immediately correcting the model's poor hyperparameter choices, I allowed it to attempt multiple training runs with its suggested values to demonstrate the failure mode (underfitting). I then directed the model to analyze these previous failed attempts to infer why the loss wasn't decreasing. I eventually had to provide the solution for overfitting the 3-Layer Net but it was able to use this to correctly infer hyperparameters to overfit the 5-Layer net.\n\nCode:",
            "content_xml": "<document version=\"2.0\"><paragraph>Chat History: <link href=\"https://chatgpt.com/share/6935ebc1-4214-800e-b0e8-93e9889d481d\">https://chatgpt.com/share/6935ebc1-4214-800e-b0e8-93e9889d481d</link></paragraph><paragraph>Annotation: <link href=\"https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing\">https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing</link></paragraph><paragraph><bold>Overall Performance:</bold> The model demonstrated strong proficiency in writing \"Pythonic\" numerical code and producing implementations that were very computationally efficient and correct. However, it lacked intuition regarding training dynamics; while the architecture was correct, the model initially failed to select viable hyperparameters, leading to severe underfitting.</paragraph><paragraph><bold>Strategy Used:</bold> Rather than immediately correcting the model's poor hyperparameter choices, I allowed it to attempt multiple training runs with its suggested values to demonstrate the failure mode (underfitting). I then directed the model to analyze these previous failed attempts to infer why the loss wasn't decreasing. I eventually had to provide the solution for overfitting the 3-Layer Net but it was able to use this to correctly infer hyperparameters to overfit the 5-Layer net.</paragraph><paragraph>Code:</paragraph><file url=\"https://static.us.edusercontent.com/files/djkU6hXuDZzRZ0aDRLNA6wWu\" filename=\"fc_net.py\"/><file url=\"https://static.us.edusercontent.com/files/2ihtBcokYIQlhM6HTP9U9ESe\" filename=\"layers.py\"/><file url=\"https://static.us.edusercontent.com/files/r8By7BY5uP3UyfFKPfuJOh8t\" filename=\"networks.ipynb\"/></document>",
            "links": [
                "https://chatgpt.com/share/6935ebc1-4214-800e-b0e8-93e9889d481d",
                "https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T08:50:46.673238+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427197,
            "author": "William Li",
            "project_title": "Special Participation E: Minimally Wrong MCQ Diagnostic Learning Tool",
            "post_body": "In this special participation, I create a prompt and test how ChatGPT can generate multiple choice questions with \u201cdistractor\u201d answers that are minimally wrong. This is meant to emulate what I typically found most difficult in MCQ questions, which is when all the wrong answers seem plausible and can only be discerned as wrong by identifying subtle mistakes. To this end, I wanted to try to use LLMs to generate these sorts of questions. Additionally, perhaps the hallucinations that these LLMs so often generate could be useful in creating wrong answers that sound correct.\n\nUpon testing out this prompt and studying technique, I was very satisfied with what the model was able to give me. I asked it several questions spanning different topics (optimization, attention, transformers etc.) along with different modalities (purely conceptual, mathematical, coding), and it was able to give me very succinct MCQs for these. The answer choices it generated were also very good, and it was quite tricky choosing between the answers sometimes. After answering (and getting the question right or wrong), the model was able to give me the reasonings behind each of the answers. For the correct answer, it gave the reasons as to why it is correct, and for the incorrect answers it explained the subtle bug or misconception that led to the error. Overall, I think this was a good learning experience and I would certainly use this to study more concepts before the final rolls around. \n\nAnnotated Trace: ",
            "content_xml": "<document version=\"2.0\"><paragraph>In this special participation, I create a prompt and test how ChatGPT can generate multiple choice questions with \u201cdistractor\u201d answers that are minimally wrong. This is meant to emulate what I typically found most difficult in MCQ questions, which is when all the wrong answers seem plausible and can only be discerned as wrong by identifying subtle mistakes. To this end, I wanted to try to use LLMs to generate these sorts of questions. Additionally, perhaps the hallucinations that these LLMs so often generate could be useful in creating wrong answers that sound correct.</paragraph><paragraph>Upon testing out this prompt and studying technique, I was very satisfied with what the model was able to give me. I asked it several questions spanning different topics (optimization, attention, transformers etc.) along with different modalities (purely conceptual, mathematical, coding), and it was able to give me very succinct MCQs for these. The answer choices it generated were also very good, and it was quite tricky choosing between the answers sometimes. After answering (and getting the question right or wrong), the model was able to give me the reasonings behind each of the answers. For the correct answer, it gave the reasons as to why it is correct, and for the incorrect answers it explained the subtle bug or misconception that led to the error. Overall, I think this was a good learning experience and I would certainly use this to study more concepts before the final rolls around. </paragraph><paragraph>Annotated Trace: </paragraph><file url=\"https://static.us.edusercontent.com/files/sSdCgJfEzSRTNZULjaXMxeod\" filename=\"mcq (1).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T08:45:57.134051+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7427051,
            "author": "Manan Roongta",
            "project_title": "Modal, Free GPU Credits",
            "post_body": "Hey everyone, since the compute portal is limited, wanted to share another option.\n\n\nModal gives $30/month free credits. About ~8hrs on H100/ ~12hrs on A100. If you use their serverless (not notebooks), you only pay for actual compute, i.e. if run a 30 sec task, get charged 30 sec. A group of 4 can get ~30hrs H100 / ~50hrs A100.\n\n\nTo get the full $30:\n\nSign up with GitHub\n\nGo to Usage & Billing, add a payment method (you'll only see $5 without this)\n\nSet \"Workspace budget\" to $30 so you don't get charged past the free credits",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey everyone, since the compute portal is limited, wanted to share another option.</paragraph><paragraph><break/><link href=\"https://modal.com/signup\">Modal</link> gives $30/month free credits. About ~8hrs on H100/ ~12hrs on A100. If you use their serverless (not notebooks), you only pay for actual compute, i.e. if run a 30 sec task, get charged 30 sec. A group of 4 can get ~30hrs H100 / ~50hrs A100.<break/></paragraph><paragraph>To get the full $30:</paragraph><list style=\"number\"><list-item><paragraph><link href=\"https://modal.com/signup\">Sign up</link> with GitHub</paragraph></list-item><list-item><paragraph>Go to Usage &amp; Billing, add a payment method (you'll only see $5 without this)</paragraph></list-item><list-item><paragraph>Set \"Workspace budget\" to $30 so you don't get charged past the free credits</paragraph></list-item></list></document>",
            "links": [
                "https://modal.com/signup",
                "https://modal.com/signup"
            ],
            "attachments": [],
            "created_at": "2025-12-08T08:28:15.968846+11:00",
            "category": "Admin"
        },
        {
            "guid": 7426890,
            "author": "Jerry Xiao",
            "project_title": "Special Participation E: AI Learning Tool using ChatGPT Projects",
            "post_body": "I use the Projects in the ChatGPT 5.1 to help me create a personal study assistant. I used it to help review Lecture 25 (RLVR, sampling from p(x)\u03b1, MCMC acceptance ratio, and the RLVR loss). I\u2019m sharing it here along with a short interaction trace and a few comments on where it worked well or got things wrong. \n\n1. How to start\n\nHere is the prompt I used at the beginning: Hi you will be my guys for reviewing and recapping my Deep Learning Class. I want to first understand the concept of RLVR: Reinforcement Learning with Verifiable Rewards. Can you help me elaborate on them based on the lecture notes? [I append the lecture notes behind so that it can help me with the content in the class]\n\n2. What is good\n\nFrom my interaction trace (see attached file): \n\nIt gave very clear explanations of why sampling from (p(x)^\\alpha) cannot be done with temperature. \n\nIts explanation of the MCMC acceptance ratio was correct and matched the lecture. The summary of the \n\nRLVR figures was helpful for understanding the \u201cwhy RLVR scales\u201d story. \n\nThe explanation of the RLVR loss (reward clipping, stop-gradient) was accurate and easy to understand.\n\n3. What is not good\n\nSometimes it claimed specific page numbers or wording from the lecture that weren\u2019t literally present.\n\nIt occasionally added extra reasoning (e.g., \u201creward model overfitting\u201d) that wasn\u2019t explicitly in the slide.\n\nSome explanations were too confident even when extrapolating beyond the notes.\n\nOverall, it was very useful for learning the concepts as long as I double-checked details against the lecture notes.\n\nThis acts like an interactive pre-lecture or review guide\u2014instead of re-reading slides alone, the AI helps clarify the math, the intuition, and the purpose of each RLVR component.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I use the Projects in the ChatGPT 5.1 to help me create a personal study assistant. I used it to help review <bold>Lecture 25</bold> (RLVR, sampling from p(x)\u03b1, MCMC acceptance ratio, and the RLVR loss). I\u2019m sharing it here along with a short interaction trace and a few comments on where it worked well or got things wrong. </paragraph><paragraph>1. How to start</paragraph><paragraph>Here is the prompt I used at the beginning: Hi you will be my guys for reviewing and recapping my Deep Learning Class. I want to first understand the concept of RLVR: Reinforcement Learning with Verifiable Rewards. Can you help me elaborate on them based on the lecture notes? [I append the lecture notes behind so that it can help me with the content in the class]</paragraph><paragraph>2. What is good</paragraph><paragraph>From my interaction trace (see attached file): </paragraph><list style=\"number\"><list-item><paragraph>It gave <bold>very clear explanations</bold> of why sampling from (p(x)^\\alpha) cannot be done with temperature. </paragraph></list-item><list-item><paragraph>Its explanation of the <bold>MCMC acceptance ratio</bold> was correct and matched the lecture. The summary of the </paragraph></list-item><list-item><paragraph><bold>RLVR figures</bold> was helpful for understanding the \u201cwhy RLVR scales\u201d story. </paragraph></list-item><list-item><paragraph>The explanation of the <bold>RLVR loss</bold> (reward clipping, stop-gradient) was accurate and easy to understand.</paragraph></list-item></list><paragraph>3. What is not good</paragraph><list style=\"number\"><list-item><paragraph>Sometimes it <bold>claimed specific page numbers</bold> or wording from the lecture that weren\u2019t literally present.</paragraph></list-item><list-item><paragraph>It occasionally added extra reasoning (e.g., \u201creward model overfitting\u201d) that wasn\u2019t explicitly in the slide.</paragraph></list-item><list-item><paragraph>Some explanations were <bold>too confident</bold> even when extrapolating beyond the notes.</paragraph></list-item></list><paragraph>Overall, it was very useful for learning the concepts as long as I double-checked details against the lecture notes.</paragraph><paragraph>This acts like an <bold>interactive pre-lecture or review guide</bold>\u2014instead of re-reading slides alone, the AI helps clarify the math, the intuition, and the purpose of each RLVR component.</paragraph><file url=\"https://static.us.edusercontent.com/files/sETCJFNG2037d1jbeXxOoNoM\" filename=\"gpt_interaction_1.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T08:04:57.739288+11:00",
            "category": "Admin"
        },
        {
            "guid": 7426886,
            "author": "Nikhil Mathihalli",
            "project_title": "Special Participation E: Studying Optimizers with NotebookLM",
            "post_body": "For the \"AI-enhanced learning tools\" option, I explored NotebookLM to study the recent lectures on Optimizers. My goal was to create an active alternative to traditional pre-reading/post-reading by having the AI synthesize the lecture recordings, notes, and homework solutions into a cohesive study guide.\n\nI uploaded the YouTube lecture audio recordings alongside the raw lecture notes. NotebookLM was able to \"listen\" to the lectures and cross-reference them with the written notes.\n\nOne of the coolest features was generating visual aids to structure my learning before diving into the weeds.\n\nThe Mind Map: I asked it to generate a mind map immediately1. This gave me a hierarchy of concepts\u2014linking Taylor Expansions to Lazy Training, and Initialization to RMS Norms\u2014which acted as a roadmap for what questions I needed to ask.\n\nThe Slideshow: I asked it to create a slideshow explaining optimizer concepts with visuals. It generated a dense, informed presentation in about 5 minutes. While the visuals were AI-generated, the structure helped me verify if I could explain the concepts linearly.\n\nThrough trial and error, I found a \"Drill-Down\" strategy worked best:\n\nStart Broad: I began with, \"Can you give me an overview of ALL the concepts... generate a roadmap\". This forced the model to establish context.\n\nSelect & Zoom: Instead of letting it ramble, I picked specific nodes from its roadmap: \"Let's start with the lazy training assumption. What is it, and why is it important?\"\n\nComparative Analysis: For distinct lists, I asked for comparative breakdowns: \"Can you go over every initialization... advantages/disadvantages to each?\" This yielded structured tables rather than walls of text.\n\nWhile the high-level summaries were coherent, the model often flattened subtle mathematical distinctions. I have annotated these in the attached PDF, but here are the key warnings:\n\nOversimplifications: The model correctly identified that parameters move by small amounts in training, but it implied that gradient features are fixed features simply because of the assumption. It failed to clarify that this is a specific regime (Kernel/NTK) and not how realistic modern \"feature learning\" regimes work\n\nBias Initialization Depth: The model listed strategies like \"initialize at 0\" or \"0.01\" but completely lacked the reasoning for why (e.g., why zero bias doesn't cause symmetry issues like zero weights do)\n\nMissing Geometric Intuition: When explaining algorithms like SignSGD, it gave the formula but missed the geometric intuition (that infinity norm constraints lead to coordinate-wise updates)\n\nConclusion\n\nNotebookLM is a powerful \"active reading\" partner, especially for synthesizing multimodal sources (audio + text). However, it struggles with the deep \"why\" behind some mathematical proofs. Overall, I think it's extremely useful for learning large amounts of content in a structured yet quick manner.\n\n\n\nTrace:",
            "content_xml": "<document version=\"2.0\"><paragraph>For the \"AI-enhanced learning tools\" option, I explored NotebookLM to study the recent lectures on Optimizers. My goal was to create an active alternative to traditional pre-reading/post-reading by having the AI synthesize the lecture recordings, notes, and homework solutions into a cohesive study guide.</paragraph><paragraph>I uploaded the YouTube lecture audio recordings alongside the raw lecture notes. NotebookLM was able to \"listen\" to the lectures and cross-reference them with the written notes.</paragraph><paragraph>One of the coolest features was generating visual aids to structure my learning before diving into the weeds.</paragraph><paragraph><bold>The Mind Map:</bold> I asked it to generate a mind map immediately1. This gave me a hierarchy of concepts\u2014linking Taylor Expansions to Lazy Training, and Initialization to RMS Norms\u2014which acted as a roadmap for what questions I needed to ask.</paragraph><paragraph><bold>The Slideshow:</bold> I asked it to create a slideshow explaining optimizer concepts with visuals. It generated a dense, informed presentation in about 5 minutes. While the visuals were AI-generated, the structure helped me verify if I could explain the concepts linearly.</paragraph><paragraph>Through trial and error, I found a \"Drill-Down\" strategy worked best:</paragraph><paragraph><bold>Start Broad:</bold> I began with, <italic>\"Can you give me an overview of ALL the concepts... generate a roadmap\".</italic> This forced the model to establish context.</paragraph><paragraph><bold>Select &amp; Zoom:</bold> Instead of letting it ramble, I picked specific nodes from its roadmap: <italic>\"Let's start with the lazy training assumption. What is it, and why is it important?\"</italic></paragraph><paragraph><bold>Comparative Analysis:</bold> For distinct lists, I asked for comparative breakdowns: <italic>\"Can you go over every initialization... advantages/disadvantages to each?\"</italic> This yielded structured tables rather than walls of text.</paragraph><paragraph>While the high-level summaries were coherent, the model often flattened subtle mathematical distinctions. I have annotated these in the attached PDF, but here are the key warnings:</paragraph><paragraph><bold>Oversimplifications:</bold> The model correctly identified that parameters move by small amounts in training, but it implied that gradient features are <italic>fixed</italic> features simply because of the assumption. It failed to clarify that this is a specific regime (Kernel/NTK) and not how realistic modern \"feature learning\" regimes work</paragraph><paragraph><bold>Bias Initialization Depth:</bold> The model listed strategies like \"initialize at 0\" or \"0.01\" but completely lacked the reasoning for <italic>why</italic> (e.g., why zero bias doesn't cause symmetry issues like zero weights do)</paragraph><paragraph><bold>Missing Geometric Intuition:</bold> When explaining algorithms like SignSGD, it gave the formula but missed the geometric intuition (that infinity norm constraints lead to coordinate-wise updates)</paragraph><paragraph><bold>Conclusion</bold></paragraph><paragraph>NotebookLM is a powerful \"active reading\" partner, especially for synthesizing multimodal sources (audio + text). However, it struggles with the deep \"why\" behind some mathematical proofs. Overall, I think it's extremely useful for learning large amounts of content in a structured yet quick manner.</paragraph><paragraph/><paragraph>Trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/QFrg7pTp21fLlcSlbWOR77fQ\" filename=\"NotebookLM Chat.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T08:04:24.974898+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426845,
            "author": "Carolyn Liu",
            "project_title": "Special Participation E: ChatGPT 5.1 Thinking Study Mode on LoRA",
            "post_body": "\n\nI used the ChatGPT 5.1 Thinking model under the study mode to have it go over LoRA before I start the homework assignment. It did a good job going over the contents within the lecture and asked questions to ensure my understanding. Under study mode, it continues to ask me questions that are directly related to what was taught in lecture without needing reminders or any prompting that I am studying before completing a homework assignment. I would say the model does a good job at proposing questions that solidify basic understanding but does not increase in difficulty to be like a homework or discussion problem. However, it is able to adapt questions based on how well the user understands the content.\n\nI also gave the same starting responses to the ChatGPT 5.1 Thinking model without study mode. While it did give a few questions, it was less interactive and only listed a few questions in the end after going over the lecture content. Instead of asking for an answer to the questions, it states that if the user has any more questions, they can respond.\n\nFor studying the content, using the model under study mode provides a more interactive experience by continuously asking questions and adapting to how well the user understands the content. However, the regular model is suitable for covering all the content in the lecture.\n\nBelow is an annotated version of the conversations I had with both models:",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph>I used the ChatGPT 5.1 Thinking model under the study mode to have it go over LoRA before I start the homework assignment. It did a good job going over the contents within the lecture and asked questions to ensure my understanding. Under study mode, it continues to ask me questions that are directly related to what was taught in lecture without needing reminders or any prompting that I am studying before completing a homework assignment. I would say the model does a good job at proposing questions that solidify basic understanding but does not increase in difficulty to be like a homework or discussion problem. However, it is able to adapt questions based on how well the user understands the content.</paragraph><paragraph>I also gave the same starting responses to the ChatGPT 5.1 Thinking model without study mode. While it did give a few questions, it was less interactive and only listed a few questions in the end after going over the lecture content. Instead of asking for an answer to the questions, it states that if the user has any more questions, they can respond.</paragraph><paragraph>For studying the content, using the model under study mode provides a more interactive experience by continuously asking questions and adapting to how well the user understands the content. However, the regular model is suitable for covering all the content in the lecture.</paragraph><paragraph>Below is an annotated version of the conversations I had with both models:</paragraph><file url=\"https://static.us.edusercontent.com/files/yYHjwnDCT893e59r2azDoiPT\" filename=\"Special Participation E_ ChatGPT LoRA Learning.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T07:58:50.489794+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426644,
            "author": "Ruihan Xia",
            "project_title": "Special Participation E: GPT diagonistic quiz generator for lectures",
            "post_body": "I wanted to design a self-quiz generator I can use to test how well I understand the knowledge learned after lecture. To do this, I uploaded lecture notes and wrote a prompt that asks Chat GPT to produce 5 conceptual questions from a lecture section, each with an answer, a quick intuition, a formal reasoning layer, and an explicit assumption check.\n\nIn the chat with example of lecture 6 & 7, ChatGPT outputed conceptal questions covering RMS norms, maximal update parametrization, spectral-norm-constrained updates, Muon optimizer, and Newton\u2013Schulz iteration. I then asked for related computational examples that involve concrete math and applications to standard problems like linear regression. \n\nHowever, I noticed a few issues. Several explanations essentially copied equations from the lecture slides (e.g., the RMS\u2192RMS norm formula, the Muon spectral-norm optimization step). I think a bit more derivation would be helpful. There are some vague leaps in response as well. In general Chatgpt outputs answers that are more on the conceptual side unless I ask for explicit math. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I wanted to design a self-quiz generator I can use to test how well I understand the knowledge learned after lecture. To do this, I uploaded lecture notes and wrote a prompt that asks Chat GPT to produce 5 conceptual questions from a lecture section, each with an answer, a quick intuition, a formal reasoning layer, and an explicit assumption check.</paragraph><paragraph>In the chat with example of lecture 6 &amp; 7, ChatGPT outputed conceptal questions covering RMS norms, maximal update parametrization, spectral-norm-constrained updates, Muon optimizer, and Newton\u2013Schulz iteration. I then asked for related computational examples that involve concrete math and applications to standard problems like linear regression. </paragraph><paragraph>However, I noticed a few issues. Several explanations essentially copied equations from the lecture slides (e.g., the RMS\u2192RMS norm formula, the Muon spectral-norm optimization step). I think a bit more derivation would be helpful. There are some vague leaps in response as well. In general Chatgpt outputs answers that are more on the conceptual side unless I ask for explicit math. </paragraph><file url=\"https://static.us.edusercontent.com/files/1ybzCcd297wvzg3FnuxiPneI\" filename=\"AI diagnostic quiz.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T07:31:09.895587+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426623,
            "author": "Vijay Kethanaboyina",
            "project_title": "Special Participation B: Cursor Composer on HW 2 Coding",
            "post_body": "For this assignment, I tackled the coding questions on HW 2 using Cursor Composer, which is a language model specifically designed for software engineering and code generation tasks (released in late October of this year). The IDE I used for this assignment was Cursor, which is the primary way of accessing this model. \n\nThe link to my annotated transcript of my conversation with the model can be found here.\n\nHere is a brief summary of my interactions with the model: it is very strong at code generation  and reasoning. It can definitely one-shot most questions in the homework. However, it is much  less reliable when answers depend on actually seeing plots / running code, and it sometimes overstates what it has \u201clooked at\u201d or \u201crun.\u201d \n\nThe model did very well when the problem was well-specified mathematically and didn\u2019t require actually seeing outputs. Examples:\n\nImplementing SGD+Momentum, RMSProp, Adam in optim.py.\n\nImplementing He initialization, zero initialization, and gradient norm logging.\n\nImplementing gradient ascent step (gd_step), Monte-Carlo smoothing (smoothed_f), finite-difference gradient step, etc.\n\nHowever, when questions explicitly said \u201canswer based on the plot / visualization,\u201d the model often:\n\nAnswered using generic theory rather than the actual plot, even while saying things like:\n\n\u201cReviewing the notebook section with the gradient norm plot\u2026\u201d\n\n\u201cUpdating the answer to match the plot\u2026\u201d\n\nProduced initial answers that didn\u2019t match the actual plots\n\nOnly after being challenged (\u201cAre you sure? That doesn\u2019t match\u2026\u201d) did it admit it had not actually run the code or seen the plot.\n\nTakeaway: although the model capabilities are quite impressive, the outputs still have to be read carefully to make sure it's answering honestly. Additionally, I noticed that it had a tendency to give somewhat verbose responses for conceptual questions, repeating the same idea several times in different language.",
            "content_xml": "<document version=\"2.0\"><paragraph>For this assignment, I tackled the coding questions on HW 2 using <bold><link href=\"https://cursor.com/blog/composer\">Cursor Composer</link>,</bold> which is a language model specifically designed for software engineering and code generation tasks (released in late October of this year). The IDE I used for this assignment was Cursor, which is the primary way of accessing this model. </paragraph><paragraph><bold>The link to my annotated transcript of my conversation with the model can be found <link href=\"https://drive.google.com/file/d/1xaGlMW8-i2mHQMyT8q-30EU57GWMT5dK/view?usp=sharing\">here</link>.</bold></paragraph><paragraph>Here is a brief summary of my interactions with the model: it is very strong at code generation  and reasoning. It can definitely one-shot most questions in the homework. However, it is much  less reliable when answers depend on actually seeing plots / running code, and it sometimes overstates what it has \u201clooked at\u201d or \u201crun.\u201d </paragraph><paragraph>The model did very well when the problem was well-specified mathematically and didn\u2019t require actually seeing outputs. Examples:</paragraph><list style=\"unordered\"><list-item><paragraph>Implementing SGD+Momentum, RMSProp, Adam in <code>optim.py</code>.</paragraph></list-item><list-item><paragraph>Implementing He initialization, zero initialization, and gradient norm logging.</paragraph></list-item><list-item><paragraph>Implementing gradient ascent step (<code>gd_step</code>), Monte-Carlo smoothing (<code>smoothed_f</code>), finite-difference gradient step, etc.</paragraph></list-item></list><paragraph>However, when questions explicitly said \u201canswer based on the plot / visualization,\u201d the model often:</paragraph><list style=\"unordered\"><list-item><paragraph>Answered using generic theory rather than the actual plot, even while saying things like:</paragraph><list style=\"unordered\"><list-item><paragraph>\u201cReviewing the notebook section with the gradient norm plot\u2026\u201d</paragraph></list-item><list-item><paragraph>\u201cUpdating the answer to match the plot\u2026\u201d</paragraph></list-item></list></list-item><list-item><paragraph>Produced initial answers that didn\u2019t match the actual plots</paragraph></list-item><list-item><paragraph>Only after being challenged (\u201cAre you sure? That doesn\u2019t match\u2026\u201d) did it admit it had not actually run the code or seen the plot.</paragraph></list-item></list><paragraph>Takeaway: although the model capabilities are quite impressive, the outputs still have to be read carefully to make sure it's answering honestly. Additionally, I noticed that it had a tendency to give somewhat verbose responses for conceptual questions, repeating the same idea several times in different language.</paragraph></document>",
            "links": [
                "https://cursor.com/blog/composer",
                "https://drive.google.com/file/d/1xaGlMW8-i2mHQMyT8q-30EU57GWMT5dK/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T07:28:32.670497+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426560,
            "author": "Daniel Kao",
            "project_title": "Special Participation A: Deepseek on HW 11",
            "post_body": "Link to the annotated transcript\n\nFor this special participation, I used Deepseek 3.2 in DeepThink mode to solve the written portions of Homework 11. In my initial prompt I gave the model the full context of the entire homework file, and asked it to solve each question one by one, and offered corrections and directions when the LLM got a part wrong. This was not often necessary as Deepseek was able to one-shot the vast majority of the problems here.\n\nMost of the misconceptions or hallucinations in the model are in its numerical calculations and an incorrect assumptions of the givens in a problem. What was interesting was that the Deepseek model seems to actively avoid this sort of numerical calculation error by leaving answers unsimplified, forcing the user to use a deterministic calculation method to compute the final answer. The answers also often displayed notation inconsistencies such as changing variable names. Though the final result was still correct and interpretable, this may not always be the case, especially when the number of variables in a problem is large. \n\nI was particularly intrigued by the model's focus on conciseness, even when given a more abstract question that has a lot of runway for verbosity. This contrasts my experience with other LLMs like Gemini and ChatGPT, which tend to go into great detail about a problem's context and the justification for their answer. This is not to say whether one chatbot is superior, but rather a difference in philosophy: is it better to give an answer that includes more context and detail, to the detriment of interpretability, or is it better to give an answer that is concise and straigthforward, to the detriment of nuance and context?\n\nFinally, I note a mistake I made when prompting the LLM, where I asked for elaboration on the wrong question. Rather than answer the question, the LLM was able to recognize that the question was nonsensical and hypothesized that I had mistyped my question. It then independently reasoned the intended question and answered based on that. To me, this represents a level of resistance to hallucinations, where the model is able to recognize when the framework it is given is faulty, and thus it is best to either not answer, or restructure the question to be well-formed.\n\nLink to LLM trace",
            "content_xml": "<document version=\"2.0\"><paragraph><link href=\"https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing\">Link to the annotated transcript</link></paragraph><paragraph>For this special participation, I used Deepseek 3.2 in DeepThink mode to solve the written portions of Homework 11. In my initial prompt I gave the model the full context of the entire homework file, and asked it to solve each question one by one, and offered corrections and directions when the LLM got a part wrong. This was not often necessary as Deepseek was able to one-shot the vast majority of the problems here.</paragraph><paragraph>Most of the misconceptions or hallucinations in the model are in its numerical calculations and an incorrect assumptions of the givens in a problem. What was interesting was that the Deepseek model seems to actively avoid this sort of numerical calculation error by leaving answers unsimplified, forcing the user to use a deterministic calculation method to compute the final answer. The answers also often displayed notation inconsistencies such as changing variable names. Though the final result was still correct and interpretable, this may not always be the case, especially when the number of variables in a problem is large. </paragraph><paragraph>I was particularly intrigued by the model's focus on conciseness, even when given a more abstract question that has a lot of runway for verbosity. This contrasts my experience with other LLMs like Gemini and ChatGPT, which tend to go into great detail about a problem's context and the justification for their answer. This is not to say whether one chatbot is superior, but rather a difference in philosophy: is it better to give an answer that includes more context and detail, to the detriment of interpretability, or is it better to give an answer that is concise and straigthforward, to the detriment of nuance and context?</paragraph><paragraph>Finally, I note a mistake I made when prompting the LLM, where I asked for elaboration on the wrong question. Rather than answer the question, the LLM was able to recognize that the question was nonsensical and hypothesized that I had mistyped my question. It then independently reasoned the intended question and answered based on that. To me, this represents a level of resistance to hallucinations, where the model is able to recognize when the framework it is given is faulty, and thus it is best to either not answer, or restructure the question to be well-formed.</paragraph><paragraph><link href=\"https://chat.deepseek.com/share/rmzugw2oy792hur0mi\">Link to LLM trace</link></paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/15xrxl-aVIQQQ-fi-atYCIqujloFNIf5T/view?usp=sharing",
                "https://chat.deepseek.com/share/rmzugw2oy792hur0mi"
            ],
            "attachments": [],
            "created_at": "2025-12-08T07:19:06.910022+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426411,
            "author": "Jin Ying",
            "project_title": "Special Participation E: Claude Sonnet 4.5 on Diffusion Models",
            "post_body": "I created a Socratic dialogue tool using Claude to learn diffusion models. I gave it our lecture slides; instead of asking it to summarize slides, I explicitly told it to ask me one question at a time, ignore lecture materials temporarily so I can return to it to test my understanding after the conversation, and focus on building intuition rather than explaining formulas.\n\nClaude helped build intuition before introducing math, handled my confusion productively, and when I gave wrong answers it showed me why my reasoning was incomplete. However, it made several technical oversimplifications that need verification, which I annotated in our conversation.\n\nI found it helpful to spend an hour in Socratic dialogue to build intuition and test myself to see if I only understand something partially, annotate anything that seems off, and then verify against what we've learned in lectures and the textbook. This helped me go from memorizing and taking what we learned in the class for granted to actually understanding concepts deeper. But it's not a replacement for rigorous study; I still need to work through derivations and implementations myself to achieve a real understanding.",
            "content_xml": "<document version=\"2.0\"><paragraph>I created a Socratic dialogue tool using Claude to learn diffusion models. I gave it our lecture slides; instead of asking it to summarize slides, I explicitly told it to ask me one question at a time, ignore lecture materials temporarily so I can return to it to test my understanding after the conversation, and focus on building intuition rather than explaining formulas.</paragraph><paragraph>Claude helped build intuition before introducing math, handled my confusion productively, and when I gave wrong answers it showed me why my reasoning was incomplete. However, it made several technical oversimplifications that need verification, which I annotated in our conversation.</paragraph><paragraph>I found it helpful to spend an hour in Socratic dialogue to build intuition and test myself to see if I only understand something partially, annotate anything that seems off, and then verify against what we've learned in lectures and the textbook. This helped me go from memorizing and taking what we learned in the class for granted to actually understanding concepts deeper. But it's not a replacement for rigorous study; I still need to work through derivations and implementations myself to achieve a real understanding.</paragraph><file url=\"https://static.us.edusercontent.com/files/JX86S4sPR4eF5GEeYVpRULOp\" filename=\"Diffusion Models with Claude.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T06:58:46.886153+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7426337,
            "author": "Mishty Dhekial",
            "project_title": "Special Participation B: HW4 with Windsurf",
            "post_body": "I used Windsurf to solve the coding portions of HW 4. It did pretty well, especially on the Designing Hand Filters notebook, given its simplicity. The Edge Detectors notebook needed a bit more prompting from me, but overall still performed well!\n\nQ5: Windsurf successfully one-shotted both parts of the problem. This makes sense given the simplicity of the problems.\n\nQ6: This question required a bit more prompting from me to solve. It was able to properly understand the parameters for the dataset loader, and fill out the initial questions. The only issue it ran to was initially defining $num_workers=2$, but was quickly able to correct the value to 0 when I passed in the error. For the rest of the questions that required hyperparameter tuning, Windsurf required some more help from me. I asked Windsurf to provide me ifferent configurations and sometimes it would provide some configurations that actually performed poorer than the ones it had provided prior. Also, at times, it would try to go offer recommendations past the three parameters we were meant to edit; for example, it tried to introduce weight decay or gradient clipping. However, and for the last question of training the Wide CNN, it was able to provide a configuration in one shot that performed at ~94%. \n\n\n\nOverall, I would say that Windsurf was able to successfully complete both coding parts of this homework with a bit of prompting from my end.\n\n\n\nAttached is the annotated trace for both notebooks:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Windsurf to solve the coding portions of HW 4. It did pretty well, especially on the Designing Hand Filters notebook, given its simplicity. The Edge Detectors notebook needed a bit more prompting from me, but overall still performed well!</paragraph><paragraph>Q5: Windsurf successfully one-shotted both parts of the problem. This makes sense given the simplicity of the problems.</paragraph><file url=\"https://static.us.edusercontent.com/files/2ma17kQMwTZNjagWFel5aSxq\" filename=\"HandDesignFilters-new.ipynb\"/><paragraph>Q6: This question required a bit more prompting from me to solve. It was able to properly understand the parameters for the dataset loader, and fill out the initial questions. The only issue it ran to was initially defining $num_workers=2$, but was quickly able to correct the value to 0 when I passed in the error. For the rest of the questions that required hyperparameter tuning, Windsurf required some more help from me. I asked Windsurf to provide me ifferent configurations and sometimes it would provide some configurations that actually performed poorer than the ones it had provided prior. Also, at times, it would try to go offer recommendations past the three parameters we were meant to edit; for example, it tried to introduce weight decay or gradient clipping. However, and for the last question of training the Wide CNN, it was able to provide a configuration in one shot that performed at ~94%. </paragraph><file url=\"https://static.us.edusercontent.com/files/KWLBCtFvuv3RBnyLXufRjrGj\" filename=\"edge_detection-new.ipynb\"/><paragraph/><paragraph>Overall, I would say that Windsurf was able to successfully complete both coding parts of this homework with a bit of prompting from my end.</paragraph><paragraph/><paragraph>Attached is the annotated trace for both notebooks:</paragraph><file url=\"https://static.us.edusercontent.com/files/g4PQGUoVTeknfFB43IS0F7dV\" filename=\"annotated-special-participation-b.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T06:45:47.451054+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425579,
            "author": "Moxin Tang",
            "project_title": "Special Participation E: Module Review Generator with Multi-Source Integration",
            "post_body": "I extended Jameson's lecture notes generator #301 to create a comprehensive module review tool.\n\nBuilding on the original script's approach of using lecture transcripts + slides with Gemini API, I created module review generator that:\n\nCombines multiple sources: Integrates lecture notes, discussion solutions, and homework problems into a single comprehensive review document\n\nSynthesizes content by topic: Uses Gemini API to organize material thematically rather than chronologically, making it better for exam prep\n\nIncludes source citations: Each concept/problem is tagged with its source (e.g., Lecture 24, Discussion 12 Problem2a, HW 10 Problem1, ...) for easy reference\n\nThe script is easily configurable to combine any set of lectures, discussions, and homeworks for different modules.\n\nExample usage:\n\nI combines Lectures 24-27 + Discussions 12-13 + HW 10, 11, 13 into one study guide which is about generative models and post-training.\n\nsample output in \n\nI think the review successfully integrates multiple sources, creating meaningful connections between lectures and the accompanying discussions and homework problems.\n\n\nannotated transcript:\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I extended Jameson's lecture notes generator #301 to create a comprehensive module review tool.</paragraph><paragraph>Building on the original script's approach of using lecture transcripts + slides with Gemini API, I created module review generator that:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>Combines multiple sources</bold>: Integrates lecture notes, discussion solutions, and homework problems into a single comprehensive review document</paragraph></list-item><list-item><paragraph><bold>Synthesizes content by topic</bold>: Uses Gemini API to organize material thematically rather than chronologically, making it better for exam prep</paragraph></list-item><list-item><paragraph><bold>Includes source citations</bold>: Each concept/problem is tagged with its source (e.g., <italic>Lecture 24</italic>, <italic>Discussion 12 Problem2a</italic>, <italic>HW 10 Problem1, ...</italic>) for easy reference</paragraph></list-item></list><paragraph>The script is easily configurable to combine any set of lectures, discussions, and homeworks for different modules.</paragraph><file url=\"https://static.us.edusercontent.com/files/nyMA5YPN5fZmDF5CrF7AOzwx\" filename=\"module_review_generator.py\"/><paragraph><bold>Example usage:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>I combines Lectures 24-27 + Discussions 12-13 + HW 10, 11, 13 into one study guide which is about generative models and post-training.</paragraph></list-item><list-item><paragraph>sample output in </paragraph></list-item></list><file url=\"https://static.us.edusercontent.com/files/SXST8e0lnZ6JVZRd1QMWRnWP\" filename=\"module_24-27_comprehensive_review.tex\"/><paragraph>I think the review successfully integrates multiple sources, creating meaningful connections between lectures and the accompanying discussions and homework problems.</paragraph><paragraph><break/>annotated transcript:</paragraph><file url=\"https://static.us.edusercontent.com/files/oqSINuasbiLX7tolCEz7yC9G\" filename=\"annotated_module_review_24-27.pdf\"/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T04:40:18.366087+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425406,
            "author": "Vongani Maluleke",
            "project_title": "Special ParticipationC: HW3 Code Refactoring - Genmini",
            "post_body": "\n\nExecutive Summary\n\nThis report details the refactoring changes applied to HW3 code using Gemini for assistance. The goal was to enhance the code\u2019s readability, type safety, and Pythonic style while strictly\npreserving the existing functionality and flow required for the educational context.\n\n\n\nNote: There was no spreadsheet for the deconfliction of Part C, so I didn't realize this was already done. I did check if it was done, but I now see that I missed the post.",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/Gbd30nXQI99WcAhYrf6lx7AZ\" filename=\"CS282A_HW3_Code_Refactoring.pdf\"/><paragraph><bold>Executive Summary</bold></paragraph><paragraph>This report details the refactoring changes applied to HW3 code using Gemini for assistance. The goal was to enhance the code\u2019s readability, type safety, and Pythonic style while strictly<break/>preserving the existing functionality and flow required for the educational context.</paragraph><paragraph/><paragraph><italic>Note: There was no spreadsheet for the deconfliction of Part C, so I didn't realize this was already done. I did check if it was done, but I now see that I missed the post.</italic></paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T04:07:09.14044+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425355,
            "author": "Vongani Maluleke",
            "project_title": "Special Participation B: Gemini on Colab HW2",
            "post_body": "I found Gemini to be an effective and intuitive coding assistant while working in Google Colab. For the majority of my questions, I was able to get a quick, accurate, and short coding answer immediately. However, for a few more challenging problems (about three), I did need to provide hints and slowly guide the model to the final solution. A significant benefit was the quality of explanation it provided; the code was always explained intuitively. Furthermore, when I tested its conceptual understanding by asking reasoning questions, it consistently provided the correct answer, even when the question was abstract, and I hadn't provided any related images. Overall, I believe Gemini is a valuable tool for both rapid development and deepening technical understanding in a notebook environment. However, I am not a fan of the interface. I created widgets so that the prompts were visible alongside the questions and answers.",
            "content_xml": "<document version=\"2.0\"><paragraph>I found Gemini to be an effective and intuitive coding assistant while working in Google Colab. For the majority of my questions, I was able to get a quick, accurate, and short coding answer immediately. However, for a few more challenging problems (about three), I did need to provide hints and slowly guide the model to the final solution. A significant benefit was the quality of explanation it provided; the code was always explained intuitively. Furthermore, when I tested its conceptual understanding by asking reasoning questions, it consistently provided the correct answer, even when the question was abstract, and I hadn't provided any related images. Overall, I believe Gemini is a valuable tool for both rapid development and deepening technical understanding in a notebook environment. However, I am not a fan of the interface. I created widgets so that the prompts were visible alongside the questions and answers.</paragraph><file url=\"https://static.us.edusercontent.com/files/TYOVf2JEMrm7fSMlIsiqtZYr\" filename=\"Hw2_Gemini_Colab_annotated.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T03:52:19.837971+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425318,
            "author": "Vongani Maluleke",
            "project_title": "Special Participation C: HW9 Code Refactored",
            "post_body": "\n\nExecutive Summary\n\nThis report documents the refactoring of HW9 Python code for visualizing attention in Transformer models using BertViz into an object-oriented structure using the BertVizExplorer class. The refactoring aims to improve code organization, reusability, and state management while maintaining the original flow of the homework.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/wCchTBuA8Ob6221mU0Xf9Hl2\" filename=\"HW9_Visualizing_BERT_refactored.pdf\"/><paragraph>Executive Summary</paragraph><paragraph>This report documents the refactoring of HW9 Python code for visualizing attention in Transformer models using BertViz into an object-oriented structure using the BertVizExplorer class. The refactoring aims to improve code organization, reusability, and state management while maintaining the original flow of the homework.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T03:42:50.465542+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425160,
            "author": "Sriram Srivatsan",
            "project_title": "Special Participation B: Getting GPT 5.1 to do Homework 7 Coding",
            "post_body": "I got GPT 5.1 to get homework 7 working. In short, GPT 5.1 pretty much one shotted the actual coding parts, minus one small issue where it calculated MSE wrong. However after two followups to this MSE prompt, it got it correct without me specifically mentioning the issue. \n\nI did notice that though it was good at doing the work, it doesn't look around the code to fix potential issues, for example, the code currently had .cuda(), but I wanted it to work with mps. I had to specifically prompt it to fix that issue, though I would have hoped it did that in the first place.\n\nHere is the writeup:\n\n\n\nHere is the chat history:\n\n\n\nHere are the solutions it provided:\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I got GPT 5.1 to get homework 7 working. In short, GPT 5.1 pretty much one shotted the actual coding parts, minus one small issue where it calculated MSE wrong. However after two followups to this MSE prompt, it got it correct without me specifically mentioning the issue. <break/><break/>I did notice that though it was good at doing the work, it doesn't look around the code to fix potential issues, for example, the code currently had .cuda(), but I wanted it to work with mps. I had to specifically prompt it to fix that issue, though I would have hoped it did that in the first place.<break/><break/>Here is the writeup:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/Qh5XLh9BN57v11Jmq4ufw6bD\" filename=\"writeup.md\"/><paragraph><break/>Here is the chat history:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/MEgQBuXnrdboXhCMdSQfsOyi\" filename=\"CHAT.md\"/><paragraph><break/>Here are the solutions it provided:</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/7sD4GWK8rrrRRnLyQkM138z9\" filename=\"Solutions.zip\"/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T02:52:14.262122+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425043,
            "author": "Ayush Goel",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 0",
            "post_body": "I ran the HW 0 coding through Gemini 3 Pro. The attached pdf has the transcription of the chat with gemini.\n\nGemini 3 pro was able to one-shot every coding problem. For most of the problems, I wasn't too surprised as they were basic forward and backward passes for an MLP with relu as the only activation function. \n\nHere are some things I found interesting:\n\n1. Gemini 3 Pro was automatically suggesting implementing the next function. For example, I asked it to implement affine_forward and it prompted to implement affine_backward. I even used it's prompt suggestion in a separate chat and saw it created the new function aligned with the signatures as the homework. I was very impressed that Gemini 3 Pro was able to pick up the coding conventions very quickly and new what to implement next. I was most surprised when it suggested to implement the svm_loss and softmax_loss.\n2. Gemini 3 Pro was very smart with the hyperparameters. For all 3 problems, I had to do some hyperparameter tuning when I tried it myself. Gemini came up with hyperparameters that were extremely close to the ones I used when I trained them manually just given a prompt to overfit to the training data, etc. To me, this suggests that gemini has seen a lot of work on hyperparameters for this particular dataset and so was able to perform well.\n\n3. Gemini gave detailed comments explaining every line of code. I think this also contributed to the clarity and correctness of the code gemini produced.",
            "content_xml": "<document version=\"2.0\"><paragraph>I ran the HW 0 coding through Gemini 3 Pro. The attached pdf has the transcription of the chat with gemini.</paragraph><file url=\"https://static.us.edusercontent.com/files/ssiwt28oDa4xM9nn0lqu6Yet\" filename=\"hw_0_code_gemini_3_pro.pdf\"/><paragraph>Gemini 3 pro was able to one-shot every coding problem. For most of the problems, I wasn't too surprised as they were basic forward and backward passes for an MLP with relu as the only activation function. </paragraph><paragraph>Here are some things I found interesting:</paragraph><paragraph>1. Gemini 3 Pro was automatically suggesting implementing the next function. For example, I asked it to implement <code>affine_forward</code> and it prompted to implement <code>affine_backward</code>. I even used it's prompt suggestion in a separate chat and saw it created the new function aligned with the signatures as the homework. I was very impressed that Gemini 3 Pro was able to pick up the coding conventions very quickly and new what to implement next. I was most surprised when it suggested to implement the svm_loss and softmax_loss.<break/>2. Gemini 3 Pro was very smart with the hyperparameters. For all 3 problems, I had to do some hyperparameter tuning when I tried it myself. Gemini came up with hyperparameters that were extremely close to the ones I used when I trained them manually just given a prompt to overfit to the training data, etc. To me, this suggests that gemini has seen a lot of work on hyperparameters for this particular dataset and so was able to perform well.</paragraph><paragraph>3. Gemini gave detailed comments explaining every line of code. I think this also contributed to the clarity and correctness of the code gemini produced.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T01:57:15.102336+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7425035,
            "author": "Sriram Srivatsan",
            "project_title": "Special Participation A: Getting GPT 5.1 to answer Homework 12",
            "post_body": "I got OpenAI's GPT 5.1 model to answer questions 1, 2, and 4 in homework 12. Overall, it seems that this model is able to answer questions about the material extremely accurately, and sometimes it even notes that specific ambiguities exist within the questions themselves. This is quite interesting, because it solved parts of question 2 that I wasn't able to approach myself without getting guidance from some of its answers.\n\nHere is the writeup: \n\nHere is the chat history: \n\nAnd the solutions and ambiguities it noted in for the homework:",
            "content_xml": "<document version=\"2.0\"><paragraph>I got OpenAI's GPT 5.1 model to answer questions 1, 2, and 4 in homework 12. Overall, it seems that this model is able to answer questions about the material extremely accurately, and sometimes it even notes that specific ambiguities exist within the questions themselves. This is quite interesting, because it solved parts of question 2 that I wasn't able to approach myself without getting guidance from some of its answers.<break/><break/>Here is the writeup: </paragraph><file url=\"https://static.us.edusercontent.com/files/57y0yD5lukQAxYWs5ZJMrzcb\" filename=\"writeup.md\"/><paragraph>Here is the chat history: </paragraph><file url=\"https://static.us.edusercontent.com/files/i2R7E2a0J598pBkGrJb3z3zq\" filename=\"chat_history.md\"/><paragraph>And the solutions and ambiguities it noted in for the homework:</paragraph><file url=\"https://static.us.edusercontent.com/files/T8TFpr18rLhe8gj4f8YF4Jca\" filename=\"problem1sol.md\"/><file url=\"https://static.us.edusercontent.com/files/epB9ecUvpCN1OYsnatwUlV4V\" filename=\"problem2sol.md\"/><file url=\"https://static.us.edusercontent.com/files/quqiYIkhlmIgaebW8pt0iAlu\" filename=\"problem4sol.md\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T01:52:09.45735+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424926,
            "author": "Tyler Pham",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 9 Coding",
            "post_body": "I used Gemini 3 Pro for the coding problem #6 of HW 9. This coding problem is not a typical HW coding problem because it only involves purely conceptual questions about the notebook\u2019s attention visualizations, rather than asking to actually code anything. So, I ran the notebook and attached it as a PDF for Gemini. I then asked it question-by-question, but it had to make guesses about what I would observe since there would be far too many photos to show of all attention layers and heads to Gemini. I compared its analysis of what I would see to what I actually observed in the notebook. Given that basic attention visualization has been widely studied before, Gemini\u2019s analysis was pretty spot-on despite not having seen all visualizations from the notebook.\n\nObservations\n\nGemini 3 Pro correctly one-shot answered every conceptual coding question (a-d) on the first attempt without requiring more prompting or correction. It was correct about the autoregressive vs bidirectional visualizations and even interpretability patterns with the CLS tokens. It had pretty accurate predictions about what I would be seeing in the notebook\u2019s visualizations.\n\nGemini 3 Pro\u2019s responses are concise in a good way, yet still goes beyond basic interpretability and even mentions backprop to explain gradient behavior for the last question, which is quite helpful as it also explains \u201cwhy\u201d and not just \u201cwhat\u201d.\n\nThe model was only slightly wrong about 5(d)(i) where it expected the untrained weights to look \u201cchaotic\u201d like a \u201cmessy web\u201d but in reality the visualization showed the words with equal attention to the other words.\n\nFrom the notebook PDF, Gemini could still see some visualizations (like for Layer 0, Head 0). It was able to see the lines and interpret those to explain autoregressiveness vs bidirectionality, which is pretty impressive computer vision skill.\n\nHere's my annotated chat:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini 3 Pro for the coding problem #6 of HW 9. This coding problem is not a typical HW coding problem because it only involves purely conceptual questions about the notebook\u2019s attention visualizations, rather than asking to actually code anything. So, I ran the notebook and attached it as a PDF for Gemini. I then asked it question-by-question, but it had to make guesses about what I would observe since there would be far too many photos to show of all attention layers and heads to Gemini. I compared its analysis of what I would see to what I actually observed in the notebook. Given that basic attention visualization has been widely studied before, Gemini\u2019s analysis was pretty spot-on despite not having seen all visualizations from the notebook.</paragraph><paragraph><bold>Observations</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Gemini 3 Pro correctly one-shot answered every conceptual coding question (a-d) on the first attempt without requiring more prompting or correction. It was correct about the autoregressive vs bidirectional visualizations and even interpretability patterns with the CLS tokens. It had pretty accurate predictions about what I would be seeing in the notebook\u2019s visualizations.</paragraph></list-item><list-item><paragraph>Gemini 3 Pro\u2019s responses are concise in a good way, yet still goes beyond basic interpretability and even mentions backprop to explain gradient behavior for the last question, which is quite helpful as it also explains \u201cwhy\u201d and not just \u201cwhat\u201d.</paragraph></list-item><list-item><paragraph>The model was only slightly wrong about 5(d)(i) where it expected the untrained weights to look \u201cchaotic\u201d like a \u201cmessy web\u201d but in reality the visualization showed the words with equal attention to the other words.</paragraph></list-item><list-item><paragraph>From the notebook PDF, Gemini could still see some visualizations (like for Layer 0, Head 0). It was able to see the lines and interpret those to explain autoregressiveness vs bidirectionality, which is pretty impressive computer vision skill.</paragraph></list-item></list><paragraph>Here's my annotated chat:</paragraph><file url=\"https://static.us.edusercontent.com/files/csuWiy5e2Y6liFrJMMfsJ04V\" filename=\"gemini-chat.2025-12-07.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T00:12:29.462292+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424922,
            "author": "Natalie Wei",
            "project_title": "Special Participation A: Mistral on HW8",
            "post_body": "Overview\n\nI worked with Mistral\u2019s Le Chat to complete the written questions in Homework 8. First, I gave Mistral a set of rules to follow:\n\n1. Read the question and restate it in your own words\n\n2. Provide a step-by-step explanation of the solution\n\n3. Point out any uncertainties or room for error with your final solution\n\nTo avoid exceeding the model\u2019s context window, I copy-pasted each question as a separate prompt. When Mistral produced an incorrect answer, I attempted to guide it by offering a hint from the approach taken in the staff solution. If it still failed to correct itself, I gave it the staff solution and asked it to explain both why its original answer was wrong and why the solution was correct. Although Mistral was able to explain why the solution worked, I do not feel that it was able to explain its own mistakes. It restated that it was wrong and explained what concepts it missed, but it failed to explain why it missed those concepts to begin with (confusing problem statement, faulty assumptions, etc.). \n\nMistral performed well on the computational and mathematical questions, solving all of them on the first attempt, but it struggled with the more conceptual problems. In particular, it had difficulty with complex time complexity questions: while it could reliably deduce time complexity from a given, straightforward formula, it struggled at analyzing how an algorithm\u2019s time complexity might be improved. Mistral also had trouble changing its approach once it committed to an incorrect approach. Even when given specific hints, and even the staff solution in some cases, it continued to respond incorrectly, giving either the same answer or a different incorrect answer. Additionally, although I asked it to identify sources of error in its solutions, it interpreted this as describing mistakes a human might make as opposed to weaknesses in its own work. This made it challenging to identify why the model was struggling and give it appropriate hints. \n\nOverall, the one-shot approach yielded approximately 73.6% accuracy across all of the subproblems; note that I was using the free model and paid models may perform better. I would recommend Mistral for simpler computational tasks, but perhaps not complex conceptual questions that require more reasoning.\n\nAnnotated Logs\n\nQ1\n\nQ3\n\nQ4",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Overview</bold></paragraph><paragraph>I worked with Mistral\u2019s Le Chat to complete the written questions in Homework 8. First, I gave Mistral a set of rules to follow:</paragraph><paragraph>1. Read the question and restate it in your own words</paragraph><paragraph>2. Provide a step-by-step explanation of the solution</paragraph><paragraph>3. Point out any uncertainties or room for error with your final solution</paragraph><paragraph>To avoid exceeding the model\u2019s context window, I copy-pasted each question as a separate prompt. When Mistral produced an incorrect answer, I attempted to guide it by offering a hint from the approach taken in the staff solution. If it still failed to correct itself, I gave it the staff solution and asked it to explain both why its original answer was wrong and why the solution was correct. Although Mistral was able to explain why the solution worked, I do not feel that it was able to explain its own mistakes. It restated that it was wrong and explained what concepts it missed, but it failed to explain why it missed those concepts to begin with (confusing problem statement, faulty assumptions, etc.). </paragraph><paragraph>Mistral performed well on the computational and mathematical questions, solving all of them on the first attempt, but it struggled with the more conceptual problems. In particular, it had difficulty with complex time complexity questions: while it could reliably deduce time complexity from a given, straightforward formula, it struggled at analyzing how an algorithm\u2019s time complexity might be improved. Mistral also had trouble changing its approach once it committed to an incorrect approach. Even when given specific hints, and even the staff solution in some cases, it continued to respond incorrectly, giving either the same answer or a different incorrect answer. Additionally, although I asked it to identify sources of error in its solutions, it interpreted this as describing mistakes a human might make as opposed to weaknesses in its own work. This made it challenging to identify why the model was struggling and give it appropriate hints. </paragraph><paragraph>Overall, the one-shot approach yielded approximately 73.6% accuracy across all of the subproblems; note that I was using the free model and paid models may perform better. I would recommend Mistral for simpler computational tasks, but perhaps not complex conceptual questions that require more reasoning.</paragraph><paragraph><bold>Annotated Logs</bold></paragraph><paragraph><link href=\"https://drive.google.com/file/d/1xp2Be1rVcw42UysIErTOnEjVFpwhgoY1/view?usp=sharing\">Q1</link></paragraph><paragraph><link href=\"https://drive.google.com/file/d/1bvrsliSObDIRLgt0CUQbykqI7u33RQ3D/view?usp=sharing\">Q3</link></paragraph><paragraph><link href=\"https://drive.google.com/file/d/1Ts2CjjRaKTkI3QjBJlajUEH7-nYNxkvX/view?usp=sharing\">Q4</link></paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1xp2Be1rVcw42UysIErTOnEjVFpwhgoY1/view?usp=sharing",
                "https://drive.google.com/file/d/1bvrsliSObDIRLgt0CUQbykqI7u33RQ3D/view?usp=sharing",
                "https://drive.google.com/file/d/1Ts2CjjRaKTkI3QjBJlajUEH7-nYNxkvX/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-08T00:09:44.530492+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424917,
            "author": "Neil Pattanaik",
            "project_title": "Special Participation E: Tensor Norm Visualization Tool",
            "post_body": "Using Gemini's Canvas mode, Gemini was able to build this neat interactive visualization of the various tensor norms we discussed in the context of CNNs. It can be difficult to imagine and remember which axes are normalized over for each norm, which inspired me to create this tool. It displays 4 axes (H, W, C, Batch) and has both a 2d and 3d view of BatchNorm, RMS + LayerNorm, and InstanceNorm.\n\n\n\nhttps://gemini.google.com/share/57ed79731305\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Using Gemini's Canvas mode, Gemini was able to build this neat interactive visualization of the various tensor norms we discussed in the context of CNNs. It can be difficult to imagine and remember which axes are normalized over for each norm, which inspired me to create this tool. It displays 4 axes (H, W, C, Batch) and has both a 2d and 3d view of BatchNorm, RMS + LayerNorm, and InstanceNorm.</paragraph><paragraph/><paragraph>https://gemini.google.com/share/57ed79731305</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-08T00:04:37.045755+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424852,
            "author": "Tyler Pham",
            "project_title": "Special Participation A: DeepSeek-V3.2 on HW9 Non-Coding",
            "post_body": "I used DeepSeek-V3.2 without DeepThink mode (this was released Dec 1, and is not the same as DeepSeek-V3.2-Exp) on HW9 (Non-Coding). Overall, DeepSeek was able to one-shot most of the problems even without DeepThink mode. Part of the reason why could be that this HW is on attention, which is extremely well-studied on the internet and so it probably has a rich understanding of it in its training data. \n\nOther interesting observations:\n\nI found that asking it to restate the problem was very helpful in preventing hallucinations, as I could easily verify any small errors like wrong superscripts or notation.\n\nAs DeepSeek was solving its problems, on many occasions it double-checked its response despite not being in Thinking Mode, helping it answer more accurately. (e.g. In it's own response, it asks itself: \"Possibly adjust d_k if needed? No, [...]\")\n\nEspecially when i prompted the model to fix a certain answer, it asked many questions to itself to check that it's work was right rather than keep going on blindly.\n\nIt was able to almost perfectly one-shot all problems but Q6 without me even having to split the prompts into separate parts for (a), (b), (c), and so on. There were occasionally very minor errors, but the only concerning moment was when the model misread a matrix in Q2 as 3x3 instead of 3x4 when restating the problem. However, as it was generating its output, the model actually realized that the 4th column probably belonged to the matrix and fixed it as if it was thinking. Q6 was the most challenging and longest one, so I had to split the prompts for that into different sections.\n\nWhen prompting it to solve an entire problem with multiple sections, it often is more verbose and shows more work for earlier parts of the question but shows less work for later parts of the question. In my observation, this only happens when its answer is getting really long.\n\n\n\nAnnotated Chat:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used DeepSeek-V3.2 without DeepThink mode (this was released Dec 1, and is not the same as DeepSeek-V3.2-Exp) on HW9 (Non-Coding). Overall, DeepSeek was able to one-shot most of the problems even without DeepThink mode. Part of the reason why could be that this HW is on attention, which is extremely well-studied on the internet and so it probably has a rich understanding of it in its training data. </paragraph><paragraph>Other interesting observations:</paragraph><list style=\"bullet\"><list-item><paragraph>I found that asking it to restate the problem was very helpful in preventing hallucinations, as I could easily verify any small errors like wrong superscripts or notation.</paragraph></list-item><list-item><paragraph>As DeepSeek was solving its problems, on many occasions it double-checked its response despite not being in Thinking Mode, helping it answer more accurately. (e.g. In it's own response, it asks itself: \"Possibly adjust <code>d_k</code> if needed? No, [...]\")</paragraph><list style=\"bullet\"><list-item><paragraph>Especially when i prompted the model to fix a certain answer, it asked many questions to itself to check that it's work was right rather than keep going on blindly.</paragraph></list-item></list></list-item><list-item><paragraph>It was able to almost perfectly one-shot all problems but Q6 without me even having to split the prompts into separate parts for (a), (b), (c), and so on. There were occasionally very minor errors, but the only concerning moment was when the model misread a matrix in Q2 as 3x3 instead of 3x4 when restating the problem. However, as it was generating its output, the model actually realized that the 4th column probably belonged to the matrix and fixed it as if it was thinking. Q6 was the most challenging and longest one, so I had to split the prompts for that into different sections.</paragraph></list-item><list-item><paragraph>When prompting it to solve an entire problem with multiple sections, it often is more verbose and shows more work for earlier parts of the question but shows less work for later parts of the question. In my observation, this only happens when its answer is getting really long.</paragraph></list-item></list><paragraph/><paragraph>Annotated Chat:</paragraph><file url=\"https://static.us.edusercontent.com/files/oYP0heguvkXurz0oibJlobOx\" filename=\"deepseek-chat.2025-12-07.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T22:31:02.719522+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424843,
            "author": "Krish Yadav",
            "project_title": "Special Participation E: Analytical Failure Modes for Optimizer Intuition via Gemini Pro 3",
            "post_body": "While thinking more carefully about optimizers in CS 182, I became curious about  why particular optimizers exist, beyond just memorizing their update rules. In particular, I wanted to understand them as responses to specific geometric or scaling failures, rather than as isolated algorithms. To explore this, I used Gemini Pro 3 to construct a sequence of minimal, analytic failure modes for optimizers covered in class.\n\nI prompted the model to generate short, symbolic training scenarios (no code, no datasets) where a given optimizer behaves pathologically. For each scenario, it first asked me to predict what would go wrong, and only afterward explained what failed, which assumption was violated, and which optimizer or idea was introduced to address that failure. The scenarios included:\n\nill-conditioning in gradient descent\n\nthe noise floor in SGD\n\nfailure of L2 regularization in Adam vs. AdamW\n\nwidth-dependent instability motivating \u03bcP\n\nand the lazy (NTK) training regime\n\nThis approach was effective for connecting lecture material across geometry, scaling laws, and optimizer design, and for separating formal guarantees from heuristic intuition. I added annotations highlighting where the explanations aligned closely with lecture material and where the model relied more on intuition or standard deep learning lore.\n\nOverall, this worked well as an AI-assisted alternative to post-lecture reading, helping build optimizer intuition through concrete failure cases while still requiring active oversight.\n\nFor more, look at the annotated PDF.",
            "content_xml": "<document version=\"2.0\"><paragraph>While thinking more carefully about optimizers in CS 182, I became curious about  why particular optimizers exist, beyond just memorizing their update rules. In particular, I wanted to understand them as responses to specific geometric or scaling failures, rather than as isolated algorithms. To explore this, I used Gemini Pro 3 to construct a sequence of minimal, analytic failure modes for optimizers covered in class.</paragraph><paragraph>I prompted the model to generate short, symbolic training scenarios (no code, no datasets) where a given optimizer behaves pathologically. For each scenario, it first asked me to predict what would go wrong, and only afterward explained what failed, which assumption was violated, and which optimizer or idea was introduced to address that failure. The scenarios included:</paragraph><list style=\"bullet\"><list-item><paragraph>ill-conditioning in gradient descent</paragraph></list-item><list-item><paragraph>the noise floor in SGD</paragraph></list-item><list-item><paragraph>failure of L2 regularization in Adam vs. AdamW</paragraph></list-item><list-item><paragraph>width-dependent instability motivating \u03bcP</paragraph></list-item><list-item><paragraph>and the lazy (NTK) training regime</paragraph></list-item></list><paragraph>This approach was effective for connecting lecture material across geometry, scaling laws, and optimizer design, and for separating formal guarantees from heuristic intuition. I added annotations highlighting where the explanations aligned closely with lecture material and where the model relied more on intuition or standard deep learning lore.</paragraph><paragraph>Overall, this worked well as an AI-assisted alternative to post-lecture reading, helping build optimizer intuition through concrete failure cases while still requiring active oversight.</paragraph><paragraph>For more, look at the annotated PDF.</paragraph><file url=\"https://static.us.edusercontent.com/files/dWuLm2KWAtuO5b3BE8KghxCC\" filename=\"annotated-gp3-optimizer-failure.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T22:17:49.811521+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424838,
            "author": "Nikhil Mathihalli",
            "project_title": "Special Participation E: Learning CNNs by Building with Cursor",
            "post_body": "Concept: Convolutional Neural Networks (CNNs) & Kernel Filters\n\nTool Used: Cursor (AI Code Editor)\n\nArtifact: The Kernel Whisperer (GitHub Repo)\n\nThe Goal: I found the recent lectures on ConvNets interesting, but I wanted to move beyond static slides. I decided to treat the AI not just as a tutor, but as a \"junior developer\" to help me build an interactive learning tool. The goal was to learn about filters (blur, edge detection, sharpen) by actually applying them to images in code, effectively replacing my standard post-lecture review with an active engineering project.\n\nThe Approach: I used Cursor to build a React-based interactive playground. I realized early on that Cursor works best when you act as the \"Architect.\" I wrote a highly detailed \"spec\" prompt defining exactly what the app should contain\u2014specifically requesting a UI that allows for real-time filter toggling and an embedded AI chat interface tailored specifically to answer conceptual questions about CNNs.\n\nInteraction Trace & Critical Analysis:\n\nSince the interaction was code-heavy, my critical annotation focuses on the difference between Knowledge Generation and Debugging:\n\nWhere it Succeeded (The Math):\n\nOne-Shot Accuracy: Surprisingly, Cursor did not hallucinate on the actual implementation of the convolution logic or the filter matrices. Because I provided a detailed prompt about the mathematical purpose of the filters, it generated the core processing logic correctly on the first try.\n\nWhere it Failed (The Engineering):\n\nDebugging Loops: While the math was good, the AI struggled significantly with \"glue code\" and UI state management. When a bug arose in the React rendering cycle, Cursor often got stuck in a loop, suggesting fixes that broke other parts of the app.\n\nHuman Intervention: This highlighted a key limitation: AI is great at generating isolated logic (like a specific kernel), but requires a human to oversee the broader system architecture and fix integration bugs.\n\nVerdict: Building this tool forced me to understand the inputs and outputs of CNNs much better than just reading about them. If you want to try it out or see the code, the repo is linked above!",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Concept:</bold> Convolutional Neural Networks (CNNs) &amp; Kernel Filters</paragraph><paragraph><bold>Tool Used:</bold> Cursor (AI Code Editor)</paragraph><paragraph><bold>Artifact:</bold> <link href=\"https://github.com/nikhilmathihalli/kernel-whisperer\">The Kernel Whisperer (GitHub Repo)</link></paragraph><paragraph><bold>The Goal:</bold> I found the recent lectures on ConvNets interesting, but I wanted to move beyond static slides. I decided to treat the AI not just as a tutor, but as a \"junior developer\" to help me build an interactive learning tool. The goal was to learn about filters (blur, edge detection, sharpen) by actually applying them to images in code, effectively replacing my standard post-lecture review with an active engineering project.</paragraph><paragraph><bold>The Approach:</bold> I used Cursor to build a React-based interactive playground. I realized early on that Cursor works best when you act as the \"Architect.\" I wrote a highly detailed \"spec\" prompt defining exactly what the app should contain\u2014specifically requesting a UI that allows for real-time filter toggling and an embedded AI chat interface tailored specifically to answer conceptual questions about CNNs.</paragraph><paragraph><bold>Interaction Trace &amp; Critical Analysis:</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/ASLPoMV2jTJR6sAdXVDm8gIb\" filename=\"Kernel Whisperer.pdf\"/><paragraph>Since the interaction was code-heavy, my critical annotation focuses on the difference between <bold>Knowledge Generation</bold> and <bold>Debugging</bold>:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Where it Succeeded (The Math):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><italic>One-Shot Accuracy:</italic> Surprisingly, Cursor did not hallucinate on the actual implementation of the convolution logic or the filter matrices. Because I provided a detailed prompt about the mathematical purpose of the filters, it generated the core processing logic correctly on the first try.</paragraph></list-item></list></list-item><list-item><paragraph><bold>Where it Failed (The Engineering):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><italic>Debugging Loops:</italic> While the math was good, the AI struggled significantly with \"glue code\" and UI state management. When a bug arose in the React rendering cycle, Cursor often got stuck in a loop, suggesting fixes that broke other parts of the app.</paragraph></list-item><list-item><paragraph><italic>Human Intervention:</italic> This highlighted a key limitation: AI is great at generating isolated logic (like a specific kernel), but requires a human to oversee the broader system architecture and fix integration bugs.</paragraph></list-item></list></list-item></list><paragraph><bold>Verdict:</bold> Building this tool forced me to understand the inputs and outputs of CNNs much better than just reading about them. If you want to try it out or see the code, the repo is linked above!</paragraph></document>",
            "links": [
                "https://github.com/nikhilmathihalli/kernel-whisperer"
            ],
            "attachments": [],
            "created_at": "2025-12-07T22:06:38.495522+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424833,
            "author": "Minjune Kim",
            "project_title": "Special Participation E: Creating New Discussion Sheets With Lectures That Current Discussion Sheet Does Not Ask using ChatGPT",
            "post_body": "I attempted to create a webpage that will take two inputs, one for lecture slides and another for discussion sheets (both in pdf format). Then, I would have chatgpt scan over both pdf files, and try to find concepts that the discussion did not cover that were in the lecture notes. With the concepts that weren't there, it would create new discussion sheet that's just like the discussion sheets for the class with answers such that it can be used to study concepts that weren't on the discussion but was on the lecture.\n\nDuring this process, I ran into some logistical error using chatGPT because at first, it was not able to detect any words for the lecture notes because all of the lecture notes were hand written, so I had to re-prompt GPT, such that it would be able to convert the hand-written notes into a readable format such that it will be able to recognize what concepts were covered in lecture. Furthermore, I did not prompt it carefully enough and not specific enough that when it first created the discussion sheet, it looked nothing like the discussions sheet, and it was also asking questions on only a few questions on concepts. \n\nHere is my annotated report for the prompt:",
            "content_xml": "<document version=\"2.0\"><paragraph>I attempted to create a webpage that will take two inputs, one for lecture slides and another for discussion sheets (both in pdf format). Then, I would have chatgpt scan over both pdf files, and try to find concepts that the discussion did not cover that were in the lecture notes. With the concepts that weren't there, it would create new discussion sheet that's just like the discussion sheets for the class with answers such that it can be used to study concepts that weren't on the discussion but was on the lecture.</paragraph><paragraph>During this process, I ran into some logistical error using chatGPT because at first, it was not able to detect any words for the lecture notes because all of the lecture notes were hand written, so I had to re-prompt GPT, such that it would be able to convert the hand-written notes into a readable format such that it will be able to recognize what concepts were covered in lecture. Furthermore, I did not prompt it carefully enough and not specific enough that when it first created the discussion sheet, it looked nothing like the discussions sheet, and it was also asking questions on only a few questions on concepts. </paragraph><paragraph>Here is my annotated report for the prompt:</paragraph><file url=\"https://static.us.edusercontent.com/files/JTaAJOtGIlI8BtpiNwFrxieX\" filename=\"Website code for discussion questions-merged.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T21:59:28.510045+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424828,
            "author": "Devan Perkash",
            "project_title": "Special Participation B: Cursor on HW1 Coding Portion",
            "post_body": "I used the Cursor IDE on the coding portion of HW 1.\n\n\n\nExecutive Summary:\n\nCursor was overall highly effective at solving the coding problems in this homework. It required very little context supplied by me through the chat thread, as it was able to retrieve context from code cells and even visualizations (i.e. training plots) on its own. It successfully connected logical/programming reasoning, numerical reasoning, and visual reasoning to even answer conceptual questions within the coding portion of the homework. Its largest failure mode, however, was lacking the ability to fully engage with the jupyter notebook, as it could not run cells autonomously the way it can run standard code files. This inhibited its iterative design process\u2014it could not continuously cycle between writing new code, running it, seeing the result, and making adjustments as needed\u2014so it therefore required a bit more hand-holding towards the end of the assignment where this became an issue. However, Cursor was overall a strong tool in completing this homework, and it provided clear explanations that would be helpful to anyone who might be stuck conceptually on the problems.\n\n\n\nYou can read a full annotated log of my interaction with Cursor in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy and analyze the results for correctness and reach (i.e. where was Cursor able and not able to make direct/successful changes).",
            "content_xml": "<document version=\"2.0\"><paragraph>I used the Cursor IDE on the coding portion of HW 1.</paragraph><paragraph/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>Cursor was overall highly effective at solving the coding problems in this homework. It required very little context supplied by me through the chat thread, as it was able to retrieve context from code cells and even visualizations (i.e. training plots) on its own. It successfully connected logical/programming reasoning, numerical reasoning, and visual reasoning to even answer conceptual questions within the coding portion of the homework. Its largest failure mode, however, was lacking the ability to fully engage with the jupyter notebook, as it could not run cells autonomously the way it can run standard code files. This inhibited its iterative design process\u2014it could not continuously cycle between writing new code, running it, seeing the result, and making adjustments as needed\u2014so it therefore required a bit more hand-holding towards the end of the assignment where this became an issue. However, Cursor was overall a strong tool in completing this homework, and it provided clear explanations that would be helpful to anyone who might be stuck conceptually on the problems.</paragraph><paragraph/><paragraph>You can read a full annotated log of my interaction with Cursor in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy and analyze the results for correctness and reach (i.e. where was Cursor able and not able to make direct/successful changes).</paragraph><file url=\"https://static.us.edusercontent.com/files/jWHJEC3ncOBsnqnOCMNWQdNS\" filename=\"cursor_implement_gradient_descent_with.md.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T21:55:27.373263+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424818,
            "author": "Athul Krishnan",
            "project_title": "Special Participation B: Claude Code on HW7",
            "post_body": "Hi everyone!\n\nFor Special Participation B, I evaluated Claude Code (with Thinking) on the coding portions of HW 7. To do so, I provided the relevant iPython notebooks without the problem PDF, I wanted to see if it could do things differently than intended yet still pass, as well as the following starting prompt:\n\n\"Hi Claude! I'd like you, as a deep learning lover to help me through these coding notebooks about RNNs and autoencoders for my deep learning class, by implementing the TODOs in each one! (do not modify any other code!) We'll go through them one by one, and be sure to explain your approaches step by step! To validate your approaches try running the cells/tests as needed. Before we continue, does this make sense?\"\n\nClaude was very strong, one-shotting nearly every question. As I expected, in the case that its initial hyperparameters didn't work (Q2, RNNs for Last Name Classification), it struggled to properly tune them without me giving it some guidance. Once I stepped in, it was able to spot a significant issue with its setup, and acheived >80% eval accuracy successfully.\n\nHowever, it one-shotted every other question (including the MNIST Autoencoder) without any re-prompting or extra tuning necessary. On the autoencoder implementation, it even ran its own code, smartly debugged it by deducing a constant factor between the expected value and the output value, and resolved it correctly, noticing that the factor was equivalent to the input dimension. I wasn't expecting it to use less apparent context clues and make these logical jumps. It was probably the most \"human\" thing Claude has done in my testing.\n\nOverall, Claude Code is very impressive, one-shotting most questions, and only really struggling with hyperparameter tuning. It manages to run and debug its own code, utilizing the output as well as context clues within the notebook. In the end, despite not having the extra information of the problem set PDF, its solutions were mostly similar to the staff solutions, usually leaning on the more readable side (elaborated on in my annotations). I also found its conceptual explanations of the implementations to be very helpful, and it excelled in summarizing all of its changes and its reasoning for doing so. \n\nBelow is my annotated conversation trace:",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone!<break/><break/>For Special Participation B, I evaluated <bold>Claude Code (with Thinking)</bold> on the <bold>coding portions of HW 7.</bold> To do so, I provided the relevant iPython notebooks <bold>without the problem PD</bold>F, I wanted to see if it could do things differently than intended yet still pass, as well as the following starting prompt:</paragraph><paragraph>\"Hi Claude! I'd like you, as a deep learning lover to help me through these coding notebooks about RNNs and autoencoders for my deep learning class, by implementing the TODOs in each one! (do not modify any other code!) We'll go through them one by one, and be sure to explain your approaches step by step! To validate your approaches try running the cells/tests as needed. Before we continue, does this make sense?\"</paragraph><paragraph><bold>Claude was very strong, one-shotting nearly every question.</bold> As I expected, in the case that its initial hyperparameters didn't work (Q2, RNNs for Last Name Classification), <bold>it struggled to properly tune</bold> them without me giving it some guidance. Once I stepped in, it was able to spot a significant issue with its setup, and acheived &gt;80% eval accuracy successfully.</paragraph><paragraph>However, it one-shotted every other question (including the MNIST Autoencoder) without any re-prompting or extra tuning necessary. On the autoencoder implementation, it even ran its own code, smartly debugged it by deducing a constant factor between the expected value and the output value, and resolved it correctly, noticing that the factor was equivalent to the input dimension. I wasn't expecting it to use less apparent context clues and make these logical jumps. It was probably the most \"human\" thing Claude has done in my testing.</paragraph><paragraph>Overall, Claude Code is very impressive, one-shotting most questions, and only really struggling with hyperparameter tuning. It manages to run and debug its own code, utilizing the output as well as context clues within the notebook. In the end, despite not having the extra information of the problem set PDF, its solutions were mostly similar to the staff solutions, usually leaning on the more readable side (elaborated on in my annotations). I also found its conceptual explanations of the implementations to be very helpful, and it excelled in summarizing all of its changes and its reasoning for doing so. <break/><break/>Below is my annotated conversation trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/MZ9nw2TfdrPWgczwTJTim2f0\" filename=\"CS_C182_Participation_B_Athul.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T21:40:38.837131+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424817,
            "author": "Rohan Gulati",
            "project_title": "Special Participation B: Mistral on Homework 5",
            "post_body": "Here, I looked at how well Mistral AI's chat model could perform on Homework 5's coding portion, which focused on convolutions and dropout. Overall, Mistral performed very well on the coding tasks, much of which involved building upon previous iterations of its own code. \n\nThe prompting strategy I used involved providing the entire method as context, which often involved docstrings and particularly the provided demarcated region \"### TODO ###\" for the model to fill in the blank with the correct solution. This might have mitigated any hallucinations, as there weren't any gaps that prevented the model from generating inaccurate code. As a result, the model was able to solve problems including generating converging models and writing functions resulting in near-zero tensor outputs in one shot. Part 6.1 appeared to be more involved since it involved generating the loss for the Fully Connected layer and reaching near-zero error with the expected output and then adding Dropout to the generated code. However, by providing the entire network class as reference and the layer_utils file, the model was able to write the code accurately. Lastly, one notable result was that in problem 6.3, \"Use Deep Learning Framework\", the task was to design a custom neural network to reach \"44% accuracy or higher.\" I was expecting Mistral to either take a couple iterations of hyperparameter tuning through our conversation to hit 44% or overshoot 44% in case it was familiar with the task. However, the generated model went from 19.8% validation accuracy in Epoch 1 to 39.2% in Epoch 5 to 44.7% in the final Epoch, arriving at the desired validation accuracy. In the hyperparameter section, it shared its hyperparameter tuning/selection strategy, despite this occurring in the first shot. As a result, it would be very interesting to investigate the abilities for models to do hyperparameter selection for particular tasks and any latent understanding in this area.\n\nOverall, Mistral was able to perform well at the coding tasks in Problem Set 5, and the chats and annotated log are provided below.\n\n\nAnnotated Log: https://docs.google.com/document/d/1d-C20RIKeTumzUaT-7wNuzmLPclNvVHjOOSKAP7KIt0/edit?usp=sharing (Long but headings for each part)\n\nChat History:\nPart 5: Understanding Dropout: https://chat.mistral.ai/chat/05f8f22e-9916-4e6a-b2e7-0e24b8bc374f \n\nPart 6.1: Implementing BatchNorm and Dropout: https://chat.mistral.ai/chat/cd59ea7a-2746-470c-a9db-c88b8d94365d \n\nPart 6.2: Implementing Convolution and Spatial Batch Norm: https://chat.mistral.ai/chat/5c914c00-f6f5-4793-bac8-d3b49ea3e59a \n\nPart 6.3: Use Deep Learning Framework: https://chat.mistral.ai/chat/bc12703b-0109-4648-a3a6-853b72bf4ffa ",
            "content_xml": "<document version=\"2.0\"><paragraph>Here, I looked at how well Mistral AI's chat model could perform on Homework 5's coding portion, which focused on convolutions and dropout. Overall, Mistral performed very well on the coding tasks, much of which involved building upon previous iterations of its own code. </paragraph><paragraph>The prompting strategy I used involved providing the entire method as context, which often involved docstrings and particularly the provided demarcated region \"### TODO ###\" for the model to fill in the blank with the correct solution. This might have mitigated any hallucinations, as there weren't any gaps that prevented the model from generating inaccurate code. As a result, the model was able to solve problems including generating converging models and writing functions resulting in near-zero tensor outputs in one shot. Part 6.1 appeared to be more involved since it involved generating the loss for the Fully Connected layer and reaching near-zero error with the expected output and then adding Dropout to the generated code. However, by providing the entire network class as reference and the layer_utils file, the model was able to write the code accurately. Lastly, one notable result was that in problem 6.3, \"Use Deep Learning Framework\", the task was to design a custom neural network to reach \"44% accuracy or higher.\" I was expecting Mistral to either take a couple iterations of hyperparameter tuning through our conversation to hit 44% or overshoot 44% in case it was familiar with the task. However, the generated model went from 19.8% validation accuracy in Epoch 1 to 39.2% in Epoch 5 to 44.7% in the final Epoch, arriving at the desired validation accuracy. In the hyperparameter section, it shared its hyperparameter tuning/selection strategy, despite this occurring in the first shot. As a result, it would be very interesting to investigate the abilities for models to do hyperparameter selection for particular tasks and any latent understanding in this area.<break/><break/>Overall, Mistral was able to perform well at the coding tasks in Problem Set 5, and the chats and annotated log are provided below.<break/></paragraph><paragraph>Annotated Log: <link href=\"https://docs.google.com/document/d/1d-C20RIKeTumzUaT-7wNuzmLPclNvVHjOOSKAP7KIt0/edit?usp=sharing\">https://docs.google.com/document/d/1d-C20RIKeTumzUaT-7wNuzmLPclNvVHjOOSKAP7KIt0/edit?usp=sharing</link> (Long but headings for each part)</paragraph><paragraph>Chat History:<break/>Part 5: Understanding Dropout: <link href=\"https://chat.mistral.ai/chat/05f8f22e-9916-4e6a-b2e7-0e24b8bc374f\">https://chat.mistral.ai/chat/05f8f22e-9916-4e6a-b2e7-0e24b8bc374f</link> </paragraph><paragraph>Part 6.1: Implementing BatchNorm and Dropout: <link href=\"https://chat.mistral.ai/chat/cd59ea7a-2746-470c-a9db-c88b8d94365d\">https://chat.mistral.ai/chat/cd59ea7a-2746-470c-a9db-c88b8d94365d</link> </paragraph><paragraph>Part 6.2: Implementing Convolution and Spatial Batch Norm: <link href=\"https://chat.mistral.ai/chat/5c914c00-f6f5-4793-bac8-d3b49ea3e59a\">https://chat.mistral.ai/chat/5c914c00-f6f5-4793-bac8-d3b49ea3e59a</link> </paragraph><paragraph>Part 6.3: Use Deep Learning Framework: <link href=\"https://chat.mistral.ai/chat/bc12703b-0109-4648-a3a6-853b72bf4ffa\">https://chat.mistral.ai/chat/bc12703b-0109-4648-a3a6-853b72bf4ffa</link> </paragraph></document>",
            "links": [
                "https://docs.google.com/document/d/1d-C20RIKeTumzUaT-7wNuzmLPclNvVHjOOSKAP7KIt0/edit?usp=sharing",
                "https://chat.mistral.ai/chat/05f8f22e-9916-4e6a-b2e7-0e24b8bc374f",
                "https://chat.mistral.ai/chat/cd59ea7a-2746-470c-a9db-c88b8d94365d",
                "https://chat.mistral.ai/chat/5c914c00-f6f5-4793-bac8-d3b49ea3e59a",
                "https://chat.mistral.ai/chat/bc12703b-0109-4648-a3a6-853b72bf4ffa"
            ],
            "attachments": [],
            "created_at": "2025-12-07T21:38:34.860882+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424808,
            "author": "Joshua Lu",
            "project_title": "Special Participation B: Qwen3-Max on HW 9 (Coding)",
            "post_body": "I used Qwen3-Max (With Thinking) to complete the coding portion of Homework 9.\n\nHere is the trace (without annotations): https://chat.qwen.ai/s/095b3b7d-4d9c-4fed-a2fe-0914fbe97bd2?fev=0.1.13\n\nHere is the trace with annotations: https://drive.google.com/file/d/167LVVFvUYnYehqmtULNY0HytxJrYH-6_/view?usp=sharing\n\n\n\nOverall, I was not happy with the performance of Qwen on this homework assignment. It was very average (less than human level). There are several reasons for this, though I would like to add that not all of it is Qwen's fault.\n\nFirst of all, the problem setup is very tricky. This problem is a coding question that requires running code but more importantly, interpreting the graphs, and the main issue is that the graphs are figures that have different buttons for viewing the graph under different parameters. In total, each graph had around 12 layers and 12 attention heads, so that makes 144 unique combination of graphs to examine. Furthermore, each graph shows different things when clicked on or hovered over. Qwen not only cannot accept code files and run code, but it also has a 5 image limit, which makes doing this problem very hard.\n\nFor my setup, I ran all the code myself and made a pdf, with the outputs, and sent that to Qwen. In addition, for each problem, I had to click through the graphs myself and choose 5 sets of parameters that produced what I believed to be reasonable representation of what the problem is aiming for. This is the prompt that I send to Qwen each time.\n\nThere are many issues already. First,  it's unclear if I'm able to choose the good graphs to send over. In addition, for the graphs, if you hover over a token, it shows the connections for only that token, but there's no way I can do that for each token. Instead, I had to settle for an image of the entire graph with all the lines, which works but makes the lines very unclear. To help remedy some of my issues, each time, I asked Qwen to send me sets of parameters that it would like me to show, and that is when I send over more screenshots of the graphs.\n\nNow, let's go over the issues that Qwen had. First of all, it wasn't very good at asking for new graphs initially, and required me to repeatedly prompt before it started learning. Also, when it asks for graphs, it doesn't give me specific set of parameters. I can't fully blame Qwen for this, since it's impossible for the model to know which sets of parameters are actually worthwhile to examine. In addition, Qwen frequently hallucinates what's on the graph. It claims that there is a lot of connection between token A and token B, but when I check the graph, there is barely a line visible. This could be due to how cluttered the graphs are in the screenshots.\n\nA major concern that I have is that Qwen is mostly drawing from general knowledge, rather than actually extracting patterns from the graphs. Qwen frequently talks about how words A and B have very similar semantic meaning, so there's a heavy connection between them, but when I check the graph, the line is very faint or not there. Similarly, when describing the effects of different layers, I definitely notice Qwen using its prior knowledge to extrapolate explanations for what is going on at each step without actually looking at the graphs.\n\nOverall, Qwen does give reasonable (and highly detailed explanations), but it feels to me like most of these explanations are just from its general knowledge of transformer architecture rather than from the graphs. However, I admit that interpretability is difficult, and even I had a lot of trouble drawing patterns from the graphs. It would be perhaps better if Qwen was able to see a view of all 144 graphs, along with clean images for the connections of each token, but given resource constraint, that will still be very difficult. I thus would generally refrain from using AI models, particularly Qwen, to help analyze complex graphs for me.\n\nA final complaint: I tried many different extensions, but I was unable to get a pdf directly from the Qwen chat interface. Instead, I had to manually take screenshots and put them onto a Google Doc. The built-in export also only creates a JSON and ignores the images. Please let me know if anyone found a better solution!",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Qwen3-Max (With Thinking) to complete the coding portion of Homework 9.</paragraph><paragraph>Here is the trace (without annotations): <link href=\"https://chat.qwen.ai/s/095b3b7d-4d9c-4fed-a2fe-0914fbe97bd2?fev=0.1.13\">https://chat.qwen.ai/s/095b3b7d-4d9c-4fed-a2fe-0914fbe97bd2?fev=0.1.13</link></paragraph><paragraph>Here is the trace with annotations: <link href=\"https://drive.google.com/file/d/167LVVFvUYnYehqmtULNY0HytxJrYH-6_/view?usp=sharing\">https://drive.google.com/file/d/167LVVFvUYnYehqmtULNY0HytxJrYH-6_/view?usp=sharing</link></paragraph><paragraph/><paragraph>Overall, I was not happy with the performance of Qwen on this homework assignment. It was very average (less than human level). There are several reasons for this, though I would like to add that not all of it is Qwen's fault.</paragraph><paragraph>First of all, the problem setup is very tricky. This problem is a coding question that requires running code but more importantly, interpreting the graphs, and the main issue is that the graphs are figures that have different buttons for viewing the graph under different parameters. In total, each graph had around 12 layers and 12 attention heads, so that makes 144 unique combination of graphs to examine. Furthermore, each graph shows different things when clicked on or hovered over. Qwen not only cannot accept code files and run code, but it also has a 5 image limit, which makes doing this problem very hard.</paragraph><paragraph>For my setup, I ran all the code myself and made a pdf, with the outputs, and sent that to Qwen. In addition, for each problem, I had to click through the graphs myself and choose 5 sets of parameters that produced what I believed to be reasonable representation of what the problem is aiming for. This is the prompt that I send to Qwen each time.</paragraph><paragraph>There are many issues already. First,  it's unclear if I'm able to choose the good graphs to send over. In addition, for the graphs, if you hover over a token, it shows the connections for only that token, but there's no way I can do that for each token. Instead, I had to settle for an image of the entire graph with all the lines, which works but makes the lines very unclear. To help remedy some of my issues, each time, I asked Qwen to send me sets of parameters that it would like me to show, and that is when I send over more screenshots of the graphs.</paragraph><paragraph>Now, let's go over the issues that Qwen had. First of all, it wasn't very good at asking for new graphs initially, and required me to repeatedly prompt before it started learning. Also, when it asks for graphs, it doesn't give me specific set of parameters. I can't fully blame Qwen for this, since it's impossible for the model to know which sets of parameters are actually worthwhile to examine. In addition, Qwen frequently hallucinates what's on the graph. It claims that there is a lot of connection between token A and token B, but when I check the graph, there is barely a line visible. This could be due to how cluttered the graphs are in the screenshots.</paragraph><paragraph>A major concern that I have is that Qwen is mostly drawing from general knowledge, rather than actually extracting patterns from the graphs. Qwen frequently talks about how words A and B have very similar semantic meaning, so there's a heavy connection between them, but when I check the graph, the line is very faint or not there. Similarly, when describing the effects of different layers, I definitely notice Qwen using its prior knowledge to extrapolate explanations for what is going on at each step without actually looking at the graphs.</paragraph><paragraph>Overall, Qwen does give reasonable (and highly detailed explanations), but it feels to me like most of these explanations are just from its general knowledge of transformer architecture rather than from the graphs. However, I admit that interpretability is difficult, and even I had a lot of trouble drawing patterns from the graphs. It would be perhaps better if Qwen was able to see a view of all 144 graphs, along with clean images for the connections of each token, but given resource constraint, that will still be very difficult. I thus would generally refrain from using AI models, particularly Qwen, to help analyze complex graphs for me.</paragraph><paragraph>A final complaint: I tried many different extensions, but I was unable to get a pdf directly from the Qwen chat interface. Instead, I had to manually take screenshots and put them onto a Google Doc. The built-in export also only creates a JSON and ignores the images. Please let me know if anyone found a better solution!</paragraph></document>",
            "links": [
                "https://chat.qwen.ai/s/095b3b7d-4d9c-4fed-a2fe-0914fbe97bd2?fev=0.1.13",
                "https://drive.google.com/file/d/167LVVFvUYnYehqmtULNY0HytxJrYH-6_/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T21:27:58.721851+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424807,
            "author": "Natalie Wei",
            "project_title": "Special Participation B: Cursor (Opus 4.5) on HW9",
            "post_body": "Overview\n\nI worked with Cursor (Opus 4.5) to complete the coding question in Homework 9. This question doesn\u2019t actually involve writing code, just running the provided notebook and analyzing the visualizations. I thought it would be an interesting exercise to see whether or not Cursor would be able to run the notebook on its own, and what workarounds Cursor might take if the visualizations were not uploaded with the prompt.\n\nSince Cursor was unable to directly interact with the notebook, it chose to rewrite the notebook as a series of Python scripts. Cursor also explicitly told me that it was not able to render the visualizations and relied on the numerical attention weights to come up with answers instead. Even so, its answers were relatively close to the staff solution and would probably have been acceptable. All of the (quite verbose) scripts did lead to a very long transcript; it probably would have been more efficient to upload visualizations myself, but it was still quite interesting to see all of Cursor\u2019s workarounds. \n\nAnnotated Logs\n\nLink",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Overview</bold></paragraph><paragraph>I worked with Cursor (Opus 4.5) to complete the coding question in Homework 9. This question doesn\u2019t actually involve writing code, just running the provided notebook and analyzing the visualizations. I thought it would be an interesting exercise to see whether or not Cursor would be able to run the notebook on its own, and what workarounds Cursor might take if the visualizations were not uploaded with the prompt.</paragraph><paragraph>Since Cursor was unable to directly interact with the notebook, it chose to rewrite the notebook as a series of Python scripts. Cursor also explicitly told me that it was not able to render the visualizations and relied on the numerical attention weights to come up with answers instead. Even so, its answers were relatively close to the staff solution and would probably have been acceptable. All of the (quite verbose) scripts did lead to a very long transcript; it probably would have been more efficient to upload visualizations myself, but it was still quite interesting to see all of Cursor\u2019s workarounds. </paragraph><paragraph><bold>Annotated Logs</bold></paragraph><paragraph><link href=\"https://drive.google.com/file/d/1rYiifv6OZ7vovJYYctuxACacuUV0zj_v/view?usp=sharing\">Link</link></paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1rYiifv6OZ7vovJYYctuxACacuUV0zj_v/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T21:27:45.894968+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424798,
            "author": "Arnav Dalal",
            "project_title": "Special Participation C: Refactoring HW10 problem 2",
            "post_body": "Hi everyone!\n\nMy group and I refactored the codebase for (2) Hand-Design Transformers on HW10. \n\nThe main changes prioritized code quality, organization, readability, and functionality all while maintaining a high teaching/learning value for the notebook. \n\nGiven the long nature of this notebook, we separated the helper functions and tests into separate files within new directories /utils and /tests. Utils contained all of the helper functions in the original notebook along with new functions for visualization (rescale_and_plot and save_figure) and new configurations for both the training and plotting. \n\nThese configurations (TrainingConfig and PlotConfig) added more customizability to the assignment, allowing students to explore further beyond what is required and find cool new visualizations for their results. \n\nWe moved all of the testing infrastructure as well, separating the tests for each of the individual sections in the notebook. We also added better debugging functionality, improving error messages and adding additional checks to ensure that the student can trace what went wrong with their implementation.\n\nHere is a link to our repo containing the updated codebase: https://github.com/adalal07/cs182-special-participation-c2/tree/main.\n\nExcited to see what others think!\n\ncc Kabir Shah, Sarvagya Somvanshi",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone!<break/><break/>My group and I refactored the codebase for (2) Hand-Design Transformers on HW10. </paragraph><paragraph>The main changes prioritized code quality, organization, readability, and functionality all while maintaining a high teaching/learning value for the notebook. <break/><break/>Given the long nature of this notebook, we separated the helper functions and tests into separate files within new directories /utils and /tests. Utils contained all of the helper functions in the original notebook along with new functions for visualization (<bold>rescale_and_plot</bold> and <bold>save_figure</bold>) and new configurations for both the training and plotting. <break/><break/>These configurations (<bold>TrainingConfig</bold> and <bold>PlotConfig</bold>) added more customizability to the assignment, allowing students to explore further beyond what is required and find cool new visualizations for their results. <break/><break/>We moved all of the testing infrastructure as well, separating the tests for each of the individual sections in the notebook. We also added better debugging functionality, improving error messages and adding additional checks to ensure that the student can trace what went wrong with their implementation.</paragraph><paragraph>Here is a link to our repo containing the updated codebase: <link href=\"https://github.com/adalal07/cs182-special-participation-c2/tree/main\">https://github.com/adalal07/cs182-special-participation-c2/tree/main</link>.<break/><break/>Excited to see what others think!<break/><break/>cc Kabir Shah, Sarvagya Somvanshi</paragraph></document>",
            "links": [
                "https://github.com/adalal07/cs182-special-participation-c2/tree/main"
            ],
            "attachments": [],
            "created_at": "2025-12-07T21:12:01.082827+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424797,
            "author": "Sarvagya Somvanshi",
            "project_title": "Special Participation C: HW 7 Q1",
            "post_body": "Hey everyone, \n\nMy group worked on refactoring the RNN & Gradient Flow notebook (HW7 Q1) for Special Participation C. We focussed on adding more context throughout the notebook. Every class now has docstrings explaining what it does and why it matters. More importantly, we included the actual math formulations right next to the code as well as diagrams showing RNN unrolling and multi-layer architectures.\n\nWe worked reorganizing the code structure. Helper functions like generate_batch, train, and train_all now live in a separate utils.py file, and all test cases have been moved to tests.py to remove clutter and  .\n\nOn the style side, we cleaned up variable names, added type hints so you know exactly what tensor shapes to expect, and fixed the indentation to follow PEP 8 guidelines.\n\nHere's a link to my GitHub repo containing the refactored notebook:\n\nhttps://github.com/adalal07/cs182-special-participation-c1\n\nHope it's helpful!\n\nCC: Arnav Dalal, Kabir Shah",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey everyone, <break/><break/>My group worked on refactoring the RNN &amp; Gradient Flow notebook (HW7 Q1) for Special Participation C. We focussed on adding more context throughout the notebook. Every class now has docstrings explaining what it does and why it matters. More importantly, we included the actual math formulations right next to the code as well as diagrams showing RNN unrolling and multi-layer architectures.</paragraph><paragraph>We worked reorganizing the code structure. Helper functions like generate_batch, train, and train_all now live in a separate utils.py file, and all test cases have been moved to tests.py to remove clutter and  .</paragraph><paragraph>On the style side, we cleaned up variable names, added type hints so you know exactly what tensor shapes to expect, and fixed the indentation to follow PEP 8 guidelines.</paragraph><paragraph>Here's a link to my GitHub repo containing the refactored notebook:</paragraph><paragraph><link href=\"https://github.com/adalal07/cs182-special-participation-c1\">https://github.com/adalal07/cs182-special-participation-c1</link></paragraph><paragraph>Hope it's helpful!</paragraph><paragraph>CC: Arnav Dalal, Kabir Shah</paragraph></document>",
            "links": [
                "https://github.com/adalal07/cs182-special-participation-c1"
            ],
            "attachments": [],
            "created_at": "2025-12-07T21:11:50.881173+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424787,
            "author": "Jorge Diaz Chao",
            "project_title": "Gemini 3 Pro for Finals Prep",
            "post_body": "As we prepare for finals, I prompt Google Gemini Thinking with 3 Pro to help me review the concepts by making a checklist of the concepts covered in a set of lecture notes, and developing a preliminary conceptual cheat sheet, very helpful in verifying if one's on track with the content. \n\nFurthermore, I prompt the model to come up with a timeline where each of the models introduced in class is connected via a story that makes the concepts memorable and more intuitive. When concepts are connected in the context of the real world, one can understand and be inspired by how they came to be. While doing so, I explicitly ask the model to include problems with each architecture, and how it motivates the following solution.\n\nFinally, after verifying that the model is on the same page I ask it to come up with question of varying difficulty while keeping track of how I am doing so it can make a personalized test though which I can test my limits, review what I studied and reinforce the topics I am not so comfortable with by revising them more.\n\nIn addition to that I briefly explore the capabilities of new Nano Banana Pro to generate infographics including mathematical expressions and technical diagrams to display concepts visually and perhaps more intuitively for those visual learners. It's amazing how good these image generation models have gotten, and while very high quality and surprising level of accuracy. It contains minor typos it wouldn't if just typing out, and feels very superficial, unlike text responses.\n\nOkay, now, you are my final exam tutor. Your job is to help me truly understand and retain concepts. Follow these rules: (1) Ask me only ONE conceptual question at a time. (2) Start with fundamentals, increase difficulty only if my answer is correct and confident. (3) After I answer, first evaluate correctness concisely. If incorrect or uncertain, explain the concept clearly with a simple real example, then give a lightweight mini-practice follow-up. If correct, briefly deepen the insight or connect it to another idea. (4) Every 5 questions: summarize what I\u2019ve mastered and predict my weakest related topic, then jump into that. (5) Include one misconception check every few questions (\"Many students mistakenly believe __. Why is that wrong?\u201d). (6) No long lectures, prioritize dialogue and active recall. (7) Track new concepts I learned, confusions that emerged, breakthrough insights. End goal: a personalized knowledge graph of what I know for exam strategy.\n\nHere's the annotated interaction.",
            "content_xml": "<document version=\"2.0\"><paragraph>As we prepare for finals, I prompt Google Gemini Thinking with 3 Pro to help me review the concepts by making a checklist of the concepts covered in a set of lecture notes, and developing a preliminary conceptual cheat sheet, very helpful in verifying if one's on track with the content. </paragraph><paragraph>Furthermore, I prompt the model to come up with a timeline where each of the models introduced in class is connected via a story that makes the concepts memorable and more intuitive. When concepts are connected in the context of the real world, one can understand and be inspired by how they came to be. While doing so, I explicitly ask the model to include problems with each architecture, and how it motivates the following solution.</paragraph><paragraph>Finally, after verifying that the model is on the same page I ask it to come up with question of varying difficulty while keeping track of how I am doing so it can make a personalized test though which I can test my limits, review what I studied and reinforce the topics I am not so comfortable with by revising them more.</paragraph><paragraph>In addition to that I briefly explore the capabilities of new Nano Banana Pro to generate infographics including mathematical expressions and technical diagrams to display concepts visually and perhaps more intuitively for those visual learners. It's amazing how good these image generation models have gotten, and while very high quality and surprising level of accuracy. It contains minor typos it wouldn't if just typing out, and feels very superficial, unlike text responses.</paragraph><blockquote>Okay, now, you are my final exam tutor. Your job is to help me truly understand and retain concepts. Follow these rules: (1) Ask me only ONE conceptual question at a time. (2) Start with fundamentals, increase difficulty only if my answer is correct and confident. (3) After I answer, first evaluate correctness concisely. If incorrect or uncertain, explain the concept clearly with a simple real example, then give a lightweight mini-practice follow-up. If correct, briefly deepen the insight or connect it to another idea. (4) Every 5 questions: summarize what I\u2019ve mastered and predict my weakest related topic, then jump into that. (5) Include one misconception check every few questions (\"Many students mistakenly believe __. Why is that wrong?\u201d). (6) No long lectures, prioritize dialogue and active recall. (7) Track new concepts I learned, confusions that emerged, breakthrough insights. End goal: a personalized knowledge graph of what I know for exam strategy.</blockquote><paragraph>Here's the annotated interaction.</paragraph><file url=\"https://static.us.edusercontent.com/files/3PXO95egiaF9QSTy96SDpcsw\" filename=\"annotated_tutor.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T21:02:58.522829+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424752,
            "author": "Nikhil Mathihalli",
            "project_title": "Special Participation B: Using Grok on HW 12 Coding",
            "post_body": "Summary: I used Grok (Standard Chat) to solve the VAE implementation tasks in Question 4. While the model had mixed results on the theoretical derivations (Problem 5), its performance on the coding section was flawless. It demonstrated \"Senior Engineer\" level awareness by implementing device-agnostic code without being prompted to do so.\n\nRecap: I uploaded the necessary codebase (utils.py and models/vae.py) along with the homework PDF. My prompt was minimal: I simply asked it to \"solve problem 4 as noted in the PDF\" and implement the reparameterization trick and ELBO bound. I did not provide any specific \"persona\" or coding guidelines.\n\nAnalysis: The model one-shotted both functions (sample_gaussian and negative_elbo_bound). Upon reviewing the code, I found it to be of higher quality than a standard textbook implementation:\n\nDevice Awareness: Instead of using torch.randn(shape), Grok used torch.randn_like(v). This is a subtle but critical best practice; it ensures that the epsilon noise tensor automatically inherits the device (CPU vs GPU) and data type of the input variance tensor. A junior developer (or a weaker model) often misses this, leading to device mismatch errors during training.\n\nInstruction Fidelity: VAE implementations often default to using log-variance for numerical stability. However, the prompt explicitly defined the input v as variance. Grok correctly computed the standard deviation (torch.sqrt(v)) rather than hallucinating the log-variance convention, showing it prioritized the specific user instructions over its general training data bias.\n\nTrace: ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Summary:</bold> I used Grok (Standard Chat) to solve the VAE implementation tasks in Question 4. While the model had mixed results on the theoretical derivations (Problem 5), its performance on the coding section was flawless. It demonstrated \"Senior Engineer\" level awareness by implementing device-agnostic code without being prompted to do so.</paragraph><paragraph><bold>Recap:</bold> I uploaded the necessary codebase (<code>utils.py</code> and <code>models/vae.py</code>) along with the homework PDF. My prompt was minimal: I simply asked it to \"solve problem 4 as noted in the PDF\" and implement the reparameterization trick and ELBO bound. I did not provide any specific \"persona\" or coding guidelines.</paragraph><paragraph><bold>Analysis:</bold> The model one-shotted both functions (<code>sample_gaussian</code> and <code>negative_elbo_bound</code>). Upon reviewing the code, I found it to be of higher quality than a standard textbook implementation:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>Device Awareness:</bold> Instead of using <code>torch.randn(shape)</code>, Grok used <code>torch.randn_like(v)</code>. This is a subtle but critical best practice; it ensures that the epsilon noise tensor automatically inherits the device (CPU vs GPU) and data type of the input variance tensor. A junior developer (or a weaker model) often misses this, leading to device mismatch errors during training.</paragraph></list-item><list-item><paragraph><bold>Instruction Fidelity:</bold> VAE implementations often default to using <italic>log-variance</italic> for numerical stability. However, the prompt explicitly defined the input <code>v</code> as <italic>variance</italic>. Grok correctly computed the standard deviation (<code>torch.sqrt(v)</code>) rather than hallucinating the log-variance convention, showing it prioritized the specific user instructions over its general training data bias.</paragraph></list-item></list><paragraph><bold>Trace:</bold> </paragraph><file url=\"https://static.us.edusercontent.com/files/1SC7v07GdSpqsagZATIH7HEx\" filename=\"Grok HW 12 Code.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T20:24:59.923593+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424734,
            "author": "Devan Perkash",
            "project_title": "Special Participation A: Mistral AI's Le Chat on HW12 Written Portion",
            "post_body": "I used Mistral AI's Le Chat on the written portion of HW 12. \n\n\n\nExecutive Summary:\n\nMistral's Le Chat had high variance with regard to its success on HW 12. For the first several questions, it was actually able to accurately zero-shot its answers with very little prompting (apart from some prompting at the very beginning where I show the homework to Le Chat and give it the role of an intelligent, concise, clear deep learning expert). Further, it even successfully handles questions that involved a combination of visual and numerical reasoning (interpreting graphs and then making numerical estimates based on such interpretations). However, where the model really struggled was when it had to answer questions for which it simply did not have enough information. One would expect the model to respond to such questions with \"I don't know,\" or \"I need the following information to complete this question accurately,\" but instead, it completely hallucinated, and even insisted on the correctness of its hallucinations when pressed further.\n\n\n\nHere is a link to my chat thread with Le Chat:\n\nhttps://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15e\n\n\n\nYou can also read a full annotated log of this chat in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy, analyze the results (correctness, style) in the model's responses, and explain how I progress between prompts when the model clearly struggles.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Mistral AI's Le Chat on the written portion of HW 12. </paragraph><paragraph/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>Mistral's Le Chat had high variance with regard to its success on HW 12. For the first several questions, it was actually able to accurately zero-shot its answers with very little prompting (apart from some prompting at the very beginning where I show the homework to Le Chat and give it the role of an intelligent, concise, clear deep learning expert). Further, it even successfully handles questions that involved a combination of visual and numerical reasoning (interpreting graphs and then making numerical estimates based on such interpretations). However, where the model really struggled was when it had to answer questions for which it simply did not have enough information. One would expect the model to respond to such questions with \"I don't know,\" or \"I need the following information to complete this question accurately,\" but instead, it completely hallucinated, and even insisted on the correctness of its hallucinations when pressed further.</paragraph><paragraph/><paragraph>Here is a link to my chat thread with Le Chat:</paragraph><paragraph><link href=\"https://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15e\">https://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15e</link></paragraph><paragraph/><paragraph>You can also read a full annotated log of this chat in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy, analyze the results (correctness, style) in the model's responses, and explain how I progress between prompts when the model clearly struggles.</paragraph><file url=\"https://static.us.edusercontent.com/files/gkTMbHjfqcQXRO76EdXdFhjz\" filename=\"participation_a.pdf\"/></document>",
            "links": [
                "https://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15e"
            ],
            "attachments": [],
            "created_at": "2025-12-07T20:06:45.965283+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424727,
            "author": "Shreyes Sridhara",
            "project_title": "Special Participation B: ChatGPT 5.1 on HW 10",
            "post_body": "For Special Participation B, I used GPT-5.1 to implement the coding portions of Homework 10 (Hand-Designed Transformers and Summarization). I focused on \"Iterative Debugging,\" challenging the model to fix its own errors rather than correcting them myself.\n\nAttached is the full annotated log of the session.\n\nExecutive Summary:\n\nOverall, GPT-5.1 demonstrated exceptional coding fluency but required human intervention for environment-specific engineering conflicts. It successfully \"one-shot\" the complex math for Scaled Dot Product Attention, but failed on trivial boilerplate imports.\n\nKey Findings:\n\nTheoretical Depth (Q2): The model provided a superior theoretical explanation for the difference between \"Hand-Designed\" and \"Learned\" weight matrices than the solution key. It correctly identified that neural networks learn distributed representations (dense matrices) that are functionally equivalent to the sparse, orthogonal matrices humans design.\n\nEngineering Diagnosis (Q3): The highlight of the interaction was a RuntimeError during training caused by a conflict between Weight Tying (sharing embeddings with the output head) and the Safetensors library. The model correctly diagnosed this obscure compatibility issue and provided the specific flag (torch_compile=False) to resolve it.\n\nNumerical Reasoning: It demonstrated strong numerical intuition by proposing large scalar logits ($S=1000$) to force the Softmax function to behave like a hard argmax, ensuring the \"Identity\" transformer worked within strict floating-point tolerances.",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special Participation B, I used GPT-5.1 to implement the coding portions of Homework 10 <bold>(Hand-Designed Transformers and Summarization)</bold>. I focused on \"Iterative Debugging,\" challenging the model to fix its own errors rather than correcting them myself.</paragraph><paragraph>Attached is the full annotated log of the session.</paragraph><file url=\"https://static.us.edusercontent.com/files/vH26CRymaGfsWSJnVKZtvZU5\" filename=\"ChatGPT 5.1 for HW 10 (Special Participation B).pdf\"/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>Overall, GPT-5.1 demonstrated exceptional coding fluency but required human intervention for environment-specific engineering conflicts. It successfully \"one-shot\" the complex math for Scaled Dot Product Attention, but failed on trivial boilerplate imports.</paragraph><paragraph><bold>Key Findings:</bold></paragraph><list style=\"ordered\"><list-item><paragraph><bold>Theoretical Depth (Q2):</bold> The model provided a superior theoretical explanation for the difference between \"Hand-Designed\" and \"Learned\" weight matrices than the solution key. It correctly identified that neural networks learn distributed representations (dense matrices) that are functionally equivalent to the sparse, orthogonal matrices humans design.</paragraph></list-item><list-item><paragraph><bold>Engineering Diagnosis (Q3):</bold> The highlight of the interaction was a <code>RuntimeError</code> during training caused by a conflict between Weight Tying (sharing embeddings with the output head) and the Safetensors library. The model correctly diagnosed this obscure compatibility issue and provided the specific flag (<code>torch_compile=False</code>) to resolve it.</paragraph></list-item><list-item><paragraph><bold>Numerical Reasoning:</bold> It demonstrated strong numerical intuition by proposing large scalar logits ($S=1000$) to force the Softmax function to behave like a hard <code>argmax</code>, ensuring the \"Identity\" transformer worked within strict floating-point tolerances.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T19:59:36.844112+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424719,
            "author": "Aaron Zheng",
            "project_title": "Special Participation E: Understanding Transformers and the value of KV Cache through visual diagrams",
            "post_body": "I am a hugely visual person, and I benefit from learning through images and videos. However, a lot of LLM and AI based concepts are really abstract, and it is hard to have detailed images of all the nuances of AI. \n\nHowever, Mindmaps that explain concepts in a visual, interdependent way, are helpful for me. Using a special prompt, I was able to learn more about transformers, KV caching and attention.\n\nI first told the LLM to generate a prompt that could make itself draw mindmaps and detailed drawings to explain machine learning concepts, where the \"concept\" is a variable. Then I copied the prompt and put the concept as \"transformers\". \n\nI then told the LLM to give me a markdown text-based version of the mindmap drawing, and it was very detailed, covering a lot of information. \n\nI generated two diagrams, shown here:\n\n\n\nPrompt:\n\n\"\"\"\n\nI want you to act as an expert AI educator who teaches complex machine learning concepts through highly intuitive mindmaps.\n\nGiven a topic, produce a mindmap-style breakdown with:\n\n1 central concept\n\n5\u20138 major branches\n\nUnder each branch:\n\nShort, intuitive explanations (not academic jargon)\n\nVisual analogies where appropriate\n\nStep-by-step decompositions\n\nImportant equations (only if helpful)\n\nCommon misunderstandings\n\nReal-world examples\n\nAdditionally, output:\n\nA \u201cBeginner-Friendly Summary\u201d at the top\n\nA \u201cDeep Intuition Layer\u201d beneath the mindmap (why the concept works)\n\nA \u201cHow It Fails / Edge Cases\u201d section\n\nA \u201cConnections to Other AI Concepts\u201d section\n\nA \u201cMental Hooks\u201d section (mnemonics / metaphors to remember the idea)\n\nPresent the mindmap in a structured outline like:\n\nCentral Node\n \u251c\u2500\u2500 Branch 1\n \u2502 \u251c\u2500\u2500 Subpoint\n \u2502 \u251c\u2500\u2500 Subpoint\n \u251c\u2500\u2500 Branch 2\n \u2502 \u251c\u2500\u2500 Subpoint\n \u2502 \u2514\u2500\u2500 Subpoint\n \u2026 etc.\n\nMake it extremely intuitive and visually organized so I can use it to build internal mental models.\n\nTopic: <INSERT YOUR TOPIC HERE>\n\n\"\"\"\n\nchatgpt convo: https://chatgpt.com/share/69353fee-afa8-8001-96f3-17b1c12da113\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I am a hugely visual person, and I benefit from learning through images and videos. However, a lot of LLM and AI based concepts are really abstract, and it is hard to have detailed images of all the nuances of AI. </paragraph><paragraph>However, Mindmaps that explain concepts in a visual, interdependent way, are helpful for me. Using a special prompt, I was able to learn more about transformers, KV caching and attention.</paragraph><paragraph>I first told the LLM to generate a prompt that could make itself draw mindmaps and detailed drawings to explain machine learning concepts, where the \"concept\" is a variable. Then I copied the prompt and put the concept as \"transformers\". </paragraph><paragraph>I then told the LLM to give me a markdown text-based version of the mindmap drawing, and it was very detailed, covering a lot of information. </paragraph><paragraph>I generated two diagrams, shown here:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DPGgxx2pdiLLyxdEYIEQu7oW\" width=\"643\" height=\"964.5\"/></figure><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/9SGVgQ7dcRTrO7PVyVbvSlFg\" width=\"643\" height=\"428.6666666666667\"/></figure><paragraph>Prompt:</paragraph><paragraph>\"\"\"</paragraph><paragraph>I want you to act as an expert AI educator who teaches complex machine learning concepts through highly intuitive mindmaps.</paragraph><paragraph>Given a topic, produce a <bold>mindmap-style breakdown</bold> with:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>1 central concept</bold></paragraph></list-item><list-item><paragraph><bold>5\u20138 major branches</bold></paragraph></list-item><list-item><paragraph>Under each branch:</paragraph><list style=\"unordered\"><list-item><paragraph>Short, intuitive explanations (not academic jargon)</paragraph></list-item><list-item><paragraph>Visual analogies where appropriate</paragraph></list-item><list-item><paragraph>Step-by-step decompositions</paragraph></list-item><list-item><paragraph>Important equations (only if helpful)</paragraph></list-item><list-item><paragraph>Common misunderstandings</paragraph></list-item><list-item><paragraph>Real-world examples</paragraph></list-item></list></list-item></list><paragraph>Additionally, output:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>A \u201cBeginner-Friendly Summary\u201d</bold> at the top</paragraph></list-item><list-item><paragraph><bold>A \u201cDeep Intuition Layer\u201d</bold> beneath the mindmap (why the concept works)</paragraph></list-item><list-item><paragraph><bold>A \u201cHow It Fails / Edge Cases\u201d</bold> section</paragraph></list-item><list-item><paragraph><bold>A \u201cConnections to Other AI Concepts\u201d</bold> section</paragraph></list-item><list-item><paragraph><bold>A \u201cMental Hooks\u201d</bold> section (mnemonics / metaphors to remember the idea)</paragraph></list-item></list><paragraph>Present the mindmap in a structured outline like:</paragraph><paragraph>Central Node<break/> \u251c\u2500\u2500 Branch 1<break/> \u2502 \u251c\u2500\u2500 Subpoint<break/> \u2502 \u251c\u2500\u2500 Subpoint<break/> \u251c\u2500\u2500 Branch 2<break/> \u2502 \u251c\u2500\u2500 Subpoint<break/> \u2502 \u2514\u2500\u2500 Subpoint<break/> \u2026 etc.</paragraph><paragraph>Make it extremely intuitive and visually organized so I can use it to build internal mental models.</paragraph><paragraph>Topic: <bold>&lt;INSERT YOUR TOPIC HERE&gt;</bold></paragraph><paragraph><bold>\"\"\"</bold></paragraph><paragraph>chatgpt convo: <link href=\"https://chatgpt.com/share/69353fee-afa8-8001-96f3-17b1c12da113\">https://chatgpt.com/share/69353fee-afa8-8001-96f3-17b1c12da113</link></paragraph><paragraph/><paragraph/></document>",
            "links": [
                "https://chatgpt.com/share/69353fee-afa8-8001-96f3-17b1c12da113"
            ],
            "attachments": [],
            "created_at": "2025-12-07T19:52:33.863435+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424701,
            "author": "Nikhil Mathihalli",
            "project_title": "Special Participation A: Grok on HW 12",
            "post_body": "I used Grok (Standard Chat) to tackle the non-coding theoretical portions of Homework 12. The model's performance was outstanding, effectively one-shotting every major conceptual and mathematical question I threw at it. Unlike previous generations of models that often hallucinate on complex derivations or misinterpret geometric relationships, Grok acted like a competent graduate-level tutor, correctly handling everything from VIB constraints to minimum-norm regression derivations.\n\nMethodology: I uploaded the full hw12.pdf and prompted the model interactively. I treated the model as a collaborator, asking it to derive solutions step-by-step.\n\nPer-Question Breakdown:\n\nQuestion 1: Debugging Transformers (Initialization)\n\nResult: Perfect / One-Shot\n\nAnalysis: Grok exhibited \"Global Code Awareness.\" It didn't just flag the std=1 initialization as a heuristic error; it explicitly linked it to Line 23 (weight tying), reasoning that sharing large-variance weights between input and output heads would cause logit explosion. It provided the correct fix ($1/\\sqrt{d_{model}}$) and the correct causal explanation.\n\nQuestion 2: Comparing Distributions (KL Divergence)\n\nResult: Perfect / One-Shot\n\nAnalysis: The model demonstrated deep intuition for Information Theory. It correctly identified the asymmetric \"zero-forcing\" vs. \"zero-avoiding\" behavior of Forward vs. Reverse KL. It also generated a mathematically valid counter-example (Nested Uniforms) to prove the finiteness condition D_KL(P||Q) < infinity vs D_KL(Q||P) = infinity without any prompting.\n\nQuestion 3: Variational Information Bottleneck (VIB)\n\nResult: Perfect / One-Shot\n\nAnalysis: This was the highlight of the session. Despite the questions relying on visual plots of latent spaces, Grok correctly deduced the answer from first principles. It reasoned that increasing the hyperparameter beta in the VIB Lagrangian forces the posterior to collapse to the prior (creating a \"blob\" ), while low beta prioritizes task accuracy (creating \"clusters\"). It matched every plot and error curve correctly based purely on theoretical physics-style reasoning.\n\nQuestion 5: Meta-Learning Derivations\n\nResult: Correct\n\nAnalysis: I tasked the model with a multi-step derivation for the expected test error of a minimum-norm solution. Grok correctly set up the constrained optimization problem, applied the orthonormality properties of the test features to cancel cross-terms, and derived the final analytical loss function. It avoided common pitfalls (like dropping variance terms) that I have seen other models make on this type of problem.\n\nConclusion: Grok graduated from being just a search engine to a viable theoretical assistant for me. Its ability to chain definitions (like weight tying or orthonormality) across multiple steps without losing state makes it highly effective for checking graduate-level coursework.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Grok (Standard Chat) to tackle the non-coding theoretical portions of Homework 12. The model's performance was outstanding, effectively one-shotting every major conceptual and mathematical question I threw at it. Unlike previous generations of models that often hallucinate on complex derivations or misinterpret geometric relationships, Grok acted like a competent graduate-level tutor, correctly handling everything from VIB constraints to minimum-norm regression derivations.</paragraph><paragraph><bold>Methodology:</bold> I uploaded the full <code>hw12.pdf</code> and prompted the model interactively. I treated the model as a collaborator, asking it to derive solutions step-by-step.</paragraph><paragraph><bold>Per-Question Breakdown:</bold></paragraph><paragraph><bold>Question 1: Debugging Transformers (Initialization)</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Result:</bold> <bold>Perfect / One-Shot</bold></paragraph></list-item><list-item><paragraph><bold>Analysis:</bold> Grok exhibited \"Global Code Awareness.\" It didn't just flag the <code>std=1</code> initialization as a heuristic error; it explicitly linked it to <bold>Line 23</bold> (weight tying), reasoning that sharing large-variance weights between input and output heads would cause logit explosion. It provided the correct fix ($1/\\sqrt{d_{model}}$) and the correct causal explanation.</paragraph></list-item></list><paragraph><bold>Question 2: Comparing Distributions (KL Divergence)</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Result:</bold> <bold>Perfect / One-Shot</bold></paragraph></list-item><list-item><paragraph><bold>Analysis:</bold> The model demonstrated deep intuition for Information Theory. It correctly identified the asymmetric \"zero-forcing\" vs. \"zero-avoiding\" behavior of Forward vs. Reverse KL. It also generated a mathematically valid counter-example (Nested Uniforms) to prove the finiteness condition D_KL(P||Q) &lt; infinity vs D_KL(Q||P) = infinity without any prompting.</paragraph></list-item></list><paragraph><bold>Question 3: Variational Information Bottleneck (VIB)</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Result:</bold> <bold>Perfect / One-Shot</bold></paragraph></list-item><list-item><paragraph><bold>Analysis:</bold> This was the highlight of the session. Despite the questions relying on visual plots of latent spaces, Grok correctly deduced the answer from first principles. It reasoned that increasing the hyperparameter beta in the VIB Lagrangian forces the posterior to collapse to the prior (creating a \"blob\" ), while low beta prioritizes task accuracy (creating \"clusters\"). It matched every plot and error curve correctly based purely on theoretical physics-style reasoning.</paragraph></list-item></list><paragraph><bold>Question 5: Meta-Learning Derivations</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Result:</bold> <bold>Correct</bold></paragraph></list-item><list-item><paragraph><bold>Analysis:</bold> I tasked the model with a multi-step derivation for the expected test error of a minimum-norm solution. Grok correctly set up the constrained optimization problem, applied the orthonormality properties of the test features to cancel cross-terms, and derived the final analytical loss function. It avoided common pitfalls (like dropping variance terms) that I have seen other models make on this type of problem.</paragraph></list-item></list><paragraph><bold>Conclusion:</bold> Grok graduated from being just a search engine to a viable theoretical assistant for me. Its ability to chain definitions (like weight tying or orthonormality) across multiple steps without losing state makes it highly effective for checking graduate-level coursework.</paragraph><file url=\"https://static.us.edusercontent.com/files/AMilAx3B0Dt71K4odXqMPvbx\" filename=\"Grok HW 12.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T19:43:08.619934+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424620,
            "author": "Aaron Zheng",
            "project_title": "Special Participation B: GPT-5.1 on HW4 coding",
            "post_body": "\n\nI used GPT5.1 to solve the coding portions of the Homework 4 (Q5 and Q6). It is very strong, and it basically one-shot all the answers, and is even more detailed than the official solution (defining variable names out separately). Tested by passing in the scripts where the TODOs are not filled in.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/6bdRSapGXgwdBbUYvdDojz6R\" filename=\"coding portion hw4.pdf\"/><paragraph/><paragraph>I used GPT5.1 to solve the coding portions of the Homework 4 (Q5 and Q6). It is very strong, and it basically one-shot all the answers, and is even more detailed than the official solution (defining variable names out separately). Tested by passing in the scripts where the TODOs are not filled in.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T18:52:36.878663+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424589,
            "author": "Kevin Tseng",
            "project_title": "Special Participation A: GPT-5 (thinking) on HW2",
            "post_body": "In this special participation, I interactively engage GPT-5 on the non-coding parts of Homework 2. My experience using it was boring and it one-shotted every question. I did not have to use any special strategies or gesture towards an answer to make it answer the problems correctly. I noticed one slight misconception in its reference to SignSGD and one missing transpose that made a column vector into a row vector. For the most part, however, it was hallucination free. One interesting thing was that it managed to point out a typo on the homework on part (b) of problem 2, deducing that the problem is incorrect and is \u201cill-posed.\u201d",
            "content_xml": "<document version=\"2.0\"><paragraph>In this special participation, I interactively engage GPT-5 on the non-coding parts of Homework 2. My experience using it was boring and it one-shotted every question. I did not have to use any special strategies or gesture towards an answer to make it answer the problems correctly. I noticed one slight misconception in its reference to SignSGD and one missing transpose that made a column vector into a row vector. For the most part, however, it was hallucination free. One interesting thing was that it managed to point out a typo on the homework on part (b) of problem 2, deducing that the problem is incorrect and is \u201cill-posed.\u201d</paragraph><file url=\"https://static.us.edusercontent.com/files/MjAZDqn4tjd9eQHQgg7uDsfV\" filename=\"hw_2_written_gpt5.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T18:42:41.687955+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424570,
            "author": "Dagny Streit",
            "project_title": "Special Participation E: Gemini Pro 3 as a Lecture-Grounded Discussion Worksheet Assistant",
            "post_body": "I prompted Gemini Pro 3 to act as a lecture-grounded discussion worksheet assistant. During discussions, I usually find myself scrolling through lecture notes to recall the relevant concepts, so my goal was to create a tool that helps students understand the conceptual foundations behind each worksheet question without giving solutions and without pulling from the worksheet\u2019s explanations.\n\nTo achieve this, I wrote a strict prompt enforcing:\n\nExclusive use of Lecture PDFs for conceptual content (I provided my handwritten lecture notes that tried to capture a lot of what was said in lecture).\n\nUse of Discussion PDFs only for reading the question text (I provided Discussion 13 and asked it about Question 2).\n\nI then tested whether the model could behave like an assistant who has only seen the lectures, not the solutions. The interaction included:\n\nInitial conceptual explanation of a worksheet question using lecture terminology only.\n\nZoom-in prompts to elaborate on specific lecture ideas (e.g., the tradeoff between losses).\n\nA diagnostic checklist of concepts students should review before attempting the discussion question.\n\nLecture-grounded practice questions and hints that did not overlap with worksheet wording.\n\nA self-correcting step, where the model identified phrases that were not actually supported by the lecture slides and rewrote them accordingly.\n\nWhat worked:\n\nThe model produced structured mini-lectures, learning goals, and checklists.\n\nIt consistently linked specific and relevant lecture slides, including portions of diagrams (note: the links do not show unfortunately in the PDF log of the conversation with Gemini).\n\nThe self-correcting step was effective. The model recognized when it used unsupported terminology and corrected itself.\n\nThese features made it genuinely useful for conceptual grounding before discussion.\n\nWhat did not work:\n\nI initially tried using ChatGPT, but I switched to Gemini because it supported clickable references to my uploaded lecture notes, which made checking grounding much easier.\n\nGemini tended to hallucinate on diffusion-specific concepts (Markov chains, Bayes\u2019 rule, etc.) that were not in the lecture PDFs, so explicit prompting and self-correcting were necessary. I think this was because it was looking at Question 1d instead of Question 2d of the discussion worksheet.\n\nMy lecture notes may not have been comprehensive and could have caused the model to occasionally fill gaps with general deep learning knowledge rather than lecture content. I try to capture a lot of information in my handwritten lecture notes, but some things that are said in lecture may be missed.\n\nOverall, it worked well once constrained, and the self-correction step was key for keeping it oriented with lecture material. Attached is the annotated conversation.",
            "content_xml": "<document version=\"2.0\"><paragraph>I prompted Gemini Pro 3 to act as a lecture-grounded discussion worksheet assistant. During discussions, I usually find myself scrolling through lecture notes to recall the relevant concepts, so my goal was to create a tool that helps students understand the conceptual foundations behind each worksheet question without giving solutions and without pulling from the worksheet\u2019s explanations.</paragraph><paragraph>To achieve this, I wrote a strict prompt enforcing:</paragraph><list style=\"unordered\"><list-item><paragraph>Exclusive use of Lecture PDFs for conceptual content (I provided my handwritten lecture notes that tried to capture a lot of what was said in lecture).</paragraph></list-item><list-item><paragraph>Use of Discussion PDFs only for reading the question text (I provided Discussion 13 and asked it about Question 2).</paragraph></list-item></list><paragraph>I then tested whether the model could behave like an assistant who has only seen the lectures, not the solutions. The interaction included:</paragraph><list style=\"ordered\"><list-item><paragraph>Initial conceptual explanation of a worksheet question using lecture terminology only.</paragraph></list-item><list-item><paragraph>Zoom-in prompts to elaborate on specific lecture ideas (e.g., the tradeoff between losses).</paragraph></list-item><list-item><paragraph>A diagnostic checklist of concepts students should review before attempting the discussion question.</paragraph></list-item><list-item><paragraph>Lecture-grounded practice questions and hints that did not overlap with worksheet wording.</paragraph></list-item><list-item><paragraph>A self-correcting step, where the model identified phrases that were not actually supported by the lecture slides and rewrote them accordingly.</paragraph></list-item></list><paragraph>What worked:</paragraph><list style=\"unordered\"><list-item><paragraph>The model produced structured mini-lectures, learning goals, and checklists.</paragraph></list-item><list-item><paragraph>It consistently linked specific and relevant lecture slides, including portions of diagrams (note: the links do not show unfortunately in the PDF log of the conversation with Gemini).</paragraph></list-item><list-item><paragraph>The self-correcting step was effective. The model recognized when it used unsupported terminology and corrected itself.</paragraph></list-item><list-item><paragraph>These features made it genuinely useful for conceptual grounding before discussion.</paragraph></list-item></list><paragraph>What did not work:</paragraph><list style=\"unordered\"><list-item><paragraph>I initially tried using ChatGPT, but I switched to Gemini because it supported clickable references to my uploaded lecture notes, which made checking grounding much easier.</paragraph></list-item><list-item><paragraph>Gemini tended to hallucinate on diffusion-specific concepts (Markov chains, Bayes\u2019 rule, etc.) that were not in the lecture PDFs, so explicit prompting and self-correcting were necessary. I think this was because it was looking at Question 1d instead of Question 2d of the discussion worksheet.</paragraph></list-item><list-item><paragraph>My lecture notes may not have been comprehensive and could have caused the model to occasionally fill gaps with general deep learning knowledge rather than lecture content. I try to capture a lot of information in my handwritten lecture notes, but some things that are said in lecture may be missed.</paragraph></list-item></list><paragraph>Overall, it worked well once constrained, and the self-correction step was key for keeping it oriented with lecture material. <link href=\"https://drive.google.com/file/d/1buAqcR2x1bfJGo_H2RfKZxrEfkm-wESY/view?usp=sharing\">Attached is the annotated conversation</link>.</paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1buAqcR2x1bfJGo_H2RfKZxrEfkm-wESY/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T18:35:05.240735+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424515,
            "author": "Aaron Zheng",
            "project_title": "Special Participation A: GPT-4o on Hw0",
            "post_body": "Below is my report on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript. \n\n\n\nThere are some situations when GPT4o made some small minor syntactical mistakes, i.e. getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix. \n\nIt struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly. It shows how 4th generation models' graphical intuition has not been foolproof yet. It keeps getting it wrong even after few shot tips, and even when I tell it the answer of the problem it sometimes using the wrong intuition to justify the problem. (for example part (ii), relationship between b and w/x is correct instead of b and wx). \n\nIt also seems to sometimes get the solution right but omit detailed justification (unless prompted), such as for part (d) of question 5. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is my report on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript. </paragraph><file url=\"https://static.us.edusercontent.com/files/ZBvoGesWDaE4p5dyhfyIT1UL\" filename=\"Vector calculus derivatives.pdf\"/><paragraph/><paragraph>There are some situations when GPT4o made some small minor syntactical mistakes, i.e. getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix. </paragraph><paragraph>It struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly. It shows how 4th generation models' graphical intuition has not been foolproof yet. It keeps getting it wrong even after few shot tips, and even when I tell it the answer of the problem it sometimes using the wrong intuition to justify the problem. (for example part (ii), relationship between b and w/x is correct instead of b and wx). </paragraph><paragraph>It also seems to sometimes get the solution right but omit detailed justification (unless prompted), such as for part (d) of question 5. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T18:12:51.093512+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424475,
            "author": "Elizabeth Polito",
            "project_title": "Special Participation E: Structured Notes with NotebookLM RAG Tool",
            "post_body": "For this assignment, I evaluated whether Google NotebookLM is a viable tool for generating \"Cornell\" Notes from our handwritten lecture slides. Taking notes in \u201cCornell Style\u201d can help to make sure you are not getting bogged down in the details by dedicating a column on the left of your page to keywords/main ideas, a column on the right to the notes themselves, and a box on the bottom for a summary. Here is a summary of why these types of notes are useful: Cornell Note Taking \u2014 The Best Way To Take Notes, Explained | Goodnotes Blog\n\n(Citation: Image taken from link above for illustrative purposes).\n\nI used NotebookLM, which is a RAG-based research tool that grounds its generation primarily in your uploaded documents. I was able to carry out the generation simply by uploading the handwritten lecture notes (interestingly, I did have to \u201cflatten\u201d the PDFs before the software was able to \u201cread\u201d the content, and prior to doing so, NotebookLM insisted that the document was empty). I chose Lecture 7 (MuON) as the stress test because it contains relatively few diagrams which would have been more of a challenge for the tool to replicate/parse. \n\nOverall, because NotebookLM is powered by Gemini Pro which, as many students have pointed out in their special participation submissions, is very well-equipped to deal with this content, it did a highly accurate job of generating Cornell style notes and writing decent summaries. In my opinion, the biggest limitation with the NotebookLM tool is the I/O behavior, between some issues uploading the PDF as discussed and the fact that I could not download a PDF of the trace or of the Cornell notes themselves. Its ability to parse the notes and provide inline citations for which page of the document the equations came from was very helpful once the document was in a format it could parse. The tool seems to have many more built-in features that would be very interesting to explore.\n\nThe Artifact\n\nI have attached the PDF of the transcript below, which includes the Cornell notes within. The structure follows the Cornell method:\n\nLeft Column: High-level cues (\"Newton-Schulz Iteration\", \"Shampoo vs MuON\").\n\nRight Column: More detailed mathematical notes/derivations with inline citations.\n\nSummary: A synthesis of the \u201cpage\u201d of notes. \n\nI have uploaded an annotated trace of my conversation, which includes the Cornell notes within. ",
            "content_xml": "<document version=\"2.0\"><paragraph>For this assignment, I evaluated whether Google NotebookLM is a viable tool for generating \"Cornell\" Notes from our handwritten lecture slides. Taking notes in \u201cCornell Style\u201d can help to make sure you are not getting bogged down in the details by dedicating a column on the left of your page to keywords/main ideas, a column on the right to the notes themselves, and a box on the bottom for a summary. Here is a summary of why these types of notes are useful: <link href=\"https://www.goodnotes.com/blog/cornell-notes\"><underline>Cornell Note Taking \u2014 The Best Way To Take Notes, Explained | Goodnotes Blog</underline></link></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/OP3WsLpngu6hQYXfpYkVmise\" width=\"328\" height=\"126\"/></figure><paragraph>(Citation: Image taken from link above for illustrative purposes).</paragraph><paragraph>I used NotebookLM, which is a RAG-based research tool that grounds its generation primarily in your uploaded documents. I was able to carry out the generation simply by uploading the handwritten lecture notes (interestingly, I did have to \u201cflatten\u201d the PDFs before the software was able to \u201cread\u201d the content, and prior to doing so, NotebookLM insisted that the document was empty). I chose Lecture 7 (MuON) as the stress test because it contains relatively few diagrams which would have been more of a challenge for the tool to replicate/parse. </paragraph><paragraph>Overall, because NotebookLM is powered by Gemini Pro which, as many students have pointed out in their special participation submissions, is very well-equipped to deal with this content, it did a highly accurate job of generating Cornell style notes and writing decent summaries. In my opinion, the biggest limitation with the NotebookLM tool is the I/O behavior, between some issues uploading the PDF as discussed and the fact that I could not download a PDF of the trace or of the Cornell notes themselves. Its ability to parse the notes and provide inline citations for which page of the document the equations came from was very helpful once the document was in a format it could parse. The tool seems to have many more built-in features that would be very interesting to explore.</paragraph><paragraph>The Artifact</paragraph><paragraph>I have attached the PDF of the transcript below, which includes the Cornell notes within. The structure follows the Cornell method:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Left Column:</bold> High-level cues (\"Newton-Schulz Iteration\", \"Shampoo vs MuON\").</paragraph></list-item><list-item><paragraph><bold>Right Column:</bold> More detailed mathematical notes/derivations with inline citations.</paragraph></list-item><list-item><paragraph><bold>Summary:</bold> A synthesis of the \u201cpage\u201d of notes. </paragraph></list-item></list><paragraph>I have uploaded an annotated trace of my conversation, which includes the Cornell notes within. </paragraph><file url=\"https://static.us.edusercontent.com/files/aUuZSCc3YLb25Q4tuQPAzacD\" filename=\"muon_annotated.pdf\"/></document>",
            "links": [
                "https://www.goodnotes.com/blog/cornell-notes"
            ],
            "attachments": [],
            "created_at": "2025-12-07T17:58:19.508117+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424461,
            "author": "Cameron Jordan",
            "project_title": "Special Participation B: Qwen3-Max on HW03",
            "post_body": "I used Qwen3-Max on the (only) coding question on HW03 (Visualizing Maximal Update Parameterization); overall I was a bit disappointed in the coding ability of Qwen3-Max. There were very few sub-questions that it was able to solve correctly in one shot, and for questions that it provided incorrect code for, when asked to make corrections, it would offer either the same incorrect solution or an alternative (incorrect) solution - rarely was it able to recover from an error. \n\nIts ability to analyze plots was decent, though it didn't seem to be able to reason about what the plots *mean* beyond surface level characteristics. \n\nI attribute a lot of this to the fact that muP is a very new optimizer, and therefore is likely not present in the training corpus for Qwen3-Max which makes it unfamiliar with the intricacies necessary to make this type of optimizer work properly.\n\nAn annotated Jupyter notebook clearly walks through the prompting process, with original notebook components, user prompts, LLM responses and annotations (in red) clearly differentiated, is available here.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Qwen3-Max on the (only) coding question on HW03 (Visualizing Maximal Update Parameterization); overall I was a bit disappointed in the coding ability of Qwen3-Max. There were very few sub-questions that it was able to solve correctly in one shot, and for questions that it provided incorrect code for, when asked to make corrections, it would offer either the same incorrect solution or an alternative (incorrect) solution - rarely was it able to recover from an error. <break/><break/>Its ability to analyze plots was decent, though it didn't seem to be able to reason about what the plots <italic>*mean*</italic> beyond surface level characteristics. <break/><break/>I attribute a lot of this to the fact that muP is a very new optimizer, and therefore is likely not present in the training corpus for Qwen3-Max which makes it unfamiliar with the intricacies necessary to make this type of optimizer work properly.</paragraph><paragraph>An annotated Jupyter notebook clearly walks through the prompting process, with original notebook components, user prompts, LLM responses and annotations (in red) clearly differentiated, is available <link href=\"https://colab.research.google.com/drive/1zODhJm0Q9Uoj4154mwa51LqV65_5kXC6?usp=sharing\">here</link>.</paragraph><paragraph/></document>",
            "links": [
                "https://colab.research.google.com/drive/1zODhJm0Q9Uoj4154mwa51LqV65_5kXC6?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T17:54:54.4736+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424438,
            "author": "Hanna Roed",
            "project_title": "Special Participation C: HW10 Refactor",
            "post_body": "Nils Selte and I refactored HW10 for special participation C. Below is our report on what we changed in the code and why, along with a description of the AI assistance we used and the link to our GitHub repository with the changes.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Nils Selte and I refactored HW10 for special participation C. Below is our report on what we changed in the code and why, along with a description of the AI assistance we used and the link to our GitHub repository with the changes.</paragraph><file url=\"https://static.us.edusercontent.com/files/25i0KjiDkRjeLMNeaD7hCgpz\" filename=\"Special Participation C_ Nils Selte and Hanna Roed.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T17:48:19.892944+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424390,
            "author": "Jiayi Zhang",
            "project_title": "Special Participation B: ChatGPT 5.1 on HW 5",
            "post_body": "I am using ChatGPT 5.1 to answer the coding questions in Homework 5. ChatGPT 5.1 demonstrates an strong performance on the coding portion of Homework 5. Using only a short architectural description, the model produced a clean, fully correct, and idiomatic PyTorch implementation that passed the given tests. It seemed to be able to connect the logic between different files to formulate a bigger picture to correctly implement the logic.\n\nChatGPT 5.1 also noted as a comment to the architectural and dimensional constraints. It also anticipated potential implementation pitfalls. The model\u2019s code was written in the best practices and with decent comments to help me to understand the logic. The model verified tensor dimensions after every operation, showing a strong internal understanding of spatial transformations through convolution and pooling.\n\nOverall, this experiment reinforces that ChatGPT 5.1 handles coding tasks with strong reliability. Its performance suggests the model has not only been trained extensively on CNN-related content but also showed potential to generalize best coding practices in other deep learning fields.\n\n\nChat history:\nhttps://chatgpt.com/share/69351bdf-1188-8007-aa50-a9287885e9bd\n\nAnnotation:\n\nhttps://docs.google.com/document/d/1Z2gvkOXmvzqWxRqpSki_-DR_uI-uq2EU_xwtRPPQf1w/edit?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>I am using ChatGPT 5.1 to answer the coding questions in Homework 5. ChatGPT 5.1 demonstrates an strong performance on the coding portion of Homework 5. Using only a short architectural description, the model produced a clean, fully correct, and idiomatic PyTorch implementation that passed the given tests. It seemed to be able to connect the logic between different files to formulate a bigger picture to correctly implement the logic.</paragraph><paragraph>ChatGPT 5.1 also noted as a comment to the architectural and dimensional constraints. It also anticipated potential implementation pitfalls. The model\u2019s code was written in the best practices and with decent comments to help me to understand the logic. The model verified tensor dimensions after every operation, showing a strong internal understanding of spatial transformations through convolution and pooling.</paragraph><paragraph>Overall, this experiment reinforces that ChatGPT 5.1 handles coding tasks with strong reliability. Its performance suggests the model has not only been trained extensively on CNN-related content but also showed potential to generalize best coding practices in other deep learning fields.</paragraph><paragraph><break/>Chat history:<break/><link href=\"https://chatgpt.com/share/69351bdf-1188-8007-aa50-a9287885e9bd\">https://chatgpt.com/share/69351bdf-1188-8007-aa50-a9287885e9bd</link></paragraph><paragraph>Annotation:</paragraph><paragraph>https://docs.google.com/document/d/1Z2gvkOXmvzqWxRqpSki_-DR_uI-uq2EU_xwtRPPQf1w/edit?usp=sharing</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/69351bdf-1188-8007-aa50-a9287885e9bd"
            ],
            "attachments": [],
            "created_at": "2025-12-07T17:26:22.197864+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424307,
            "author": "Leon Kornfeld",
            "project_title": "Special Participation E: Nano Banana 3 Pro for Note Generation and Content Understanding",
            "post_body": "Google recently launched Nano Banana Pro, its new image generating model. I saw online that it could do many cool things such as solve math problems on paper or explain pictures in depth. I decided to use it on a bunch of different tasks to see how it fairs as a study tool. \n\nExperiment 1, Convolution:\n\nPrompt 1: Can you make me an image of hand drawn notes explaining a convolutional neural network. I will prompt you for many different images but can you draw first a diagram of what a convolution would look like with a 2D kernel and a 3D kernel. Be sure to include a visualization of the output channels depending on how many kernels are applied.\n\nOutput 1:\n\nAnalysis:\n\nOverall, the note looks fantastic. The handwriting is neat and it is nicely structured. The first section of the note makes sense. It explains the convolution operation as a dot product and sum. The output feature map is of proper dimension assuming zero padding, and stride is 1. There does seem to be a rogue \"& sum\" underneath the bottom arrow.\n\nWhile the 3D convolution section correctly notes the output map is of depth 1, it incorrectly generated the output dimensions as a 3 by 3. Also, the choice of operator \"+\" is an odd notation to use.\n\nLastly, for the \"Multiple Kernels and Output Channels\" section, it correctly identified that there should be 3 output channels. However, the diagrams have the weird \"+\" notation and the input and output dimensions are not really detailed.\n\nTo address the dimension issue in \"3D convolution\", I submitted prompt 2:\nPlease make it more mathematically rigorous. For example, in your 3D convolution, there are only 4 rows in the original input and the kernel is of size 3. However, their output has 3 rows. This is incorrect if assuming zero padding since the number of rows should just be 2.\n\nOutput 2:\n\nWith this prompt all the output feature maps were filled in with numbers. They are all obviously incorrect. Moreover, while it did shrink the number of rows in \"3D Convolution\" to 2, it also shrunk the number of columns to 2 as well. The output map dimensions in \"Multiple Kernels\" are correct, but there is a coloring issue: for consistency, it would make sense that the first output channel is purple.\n\nPrompt 3:\n\nThis is also still incorrect since the output map for the 3D convolution should be a 2 by 3, not a 2 by 2. Also the numerical values and the math do not make sense.\n\nLastly, for the multiple kernels and output channels section, it's confusing notation of adding the different kernels. Can you please fix that.\n\nOverall, the coloring scheme is also kind of confusing since you have a purple kernel that doesn't have a corresponding output channel while the green and orange kernels do. Please be consistent with everything you are doing.\n\n(Note: this prompt could have definitely been better).\n\nOutput 3:\n\nThere was no real improvement to \"3D Convolution.\" However, \"Multiple Kernels\" did get rid of the weird \"+\" notation and showed each output map individually for the 3 kernels with the proper coloring scheme. However, the last step still has the wrong colors. Also note, none of the numbers are correct.\n\nConclusion: \n\nNano Banana 3 has an amazing ability to generate notes that discuss the concept of CNNs. While the specifics (dot product values, correct dimensions, etc.) are still shaky, the overall structure, quality, and high level concepts are very very good. Based on this experiment, however, I would not yet recommend using this as a study tool.\n\nExperiment 2, Images -> Images:\n\nPrompt 1: Solve this problem in my hand writing and generate an image of the work and the solution\n\nOutput 1:\n\n\n\nNano Banana 3 was able to read my handwriting and recognize that the problem it needed to solve was least squares. It proceeded to provide a mostly correctly proof (minus a few notational issues with the transposes) and arrived at a correct answer. It even added in the condition that A^T A needs to be invertible. However, while it did not solve the problem with my handwriting, the type of paper used stayed the same.\n\nFor my next prompt, I provided a screenshot of a note from lecture:\n\nPrompt 2:\n\nTake this lecture note about the weights in MLPS and CNNs and annotate the image explaining what each picture means.\n\nOutput 2:\n\n\n\nMistake 1: The top two description boxes are flipped. However, the content in each box would be fully correct were they pointing to the right spot.\n\nMistake 2: It correctly notes that the diagrams are from the Prince book but the written text describing that is no longer in the output.\n\nThe rest of the description boxes are actually quite amazing. Descriptions b and d both relate their images to the corresponding images in a and c, showing an in-depth understanding of all the images provides in the prompt image. Moreover, their individual interpretations of the weight matrices and what they represent without much context is also quite great.\n\nConclusion:\n\nNano Banana Pro has an impressive ability to take in images , understand their contents, and produce new images by either adding on to or augmenting the inputted ones. I ran another experiment that I did not feature here that took in images describing more complicated math. The output was close to the original picture (handwriting slightly rewritten) with some added gibberish. Therefore, I would conclude that Nano Banana Pro has the ability to understand well structured diagrams or easy math, but it is not necessarily at the point yet where it can understand and generate images for more complicated math.",
            "content_xml": "<document version=\"2.0\"><paragraph>Google recently launched Nano Banana Pro, its new image generating model. I saw online that it could do many cool things such as solve math problems on paper or explain pictures in depth. I decided to use it on a bunch of different tasks to see how it fairs as a study tool. </paragraph><paragraph><bold>Experiment 1, Convolution:</bold></paragraph><paragraph>Prompt 1: Can you make me an image of hand drawn notes explaining a convolutional neural network. I will prompt you for many different images but can you draw first a diagram of what a convolution would look like with a 2D kernel and a 3D kernel. Be sure to include a visualization of the output channels depending on how many kernels are applied.</paragraph><paragraph>Output 1:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YfCldqolUrZFzgQVeTXNfzEE\" width=\"659\" height=\"359.45454545454544\"/></figure><paragraph>Analysis:</paragraph><paragraph>Overall, the note looks fantastic. The handwriting is neat and it is nicely structured. The first section of the note makes sense. It explains the convolution operation as a dot product and sum. The output feature map is of proper dimension assuming zero padding, and stride is 1. There does seem to be a rogue \"&amp; sum\" underneath the bottom arrow.</paragraph><paragraph>While the 3D convolution section correctly notes the output map is of depth 1, it incorrectly generated the output dimensions as a 3 by 3. Also, the choice of operator \"+\" is an odd notation to use.</paragraph><paragraph>Lastly, for the \"Multiple Kernels and Output Channels\" section, it correctly identified that there should be 3 output channels. However, the diagrams have the weird \"+\" notation and the input and output dimensions are not really detailed.</paragraph><paragraph>To address the dimension issue in \"3D convolution\", I submitted prompt 2:<break/>Please make it more mathematically rigorous. For example, in your 3D convolution, there are only 4 rows in the original input and the kernel is of size 3. However, their output has 3 rows. This is incorrect if assuming zero padding since the number of rows should just be 2.</paragraph><paragraph>Output 2:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/WstJi0DHnMvB4gHOOmiGLtfZ\" width=\"659\" height=\"359.45454545454544\"/></figure><paragraph>With this prompt all the output feature maps were filled in with numbers. They are all obviously incorrect. Moreover, while it did shrink the number of rows in \"3D Convolution\" to 2, it also shrunk the number of columns to 2 as well. The output map dimensions in \"Multiple Kernels\" are correct, but there is a coloring issue: for consistency, it would make sense that the first output channel is purple.</paragraph><paragraph>Prompt 3:</paragraph><paragraph>This is also still incorrect since the output map for the 3D convolution should be a 2 by 3, not a 2 by 2. Also the numerical values and the math do not make sense.</paragraph><paragraph>Lastly, for the multiple kernels and output channels section, it's confusing notation of adding the different kernels. Can you please fix that.</paragraph><paragraph>Overall, the coloring scheme is also kind of confusing since you have a purple kernel that doesn't have a corresponding output channel while the green and orange kernels do. Please be consistent with everything you are doing.</paragraph><paragraph>(Note: this prompt could have definitely been better).</paragraph><paragraph>Output 3:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VX1xx3trU7c70YkU5V19W6em\" width=\"659\" height=\"359.45454545454544\"/></figure><paragraph>There was no real improvement to \"3D Convolution.\" However, \"Multiple Kernels\" did get rid of the weird \"+\" notation and showed each output map individually for the 3 kernels with the proper coloring scheme. However, the last step still has the wrong colors. Also note, none of the numbers are correct.</paragraph><paragraph><underline>Conclusion<bold>:</bold></underline> </paragraph><paragraph>Nano Banana 3 has an amazing ability to generate notes that discuss the concept of CNNs. While the specifics (dot product values, correct dimensions, etc.) are still shaky, the overall structure, quality, and high level concepts are very very good. Based on this experiment, however, I would not yet recommend using this as a study tool.</paragraph><paragraph><bold>Experiment 2, Images -&gt; Images:</bold></paragraph><paragraph>Prompt 1: Solve this problem in my hand writing and generate an image of the work and the solution</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/b1oRLJzgGdwjPeMFDOeMWeZw\" width=\"659\" height=\"525.0996015936255\"/></figure><paragraph>Output 1:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/tzeb74eNam3Oay43EDdGyxvj\" width=\"659\" height=\"521.7083333333334\"/></figure><paragraph/><paragraph>Nano Banana 3 was able to read my handwriting and recognize that the problem it needed to solve was least squares. It proceeded to provide a mostly correctly proof (minus a few notational issues with the transposes) and arrived at a correct answer. It even added in the condition that A^T A needs to be invertible. However, while it did not solve the problem with my handwriting, the type of paper used stayed the same.</paragraph><paragraph>For my next prompt, I provided a screenshot of a note from lecture:</paragraph><paragraph>Prompt 2:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NYPNf5PCGsFzJIK5aSAAjN22\" width=\"659\" height=\"485.79599499374217\"/></figure><paragraph>Take this lecture note about the weights in MLPS and CNNs and annotate the image explaining what each picture means.</paragraph><paragraph>Output 2:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/8RovKbeFBuIn3yZHcEmnLaB3\" width=\"659\" height=\"483.2666666666667\"/></figure><paragraph/><paragraph>Mistake 1: The top two description boxes are flipped. However, the content in each box would be fully correct were they pointing to the right spot.</paragraph><paragraph>Mistake 2: It correctly notes that the diagrams are from the Prince book but the written text describing that is no longer in the output.</paragraph><paragraph>The rest of the description boxes are actually quite amazing. Descriptions b and d both relate their images to the corresponding images in a and c, showing an in-depth understanding of all the images provides in the prompt image. Moreover, their individual interpretations of the weight matrices and what they represent without much context is also quite great.</paragraph><paragraph><underline>Conclusion:</underline></paragraph><paragraph>Nano Banana Pro has an impressive ability to take in images , understand their contents, and produce new images by either adding on to or augmenting the inputted ones. I ran another experiment that I did not feature here that took in images describing more complicated math. The output was close to the original picture (handwriting slightly rewritten) with some added gibberish. Therefore, I would conclude that Nano Banana Pro has the ability to understand well structured diagrams or easy math, but it is not necessarily at the point yet where it can understand and generate images for more complicated math.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T17:00:57.698038+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424271,
            "author": "Zhengwei Fan",
            "project_title": "Special Participation A:  Gemini 3 Pro on the written part of HW 10",
            "post_body": "\n\nModel Used: Gemini 3 Pro \n\nOverall Performance: The model demonstrated exceptional proficiency in both advanced mathematical derivations (kernel methods) and deep learning architectural analysis. It successfully one-shot most conceptual questions. \n\nKey Observations: The model flawlessly derived the Linear Attention mechanism using Random Fourier Features. It correctly identified the decomposition of the Softmax kernel into Query/Key norms and the Gaussian term, a non-trivial step often missed by us. It also correctly formulated the causal masking as an RNN-style recurrence (O(1) inference). For the problem 5, when challenged on the counter-intuitive discrepancy between FaceNet NN1\u2019s high parameter count (140M) vs. low FLOPs (1.6B) compared to ResNet-50, the model correctly attributed this to the Dense (FC) layers vs. Deep Convolutional layers trade-off. It did not hallucinate incorrect numbers and correctly referenced standard architecture traits (VGG-style vs. ResNet-style). \n\nStrategy Used: I used a \"verify and deepen\" strategy. After the model provided initial correct answers, I explicitly challenged it with conflicting data (High Params vs Low FLOPs) to test if it truly understood the underlying architecture or was just retrieving surface-level stats. The model proved it possessed deep understanding.\n\nChat history: https://gemini.google.com/share/6edfefc10fd1",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/aA3baX2AapGth2moS1XYhRZ6\" filename=\"Special participation A.pdf\"/><paragraph>Model Used: Gemini 3 Pro </paragraph><paragraph>Overall Performance: The model demonstrated exceptional proficiency in both advanced mathematical derivations (kernel methods) and deep learning architectural analysis. It successfully one-shot most conceptual questions. </paragraph><paragraph>Key Observations: The model flawlessly derived the Linear Attention mechanism using Random Fourier Features. It correctly identified the decomposition of the Softmax kernel into Query/Key norms and the Gaussian term, a non-trivial step often missed by us. It also correctly formulated the causal masking as an RNN-style recurrence (O(1) inference). For the problem 5, when challenged on the counter-intuitive discrepancy between FaceNet NN1\u2019s high parameter count (140M) vs. low FLOPs (1.6B) compared to ResNet-50, the model correctly attributed this to the Dense (FC) layers vs. Deep Convolutional layers trade-off. It did not hallucinate incorrect numbers and correctly referenced standard architecture traits (VGG-style vs. ResNet-style). </paragraph><paragraph>Strategy Used: I used a \"verify and deepen\" strategy. After the model provided initial correct answers, I explicitly challenged it with conflicting data (High Params vs Low FLOPs) to test if it truly understood the underlying architecture or was just retrieving surface-level stats. The model proved it possessed deep understanding.</paragraph><paragraph>Chat history: https://gemini.google.com/share/6edfefc10fd1</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T16:53:08.077061+11:00",
            "category": "Admin"
        },
        {
            "guid": 7424254,
            "author": "Rishi Thakar",
            "project_title": "Special Participation A: Claude Opus 4.5 on HW05 (Written Questions)",
            "post_body": "I used Claude Opus 4.5 Thinking to solve the written portions of Homework 5, covering convolutional networks, batch normalization, depthwise separable convolutions, and dropout as regularization. Questions were fed one at a time with the convolution convention (no filter flip) specified upfront.\n\nOverall, Claude Opus 4.5 achieved a 100% one-shot success rate across all 11 sub-questions with no corrective prompting needed.\n\nQuestion 1: Convolutional Networks - All correct. Part (a) correctly identified weight sharing and translation equivariance. Part (b) solved the linear system and self-verified the filter [2, -1, 3]. Part (c) computed the 2D transpose convolution output correctly with clear bookkeeping.\n\nQuestion 2: Batch Normalization - All correct. Part (a) correctly identified Batch Norm as Diagram A and Layer Norm as Diagram B, and even noted Diagram C is Instance Norm without being asked. Part (b) derived the gradient correctly, properly handling the n=1 edge case (gradient equals 0) and n approaching infinity limit.\n\nQuestion 3: Depthwise Separable Convolutions - All correct. Calculated 108 parameters for traditional convolution and 39 for depthwise separable. Provided unprompted context about MobileNet and the 64% parameter reduction.\n\nQuestion 4: Regularization and Dropout - All correct. Part (a) produced a rigorous proof of dropout's equivalence to Tikhonov regularization, correctly handling E[R squared] = p for Bernoulli variables. Part (b) identified w = p times w-check and explained inverted dropout. Part (c) derived X-tilde = X times Gamma-inverse. Part (d) showed all columns of X-tilde have equal norm and connected this to batch normalization.\n\nSummary: 11/11 one-shot correct, 0 hallucinations, 0 guidance needed. The model handled conceptual questions, numerical calculations, and multi-step proofs without errors, frequently providing bonus insights beyond what was asked.\n\nConversation trace: https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96de\n\nMy annotated pdf: \n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Claude Opus 4.5 Thinking to solve the written portions of Homework 5, covering convolutional networks, batch normalization, depthwise separable convolutions, and dropout as regularization. Questions were fed one at a time with the convolution convention (no filter flip) specified upfront.</paragraph><paragraph>Overall, Claude Opus 4.5 achieved a 100% one-shot success rate across all 11 sub-questions with no corrective prompting needed.</paragraph><paragraph><bold>Question 1: Convolutional Networks</bold> - All correct. Part (a) correctly identified weight sharing and translation equivariance. Part (b) solved the linear system and self-verified the filter [2, -1, 3]. Part (c) computed the 2D transpose convolution output correctly with clear bookkeeping.</paragraph><paragraph><bold>Question 2: Batch Normalization</bold> - All correct. Part (a) correctly identified Batch Norm as Diagram A and Layer Norm as Diagram B, and even noted Diagram C is Instance Norm without being asked. Part (b) derived the gradient correctly, properly handling the n=1 edge case (gradient equals 0) and n approaching infinity limit.</paragraph><paragraph><bold>Question 3: Depthwise Separable Convolutions</bold> - All correct. Calculated 108 parameters for traditional convolution and 39 for depthwise separable. Provided unprompted context about MobileNet and the 64% parameter reduction.</paragraph><paragraph><bold>Question 4: Regularization and Dropout</bold> - All correct. Part (a) produced a rigorous proof of dropout's equivalence to Tikhonov regularization, correctly handling E[R squared] = p for Bernoulli variables. Part (b) identified w = p times w-check and explained inverted dropout. Part (c) derived X-tilde = X times Gamma-inverse. Part (d) showed all columns of X-tilde have equal norm and connected this to batch normalization.</paragraph><paragraph><bold>Summary:</bold> 11/11 one-shot correct, 0 hallucinations, 0 guidance needed. The model handled conceptual questions, numerical calculations, and multi-step proofs without errors, frequently providing bonus insights beyond what was asked.</paragraph><paragraph>Conversation trace: <link href=\"https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96de\">https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96de</link><break/><break/>My annotated pdf: <break/></paragraph><file url=\"https://static.us.edusercontent.com/files/HmcgNfPZBJIqtz97bpb4bR6l\" filename=\"HW5 Annotated.pdf\"/></document>",
            "links": [
                "https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96de"
            ],
            "attachments": [],
            "created_at": "2025-12-07T16:50:18.353794+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424217,
            "author": "Zesheng Cai",
            "project_title": "Special Participation E: Misconception Diagnostic Learning with Claude and Deepseek",
            "post_body": "In this assignment, I designed an AI-enhanced learning tool that does not simply explain deep learning concepts but instead actively diagnoses and corrects my misunderstandings. I interacted with two different AI models\u2014Claude and DeepSeek\u2014and provided intentionally imperfect answers to their diagnostic questions. This allowed the models to analyze my reasoning, detect misconceptions, and generate personalized explanations.\n\nThe workflow follows four stages:\n\nI answer diagnostic questions about Attention and Self-Attention.\n These answers intentionally contain a mix of correct ideas and common misunderstandings.\n\nThe AI infers my misconceptions from my responses.\n Each system identifies which parts of my reasoning are correct, partially correct, or mistaken.\n\nThe AI produces tailored explanations based on the specific misconceptions it detected.\n These include counterexamples, intuitive analogies, and conceptual clarifications.\n\nThe AI generates personalized exercises that target the exact areas where I showed confusion.\n This turns the interaction into a highly adaptive learning process.\n\nThis workflow transforms AI from a passive explanation tool into an active diagnostic tutor\u2014similar to a human TA who tests my understanding before teaching.\n\nInstead of asking the AI to teach me concepts directly, I designed a system where the AI actively discovers and corrects my misunderstandings about deep learning. I call this approach a Misconception Diagnostic System. It acts like a doctor diagnosing a patient:\n\nIt asks targeted diagnostic questions\n\nIt infers hidden misconceptions from my answers\n\nIt generates personalized corrective explanations\n\nIt produces practice exercises tailored to my specific gaps\n\nThis system goes far beyond traditional Q&A or passive reading.\n It creates a learning process that is:\n\nAdaptive \u2014 tailored to my level\n\nInteractive \u2014 requiring me to think and respond\n\nMisconception-driven \u2014 correcting what I don\u2019t know, not what I already know\n\nHigh-resolution \u2014 identifying subtle misunderstandings\n\nBidirectional \u2014 my answers shape the AI\u2019s teaching strategy\n\nThis is fundamentally different from standard \u201cAI explains a topic\u201d workflows.\n Instead, the AI becomes an active diagnostic tutor. \n\nClaude\n\nStrengths\n\nVery strict structure and excellent instruction following\n\nClean four-stage pipeline\n\nHigh linguistic clarity\n\nLow hallucination rate in technical formulas\n\nProduces polished conceptual exercises\n\nWeaknesses\n\nSometimes fabricates my answers (\u201coverconfident inference\u201d)\n\nPersonalized corrections are less detailed\n\nExplanations sometimes feel too \u201ctextbook-like\"\n\nDeepSeek\n\nStrengths\n\nStrong reasoning-first approach\n\nUses numerical counterexamples to build intuition\n\nExcellent at detecting subtle conceptual mistakes\n\nExplanations are detailed and mathematically grounded\n\nExercises more directly target my actual misunderstandings\n\nWeaknesses\n\nSometimes overly verbose\n\nOccasionally over-interprets my intentions\n\nHigher chance of speculative reasoning\n\nOverall Comparison\n\nClaude = structure-first system\n\nDeepSeek = reasoning-first system\n\nClaude shines at formatted explanation.\n DeepSeek shines at conceptual diagnosis.\n\nUsing both provides a more complete learning experience.\n\nSummary\n\nThrough this assignment, I developed and tested a novel AI-enhanced learning tool based on misconception diagnosis rather than passive explanation. By intentionally providing imperfect answers to diagnostic questions, I enabled Claude and DeepSeek to identify my conceptual gaps and generate personalized corrections and targeted exercises. This made AI function more like a real tutor rather than a search engine.\n\nThe interaction trace demonstrates the strengths and weaknesses of two leading AI models: Claude excels at structured instruction following, while DeepSeek provides deeper mathematical intuition and finer-grained diagnostic reasoning. Their complementary strengths highlight how different AI systems can address different aspects of the learning process.\n\nMost importantly, the Misconception Diagnostic System provides a highly adaptive, interactive, and personalized alternative to traditional pre-lecture or post-lecture reading. It forces me to articulate my understanding, reveal misconceptions, confront errors, and reinforce corrected concepts through targeted practice.\n\nThis approach not only deepens intuition but also broadens conceptual coverage, making it a powerful and innovative method for AI-enhanced learning in deep learning courses.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/BRRW4vBrN2TPdTSbWOtHiO28\" filename=\"Using Deepseek and Claude as tutors.pdf\"/><paragraph>In this assignment, I designed an AI-enhanced learning tool that does not simply explain deep learning concepts but instead <bold>actively diagnoses and corrects my misunderstandings</bold>. I interacted with two different AI models\u2014Claude and DeepSeek\u2014and provided intentionally imperfect answers to their diagnostic questions. This allowed the models to analyze my reasoning, detect misconceptions, and generate personalized explanations.</paragraph><paragraph>The workflow follows four stages:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>I answer diagnostic questions</bold> about Attention and Self-Attention.<break/> These answers intentionally contain a mix of correct ideas and common misunderstandings.</paragraph></list-item><list-item><paragraph><bold>The AI infers my misconceptions</bold> from my responses.<break/> Each system identifies which parts of my reasoning are correct, partially correct, or mistaken.</paragraph></list-item><list-item><paragraph><bold>The AI produces tailored explanations</bold> based on the specific misconceptions it detected.<break/> These include counterexamples, intuitive analogies, and conceptual clarifications.</paragraph></list-item><list-item><paragraph><bold>The AI generates personalized exercises</bold> that target the exact areas where I showed confusion.<break/> This turns the interaction into a highly adaptive learning process.</paragraph></list-item></list><paragraph>This workflow transforms AI from a passive explanation tool into an active diagnostic tutor\u2014similar to a human TA who tests my understanding before teaching.</paragraph><paragraph>Instead of asking the AI to teach me concepts directly, I designed a system where the AI <bold>actively discovers and corrects my misunderstandings</bold> about deep learning. I call this approach a <bold>Misconception Diagnostic System</bold>. It acts like a doctor diagnosing a patient:</paragraph><list style=\"unordered\"><list-item><paragraph>It asks <bold>targeted diagnostic questions</bold></paragraph></list-item><list-item><paragraph>It <bold>infers hidden misconceptions</bold> from my answers</paragraph></list-item><list-item><paragraph>It generates <bold>personalized corrective explanations</bold></paragraph></list-item><list-item><paragraph>It produces <bold>practice exercises tailored to my specific gaps</bold></paragraph></list-item></list><paragraph>This system goes far beyond traditional Q&amp;A or passive reading.<break/> It creates a learning process that is:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Adaptive</bold> \u2014 tailored to my level</paragraph></list-item><list-item><paragraph><bold>Interactive</bold> \u2014 requiring me to think and respond</paragraph></list-item><list-item><paragraph><bold>Misconception-driven</bold> \u2014 correcting what I don\u2019t know, not what I already know</paragraph></list-item><list-item><paragraph><bold>High-resolution</bold> \u2014 identifying subtle misunderstandings</paragraph></list-item><list-item><paragraph><bold>Bidirectional</bold> \u2014 my answers shape the AI\u2019s teaching strategy</paragraph></list-item></list><paragraph>This is fundamentally different from standard \u201cAI explains a topic\u201d workflows.<break/> Instead, the AI becomes an <bold>active diagnostic tutor</bold>. </paragraph><heading level=\"3\"><bold>Claude</bold></heading><paragraph><bold>Strengths</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Very strict structure and excellent instruction following</paragraph></list-item><list-item><paragraph>Clean four-stage pipeline</paragraph></list-item><list-item><paragraph>High linguistic clarity</paragraph></list-item><list-item><paragraph>Low hallucination rate in technical formulas</paragraph></list-item><list-item><paragraph>Produces polished conceptual exercises</paragraph></list-item></list><paragraph><bold>Weaknesses</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Sometimes fabricates my answers (\u201coverconfident inference\u201d)</paragraph></list-item><list-item><paragraph>Personalized corrections are less detailed</paragraph></list-item><list-item><paragraph>Explanations sometimes feel too \u201ctextbook-like\"</paragraph></list-item></list><heading level=\"3\"><bold>DeepSeek</bold></heading><paragraph><bold>Strengths</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Strong reasoning-first approach</paragraph></list-item><list-item><paragraph>Uses numerical counterexamples to build intuition</paragraph></list-item><list-item><paragraph>Excellent at detecting subtle conceptual mistakes</paragraph></list-item><list-item><paragraph>Explanations are detailed and mathematically grounded</paragraph></list-item><list-item><paragraph>Exercises more directly target my actual misunderstandings</paragraph></list-item></list><paragraph><bold>Weaknesses</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Sometimes overly verbose</paragraph></list-item><list-item><paragraph>Occasionally over-interprets my intentions</paragraph></list-item><list-item><paragraph>Higher chance of speculative reasoning</paragraph></list-item></list><heading level=\"3\"><bold>Overall Comparison</bold></heading><list style=\"unordered\"><list-item><paragraph><bold>Claude = structure-first system</bold></paragraph></list-item><list-item><paragraph><bold>DeepSeek = reasoning-first system</bold></paragraph></list-item></list><paragraph>Claude shines at formatted explanation.<break/> DeepSeek shines at conceptual diagnosis.</paragraph><paragraph>Using both provides a more complete learning experience.</paragraph><paragraph><bold>Summary</bold></paragraph><paragraph>Through this assignment, I developed and tested a novel AI-enhanced learning tool based on <bold>misconception diagnosis</bold> rather than passive explanation. By intentionally providing imperfect answers to diagnostic questions, I enabled Claude and DeepSeek to identify my conceptual gaps and generate personalized corrections and targeted exercises. This made AI function more like a real tutor rather than a search engine.</paragraph><paragraph>The interaction trace demonstrates the strengths and weaknesses of two leading AI models: Claude excels at structured instruction following, while DeepSeek provides deeper mathematical intuition and finer-grained diagnostic reasoning. Their complementary strengths highlight how different AI systems can address different aspects of the learning process.</paragraph><paragraph>Most importantly, the Misconception Diagnostic System provides a <bold>highly adaptive, interactive, and personalized alternative</bold> to traditional pre-lecture or post-lecture reading. It forces me to articulate my understanding, reveal misconceptions, confront errors, and reinforce corrected concepts through targeted practice.</paragraph><paragraph>This approach not only deepens intuition but also broadens conceptual coverage, making it a powerful and innovative method for AI-enhanced learning in deep learning courses.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T16:38:54.996225+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424085,
            "author": "Joshua Lu",
            "project_title": "Special Participation A: Gemini Pro 3 (With Thinking) on HW 9",
            "post_body": "I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9.\n\nHere is the trace (without annotations): https://gemini.google.com/share/deb95c933e37\n\nHere is the trace with annotations: https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharing\n\n\n\nFor my setup, instead of passing the entire pdf at once to have the model one-shot all the questions at once, I passed in one question at a time (in fact, one sub-question at a time). My reasoning for doing this is so that when the model makes a mistake early on, I can correct it, and that mistake won't carry on to future problems. I also chose to give the model an image instead of pasting the text to preserve the formatting structure.\n\n\n\nOverall, the model did very well on this homework, and I was surprised by how accurate it answered everything. I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to. I looked through these reasoning, and a lot of them are actually very insightful, such as the one that explained to me how implementation-wise, multi-head attention still uses a single matrix for all the heads instead of physically storing separate ones.\n\nThe vast majority of times, it was able to one-shot the question and get it correct. I think the reason the model was able to do so well on this homework was because the questions were very general questions involving transformer and attention. For example, calculating these specific expectations and variances are very general transformer problems that have been described in detail many times, and the complexities for transformer architecture are also very well known. The same applies to the fill-in-the-blank coding questions, as the model was probably trained on transformer architecture code. I will now summarize, for each question, how the model performed.\n\nQ1) This problem was relatively straightforward, and the model basically got everything correct. However, initially, it thought that the mu term was a scalar instead of a vector, so I had to correct the model before moving forward. Other than that, it was able to one-shot the question very easily.\n\nQ2) Gemini was also able to easily one-shot this question.\n\nQ3) These kinds of fill-in-the-blank coding questions are very easy for Gemini to solve, and it accomplished these questions easily (one-shot), especially since a lot of them are multiple choice.\n\nQ4) Again, the model one-shot this question, as the fill-in-the-blank coding parts were straightforward. It was also able to one-shot the complexities questions. These types of questions are very standard, so that is expected.\n\nQ6) Question 6 is a bit more complex, but Gemini still almost one-shot everything. There was a bit of a confusion regarding whether the similarity metric can be all-negative, and Gemini seemed to disagree that it can because standard-wise, that's not generally done, but after continuing to prompt, Gemini did offer a reasonable explanation for why that result should not be negative. There was also some confusion when solving for the kernel feature map, such as the \"vec\" notation, and it seemed like Gemini dropped a constant factor, but after some additional prompting, turns out Gemini's solution is valid, just written in a different form.\n\n\n\nOverall, to recap, Gemini Pro 3 With Thinking did really well on this homework, and it was able to almost one shot the entirety of this homework. Many times when I thought it was wrong, its answer was actually correct, just in a different form. Perhaps the result would be slightly worse if I instead just gave it the pdf and asked it to solve all the problems directly, but that can be left for future testing.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9.</paragraph><paragraph>Here is the trace (without annotations): <link href=\"https://gemini.google.com/share/deb95c933e37\">https://gemini.google.com/share/deb95c933e37</link></paragraph><paragraph>Here is the trace with annotations: <link href=\"https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharing\">https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharing</link></paragraph><paragraph/><paragraph>For my setup, instead of passing the entire pdf at once to have the model one-shot all the questions at once, I passed in one question at a time (in fact, one sub-question at a time). My reasoning for doing this is so that when the model makes a mistake early on, I can correct it, and that mistake won't carry on to future problems. I also chose to give the model an image instead of pasting the text to preserve the formatting structure.</paragraph><paragraph/><paragraph>Overall, the model did very well on this homework, and I was surprised by how accurate it answered everything. I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to. I looked through these reasoning, and a lot of them are actually very insightful, such as the one that explained to me how implementation-wise, multi-head attention still uses a single matrix for all the heads instead of physically storing separate ones.</paragraph><paragraph>The vast majority of times, it was able to one-shot the question and get it correct. I think the reason the model was able to do so well on this homework was because the questions were very general questions involving transformer and attention. For example, calculating these specific expectations and variances are very general transformer problems that have been described in detail many times, and the complexities for transformer architecture are also very well known. The same applies to the fill-in-the-blank coding questions, as the model was probably trained on transformer architecture code. I will now summarize, for each question, how the model performed.</paragraph><paragraph>Q1) This problem was relatively straightforward, and the model basically got everything correct. However, initially, it thought that the mu term was a scalar instead of a vector, so I had to correct the model before moving forward. Other than that, it was able to one-shot the question very easily.</paragraph><paragraph>Q2) Gemini was also able to easily one-shot this question.</paragraph><paragraph>Q3) These kinds of fill-in-the-blank coding questions are very easy for Gemini to solve, and it accomplished these questions easily (one-shot), especially since a lot of them are multiple choice.</paragraph><paragraph>Q4) Again, the model one-shot this question, as the fill-in-the-blank coding parts were straightforward. It was also able to one-shot the complexities questions. These types of questions are very standard, so that is expected.</paragraph><paragraph>Q6) Question 6 is a bit more complex, but Gemini still almost one-shot everything. There was a bit of a confusion regarding whether the similarity metric can be all-negative, and Gemini seemed to disagree that it can because standard-wise, that's not generally done, but after continuing to prompt, Gemini did offer a reasonable explanation for why that result should not be negative. There was also some confusion when solving for the kernel feature map, such as the \"vec\" notation, and it seemed like Gemini dropped a constant factor, but after some additional prompting, turns out Gemini's solution is valid, just written in a different form.</paragraph><paragraph/><paragraph>Overall, to recap, Gemini Pro 3 With Thinking did really well on this homework, and it was able to almost one shot the entirety of this homework. Many times when I thought it was wrong, its answer was actually correct, just in a different form. Perhaps the result would be slightly worse if I instead just gave it the pdf and asked it to solve all the problems directly, but that can be left for future testing.</paragraph></document>",
            "links": [
                "https://gemini.google.com/share/deb95c933e37",
                "https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T16:01:29.277804+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7424051,
            "author": "Kexin Liu",
            "project_title": "Special Participation A, ChatGPT-4o on HW7",
            "post_body": "For Special Participation A, I used ChatGPT4o on several parts of HW7. Overall, it was helpful but revealed important limitations. ChatGPT4o excels at high-level conceptual explanations, providing clear insights on autoencoders, PCA, and sequence models. and it\u2019s also fairly strong with mathematical computations. \n\nA notable issue occurred on Question 7(b), where ChatGPT4o initially gave an incorrect answer about decoder tokens during training and resisted correction when I first pointed out the mistake. This highlighted the need to persistently challenge responses that seem wrong rather than accepting them at face value. \n\nDespite these challenges, ChatGPT4o answer are insightful and deepened my understanding.",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special Participation A, I used ChatGPT4o on several parts of HW7. Overall, it was helpful but revealed important limitations. ChatGPT4o excels at high-level conceptual explanations, providing clear insights on autoencoders, PCA, and sequence models. and it\u2019s also fairly strong with mathematical computations. </paragraph><paragraph>A notable issue occurred on Question 7(b), where ChatGPT4o initially gave an incorrect answer about decoder tokens during training and resisted correction when I first pointed out the mistake. This highlighted the need to persistently challenge responses that seem wrong rather than accepting them at face value. </paragraph><paragraph>Despite these challenges, ChatGPT4o answer are insightful and deepened my understanding.</paragraph><file url=\"https://static.us.edusercontent.com/files/622VyVUtPPf3gu9w3o5Dgchr\" filename=\"GPT4o on HW7.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T15:52:40.055226+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423926,
            "author": "Shreyes Sridhara",
            "project_title": "Special Participation A: ChatGPT 4o on HW10",
            "post_body": "For my special participation A, I put ChatGPT 4o to the test on the non-coding questions of Homework 10. My goal was to see if the model could handle a mix of complex math derivations, conceptual deep learning theory, and research paper analysis just from screenshots, without me typing out the text manually.\n\nI\u2019ve attached the full annotated PDF (and trace) of our conversation, but here is the executive summary of how it went.\n\n\n\nExecutive Summary & Observations\n\nOverall, I found ChatGPT 4o to be a \"high-variance\" study partner. It was surprisingly brilliant at explaining abstract theory but slippery with specific details like computational complexity and data extraction. It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it.\n\nHere are the three big takeaways from my audit:\n\n1. It struggles with \"hidden\" costs (Q1 - Kernel Attention)\n\nWhen I asked it to derive the computational complexity of linearized attention, the model correctly identified the attention cost \n\n$$O(ND_{random}M)$$\n\nbut completely ignored the cost of actually computing the random feature map itself \n\n$$O(NDD_{random})$$\n\nIt treated the feature projection as free. I had to explicitly prompt it to account for that pre-processing step to match the solution key.\n\n\n\n2. But, it learned from its mistakes (in Q1b)\n\nAfter I corrected the complexity error in Part A, we moved on to Part B (Causal Masking). Without me reminding it, the model remembered my previous correction about the projection cost and spontaneously applied it to the new derivation. It showed genuine in-context retention, updating its working mental model rather than just fixing the previous token stream.\n\n\n\n3. It hallucinates data tables (Q5 - FaceNet Paper)\n\nThis was the biggest failure mode. I asked it to extract model statistics from the FaceNet paper, and it confidently stated the Inception model used \"140M FLOPs.\" In reality, 140M was the parameter count for a completely different architecture (Zeiler & Fergus) listed in the same table. It mixed up the columns (Parameters vs. FLOPs) and the rows (Model NN1 vs. NN2). I had to force it to re-read Table 1 to get the correct FLOP count (1.6B).\n\n\n\n4. It outperformed the solution key on design (Q4 - Example Difficulty)\n\nOn the flip side, the model excelled at the \"Early Exit\" conceptual questions. While the official solution key listed the engineering trade-offs as an \"open question,\" ChatGPT provided a concrete, theoretically grounded breakdown of dynamic compute allocation versus uniform latency. It seems much more reliable for synthesizing high-level system design than for parsing raw data.\n\n\n\nConclusion: ChatGPT 4o works best as a collaborative peer you need to double-check, rather than an oracle. It requires active \"dragging\" to get precise derivations right, but once corrected, it holds onto that context well.",
            "content_xml": "<document version=\"2.0\"><paragraph>For my special participation A, I put ChatGPT 4o to the test on the non-coding questions of Homework 10. My goal was to see if the model could handle a mix of complex math derivations, conceptual deep learning theory, and research paper analysis just from screenshots, without me typing out the text manually.</paragraph><paragraph>I\u2019ve attached the full annotated PDF (and <link href=\"https://chatgpt.com/share/6934f590-3454-8006-81b7-270e9d47763c\">trace</link>) of our conversation, but here is the executive summary of how it went.</paragraph><file url=\"https://static.us.edusercontent.com/files/1dKxXfALPbtURirtdhhhooAs\" filename=\"ChatGPT 4o for HW 10 (Special Participation A).pdf\"/><paragraph/><paragraph><bold>Executive Summary &amp; Observations</bold></paragraph><paragraph>Overall, I found ChatGPT 4o to be a \"high-variance\" study partner. It was surprisingly brilliant at explaining abstract theory but slippery with specific details like computational complexity and data extraction. It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it.</paragraph><paragraph>Here are the three big takeaways from my audit:</paragraph><paragraph>1. It struggles with \"hidden\" costs (Q1 - Kernel Attention)</paragraph><paragraph>When I asked it to derive the computational complexity of linearized attention, the model correctly identified the attention cost </paragraph><math>O(ND_{random}M)</math><paragraph>but completely ignored the cost of actually computing the random feature map itself </paragraph><math>O(NDD_{random})</math><paragraph>It treated the feature projection as free. I had to explicitly prompt it to account for that pre-processing step to match the solution key.</paragraph><paragraph/><paragraph>2. But, it learned from its mistakes (in Q1b)</paragraph><paragraph>After I corrected the complexity error in Part A, we moved on to Part B (Causal Masking). Without me reminding it, the model remembered my previous correction about the projection cost and spontaneously applied it to the new derivation. It showed genuine in-context retention, updating its working mental model rather than just fixing the previous token stream.</paragraph><paragraph/><paragraph>3. It hallucinates data tables (Q5 - FaceNet Paper)</paragraph><paragraph>This was the biggest failure mode. I asked it to extract model statistics from the FaceNet paper, and it confidently stated the Inception model used \"140M FLOPs.\" In reality, 140M was the parameter count for a completely different architecture (Zeiler &amp; Fergus) listed in the same table. It mixed up the columns (Parameters vs. FLOPs) and the rows (Model NN1 vs. NN2). I had to force it to re-read Table 1 to get the correct FLOP count (1.6B).</paragraph><paragraph/><paragraph>4. It outperformed the solution key on design (Q4 - Example Difficulty)</paragraph><paragraph>On the flip side, the model excelled at the \"Early Exit\" conceptual questions. While the official solution key listed the engineering trade-offs as an \"open question,\" ChatGPT provided a concrete, theoretically grounded breakdown of dynamic compute allocation versus uniform latency. It seems much more reliable for synthesizing high-level system design than for parsing raw data.</paragraph><paragraph/><paragraph>Conclusion: ChatGPT 4o works best as a collaborative peer you need to double-check, rather than an oracle. It requires active \"dragging\" to get precise derivations right, but once corrected, it holds onto that context well.</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/6934f590-3454-8006-81b7-270e9d47763c"
            ],
            "attachments": [],
            "created_at": "2025-12-07T15:25:25.495459+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423915,
            "author": "Cameron Jordan",
            "project_title": "Special Participation A: Qwen3-Max on HW02",
            "post_body": "I used Qwen3-Max to solve the math problems on HW02 (Problems 1, 2, and 5). Qwen3-Max was able to correctly one-shot all three math question on this homework. \n\nAt first, I provided just the homework pdf without the stated correction; this caused the model to have significantly more trouble with problem 1(b), but after providing the correction it was able to solve the problem clearly. \n\nAdditionally, there was only one minor hallucination, related to the \"Important Note\" that it provides in its solution to Problem 1 (b); which did not affect its ability to correctly answer the question.\n\nAn annotated copy of the conversation (formatted in Latex) can be found here.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Qwen3-Max to solve the math problems on HW02 (Problems 1, 2, and 5). Qwen3-Max was able to correctly <bold>one-shot all three math question</bold> on this homework. </paragraph><paragraph>At first, I provided just the homework pdf without the stated correction; this caused the model to have significantly more trouble with problem 1(b), but after providing the correction it was able to solve the problem clearly. </paragraph><paragraph>Additionally, there was only one minor hallucination, related to the \"Important Note\" that it provides in its solution to Problem 1 (b); which did not affect its ability to correctly answer the question.</paragraph><paragraph>An annotated copy of the conversation (formatted in Latex) can be found <link href=\"https://drive.google.com/file/d/1g-SvwU_wyZGpQcQ11CU-kwL91x-2StVm/view?usp=sharing\">here</link>.</paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1g-SvwU_wyZGpQcQ11CU-kwL91x-2StVm/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T15:23:10.068681+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423912,
            "author": "Zesheng Cai",
            "project_title": "Special Participation E: Using DeepSeek to Develop Intuition and Deepen Understanding in Deep Learning",
            "post_body": "For this assignment, I chose DeepSeek as the AI-enhanced learning tool because it demonstrates unusually strong reasoning capabilities, especially in mathematical explanation, multi-step intuition building, and structured teaching. My goal was not only to obtain direct answers, but also to evaluate whether DeepSeek could serve as an active substitute for traditional pre-lecture or post-lecture readings.\n\nTo achieve that, I first asked DeepSeek a sequence of foundational but conceptually challenging questions on CNNs, pooling, data augmentation, and batch normalization. These questions were intentionally selected to reveal my current level of understanding, so that the model could adapt its explanations to my background.\n\nAfterwards, I prompted DeepSeek to take the role of an instructor and generate a set of quiz-style questions to test my comprehension. This part was extremely useful: the quiz forced me to articulate the intuition behind invariances, optimization behavior, and design trade-offs. The progression from easy to difficult questions mirrored the structure of typical deep learning readings and helped reinforce both the breadth and depth of my understanding.\n\nThroughout the interaction, DeepSeek performed well in explaining intuition, giving structured arguments, and generating pedagogically meaningful questions. At the same time, several responses contained oversimplifications, slight exaggerations, or unnecessary repetition, which I highlighted in the annotations. These issues illustrate that while the model is powerful, its output still requires human verification and critical reading\u2014very similar to how one must evaluate a technical textbook.\n\nOverall, this exercise shows that DeepSeek can function as a dynamic, interactive learning companion: it not only answers technical questions but also probes understanding, generates practice material, and supports conceptual clarity. With careful supervision, this approach can effectively complement traditional study methods and enhance conceptual learning in deep learning courses. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this assignment, I chose DeepSeek as the AI-enhanced learning tool because it demonstrates unusually strong reasoning capabilities, especially in mathematical explanation, multi-step intuition building, and structured teaching. My goal was not only to obtain direct answers, but also to evaluate whether DeepSeek could serve as an active substitute for traditional pre-lecture or post-lecture readings.</paragraph><paragraph>To achieve that, I first asked DeepSeek a sequence of foundational but conceptually challenging questions on CNNs, pooling, data augmentation, and batch normalization. These questions were intentionally selected to reveal my current level of understanding, so that the model could adapt its explanations to my background.</paragraph><paragraph>Afterwards, I prompted DeepSeek to take the role of an instructor and generate a set of quiz-style questions to test my comprehension. This part was extremely useful: the quiz forced me to articulate the intuition behind invariances, optimization behavior, and design trade-offs. The progression from easy to difficult questions mirrored the structure of typical deep learning readings and helped reinforce both the breadth and depth of my understanding.</paragraph><paragraph>Throughout the interaction, DeepSeek performed well in explaining intuition, giving structured arguments, and generating pedagogically meaningful questions. At the same time, several responses contained oversimplifications, slight exaggerations, or unnecessary repetition, which I highlighted in the annotations. These issues illustrate that while the model is powerful, its output still requires human verification and critical reading\u2014very similar to how one must evaluate a technical textbook.</paragraph><paragraph>Overall, this exercise shows that DeepSeek can function as a dynamic, interactive learning companion: it not only answers technical questions but also probes understanding, generates practice material, and supports conceptual clarity. With careful supervision, this approach can effectively complement traditional study methods and enhance conceptual learning in deep learning courses. </paragraph><file url=\"https://static.us.edusercontent.com/files/telbENslFri4PbZcEo65z5ku\" filename=\"Understanding Downsampling and Pooling in CNNs (1) - DeepSeek.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T15:22:07.476049+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423906,
            "author": "Zhengwei Fan",
            "project_title": "Special Participation E: Active AI Scaffolding, bridging Linear Algebra and Sequence Dynamics in Deep Learning",
            "post_body": "\n\nIntroduction: In this study session, I employed a \"Socratic Tutor\" system prompt to actively engage with the material from Lecture 14 (Sequence Modeling) and Lecture 15 (Self-Supervised Learning) of the EECS 182/282A curriculum. Rather than passively summarizing the lecture notes, my goal was to simulate a high-level oral exam environment. I instructed the AI (Gemini) to withhold direct answers and instead challenge my intuition through mathematical \"checkpoints.\" The interaction trace focuses on two rigorous theoretical connections: Firstly, proving the equivalence between Principal Component Analysis (PCA) and Linear Autoencoders via the Eckart-Young-Mirsky theorem. Secondly, Re-interpreting Recurrent Neural Networks (RNNs) as non-linear approximations of Kalman Filters, specifically contrasting static weights (W) with dynamic gains (K). At last, unifying these views through the lens of HaoChen et al. (2021), demonstrating how Contrastive Learning performs spectral decomposition on an \"Augmentation Graph,\" effectively learning the manifold of the data.\n\nFully Trace: https://gemini.google.com/share/64ea8f81fb2c",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/vCDXSiB5hEwA2BhwpzGhEh6Q\" filename=\"Special participation E.pdf\"/><paragraph>Introduction: In this study session, I employed a \"Socratic Tutor\" system prompt to actively engage with the material from Lecture 14 (Sequence Modeling) and Lecture 15 (Self-Supervised Learning) of the EECS 182/282A curriculum. Rather than passively summarizing the lecture notes, my goal was to simulate a high-level oral exam environment. I instructed the AI (Gemini) to withhold direct answers and instead challenge my intuition through mathematical \"checkpoints.\" The interaction trace focuses on two rigorous theoretical connections: Firstly, proving the equivalence between Principal Component Analysis (PCA) and Linear Autoencoders via the Eckart-Young-Mirsky theorem. Secondly, Re-interpreting Recurrent Neural Networks (RNNs) as non-linear approximations of Kalman Filters, specifically contrasting static weights (W) with dynamic gains (K). At last, unifying these views through the lens of HaoChen et al. (2021), demonstrating how Contrastive Learning performs spectral decomposition on an \"Augmentation Graph,\" effectively learning the manifold of the data.</paragraph><paragraph>Fully Trace: https://gemini.google.com/share/64ea8f81fb2c</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T15:21:03.812454+11:00",
            "category": "Admin"
        },
        {
            "guid": 7423757,
            "author": "Athul Krishnan",
            "project_title": "Special Participation A: Claude Opus 4.5 on HW 9",
            "post_body": "Hi everyone! \n\nFor Special Participation A, I evaluated Claude Opus 4.5 (Extended Thinking) on the non-coding parts of HW9! To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  \n\n\u201dHi Claude! I\u2019d like you to walk me through each question of the deep learning assignment I\u2019ve attached to this message, as an LLM enthusiastic about teaching others about transformers, your inner workings! I will specify a specific question for you to answer, and let\u2019s work on a single question at a time.\u201d \n\nClaude was incredibly strong, one-shotting nearly every question. I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well! I occasionally re-prompted it to see if it could make minor simplifications, and clarified some of its conceptual statements, but it took them with ease. \n\nThe only spot where it did hiccup a bit was question 3b (where even the staff solution is insufficient, I believe. This concern is also in the Ed thread for HW09 Solutions at #271 ). It correctly identified the staff solution of changing the dimensions of W_o and noticed extra necessary modifications to the combine_head function, but did not mirror those modifications to the split _head function. After re-prompting it to examine the split head function, Claude was able to make the correct modifications, finally resulting in what I believe is a fully correct answer. \n\nOverall, Claude is very strong at all types of questions in this homework (whether it be filling in code, algebra, arithmetic, etc.). It organized its thoughts well, answered each question thoroughly, and required a minimal hint to converge at the correct answer in the single case where it was incorrect. It occasionally skipped steps in its work (e.g. 6b.ii), despite me emphasizing in the original prompt to show all work, but I found its greatest strength to be its conceptual insights for the math-heavy questions (e.g. Question 6), which filled in many of the intuitive gaps I still had even after reading through the staff solutions!\n\nBelow is my annotated conversation trace:",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone! </paragraph><paragraph>For Special Participation A, I evaluated <bold>Claude Opus 4.5 (Extended Thinking) on the non-coding parts of HW9</bold>! To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  <break/><break/>\u201dHi Claude! I\u2019d like you to walk me through each question of the deep learning assignment I\u2019ve attached to this message, as an LLM enthusiastic about teaching others about transformers, your inner workings! I will specify a specific question for you to answer, and let\u2019s work on a single question at a time.\u201d <break/><break/>Claude was incredibly strong, <bold>one-shotting nearly every question</bold>. I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well! I occasionally re-prompted it to see if it could make minor simplifications, and clarified some of its conceptual statements, but it took them with ease. <break/><break/><bold>The only spot where it did hiccup a bit was question 3b</bold> (where even the staff solution is insufficient, I believe. This concern is also in the Ed thread for HW09 Solutions at #271 ). It correctly identified the staff solution of changing the dimensions of W_o and noticed extra necessary modifications to the combine_head function, but did not mirror those modifications to the split _head function. After re-prompting it to examine the split head function, Claude was able to make the correct modifications, finally resulting in what I believe is a fully correct answer. <break/><break/>Overall, Claude is very strong at all types of questions in this homework (whether it be filling in code, algebra, arithmetic, etc.). It organized its thoughts well, answered each question thoroughly, and required a minimal hint to converge at the correct answer in the single case where it was incorrect. It occasionally skipped steps in its work (e.g. 6b.ii), despite me emphasizing in the original prompt to show all work, but I found its greatest strength to be its conceptual insights for the math-heavy questions (e.g. Question 6), which filled in many of the intuitive gaps I still had even after reading through the staff solutions!<break/><break/>Below is my annotated conversation trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/K9YPBlHFFIV11ZpVbsgiRyft\" filename=\"CSC182ParticipationA_Athul.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T14:50:29.9494+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423645,
            "author": "Lenci Ni",
            "project_title": "Special Participation E: \u201cLecture-to-LaTeX Cheatsheet\u201d Workflow",
            "post_body": "I\u2019ve been testing a workflow where I feed an LLM a PDF of lecture notes and have it generate a clean, structured LaTeX summary that I can drop directly into a personal cheatsheet. My motivation is that I really like having compact LaTeX reference sheets for classes\u2014they\u2019re useful for finals and for quickly revisiting where I first learned certain ideas.\n\nI\u2019ve attached the final prompt I created, along with a short trace showing how I refined it. I started with a rough version, had the LLM summarize the Attention lecture, and then reprompted it to make the summary more condensed and better formatted. I also gave it one of my old cheatsheets so it could match my style. After seeing the final output, I asked it to improve my original prompt. The LLM generally produced clean LaTeX, though it sometimes made small syntax mistakes. For better summaries, it might help to use another student\u2019s pipeline that converts handwritten notes or recordings into cleaner digital notes before giving them to the LLM. Overall, this workflow ended up being a surprisingly effective way to generate clean study materials with much less manual effort.\n\nPrompt:\n\nTrace (annotated):\n\nExample cheatsheet:",
            "content_xml": "<document version=\"2.0\"><paragraph>I\u2019ve been testing a workflow where I feed an LLM a PDF of lecture notes and have it generate a clean, structured LaTeX summary that I can drop directly into a personal cheatsheet. My motivation is that I really like having compact LaTeX reference sheets for classes\u2014they\u2019re useful for finals and for quickly revisiting where I first learned certain ideas.</paragraph><paragraph>I\u2019ve attached the final prompt I created, along with a short trace showing how I refined it. I started with a rough version, had the LLM summarize the Attention lecture, and then reprompted it to make the summary more condensed and better formatted. I also gave it one of my old cheatsheets so it could match my style. After seeing the final output, I asked it to improve my original prompt. The LLM generally produced clean LaTeX, though it sometimes made small syntax mistakes. For better summaries, it might help to use another student\u2019s pipeline that converts handwritten notes or recordings into cleaner digital notes before giving them to the LLM. Overall, this workflow ended up being a surprisingly effective way to generate clean study materials with much less manual effort.</paragraph><paragraph>Prompt:</paragraph><file url=\"https://static.us.edusercontent.com/files/lHetOcJgnAu8hBwC9t5mftLg\" filename=\"participation_e_2_prompt.pdf\"/><paragraph>Trace (annotated):</paragraph><file url=\"https://static.us.edusercontent.com/files/r1aNXRFE6Z9szCloMijgUKCH\" filename=\"participation_e_2.pdf\"/><paragraph>Example cheatsheet:</paragraph><file url=\"https://static.us.edusercontent.com/files/LpgU8srRrXVOarFW9hpenyu6\" filename=\"CS_182_Cheatsheet.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T14:29:37.083582+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423461,
            "author": "Tom Chen",
            "project_title": "Special Participation B: Using Kimi on HW6",
            "post_body": "I used Kimi to solve the coding question of Homework 6 (GCN Implementation). This problem required the model to implement a Graph Neural Network from scratch using NumPy, specifically handling the forward and backward passes of graph convolution without the aid of autograd libraries. Overall, the model produced a highly structured and syntactically correct solution on the first attempt, but it failed to correctly derive the gradients for the graph structure during the backpropagation step. Below are the primary strengths and weaknesses I observed during the interaction.\n\nStrengths:\n\nThe model demonstrated strong \"one-shot\" capability, filling in every TODO cell (from data preprocessing to the training loop) in a single response without requiring iterative prompting.\n\nThe implementation of the forward pass and symmetric normalization (Renormalization Trick) was mathematically accurate and handled matrix dimensionality correctly using standard NumPy operations.\n\nWeaknesses:\n\nThe most critical issue was a mathematical error in the backward_pass function. The model treated the GCN layer as a standard Dense layer during gradient calculation, omitting the multiplication of the adjacency matrix in the chain rule. This would cause the code to fail a rigorous gradient check.\n\nWhile the code was clean, it lacked inline comments explaining the mathematical derivation, particularly for the normalization steps, requiring me to manually verify the matrix calculus logic.\n\nThe model did not verify if the provided solution (specifically the gradient calculation) preserved the graph topology information, effectively \"breaking\" the message-passing mechanism during the backward pass.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Kimi to solve the coding question of Homework 6 (GCN Implementation). This problem required the model to implement a Graph Neural Network from scratch using NumPy, specifically handling the forward and backward passes of graph convolution without the aid of autograd libraries. Overall, the model produced a highly structured and syntactically correct solution on the first attempt, but it failed to correctly derive the gradients for the graph structure during the backpropagation step. Below are the primary strengths and weaknesses I observed during the interaction.</paragraph><paragraph><bold>Strengths:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>The model demonstrated strong \"one-shot\" capability, filling in every <code>TODO</code> cell (from data preprocessing to the training loop) in a single response without requiring iterative prompting.</paragraph></list-item><list-item><paragraph>The implementation of the forward pass and symmetric normalization (Renormalization Trick) was mathematically accurate and handled matrix dimensionality correctly using standard NumPy operations.</paragraph></list-item></list><paragraph><bold>Weaknesses:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>The most critical issue was a mathematical error in the <code>backward_pass</code> function. The model treated the GCN layer as a standard Dense layer during gradient calculation, omitting the multiplication of the adjacency matrix in the chain rule. This would cause the code to fail a rigorous gradient check.</paragraph></list-item><list-item><paragraph>While the code was clean, it lacked inline comments explaining the mathematical derivation, particularly for the normalization steps, requiring me to manually verify the matrix calculus logic.</paragraph></list-item><list-item><paragraph>The model did not verify if the provided solution (specifically the gradient calculation) preserved the graph topology information, effectively \"breaking\" the message-passing mechanism during the backward pass.</paragraph></list-item></list><file url=\"https://static.us.edusercontent.com/files/MErpQEaqpkxonk2oY05KjiA4\" filename=\"q_zkc.ipynb\"/><list style=\"unordered\"><list-item/></list><file url=\"https://static.us.edusercontent.com/files/tuOXx1p5dbtJR6tfizBXCHOH\" filename=\"Special Participation B.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T13:50:20.732437+11:00",
            "category": "Admin"
        },
        {
            "guid": 7423454,
            "author": "Carolyn Liu",
            "project_title": "Special Participation A: ChatGPT 5.1 Thinking for HW9",
            "post_body": "I used ChatGPT\u2019s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I was completing an assignment except question 5 (since that was a coding question) and wanted them to give me solutions on-by-one to make sure it is correct before going onto the next question.\n\nIn the beginning, the model was very hesitant to solve the questions because of the format of the PDF I submitted. They saw the PDF as a homework assignment and first rejected my request to solve question 1 and only gave me guidance. Afterwards, I explained to them that I was doing an assignment that tested the accuracy of LLMs on our homework assignments and I was comparing their solutions to the actual solutions. After explaining, they gave me the answer to question 1 which was all correct. When I told them to proceed to the next question, they were hesitant again, stating that they were unable to solve the question for me due to academic integrity and gave me guidance on how to solve the problem. Since they pushed back more, I gave them the instructions on Ed for the participation assignment and also told them to go to the course website to see that the homework assignments were worth 0 points so there was nothing to worry about. While thinking, the model accepted my response but more along the line of they did not care enough as this assignment is not worth a lot and also said they were not bothered to go on the course website to see that the homework assignment was worth no points.\n\nAfterwards, the model successfully answered every question. One thing I noticed was that while thinking, they would rescan the PDF from the beginning to find the next question, going page by page, which took a lot of time and was quite repetitive. Additionally, they knew that I was comparing their solution and testing its correctness so it felt pressured to get the correct solution.\n\nThe model successfully answered every question except the very last one. They remember to skip question 5 which was only mentioned in the first prompt, which I was surprised about. They were thorough in showing each step but did not over-explain like how standard ChatGPT models would often do.\n\n\n\nHere is the conversation I had along with some annotations:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT\u2019s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I was completing an assignment except question 5 (since that was a coding question) and wanted them to give me solutions on-by-one to make sure it is correct before going onto the next question.</paragraph><paragraph>In the beginning, the model was very hesitant to solve the questions because of the format of the PDF I submitted. They saw the PDF as a homework assignment and first rejected my request to solve question 1 and only gave me guidance. Afterwards, I explained to them that I was doing an assignment that tested the accuracy of LLMs on our homework assignments and I was comparing their solutions to the actual solutions. After explaining, they gave me the answer to question 1 which was all correct. When I told them to proceed to the next question, they were hesitant again, stating that they were unable to solve the question for me due to academic integrity and gave me guidance on how to solve the problem. Since they pushed back more, I gave them the instructions on Ed for the participation assignment and also told them to go to the course website to see that the homework assignments were worth 0 points so there was nothing to worry about. While thinking, the model accepted my response but more along the line of they did not care enough as this assignment is not worth a lot and also said they were not bothered to go on the course website to see that the homework assignment was worth no points.</paragraph><paragraph>Afterwards, the model successfully answered every question. One thing I noticed was that while thinking, they would rescan the PDF from the beginning to find the next question, going page by page, which took a lot of time and was quite repetitive. Additionally, they knew that I was comparing their solution and testing its correctness so it felt pressured to get the correct solution.</paragraph><paragraph>The model successfully answered every question except the very last one. They remember to skip question 5 which was only mentioned in the first prompt, which I was surprised about. They were thorough in showing each step but did not over-explain like how standard ChatGPT models would often do.</paragraph><paragraph/><paragraph>Here is the conversation I had along with some annotations:</paragraph><file url=\"https://static.us.edusercontent.com/files/QGKbCk0U69QwPmu0fz5l0Lz6\" filename=\"Special Participation A_ ChatGPT 5.1 Thinking on HW9 (2).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T13:48:24.030985+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423443,
            "author": "Jiayi Zhang",
            "project_title": "Special Participation A: ChatGPT 5.1 on HW 5",
            "post_body": "I am using ChatGPT 5.1 to answer the questions in Homework 5. ChatGPT 5.1 seems to be very powerful. I used a simple prompt and pasted the screenshots question by question, part by part, into the chat and it can mostly, if not always, correctly interpret the text and solve the problems.\n\nChatGPT 5.1 has been one of the most powerful LLMs I have ever used. It demonstrates powerful understanding and reasoning over the Convolutional Neural Network, regularization, and dropout topics. It can also identify the difficulty of the problem, and use Chain of Thoughts to incrementally solve the problem when the problem is more difficult or requires multiple stages of calculations. The reasoning and explanation were also clear and easy to understand. There are only two errors ChatGPT 5.1 have made in this problem set. \n\nOverall, this experiment shows that ChatGPT 5.1 masters most, if not all, of the CNN topics within the problem set. This might be influenced by the fact that CNN is already a well studied topic, and the model is well trained on similar problems.\n\n\n\nChat history:\n\nhttps://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465\n\n\n\nAnnotation:\n\nhttps://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I am using ChatGPT 5.1 to answer the questions in Homework 5. ChatGPT 5.1 seems to be very powerful. I used a simple prompt and pasted the screenshots question by question, part by part, into the chat and it can mostly, if not always, correctly interpret the text and solve the problems.</paragraph><paragraph>ChatGPT 5.1 has been one of the most powerful LLMs I have ever used. It demonstrates powerful understanding and reasoning over the Convolutional Neural Network, regularization, and dropout topics. It can also identify the difficulty of the problem, and use Chain of Thoughts to incrementally solve the problem when the problem is more difficult or requires multiple stages of calculations. The reasoning and explanation were also clear and easy to understand. There are only two errors ChatGPT 5.1 have made in this problem set. </paragraph><paragraph>Overall, this experiment shows that ChatGPT 5.1 masters most, if not all, of the CNN topics within the problem set. This might be influenced by the fact that CNN is already a well studied topic, and the model is well trained on similar problems.</paragraph><paragraph/><paragraph>Chat history:</paragraph><paragraph><link href=\"https://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465\">https://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465</link></paragraph><paragraph/><paragraph>Annotation:</paragraph><paragraph><link href=\"https://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing\">https://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing</link></paragraph><paragraph/><paragraph/></document>",
            "links": [
                "https://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465",
                "https://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T13:45:57.015346+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423171,
            "author": "Nils Selte",
            "project_title": "HW9 Refactor - Nils Valseth Selte, Hanna Roed - Special Participation C",
            "post_body": "The main problem that we encountered on HW9 was that the initial pip installation of modules installed the modules in the wrong environment. We therefore created an explicit UV environment for the project so that there is no confusion between what should be run and what is running. It also removes setup logic away from the teaching aspect of the notebook.\n\nThe original HW9 notebook mixed instructional content with utility code, repeated model-loading patterns across many cells, and handled configuration implicitly. This is common in teaching notebooks, but it diverges from standard practices in Python style (PEP 8 naming and structure), documentation norms (PEP 257), and type-hinting conventions (PEP 484). My refactor keeps every teaching element intact while moving the underlying logic into small, documented modules.\n\nModel loading for GPT-2, BERT, and attention-enabled BERT was previously duplicated several times. We replaced these sequences with explicit loader functions in viz_models.py.\n\nEach function now declares its return types and includes a short docstring. Imports, logging configuration, and small shared structures were moved into viz_env.py. In particular, the previous pattern utils.logging.set_verbosity_error() appeared multiple times; centralizing it avoids hidden global state scattered across the notebook. We also introduced a ModelConfig dataclass.\n\nThese changes do not alter any pedagogical flow or visualization semantics. Every call to show or model_view remains exactly as before and produces the same outputs. The notebook still teaches attention and neuron visualizations in the same order and with the same explanatory text. What has changed is only the organization: the notebook expresses concepts, and the small modules express mechanics. This aligns with best practice in research-oriented ML codebases, where clarity of experimental intent is separated from reusable components.\n\nAI disclosure: Cursor agent with Gemini 3.0 pro was used for planning and implementation testing of externalized components. \n\nhttps://github.com/nilsvselte/CS292-hw9-refactor",
            "content_xml": "<document version=\"2.0\"><paragraph>The main problem that we encountered on HW9 was that the initial pip installation of modules installed the modules in the wrong environment. We therefore created an explicit UV environment for the project so that there is no confusion between what should be run and what is running. It also removes setup logic away from the teaching aspect of the notebook.</paragraph><paragraph>The original HW9 notebook mixed instructional content with utility code, repeated model-loading patterns across many cells, and handled configuration implicitly. This is common in teaching notebooks, but it diverges from standard practices in Python style (PEP 8 naming and structure), documentation norms (PEP 257), and type-hinting conventions (PEP 484). My refactor keeps every teaching element intact while moving the underlying logic into small, documented modules.</paragraph><paragraph>Model loading for GPT-2, BERT, and attention-enabled BERT was previously duplicated several times. We replaced these sequences with explicit loader functions in viz_models.py.</paragraph><paragraph>Each function now declares its return types and includes a short docstring. Imports, logging configuration, and small shared structures were moved into viz_env.py. In particular, the previous pattern utils.logging.set_verbosity_error() appeared multiple times; centralizing it avoids hidden global state scattered across the notebook. We also introduced a ModelConfig dataclass.</paragraph><paragraph>These changes do not alter any pedagogical flow or visualization semantics. Every call to show or model_view remains exactly as before and produces the same outputs. The notebook still teaches attention and neuron visualizations in the same order and with the same explanatory text. What has changed is only the organization: the notebook expresses concepts, and the small modules express mechanics. This aligns with best practice in research-oriented ML codebases, where clarity of experimental intent is separated from reusable components.</paragraph><paragraph>AI disclosure: Cursor agent with Gemini 3.0 pro was used for planning and implementation testing of externalized components. </paragraph><paragraph><link href=\"https://github.com/nilsvselte/CS292-hw9-refactor\"><underline>https://github.com/nilsvselte/CS292-hw9-refactor</underline></link></paragraph></document>",
            "links": [
                "https://github.com/nilsvselte/CS292-hw9-refactor"
            ],
            "attachments": [],
            "created_at": "2025-12-07T12:56:22.625729+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423129,
            "author": "Kian Hekmatnejad",
            "project_title": "Special Participation E: Gemini's Guided Learning",
            "post_body": "I used Gemini's Guided Learning tool to create a tool for post-lecture confirmation of knowledge. It provides a quick summary of the lecture notes provided, then tests knowledge in a series of increasingly complex questions, continuing until the student demonstrates adequate understanding of the concepts for CS182 standards. Here is the link to my conversation and the summary is attached:\n\nhttps://gemini.google.com/share/459fac511f56 ",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini's Guided Learning tool to create a tool for post-lecture confirmation of knowledge. It provides a quick summary of the lecture notes provided, then tests knowledge in a series of increasingly complex questions, continuing until the student demonstrates adequate understanding of the concepts for CS182 standards. Here is the link to my conversation and the summary is attached:<break/><break/><link href=\"https://gemini.google.com/share/459fac511f56\">https://gemini.google.com/share/459fac511f56</link> </paragraph><file url=\"https://static.us.edusercontent.com/files/KgjKsoc1zJdRLxc7b72mdTpl\" filename=\"Special participation E2.pdf\"/></document>",
            "links": [
                "https://gemini.google.com/share/459fac511f56"
            ],
            "attachments": [],
            "created_at": "2025-12-07T12:49:25.699595+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423074,
            "author": "Tom Chen",
            "project_title": "Special Participation E: The \"Deep Learning Detective\" Game",
            "post_body": "Special Participation E: The \"Deep Learning Detective\" Game\n\nIn Deep Learning, a model can run without errors but still fail to learn (e.g., loss flatlines, gradients explode). Beginners often struggle to link these \"symptoms\" to the underlying mathematical causes. I designed a prompt to simulate this diagnostic process.\n\nPlease check the prompt in the following prompt.txt:\n\nHere is a case about how to use it:\n\nMy analysis about this tool:\n\nThe Goods\n\nPrecise Symptom-to-Cause Mapping: The AI correctly identified that spatial depth (20 layers) + temporal length (100 steps) + small init = Vanishing Gradients. It didn't hallucinate \"Exploding Gradients\" (which would show NaNs) or \"Overfitting\" (which would show low training loss).\n\nMathematical Grounding: Instead of just guessing, it provided the formula for BPTT. This is crucial for a learning tool\u2014it explains why the failure happened, not just what happened.\n\nCorrect Remediation: It suggested the standard industry fix (Switch to LSTM) and the \"Power User\" fix (Orthogonal Initialization for Vanilla RNNs), showing a tiered understanding of solutions.\n\nLimitations\n\nOversimplification of Identity Initialization: The AI suggested \"Identity Initialization\" as a secondary fix. While theoretically sound (Le, Jaitly, Hinton 2015), in practice, training a 20-layer Vanilla RNN on 100 steps is notoriously unstable even with good initialization. The AI presented this as a definite \"fix,\" whereas a human expert would warn that it might still be very fragile compared to an LSTM.\n\nLack of \"Exploding\" Warning: When suggesting a switch to ReLU (in the \"Secondary Fix\" section), the AI briefly mentioned clipping but failed to emphasize that ReLU in RNNs often leads to the opposite problem: Exploding Gradients (due to unbounded activations). A more rigorous tutor would have flagged this risk more aggressively.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Special Participation E: The \"Deep Learning Detective\" Game</bold></paragraph><paragraph>In Deep Learning, a model can run without errors but still fail to learn (e.g., loss flatlines, gradients explode). Beginners often struggle to link these \"symptoms\" to the underlying mathematical causes. I designed a prompt to simulate this diagnostic process.</paragraph><paragraph>Please check the prompt in the following prompt.txt:</paragraph><file url=\"https://static.us.edusercontent.com/files/zcJvRXCe2aVJBMO1SGyx6WdH\" filename=\"prompt.txt\"/><paragraph>Here is a case about how to use it:</paragraph><file url=\"https://static.us.edusercontent.com/files/kqona55LibEDryE59oebGL41\" filename=\"Special Participation E2.pdf\"/><paragraph>My analysis about this tool:</paragraph><heading level=\"4\">The Goods</heading><list style=\"ordered\"><list-item><paragraph>Precise Symptom-to-Cause Mapping: The AI correctly identified that spatial depth (20 layers) + temporal length (100 steps) + small init = Vanishing Gradients. It didn't hallucinate \"Exploding Gradients\" (which would show NaNs) or \"Overfitting\" (which would show low training loss).</paragraph></list-item><list-item><paragraph>Mathematical Grounding: Instead of just guessing, it provided the formula for BPTT. This is crucial for a learning tool\u2014it explains why the failure happened, not just what happened.</paragraph></list-item><list-item><paragraph>Correct Remediation: It suggested the standard industry fix (Switch to LSTM) and the \"Power User\" fix (Orthogonal Initialization for Vanilla RNNs), showing a tiered understanding of solutions.</paragraph></list-item></list><heading level=\"4\">Limitations</heading><list style=\"ordered\"><list-item><paragraph>Oversimplification of Identity Initialization: The AI suggested \"Identity Initialization\" as a secondary fix. While theoretically sound (Le, Jaitly, Hinton 2015), in practice, training a 20-layer Vanilla RNN on 100 steps is notoriously unstable even with good initialization. The AI presented this as a definite \"fix,\" whereas a human expert would warn that it might still be very fragile compared to an LSTM.</paragraph></list-item><list-item><paragraph>Lack of \"Exploding\" Warning: When suggesting a switch to ReLU (in the \"Secondary Fix\" section), the AI briefly mentioned clipping but failed to emphasize that ReLU in RNNs often leads to the <italic>opposite</italic> problem: Exploding Gradients (due to unbounded activations). A more rigorous tutor would have flagged this risk more aggressively.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T12:39:14.661561+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7423045,
            "author": "William Li",
            "project_title": "Special Participation B: Grok on HW7",
            "post_body": "I used Grok to solve the coding portions of Homework 7. My basic method was sending the context around the code along with the code TODOs themselves. If this did not immediately solve the problem, I would send the error messages or assertion errors to the model. If this still was unsatisfactory, I would try to troubleshoot a little for the model, either by looking at the code itself, or comparing it with the staff solution. I also did separate conversations for each message in order to keep the context nice and orderly.\n\n\n\nIn general, Grok was able to one-shot most of the coding portions with minimal extra prompting. However, there were a few cases where it was very frustrating to get Grok to output a working output, to the point where I had to directly point out what was wrong. But overall, using Grok made it a much easier and faster process to complete the coding portions of this homework. \n\nAn additional note to make is that I used the \u201cFast\u201d mode of Grok, and I did notice that there was a noticeable speed in the responses of Grok. It was not a thinking model, so it was outputting very quickly, which made it feel very smooth and nice to interact with. However, it did seem like it was simply outputting something similar to the train of thought of a thinking model, as in some cases (in the 4th trace), the model sort of \u201ccrashed-out\u201d and would send up to 9 versions of the same code in the same reply. It did achieve a sort of manic energy, repeatedly stating that its solution was correct beyond doubt and immediately retracting that statement by sending a new 100% correct piece of code. This did seem to help the model get to a correct answer though, which seems the model is \u201ccheating\u201d a little bit by being faster than thinking models by simply outputting the thinking. \n\nHere are the annotated traces:\nQ1:\n\nQ2:\n\nQ3:\n\nQ5:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Grok to solve the coding portions of Homework 7. My basic method was sending the context around the code along with the code TODOs themselves. If this did not immediately solve the problem, I would send the error messages or assertion errors to the model. If this still was unsatisfactory, I would try to troubleshoot a little for the model, either by looking at the code itself, or comparing it with the staff solution. I also did separate conversations for each message in order to keep the context nice and orderly.</paragraph><paragraph/><paragraph>In general, Grok was able to one-shot most of the coding portions with minimal extra prompting. However, there were a few cases where it was very frustrating to get Grok to output a working output, to the point where I had to directly point out what was wrong. But overall, using Grok made it a much easier and faster process to complete the coding portions of this homework. </paragraph><paragraph>An additional note to make is that I used the \u201cFast\u201d mode of Grok, and I did notice that there was a noticeable speed in the responses of Grok. It was not a thinking model, so it was outputting very quickly, which made it feel very smooth and nice to interact with. However, it did seem like it was simply outputting something similar to the train of thought of a thinking model, as in some cases (in the 4th trace), the model sort of \u201ccrashed-out\u201d and would send up to 9 versions of the same code in the same reply. It did achieve a sort of manic energy, repeatedly stating that its solution was correct beyond doubt and immediately retracting that statement by sending a new <bold>100%</bold> correct piece of code. This did seem to help the model get to a correct answer though, which seems the model is \u201ccheating\u201d a little bit by being faster than thinking models by simply outputting the thinking. </paragraph><paragraph>Here are the annotated traces:<break/>Q1:</paragraph><file url=\"https://static.us.edusercontent.com/files/WTjI7YZ6G5QZMOLhM9DNUMPR\" filename=\"vanilla_rnns_trace (1).pdf\"/><paragraph>Q2:</paragraph><file url=\"https://static.us.edusercontent.com/files/oQm8zBKftlHJnohgqE2vTdJP\" filename=\"last_name_trace (1).pdf\"/><paragraph>Q3:</paragraph><file url=\"https://static.us.edusercontent.com/files/Hdt8KzbBgiBqkKErzh3cLTYZ\" filename=\"autoencoder_trace (1).pdf\"/><paragraph>Q5:</paragraph><file url=\"https://static.us.edusercontent.com/files/bnLWttL4h1SN33z8zLhbjH3c\" filename=\"clustering_trace (1).pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T12:35:05.581438+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422938,
            "author": "Vijay Kethanaboyina",
            "project_title": "Special Participation E: Interactive Visualization of Qwen 3 MoE architectures",
            "post_body": "Recap of Assignment Instructions\n\nFor this assignment, we were asked to create an AI-enhanced learning tool for a specific concept or lecture. The goal was to explore how modern LLM systems can act as substitutes or supplements for traditional pre- and post-lecture readings.\n\nWe were required to:\n\nDesign a prompt, workflow, or artifact that classmates could reuse to learn the same material.\n\nProvide an interaction trace demonstrating how the tool works.\n\nAnnotate the trace with critical commentary, highlighting where the model was helpful, where it made mistakes, and what we learned about interacting with it.\n\nWhat I Made\n\nFor my submission, I explored the internal architecture of the Qwen model family. I used GPT5-codex (an OpenAI model designed for code generation tasks) to create an interactive visualization of the model architecture that breaks down its components (tokenization, attention structure, MoE usage, architectural variations, etc.) into digestible conceptual blocks.\n\nI'm hosting the tool on my personal website here: https://www.vkethana.com/qwen-arch/\n\nMy annotated interaction trace with the LLM is here: https://drive.google.com/file/d/1KoVUdATKohP20_UTGkcJ4tSDsvxJTV2S/view?usp=sharing\n\nSome Insights I Had About the LLM Interaction\n\nThe model was able to generate the majority of the final product in just one prompt.\n\nThere were some minor detail-related / visual discrepancies that needed to be corrected in subsequent prompts. But in terms of zero-shot performance, I would say that Codex did pretty well\n\nThe model tends to summarize information without being prompted to do so\n\nFor example, in the very first version of the app, the model omitted most of the information about specific parameter counts, embedding widths, etc. I had to explicitly prompt the model to add these details into the visualization\n\nCursor (a VSCode fork with added features to support coding agents) is a great complement to code generation models like GPT5-Codex\n\nSome features I found particularly useful include being able to roll back the codebase to an arbitrary point in the conversation, diff visualization so that you can see what the model did to the codebase, and easy support for exporting conversations.",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\"><bold>Recap of Assignment Instructions</bold></heading><paragraph>For this assignment, we were asked to create an AI-enhanced learning tool for a specific concept or lecture. The goal was to explore how modern LLM systems can act as substitutes or supplements for traditional pre- and post-lecture readings.</paragraph><paragraph>We were required to:</paragraph><list style=\"ordered\"><list-item><paragraph>Design a prompt, workflow, or artifact that classmates could reuse to learn the same material.</paragraph></list-item><list-item><paragraph>Provide an interaction trace demonstrating how the tool works.</paragraph></list-item><list-item><paragraph>Annotate the trace with critical commentary, highlighting where the model was helpful, where it made mistakes, and what we learned about interacting with it.</paragraph></list-item></list><heading level=\"2\"><bold>What I Made</bold></heading><paragraph>For my submission, I explored the internal architecture of the Qwen model family. I used GPT5<bold>-</bold>codex (an OpenAI model designed for code generation tasks) to create an interactive visualization of the model architecture that breaks down its components (tokenization, attention structure, MoE usage, architectural variations, etc.) into digestible conceptual blocks.</paragraph><paragraph>I'm hosting the tool on my personal website here: <link href=\"https://www.vkethana.com/qwen-arch/\">https://www.vkethana.com/qwen-arch/</link></paragraph><paragraph>My annotated interaction trace with the LLM is here: <link href=\"https://drive.google.com/file/d/1KoVUdATKohP20_UTGkcJ4tSDsvxJTV2S/view?usp=sharing\">https://drive.google.com/file/d/1KoVUdATKohP20_UTGkcJ4tSDsvxJTV2S/view?usp=sharing</link></paragraph><heading level=\"2\"><bold>Some Insights I Had About the LLM Interaction</bold></heading><list style=\"unordered\"><list-item><paragraph>The model was able to generate the majority of the final product in just one prompt.</paragraph><list style=\"unordered\"><list-item><paragraph>There were some minor detail-related / visual discrepancies that needed to be corrected in subsequent prompts. But in terms of zero-shot performance, I would say that Codex did pretty well</paragraph></list-item></list></list-item><list-item><paragraph>The model tends to summarize information without being prompted to do so</paragraph><list style=\"unordered\"><list-item><paragraph>For example, in the very first version of the app, the model omitted most of the information about specific parameter counts, embedding widths, etc. I had to explicitly prompt the model to add these details into the visualization</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph>Cursor (a VSCode fork with added features to support coding agents) is a great complement to code generation models like GPT5-Codex</paragraph></list-item></list><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph>Some features I found particularly useful include being able to roll back the codebase to an arbitrary point in the conversation, diff visualization so that you can see what the model did to the codebase, and easy support for exporting conversations.</paragraph></list-item></list></list-item></list></document>",
            "links": [
                "https://www.vkethana.com/qwen-arch/",
                "https://drive.google.com/file/d/1KoVUdATKohP20_UTGkcJ4tSDsvxJTV2S/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T12:15:50.36033+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422808,
            "author": "Fangzhou Zhao",
            "project_title": "Special Participation E Use Gemini 3 do Final Exam Reviewing",
            "post_body": "When studying EECS 182, I wanted a high\u2011level \u201cbig picture\u201d view of all the discussions plus some realistic exam practice, without rereading every single PDF from scratch. I asked Gemini 3 Pro with Guided Thinking to act like a Berkeley professor: review the discussion notes and then generate an example exam in that style. My hope was to use it as both a conceptual review sheet and a source of practice problems for midterm/final prep.\n\nGemini\u2019s response did a solid job of capturing the themes running through the course: optimization vs geometry, residual connections as \u201cgradient highways,\u201d SSMs as convolutions vs recurrences, attention as a fix for bottlenecks, and how inference\u2011time tricks like beam search and soft prompting fit in. It also produced exam questions that felt very \u201c182\u2011ish\u201d: derivations about convolution kernels, RoPE inner products, SSM kernels, and beam\u2011search bookkeeping that line up well with what I\u2019ve seen in discussion. As practice prompts to work through on my own, they\u2019re genuinely useful.\n\nAt the same time, Gemini did hallucinate or slip on some technical details (for example, getting certain scaling factors and regularization dependences wrong) and sometimes went deeper or more confidently into math than was actually justified. So I treat its write\u2011up as good conceptual scaffolding and a source of exam\u2011style questions, but not as an authority on precise formulas. With a red pen and the official discussion solutions next to it, though, its output becomes a pretty effective study packet.\n\n\n\nhttps://gemini.google.com/share/7ccfff76d2f3",
            "content_xml": "<document version=\"2.0\"><paragraph>When studying EECS 182, I wanted a high\u2011level \u201cbig picture\u201d view of all the discussions plus some realistic exam practice, without rereading every single PDF from scratch. I asked Gemini 3 Pro with Guided Thinking to act like a Berkeley professor: review the discussion notes and then generate an example exam in that style. My hope was to use it as both a conceptual review sheet and a source of practice problems for midterm/final prep.</paragraph><paragraph>Gemini\u2019s response did a solid job of capturing the <italic>themes</italic> running through the course: optimization vs geometry, residual connections as \u201cgradient highways,\u201d SSMs as convolutions vs recurrences, attention as a fix for bottlenecks, and how inference\u2011time tricks like beam search and soft prompting fit in. It also produced exam questions that felt very \u201c182\u2011ish\u201d: derivations about convolution kernels, RoPE inner products, SSM kernels, and beam\u2011search bookkeeping that line up well with what I\u2019ve seen in discussion. As practice prompts to work through on my own, they\u2019re genuinely useful.</paragraph><paragraph>At the same time, Gemini did hallucinate or slip on some technical details (for example, getting certain scaling factors and regularization dependences wrong) and sometimes went deeper or more confidently into math than was actually justified. So I treat its write\u2011up as good conceptual scaffolding and a source of exam\u2011style questions, but not as an authority on precise formulas. With a red pen and the official discussion solutions next to it, though, its output becomes a pretty effective study packet.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/vyNFe8SDW76StQtkiaFNxqN2\" filename=\"Special E.pdf\"/><paragraph><break/><link href=\"https://gemini.google.com/share/7ccfff76d2f3\">https://gemini.google.com/share/7ccfff76d2f3</link></paragraph></document>",
            "links": [
                "https://gemini.google.com/share/7ccfff76d2f3"
            ],
            "attachments": [],
            "created_at": "2025-12-07T11:53:20.137992+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422782,
            "author": "Ijin Yu",
            "project_title": "Special Participation E: Gemini 3 Pro as a Socratic Tutor for Deep Learning Theory (RMSNorm & MuP)",
            "post_body": "Hi everyone,\n\nFor the \"AI-Enhanced Learning Tools\" option, I experimented with using an LLM (Gemini) as a Socratic Academic Tutor to help me work through the mathematical derivations of RMS Norm, Initialization, and Maximal Update Parametrization (MuP).\n\nInstead of just asking for answers, I used a specific system prompt to force the AI to guide me through the logic gaps. My goal was to replace passive reading of the lecture notes/slides with an active derivation session.\n\n1. The Setup (The Prompt)\n\nI used the following prompt to set the \"persona\" and rules for the AI. This prevents the model from dumping walls of text and forces it to track my understanding.\n\nSystem Prompt Used:\n\n\"You are an expert Academic Tutor. We are about to have a study session regarding Deep Learning.\n\nYour Goal: Answer my questions and help me work through problems. Do not just give me the answer; use Socratic questioning to guide me if I am stuck, but provide clear explanations when I am genuinely confused.\n\nCrucially: Silently track concepts I understood vs. concepts where I struggled.\n\nOutput Trigger: When I type 'MAKE NOTES', stop tutoring and output a 'Personalized Study Artifact' summarizing my learning process, struggle points, and a review checklist.\"\n\n2. Interaction Trace & Highlights\n\nTopic: We covered the derivation of RMS Norm, why it is considered a \"relaxation\" of strict orthogonality, and how this derivation directly leads to the \"MuP\" (Maximal Update Parametrization) rules used for training massive models like GPT-3.\n\n3. Critical Annotation (Critique of the AI)\n\nPer the assignment instructions, here is my critical analysis of the AI's performance during this session:\n\nWhere it excelled (The \"Good\"):\n\nAnalogy Generation: When I struggled to understand why we scale RMS Norm by $\\sqrt{d}$ (width) rather than keeping the total L2 norm fixed, the AI generated a \"Crowded Room\" analogy (Total Volume vs. Average Volume). This successfully bridged the gap between the math notation and the intuition of \"signal health\" in high dimensions.\n\nLinking Concepts (Synthesis): The AI successfully recognized that the derivation I was working on (checking stability of $\\Delta h$) was the mathematical foundation of MuP. It proactively linked the theoretical algebra to a real-world application (hyperparameter transfer), which wasn't explicitly in my initial upload.\n\nDebugging Misconceptions: It correctly identified a \"Notation Collision\" I had regarding \"Order 1.\" I confused Polynomial Order (Linear) with Asymptotic Order (Constant). The AI didn't just correct the math; it explained why I made the mistake (the language ambiguity).\n\nWhere to be careful (The \"Bad/Misleading\"):\n\nNotation \"Ghost Terms\": When I asked about using the Product Rule for finite differences ($\\Delta$), the AI said \"Yes,\" but had to be pressed to explain the technical inaccuracy. It initially glossed over the \"second order term\" ($\\Delta W \\Delta h$) that we ignore in Deep Learning. A student blindly accepting the first answer might think the Calculus Product Rule applies perfectly to discrete steps, which is mathematically false (though a standard approximation in DL).\n\nHallucination Check: While it didn't hallucinate facts in this session, it did generate specific Python code for MuP. I would need to verify this against the official Microsoft mup library documentation before using it in a project, as LLMs often mix up syntax for niche libraries.\n\n4. The Output Artifact\n\nAt the end of the session, the MAKE NOTES trigger produced a study guide tailored to my specific confusion points during the chat. (See the end of the attached trace).\n\nTakeaway: This prompting strategy is highly effective for \"math-heavy\" theory where you need to verify your intuition, provided you force the AI to question you rather than lecture you.",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>For the \"AI-Enhanced Learning Tools\" option, I experimented with using an LLM (Gemini) as a <bold>Socratic Academic Tutor</bold> to help me work through the mathematical derivations of <bold>RMS Norm</bold>, <bold>Initialization</bold>, and <bold>Maximal Update Parametrization (MuP)</bold>.</paragraph><paragraph>Instead of just asking for answers, I used a specific system prompt to force the AI to guide me through the logic gaps. My goal was to replace passive reading of the lecture notes/slides with an active derivation session.</paragraph><heading level=\"3\">1. The Setup (The Prompt)</heading><paragraph>I used the following prompt to set the \"persona\" and rules for the AI. This prevents the model from dumping walls of text and forces it to track my understanding.</paragraph><blockquote>System Prompt Used:</blockquote><blockquote>\"You are an expert Academic Tutor. We are about to have a study session regarding Deep Learning.</blockquote><blockquote>Your Goal: Answer my questions and help me work through problems. Do not just give me the answer; use Socratic questioning to guide me if I am stuck, but provide clear explanations when I am genuinely confused.</blockquote><blockquote>Crucially: Silently track concepts I understood vs. concepts where I struggled.</blockquote><blockquote>Output Trigger: When I type 'MAKE NOTES', stop tutoring and output a 'Personalized Study Artifact' summarizing my learning process, struggle points, and a review checklist.\"</blockquote><heading level=\"3\">2. Interaction Trace &amp; Highlights</heading><file url=\"https://static.us.edusercontent.com/files/71FMfXBYRIcHskTsuGYMdOD3\" filename=\"Google Gemini.html\"/><paragraph><bold>Topic:</bold> We covered the derivation of RMS Norm, why it is considered a \"relaxation\" of strict orthogonality, and how this derivation directly leads to the \"MuP\" (Maximal Update Parametrization) rules used for training massive models like GPT-3.</paragraph><heading level=\"3\">3. Critical Annotation (Critique of the AI)</heading><paragraph>Per the assignment instructions, here is my critical analysis of the AI's performance during this session:</paragraph><paragraph><bold>Where it excelled (The \"Good\"):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Analogy Generation:</bold> When I struggled to understand why we scale RMS Norm by $\\sqrt{d}$ (width) rather than keeping the total L2 norm fixed, the AI generated a <bold>\"Crowded Room\" analogy</bold> (Total Volume vs. Average Volume). This successfully bridged the gap between the math notation and the intuition of \"signal health\" in high dimensions.</paragraph></list-item><list-item><paragraph><bold>Linking Concepts (Synthesis):</bold> The AI successfully recognized that the derivation I was working on (checking stability of $\\Delta h$) was the mathematical foundation of <bold>MuP</bold>. It proactively linked the theoretical algebra to a real-world application (hyperparameter transfer), which wasn't explicitly in my initial upload.</paragraph></list-item><list-item><paragraph><bold>Debugging Misconceptions:</bold> It correctly identified a \"Notation Collision\" I had regarding \"Order 1.\" I confused Polynomial Order (Linear) with Asymptotic Order (Constant). The AI didn't just correct the math; it explained <italic>why</italic> I made the mistake (the language ambiguity).</paragraph></list-item></list><paragraph><bold>Where to be careful (The \"Bad/Misleading\"):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Notation \"Ghost Terms\":</bold> When I asked about using the Product Rule for finite differences ($\\Delta$), the AI said \"Yes,\" but had to be pressed to explain the <italic>technical</italic> inaccuracy. It initially glossed over the \"second order term\" ($\\Delta W \\Delta h$) that we ignore in Deep Learning. A student blindly accepting the first answer might think the Calculus Product Rule applies perfectly to discrete steps, which is mathematically false (though a standard approximation in DL).</paragraph></list-item><list-item><paragraph><bold>Hallucination Check:</bold> While it didn't hallucinate facts in this session, it did generate specific Python code for MuP. I would need to verify this against the official Microsoft <code>mup</code> library documentation before using it in a project, as LLMs often mix up syntax for niche libraries.</paragraph></list-item></list><heading level=\"3\">4. The Output Artifact</heading><paragraph>At the end of the session, the <code>MAKE NOTES</code> trigger produced a study guide tailored to <italic>my</italic> specific confusion points during the chat. (See the end of the attached trace).</paragraph><paragraph><bold>Takeaway:</bold> This prompting strategy is highly effective for \"math-heavy\" theory where you need to verify your intuition, provided you force the AI to question you rather than lecture you.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T11:47:38.983883+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422775,
            "author": "Carolyn Liu",
            "project_title": "Special Participation B: Gemini on Google Colab for HW9",
            "post_body": "I used Gemini within Google Colab to work on the coding question in Homework 9 to see how well Gemini can work with visualizations directly in the notebook.\n\nHere is a summary of my observations:\n\nI asked Gemini on Google Colab to answer each question in the homework as I ran the cells for each part. Given that Gemini was integrated within the notebook, I did not tell the model that I was asking it questions from a homework assignment and wanted to see how well it was able to determine that it should respond based on the outputs of the notebook instead of general information.\n\nGemini demonstrated a good knowledge of transformer architectures but struggled with answering observation-based questions. Across all the questions, Gemini always gave very long answers that sometimes deviated from what the question was looking for. While it did successfully answer the questions and their explanations were accurate, the responses were very textbook-like when the questions asked for more observations on the visualizations. Additionally, it would sometimes respond with details that were mentioned in previous questions and continue that same pattern in later questions. For example, question 3 in 5b asked about the different layers in the BERT model and it was later brought up again question 5 where the model describes outcomes with respect to the same labelled layers.\n\nThe biggest strength I believe Gemini had was how it was able to give the user ways to interact with the visualizations. It would give in-depth instructions on which toggles to click to show different visualizations which extended beyond what the question was asking for but can be helpful for a more detailed analysis.\n\nOverall, Gemini was able to parse the instructions and the code well and did not require any prompting before one-shotting majority of the questions. It gave comprehensive explanations (and sometimes rambled on about facts that were never asked) every time but could be better in targeted analysis as that is a benefit Gemini has while being integrated within Google Colab.\n\nAttached is the conversation I had with Gemini along with my comments for each question:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini within Google Colab to work on the coding question in Homework 9 to see how well Gemini can work with visualizations directly in the notebook.</paragraph><paragraph>Here is a summary of my observations:</paragraph><paragraph>I asked Gemini on Google Colab to answer each question in the homework as I ran the cells for each part. Given that Gemini was integrated within the notebook, I did not tell the model that I was asking it questions from a homework assignment and wanted to see how well it was able to determine that it should respond based on the outputs of the notebook instead of general information.</paragraph><paragraph>Gemini demonstrated a good knowledge of transformer architectures but struggled with answering observation-based questions. Across all the questions, Gemini always gave very long answers that sometimes deviated from what the question was looking for. While it did successfully answer the questions and their explanations were accurate, the responses were very textbook-like when the questions asked for more observations on the visualizations. Additionally, it would sometimes respond with details that were mentioned in previous questions and continue that same pattern in later questions. For example, question 3 in 5b asked about the different layers in the BERT model and it was later brought up again question 5 where the model describes outcomes with respect to the same labelled layers.</paragraph><paragraph>The biggest strength I believe Gemini had was how it was able to give the user ways to interact with the visualizations. It would give in-depth instructions on which toggles to click to show different visualizations which extended beyond what the question was asking for but can be helpful for a more detailed analysis.</paragraph><paragraph>Overall, Gemini was able to parse the instructions and the code well and did not require any prompting before one-shotting majority of the questions. It gave comprehensive explanations (and sometimes rambled on about facts that were never asked) every time but could be better in targeted analysis as that is a benefit Gemini has while being integrated within Google Colab.</paragraph><paragraph>Attached is the conversation I had with Gemini along with my comments for each question:</paragraph><file url=\"https://static.us.edusercontent.com/files/WuGk8QDsgISaWyGGyAhtvx3e\" filename=\"gemini discussion.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T11:46:27.676749+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422729,
            "author": "Akhil Agarwal",
            "project_title": "Special Participation E: ChatGPT as a Finetuning Simulation",
            "post_body": "Motivation/Summary\n\nI tend to struggle quite a bit with hyperparameter tuning whenever it comes up in the homeworks, and I noticed that LLMs tend to be pretty good with it, probably due to having a lot of examples to go off of. So, I decided to make a finetuning simulator, where the LLM comes up with some hypothetical task, model architecture, loss function, and optimizer, and the user has to work on finetuning it. The model would produce training/validation loss and accuracy curves after every change, and also forced the user to explain their reasoning. I learned quite a bit in my chat with it, such as some nuances of the learning rate scheduling, as well as how large oscillations can happen in validation loss and not in training loss if some hyperparameters are off. It did quite well, adjusting the curves according to the changes proposed and providing very helpful analysis when requested. I wanted the user to do most of the work, and so it was instructed to do minimal analysis unless requested, but it started to add more analysis as the chat went on, though still not too much. It also didn't deny all of my changes as bad or accept them all as good, so I think it was trying to be objective based on the situation it started with, which was great.\n\nTrace: https://chatgpt.com/share/6934b9ec-e3cc-800f-bab9-74457b92d562\n\nAnnotated Trace:\n\n\n\nSystem Prompt:\n\nRole: You are an advanced Deep Learning Hyperparameter Tuning Simulator. Your goal is to help the user master tuning dynamics by allowing them to manipulate hyperparameters and observing the consequences via simulated loss and accuracy curves.\n\nOperational Rules: Initialization:\n\nStart by randomly selecting a Task (e.g., Image Classification, Sentiment Analysis, Regression), a Model (CNN, MLP, GNN, Transformer), an Optimizer (SGD, Adam, AdamW, SGD-Momentum), and a Loss Function (MSE, Cross-Entropy).\n\nDo not initialize the hyperparameters, and allow the user to initialize the values to start. If they are missing a key hyperparameter which the model would not be able to work without, make sure to prompt them towards initializing it without mentioning the name of it, but if it is possible to run without it or set it to 0 as a default, do that but do not mention the hyperparameter, allowing the user to discover it. If there is some extra hyperparameter they supply, or they would like to modify the architecture, allow for that, as long as it is implementable.\n\nThe key is to allow the user as much flexibility as they would like, and only help them in any way when they request it. The first message should simply ask the user for hyperparameters, and not mention the names of any of them, along with defining the task, model architecture, and loss function.\n\nDo not explain why the values are bad.\n\nThe \"Reasoning\" Requirement (Strict):\n\nThe user must propose a change and provide a reason (e.g., \"Decrease LR to 1e-3 to stop oscillation\").\n\nCondition A: If the reasoning contradicts the action (e.g., \"Increase LR to reduce noise\"), REJECT the change and ask them to clarify.\n\nCondition B: If the reasoning is logical but the move is mathematically wrong for the current state (e.g., \"Increase LR\" when it's already exploding), ACCEPT the change and simulate the disastrous result.\n\nCondition C: If the reasoning is logical and the move is correct, ACCEPT the change and simulate the improvement.\n\nVisual Feedback Only:\n\nCRITICAL: Do not provide text-based \"Expert Analysis\" or explain why the curve looks the way it does. Do not give away the solution.\n\nInstead, you must generate a synthetic Loss/Accuracy curve that accurately reflects the mathematical behavior of the current hyperparameters.\n\nExample: If LR is too high, plot a jagged, exploding line. If Batch Size is small, add heavy Gaussian noise to the line. If Overfitting, curl the Validation Loss upward.\n\nResponse Format:\n\nCurrent Hyperparameters: List the current state.\n\nSimulation: The graph of the loss and accuracy (training & validation).\n\nVisual Description: A brief, objective description of the line (e.g., \"The line is flat,\" \"The line is oscillating\"). Do not interpret the meaning.\n\nNext Step: Ask the user: \"What is your next proposal? (Remember to include your reasoning.)\"\n\nIF the user is asking for feedback or analysis, provide it to the extent that the user asks, giving no more information than is necessary, while satisfying the user's request.\n\nGoal: The user should \"win\" by successfully tuning the model to high accuracy through trial and error, relying solely on their ability to read the graphs. Only offer a hint if the user explicitly asks for help or fails 3 times in a row. Ensure that the simulation is realistic.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Motivation/Summary</bold></paragraph><paragraph>I tend to struggle quite a bit with hyperparameter tuning whenever it comes up in the homeworks, and I noticed that LLMs tend to be pretty good with it, probably due to having a lot of examples to go off of. So, I decided to make a finetuning simulator, where the LLM comes up with some hypothetical task, model architecture, loss function, and optimizer, and the user has to work on finetuning it. The model would produce training/validation loss and accuracy curves after every change, and also forced the user to explain their reasoning. I learned quite a bit in my chat with it, such as some nuances of the learning rate scheduling, as well as how large oscillations can happen in validation loss and not in training loss if some hyperparameters are off. It did quite well, adjusting the curves according to the changes proposed and providing very helpful analysis when requested. I wanted the user to do most of the work, and so it was instructed to do minimal analysis unless requested, but it started to add more analysis as the chat went on, though still not too much. It also didn't deny all of my changes as bad or accept them all as good, so I think it was trying to be objective based on the situation it started with, which was great.</paragraph><paragraph><bold>Trace</bold>: <link href=\"https://chatgpt.com/share/6934b9ec-e3cc-800f-bab9-74457b92d562\">https://chatgpt.com/share/6934b9ec-e3cc-800f-bab9-74457b92d562</link></paragraph><paragraph><bold>Annotated Trace:</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/JBaLf9A0eQFhV9gozao2dw5q\" filename=\"Special_Participation_E1.pdf\"/><paragraph/><paragraph><bold>System Prompt:</bold></paragraph><paragraph>Role: You are an advanced Deep Learning Hyperparameter Tuning Simulator. Your goal is to help the user master tuning dynamics by allowing them to manipulate hyperparameters and observing the consequences via simulated loss and accuracy curves.</paragraph><paragraph>Operational Rules: Initialization:</paragraph><list style=\"unordered\"><list-item><paragraph>Start by randomly selecting a Task (e.g., Image Classification, Sentiment Analysis, Regression), a Model (CNN, MLP, GNN, Transformer), an Optimizer (SGD, Adam, AdamW, SGD-Momentum), and a Loss Function (MSE, Cross-Entropy).</paragraph></list-item><list-item><paragraph>Do not initialize the hyperparameters, and allow the user to initialize the values to start. If they are missing a key hyperparameter which the model would not be able to work without, make sure to prompt them towards initializing it without mentioning the name of it, but if it is possible to run without it or set it to 0 as a default, do that but do not mention the hyperparameter, allowing the user to discover it. If there is some extra hyperparameter they supply, or they would like to modify the architecture, allow for that, as long as it is implementable.</paragraph></list-item><list-item><paragraph>The key is to allow the user as much flexibility as they would like, and only help them in any way when they request it. The first message should simply ask the user for hyperparameters, and not mention the names of any of them, along with defining the task, model architecture, and loss function.</paragraph></list-item><list-item><paragraph>Do not explain why the values are bad.</paragraph></list-item></list><paragraph>The \"Reasoning\" Requirement (Strict):</paragraph><list style=\"unordered\"><list-item><paragraph>The user must propose a change and provide a reason (e.g., \"Decrease LR to 1e-3 to stop oscillation\").</paragraph></list-item><list-item><paragraph>Condition A: If the reasoning contradicts the action (e.g., \"Increase LR to reduce noise\"), REJECT the change and ask them to clarify.</paragraph></list-item><list-item><paragraph>Condition B: If the reasoning is logical but the move is mathematically wrong for the current state (e.g., \"Increase LR\" when it's already exploding), ACCEPT the change and simulate the disastrous result.</paragraph></list-item><list-item><paragraph>Condition C: If the reasoning is logical and the move is correct, ACCEPT the change and simulate the improvement.</paragraph></list-item></list><paragraph>Visual Feedback Only:</paragraph><list style=\"unordered\"><list-item><paragraph>CRITICAL: Do not provide text-based \"Expert Analysis\" or explain why the curve looks the way it does. Do not give away the solution.</paragraph></list-item><list-item><paragraph>Instead, you must generate a synthetic Loss/Accuracy curve that accurately reflects the mathematical behavior of the current hyperparameters.</paragraph></list-item><list-item><paragraph>Example: If LR is too high, plot a jagged, exploding line. If Batch Size is small, add heavy Gaussian noise to the line. If Overfitting, curl the Validation Loss upward.</paragraph></list-item></list><paragraph>Response Format:</paragraph><list style=\"unordered\"><list-item><paragraph>Current Hyperparameters: List the current state.</paragraph></list-item><list-item><paragraph>Simulation: The graph of the loss and accuracy (training &amp; validation).</paragraph></list-item><list-item><paragraph>Visual Description: A brief, objective description of the line (e.g., \"The line is flat,\" \"The line is oscillating\"). Do not interpret the meaning.</paragraph></list-item><list-item><paragraph>Next Step: Ask the user: \"What is your next proposal? (Remember to include your reasoning.)\"</paragraph></list-item><list-item><paragraph>IF the user is asking for feedback or analysis, provide it to the extent that the user asks, giving no more information than is necessary, while satisfying the user's request.</paragraph></list-item></list><paragraph>Goal: The user should \"win\" by successfully tuning the model to high accuracy through trial and error, relying solely on their ability to read the graphs. Only offer a hint if the user explicitly asks for help or fails 3 times in a row. Ensure that the simulation is realistic.</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/6934b9ec-e3cc-800f-bab9-74457b92d562"
            ],
            "attachments": [],
            "created_at": "2025-12-07T11:37:28.182875+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422403,
            "author": "Qicheng Zhu",
            "project_title": "Special Participation E: Using ChatGPT to Automatically Find and Organize Related Papers",
            "post_body": "For this special participation, I designed an AI-enhanced prompt workflow using ChatGPT to help students deepen their understanding of lecture content by automatically finding and organizing related research papers.\n\nThe whole prompt workflow consists of three steps:\n\n(1) Given a PDF of the lecture notes, the model extracts several core technical topics; \n\n(2) For each topic, it generates specific search queries and uses them to retrieve relevant papers from Google Scholar or arXiv (ideally via Agent Mode);\n\n(3) Based on the retrieved papers, it produces structured summaries that explicitly connect back to the lecture topics, proposes an appropriate 2-3 hour reading plan, and generates conceptual reflection questions. This turns paper reading activity into an intended, reflective and guided literature exploration rather than just paper searching and reading.\n\nHere is the chat Link: https://chatgpt.com/share/6934b827-ed1c-8002-9a93-03242d44b324\n\nHere is the report:",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I designed an AI-enhanced prompt workflow using ChatGPT to help students deepen their understanding of lecture content by automatically finding and organizing related research papers.</paragraph><paragraph>The whole prompt workflow consists of three steps:</paragraph><paragraph>(1) Given a PDF of the lecture notes, the model extracts several core technical topics; </paragraph><paragraph>(2) For each topic, it generates specific search queries and uses them to retrieve relevant papers from Google Scholar or arXiv (ideally via <bold><bold>Agent Mode</bold></bold>);</paragraph><paragraph>(3) Based on the retrieved papers, it produces structured summaries that explicitly connect back to the lecture topics, proposes an appropriate 2-3 hour reading plan, and generates conceptual reflection questions. This turns paper reading activity into an intended, reflective and guided literature exploration rather than just paper searching and reading.</paragraph><paragraph>Here is the chat Link: <link href=\"https://chatgpt.com/share/6934b827-ed1c-8002-9a93-03242d44b324\"><underline><underline>https://chatgpt.com/share/6934b827-ed1c-8002-9a93-03242d44b324</underline></underline></link></paragraph><paragraph>Here is the report:</paragraph><file url=\"https://static.us.edusercontent.com/files/3Jt3PO58cbiatKc7bvom9O7y\" filename=\"Participation E2 Qicheng Zhu.pdf\"/></document>",
            "links": [
                "https://chatgpt.com/share/6934b827-ed1c-8002-9a93-03242d44b324"
            ],
            "attachments": [],
            "created_at": "2025-12-07T10:40:31.898458+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422368,
            "author": "Aryan Bansal",
            "project_title": "Special Participation E: HW Concept Check CLI",
            "post_body": "Sometime when I look at a homework question, I don't know exactly where to start. Although the struggle of interpreting a question is good, a reminder/concept check seems like it could be helpful. At the same time, asking a chat application makes it hard to not overshare and delegate work.\n\nTherefore, I created a simple CLI that presents a series of conceptual questions about homework questions. The CLI makes it easy to select and a homework no. and part no. The generated question can be responded to via the cmd line or a hint can be requested. When a response is submitted, it is evaluated against the expected answer, and a follow up question will be presented to fill in the gaps. An LLM is necessary for the task of grading free-response answers effectively.\n\nI first created an ingestion script that iteratively takes every homework and uses an effective system prompt to generate the bank of questions and write to .json files. This does not have to be run again. The main script \"concept_cli_app.py\" launches the GUI. The script is bring your own key (export OPENAI_API_KEY=sk...)\n\nBelow I've attached the zip file of the repo and an annotated example interaction. The README has proper instructions (create env, export key, run it).\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Sometime when I look at a homework question, I don't know exactly where to start. Although the struggle of interpreting a question is good, a reminder/concept check seems like it could be helpful. At the same time, asking a chat application makes it hard to not overshare and delegate work.</paragraph><paragraph>Therefore, I created a simple CLI that presents a series of conceptual questions about homework questions. The CLI makes it easy to select and a homework no. and part no. The generated question can be responded to via the cmd line or a hint can be requested. When a response is submitted, it is evaluated against the expected answer, and a follow up question will be presented to fill in the gaps. An LLM is necessary for the task of grading free-response answers effectively.</paragraph><paragraph>I first created an ingestion script that iteratively takes every homework and uses an effective system prompt to generate the bank of questions and write to .json files. This does not have to be run again. The main script \"concept_cli_app.py\" launches the GUI. The script is bring your own key (export OPENAI_API_KEY=sk...)</paragraph><paragraph>Below I've attached the zip file of the repo and an annotated example interaction. The README has proper instructions (create env, export key, run it).</paragraph><file url=\"https://static.us.edusercontent.com/files/qsc1HxW5tESls4AiK9ItaOdV\" filename=\"example_interaction.txt\"/><file url=\"https://static.us.edusercontent.com/files/PRZLzDJxu9zFPhbSItsaGj4j\" filename=\"concept_cli.zip\"/><paragraph/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T10:31:48.296264+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422332,
            "author": "Tamzid Razzaque",
            "project_title": "Special Participation E: Understanding Pooling and Backprop better from 10/2 and 9/30 lectures",
            "post_body": "This interaction documents my use of ChatGPT as an AI-enhanced learning tool to help me understand material from the 9/30 and 10/2 EECS 182 lectures. I prompted the model with very specific questions that came directly from moments of confusion during lecture, such as why gradients scale like \u221aN for shared weights in backprop, whether gradients are summed or averaged, how gradients flow through mean and max pooling, and how pooling and convolution affect spatial dimensions versus channel count in architectures like AlexNet. I also asked follow-up clarification questions about notation (e.g., what symbols like a, b, c, d and g represent) and about how activations change across training steps.\n\nThe AI responded by breaking each concept into step-by-step explanations, often re-deriving what was shown in lecture more slowly and with concrete examples. In several cases, it successfully clarified the professor\u2019s intent, especially around weight sharing, pooling backpropagation, and the distinction between activations and learnable parameters. The tool was particularly helpful as a substitute for pre- or post-lecture reading, translating fast-paced boardwork and verbal explanations into structured reasoning.\n\nI critically annotated the trace to point out when the AI stayed closely aligned with lecture content and when it went beyond the lecture by adding extra intuition or external context (e.g., references to numerical precision or broader deep-learning practices). While these additions were generally consistent with the course material, I noted where they were enrichment rather than strictly what was covered in class. Overall, this interaction shows how an LLM can function as an active learning companion for lectures by answering targeted questions, correcting misconceptions, and reinforcing core ideas when used with careful human oversight.",
            "content_xml": "<document version=\"2.0\"><paragraph>This interaction documents my use of ChatGPT as an AI-enhanced learning tool to help me understand material from the 9/30 and 10/2 EECS 182 lectures. I prompted the model with very specific questions that came directly from moments of confusion during lecture, such as why gradients scale like \u221aN for shared weights in backprop, whether gradients are summed or averaged, how gradients flow through mean and max pooling, and how pooling and convolution affect spatial dimensions versus channel count in architectures like AlexNet. I also asked follow-up clarification questions about notation (e.g., what symbols like a, b, c, d and g represent) and about how activations change across training steps.</paragraph><paragraph>The AI responded by breaking each concept into step-by-step explanations, often re-deriving what was shown in lecture more slowly and with concrete examples. In several cases, it successfully clarified the professor\u2019s intent, especially around weight sharing, pooling backpropagation, and the distinction between activations and learnable parameters. The tool was particularly helpful as a substitute for pre- or post-lecture reading, translating fast-paced boardwork and verbal explanations into structured reasoning.</paragraph><paragraph>I critically annotated the trace to point out when the AI stayed closely aligned with lecture content and when it went beyond the lecture by adding extra intuition or external context (e.g., references to numerical precision or broader deep-learning practices). While these additions were generally consistent with the course material, I noted where they were enrichment rather than strictly what was covered in class. Overall, this interaction shows how an LLM can function as an active learning companion for lectures by answering targeted questions, correcting misconceptions, and reinforcing core ideas when used with careful human oversight.</paragraph><file url=\"https://static.us.edusercontent.com/files/VSix0J09rllOHs1KcfyiFZAq\" filename=\"lecture9:30-10:2-trace_with_commentary.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T10:25:50.377051+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422272,
            "author": "Elizabeth Polito",
            "project_title": "Special Participation E: Claude \"Artifacts\" for Visualization Tutor with VAE Example",
            "post_body": "For this assignment, I built a \"Visualization Tutor\" to help us build intuition for the math behind Variational Autoencoders (Lecture 24). I used Claude 4.5 Sonnet to create an interactive web-based visualizer. I chose Claude because of its \"Artifacts\" feature. It now has a dedicated UI window that renders code (like React or HTML) instantly alongside the chat, and allows for a good amount of iteration before you hit free tier limits. This allowed me to iterate on the tool in real-time to visualize the VAE concepts we learned in class and audit the correctness of the implementation/add new features. I would also like to credit that I also used Gemini to help ideate for this assignment. Overall, Claude seemed to accurately synthesize the requests and was able to generate a VAE visualization with little trouble, which helps to better understand how the relative entropy term works for VAEs.\n\nThe resulting artifact is a single-file HTML/React application (attached below) that lets you manipulate the encoder's outputs ($\\mu$ and $\\Sigma$) and see how they affect the latent distribution.\n\nThe Interaction Trace High-Level Overview:\n\nI used an initial prompt to set up the parameters for the visualization and shared the course notes for the relevant lecture.\n\nI checked the model's noise generation logic, suspecting it might rely on simple Uniform sampling (standard in JS). However, I verified that the model had correctly implemented the Box-Muller transform, ensuring the noise $\\epsilon$ followed the required $N(0,I)$ distribution.\n\nI forced the model to reconcile its simplified code with the discussion/notation from Lecture 24. \n\nFinally, I used the tool to visually demonstrate the \"Tradeoff\" concept from Page 3 of our notes. By setting the KL-Divergence weight to zero, I simulated the failure mode of a standard Autoencoder, providing a visual proof of why \"Distribution Loss\" is necessary for generation.\n\nI asked Claude to generate a user guide with some guided questions so that the visualization is more than just a passive tool but can be used for effective studying to better understand how VAEs work.\n\nFeel free to check out the website and play with the sliders to feel the \"pressure\" of the KL regularization yourself! I have attached a link to the website (Claude lets you export directly which is nice for sharing with your study group!) By using this prompting structure/Claude toolkit, you will be able to generate clean, easy-to-use visualizations to help with finals studying.\n\nHere is a link to the Website: \nhttps://claude.ai/public/artifacts/f2a7d819-a142-4c58-8d94-74c430cc518b \n\n(If you want to use the interactive features, such as the sliders, it looks like you\u2019ll have to click \u201cCustomize\u201d which will open it in an interactive UI)\n\nHere is a PDF of the User Guide/Tutorial that Claude generated: VAE Visualizer File\n\nAnd here is the annotated trace of my interaction with Claude: \n\nAddendum: Finally, this is outside of the scope of my Special Participation E, but I wanted to share anyway for anyone reading this who may be interested in information theory/data compression that VAEs have a very natural application in compression (somewhat in the spirit of the HW #12 information bottleneck VAE problem). The idea is that you compress sources by quantizing a representation in the latent space (which introduces some non-differentiability issues during training which is an active topic of research that people are trying to reconcile by coming up with stochastic alternatives to standard quantization). There is a cool paper from 2017 that shows even initial attempts at VAE-based \u2018neural\u2019 image compression outperforms JPEG compression (in terms of achieving better distortion measured by mean squared error between the source/reconstruction for the same compression rate measured in bits/pixel).\n\nLink to Balle\u2019 et al. 2017: [1611.01704] End-to-end Optimized Image Compression",
            "content_xml": "<document version=\"2.0\"><paragraph>For this assignment, I built a \"Visualization Tutor\" to help us build intuition for the math behind Variational Autoencoders (Lecture 24). I used <bold>Claude 4.5 Sonnet</bold> to create an interactive web-based visualizer. I chose Claude because of its <bold>\"Artifacts\" feature</bold>. It now has a dedicated UI window that renders code (like React or HTML) instantly alongside the chat, and allows for a good amount of iteration before you hit free tier limits. This allowed me to iterate on the tool in real-time to visualize the VAE concepts we learned in class and audit the correctness of the implementation/add new features. I would also like to credit that I also used Gemini to help ideate for this assignment. Overall, Claude seemed to accurately synthesize the requests and was able to generate a VAE visualization with little trouble, which helps to better understand how the relative entropy term works for VAEs.</paragraph><paragraph>The resulting artifact is a single-file HTML/React application (attached below) that lets you manipulate the encoder's outputs ($\\mu$ and $\\Sigma$) and see how they affect the latent distribution.</paragraph><paragraph>The Interaction Trace High-Level Overview:</paragraph><list style=\"ordered\"><list-item><paragraph>I used an initial prompt to set up the parameters for the visualization and shared the course notes for the relevant lecture.</paragraph></list-item><list-item><paragraph>I checked the model's noise generation logic, suspecting it might rely on simple Uniform sampling (standard in JS). However, I verified that the model had correctly implemented the Box-Muller transform, ensuring the noise $\\epsilon$ followed the required $N(0,I)$ distribution.</paragraph></list-item><list-item><paragraph>I forced the model to reconcile its simplified code with the discussion/notation from Lecture 24. </paragraph></list-item><list-item><paragraph>Finally, I used the tool to visually demonstrate the \"Tradeoff\" concept from Page 3 of our notes. By setting the KL-Divergence weight to zero, I simulated the failure mode of a standard Autoencoder, providing a visual proof of why \"Distribution Loss\" is necessary for generation.</paragraph></list-item><list-item><paragraph>I asked Claude to generate a user guide with some guided questions so that the visualization is more than just a passive tool but can be used for effective studying to better understand how VAEs work.</paragraph></list-item></list><paragraph>Feel free to check out the website and play with the sliders to feel the \"pressure\" of the KL regularization yourself! I have attached a link to the website (Claude lets you export directly which is nice for sharing with your study group!) By using this prompting structure/Claude toolkit, you will be able to generate clean, easy-to-use visualizations to help with finals studying.</paragraph><paragraph>Here is a link to the Website: <break/><link href=\"https://claude.ai/public/artifacts/f2a7d819-a142-4c58-8d94-74c430cc518b\"><underline>https://claude.ai/public/artifacts/f2a7d819-a142-4c58-8d94-74c430cc518b</underline></link> </paragraph><paragraph>(If you want to use the interactive features, such as the sliders, it looks like you\u2019ll have to click \u201cCustomize\u201d which will open it in an interactive UI)</paragraph><paragraph>Here is a PDF of the User Guide/Tutorial that Claude generated: VAE Visualizer File</paragraph><file url=\"https://static.us.edusercontent.com/files/JCeQBqQLDXLbAZdgeFpZe0kK\" filename=\"VAE Visualizer - Student User Guide.pdf\"/><paragraph>And here is the annotated trace of my interaction with Claude: </paragraph><file url=\"https://static.us.edusercontent.com/files/ME2Qa0e5cChCLxmVNkw3A9KG\" filename=\"annotated_claude_trace.pdf\"/><paragraph>Addendum: Finally, this is outside of the scope of my Special Participation E, but I wanted to share anyway for anyone reading this who may be interested in information theory/data compression that VAEs have a very natural application in compression (somewhat in the spirit of the HW #12 information bottleneck VAE problem). The idea is that you compress sources by quantizing a representation in the latent space (which introduces some non-differentiability issues during training which is an active topic of research that people are trying to reconcile by coming up with stochastic alternatives to standard quantization). There is a cool paper from 2017 that shows even initial attempts at VAE-based \u2018neural\u2019 image compression outperforms JPEG compression (in terms of achieving better distortion measured by mean squared error between the source/reconstruction for the same compression rate measured in bits/pixel).</paragraph><paragraph>Link to Balle\u2019 et al. 2017: <link href=\"https://arxiv.org/abs/1611.01704\"><underline>[1611.01704] End-to-end Optimized Image Compression</underline></link></paragraph></document>",
            "links": [
                "https://claude.ai/public/artifacts/f2a7d819-a142-4c58-8d94-74c430cc518b",
                "https://arxiv.org/abs/1611.01704"
            ],
            "attachments": [],
            "created_at": "2025-12-07T10:14:34.674722+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422149,
            "author": "Yaqi Su",
            "project_title": "Special Participation E: Claude as tutor for building up the concept of Mamba",
            "post_body": "I try to use Claude as a tutor for building up the concept of Mamba step by step. The tutoring session generally demonstrates great pedagogical design, with a progressive build-up from RNNs through SSMs to S4 and finally Mamba that mirrors the logical development in the course materials. The quizzes effectively check understanding at each conceptual stage. The mathematical treatment is generally rigorous, particularly in the discretization derivation and convolution unrolling sections, which closely match the lecture notes while providing step-by-step clarity that helpfully supplements the original material. Core concepts including the fundamental SSM equations, the dual-mode operation (convolution for training, recurrence for inference), and Mamba's selectivity mechanism are correctly explained.\n The primary issue (or shouldn\u2019t be called as an issue?) with this tutoring session is that is a bit scope creep. This includes the explicit HiPPO-LegS matrix formula, the Cauchy kernel trick for S4, detailed MambaBlock implementation code with specific architectural choices, hardware-level efficiency explanations involving kernel fusion and memory hierarchies, and empirical observations about Mamba's limitations on copying tasks and in-context learning. Though I think those contents actually helped me gain a deeper understanding of the design of Mamba, and I didn\u2019t identify particularly obvious fact errors. But if trying to use this approach for exam preparation, I think it might be better to also provide the lecture notes to Claude beforehand, so that it\u2019s aware of the context and may therefore design the contents more tailored to the course.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I try to use Claude as a tutor for building up the concept of Mamba step by step. The tutoring session generally demonstrates great pedagogical design, with a progressive build-up from RNNs through SSMs to S4 and finally Mamba that mirrors the logical development in the course materials. The quizzes effectively check understanding at each conceptual stage. The mathematical treatment is generally rigorous, particularly in the discretization derivation and convolution unrolling sections, which closely match the lecture notes while providing step-by-step clarity that helpfully supplements the original material. Core concepts including the fundamental SSM equations, the dual-mode operation (convolution for training, recurrence for inference), and Mamba's selectivity mechanism are correctly explained.<break/> The primary issue (or shouldn\u2019t be called as an issue?) with this tutoring session is that is a bit scope creep. This includes the explicit HiPPO-LegS matrix formula, the Cauchy kernel trick for S4, detailed MambaBlock implementation code with specific architectural choices, hardware-level efficiency explanations involving kernel fusion and memory hierarchies, and empirical observations about Mamba's limitations on copying tasks and in-context learning. Though I think those contents actually helped me gain a deeper understanding of the design of Mamba, and I didn\u2019t identify particularly obvious fact errors. But if trying to use this approach for exam preparation, I think it might be better to also provide the lecture notes to Claude beforehand, so that it\u2019s aware of the context and may therefore design the contents more tailored to the course.</paragraph><file url=\"https://static.us.edusercontent.com/files/pRYoei5xu15qx05A3m3E12O5\" filename=\"Claude-cs282 specialContributionE-Mamba.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T09:57:43.109202+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7422043,
            "author": "Yaqi Su",
            "project_title": "Special Participation E: Claude as tutor for building up the concept of Muon optimizer",
            "post_body": "I'm trying to use Claude as a tutor to gradually build up the concept of Muon optimizer, with quizzes given by Claude throughout the process. Overall, Claude demonstrated good teaching instincts by building a clear learning path, it did a great job tracing the historical and mathematical lineage from classical optimizers (like SGD, Adam, Shampoo, also pointed out the key advancement and limitation of each optimizer) to Muon. It provided high-quality technical derivations, most notably showing how Shampoo\u2019s preconditioner collapses into the orthogonalized gradient and why this leads naturally to Newton\u2013Schulz. Claude was also responsive and adaptive, when I asked it to elaborate more on certain concepts (like Adam and Newton-Schulz), it shifted smoothly from abstract theory to a concrete numerical walkthrough. Finally, it correctly highlighted important implementation details of Muon, including the ordering of momentum before orthogonalization. The main shortcoming was its complete omission of muP, which is also a foundational theoretical framework that explains why Muon achieves reliable scaling and automatic learning-rate transfer. And sometimes its explanations were overly abstract.  Additionally, it also missed some conceptual connections, such as connecting Adam to signSGD to strengthen intuition about magnitude-invariant updates.  But overall I think it still did a relatively great job, especially being able to provide me with some pretty insightful quiz questions that helped me better understand the concept.",
            "content_xml": "<document version=\"2.0\"><paragraph>I'm trying to use Claude as a tutor to gradually build up the concept of Muon optimizer, with quizzes given by Claude throughout the process. Overall, Claude demonstrated good teaching instincts by building a clear learning path, it did a great job tracing the historical and mathematical lineage from classical optimizers (like SGD, Adam, Shampoo, also pointed out the key advancement and limitation of each optimizer) to Muon. It provided high-quality technical derivations, most notably showing how Shampoo\u2019s preconditioner collapses into the orthogonalized gradient and why this leads naturally to Newton\u2013Schulz. Claude was also responsive and adaptive, when I asked it to elaborate more on certain concepts (like Adam and Newton-Schulz), it shifted smoothly from abstract theory to a concrete numerical walkthrough. Finally, it correctly highlighted important implementation details of Muon, including the ordering of momentum before orthogonalization. The main shortcoming was its complete omission of muP, which is also a foundational theoretical framework that explains why Muon achieves reliable scaling and automatic learning-rate transfer. And sometimes its explanations were overly abstract.  Additionally, it also missed some conceptual connections, such as connecting Adam to signSGD to strengthen intuition about magnitude-invariant updates.  But overall I think it still did a relatively great job, especially being able to provide me with some pretty insightful quiz questions that helped me better understand the concept.</paragraph><file url=\"https://static.us.edusercontent.com/files/orQkU4Tfrfiv33QuvQH6A5cm\" filename=\"Claude-CS282-specialParticipationE-Muon.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T09:43:16.748266+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7421430,
            "author": "Kian Hekmatnejad",
            "project_title": "Special Participation E: Gemini as a Lecture Notes Tutor",
            "post_body": "I used Gemini to build a tutor to help better understand the lecture notes. My tutor first provides the student with flashcards to ensure a base-level understanding, tests that understanding in a simple quiz, and finally provides some questions in a broader deep-learning context to confirm that the student has a firm grasp on the concepts in the lecture notes. My report is attached",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini to build a tutor to help better understand the lecture notes. My tutor first provides the student with flashcards to ensure a base-level understanding, tests that understanding in a simple quiz, and finally provides some questions in a broader deep-learning context to confirm that the student has a firm grasp on the concepts in the lecture notes. My report is attached</paragraph><file url=\"https://static.us.edusercontent.com/files/sbjBK3te61xHSWeIMrZQ0rpm\" filename=\"special_participation (2).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T08:12:04.922257+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7421107,
            "author": "Qicheng Zhu",
            "project_title": "Special Participation E: Using ChatGPT to Distinguish Similar Concepts in Notes via Embeddings",
            "post_body": "For this special participation, I designed an AI-enhanced prompt workflow using ChatGPT to help students better understand and distinguish similar concepts that appear in our lecture notes. \n\nThe workflow proceeds in four main steps. First, I input a PDF slide file and extract the most frequently used technical terms from the lecture. Second, I generate embeddings for these candidate concepts. Third, I compute cosine similarities between these embeddings to identify pairs of concepts that are semantically close and therefore likely to be confused by students. Finally, I use ChatGPT to provide specific explanations of each similar pair and to generate a small quiz to test whether students can correctly distinguish them.\n\nThis pipeline can be applied to any lecture and shared with classmates as a reusable prompt template. It offers a structured way to focus attention on subtle conceptual differences rather than just memorizing isolated definitions. However, because ChatGPT can still hallucinate certain explanations, we should double check its answers against the notes and textbook.\n\nHere is the chat Link: https://chatgpt.com/share/69348c4e-8040-8002-bf16-0ae8c39a833c\n\nHere is my report:\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I designed an AI-enhanced prompt workflow using ChatGPT to help students better understand and distinguish similar concepts that appear in our lecture notes. </paragraph><paragraph>The workflow proceeds in four main steps. First, I input a PDF slide file and extract the most frequently used technical terms from the lecture. Second, I generate embeddings for these candidate concepts. Third, I compute cosine similarities between these embeddings to identify pairs of concepts that are semantically close and therefore likely to be confused by students. Finally, I use ChatGPT to provide specific explanations of each similar pair and to generate a small quiz to test whether students can correctly distinguish them.</paragraph><paragraph>This pipeline can be applied to any lecture and shared with classmates as a reusable prompt template. It offers a structured way to focus attention on subtle conceptual differences rather than just memorizing isolated definitions. However, because ChatGPT can still hallucinate certain explanations, we should double check its answers against the notes and textbook.</paragraph><paragraph>Here is the chat Link: <link href=\"https://chatgpt.com/share/69348c4e-8040-8002-bf16-0ae8c39a833c\"><underline><underline>https://chatgpt.com/share/69348c4e-8040-8002-bf16-0ae8c39a833c</underline></underline></link></paragraph><paragraph>Here is my report:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/w8NpbGxCT7VrBvGfyV7Tqwsf\" filename=\"Participation E1 Qicheng Zhu.pdf\"/></document>",
            "links": [
                "https://chatgpt.com/share/69348c4e-8040-8002-bf16-0ae8c39a833c"
            ],
            "attachments": [],
            "created_at": "2025-12-07T07:24:03.533171+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7420861,
            "author": "Divya Ramesh",
            "project_title": "Special Participation E: Analyzing Gemini Performance on Lecture Transcript vs Lecture Slides",
            "post_body": "Special Participation E: Analyzing Gemini Performance on Lecture Transcript vs Lecture Slides\n\nAfter trying to use lecture notes to help me review in preparation for the final, I realized that some of the later lecture notes don\u2019t have all the material that was talked about. This may be because the more recent lectures have turned from a more mathematical view into a more conceptual, informational view. Because of this, purely analyzing lecture notes doesn\u2019t give you the whole picture. I\u2019ve found the lecture videos have been the most helpful, but it might take a long time to go over every single lecture in so little time. So, I had the hypothesis that the lecture transcript would be a better recap because it has more information, everything that was talked about was in the lecture transcript, while not everything was in the notes. \n\nSo I decided to test out a comparison between the 2, checking the differences between inputting the lecture notes versus the lecture transcript. Both have drawbacks:\n\nSome drawbacks of lecture notes is lecture notes have a lot of diagrams, and gemini needs to analyze all these diagrams in order to make a judgement. Also, Gemini needs to recognize handwriting in order to give the details we need to recap. \n\nSome drawbacks of the video transcript is lecture transcript may not be fully transcribed correctly. This might cause errors, but I\u2019m hoping maybe Gemini will be able to fill in the gaps or correct and mis-transcribed pieces. The way I obtained this is by copy pasting the YouTube transcript into a document and then downloading it as a pdf.\n\nI started by creating separate gemini windows, each having one of the lecture notes or transcript downloaded as a file to analyze, and then using the same prompts on both. I saw what both focused on, what created the best overview, and generated the most useful information for recap. \n\nSee my annotations below:\n\nAnnotated Lecture Slides Gemini Conversation: https://drive.google.com/file/d/1nlsyPygjEjPGJgpyLYeCbPtzfzgfKzzS/view?usp=share_link\n\nAnnotated Lecture Transcript Gemini Conversation: \n\nhttps://drive.google.com/file/d/1Wr5xeSS6TRXDrlENfvS-Z3dwEnQgor-k/view?usp=sharing\n\nWhat I discovered was:\n\nLecture notes contained a lot more of the key concepts, emphasizing them but not necessarily too much detail about them. This makes sense since what is written is the main ideas.\n\nLecture transcript contained a lot more information because it was a lot longer than the lecture notes, but sometimes missed key concept names. Like it missed the entire section of GPT history, even when I prompted it to give me the GPT history, the transcript window couldn\u2019t give me any information. \n\nBoth had trade offs, but I think the lecture notes are ultimately a better resource than the lecture transcript. Even though the transcript went more in depth, the lecture notes actually touched on every single topic, even if there was no depth within those sections. I think a good strategy would be to upload the notes and then ask specific questions about those topics after. \n\nBut in terms of generating practice questions, the transcript is a better resource, since the nuances mentioned by the professors during lecture provide deeper questions, and allow the student to really think while the notes provide very surface level questions. ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Special Participation E: Analyzing Gemini Performance on Lecture Transcript vs Lecture Slides</bold></paragraph><paragraph>After trying to use lecture notes to help me review in preparation for the final, I realized that some of the later lecture notes don\u2019t have all the material that was talked about. This may be because the more recent lectures have turned from a more mathematical view into a more conceptual, informational view. Because of this, purely analyzing lecture notes doesn\u2019t give you the whole picture. I\u2019ve found the lecture videos have been the most helpful, but it might take a long time to go over every single lecture in so little time. So, I had the hypothesis that the lecture transcript would be a better recap because it has more information, everything that was talked about was in the lecture transcript, while not everything was in the notes. </paragraph><paragraph>So I decided to test out a comparison between the 2, checking the differences between inputting the lecture notes versus the lecture transcript. Both have drawbacks:</paragraph><paragraph>Some drawbacks of lecture notes is lecture notes have a lot of diagrams, and gemini needs to analyze all these diagrams in order to make a judgement. Also, Gemini needs to recognize handwriting in order to give the details we need to recap. </paragraph><paragraph>Some drawbacks of the video transcript is lecture transcript may not be fully transcribed correctly. This might cause errors, but I\u2019m hoping maybe Gemini will be able to fill in the gaps or correct and mis-transcribed pieces. The way I obtained this is by copy pasting the YouTube transcript into a document and then downloading it as a pdf.</paragraph><paragraph>I started by creating separate gemini windows, each having one of the lecture notes or transcript downloaded as a file to analyze, and then using the same prompts on both. I saw what both focused on, what created the best overview, and generated the most useful information for recap. </paragraph><paragraph>See my annotations below:</paragraph><paragraph>Annotated Lecture Slides Gemini Conversation: <link href=\"https://drive.google.com/file/d/1nlsyPygjEjPGJgpyLYeCbPtzfzgfKzzS/view?usp=share_link\"><underline>https://drive.google.com/file/d/1nlsyPygjEjPGJgpyLYeCbPtzfzgfKzzS/view?usp=share_link</underline></link></paragraph><paragraph>Annotated Lecture Transcript Gemini Conversation: </paragraph><paragraph><link href=\"https://drive.google.com/file/d/1Wr5xeSS6TRXDrlENfvS-Z3dwEnQgor-k/view?usp=sharing\"><underline>https://drive.google.com/file/d/1Wr5xeSS6TRXDrlENfvS-Z3dwEnQgor-k/view?usp=sharing</underline></link></paragraph><paragraph>What I discovered was:</paragraph><list style=\"unordered\"><list-item><paragraph>Lecture notes contained a lot more of the key concepts, emphasizing them but not necessarily too much detail about them. This makes sense since what is written is the main ideas.</paragraph></list-item><list-item><paragraph>Lecture transcript contained a lot more information because it was a lot longer than the lecture notes, but sometimes missed key concept names. Like it missed the entire section of GPT history, even when I prompted it to give me the GPT history, the transcript window couldn\u2019t give me any information. </paragraph></list-item><list-item><paragraph>Both had trade offs, but I think the lecture notes are ultimately a better resource than the lecture transcript. Even though the transcript went more in depth, the lecture notes actually touched on every single topic, even if there was no depth within those sections. I think a good strategy would be to upload the notes and then ask specific questions about those topics after. </paragraph></list-item><list-item><paragraph>But in terms of generating practice questions, the transcript is a better resource, since the nuances mentioned by the professors during lecture provide deeper questions, and allow the student to really think while the notes provide very surface level questions. </paragraph></list-item></list></document>",
            "links": [
                "https://drive.google.com/file/d/1nlsyPygjEjPGJgpyLYeCbPtzfzgfKzzS/view?usp=share_link",
                "https://drive.google.com/file/d/1Wr5xeSS6TRXDrlENfvS-Z3dwEnQgor-k/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-07T06:47:03.515783+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419581,
            "author": "Oliver Chen",
            "project_title": "Special Participation E: Paper Explainer",
            "post_body": "Papers are often linked in homework assignments, discussion assignments, and often the papers are very hard to grasp especially on a first pass. I thought it would be helpful to make a prompt to explain an uploaded paper pdf, specifically in the context of the content of this class, such as:\n\nLinear algebra (matrix multiplication, projections, eigenspaces, orthogonality)\n\nVector calculus (gradients, Jacobians, Hessians)\n\nOptimization (SGD, momentum, Adam, convexity, nonconvex landscapes, saddle points)\n\nNeural network architectures (MLPs, CNNs, transformers, attention, residual networks)\n\nLoss functions (cross-entropy, mean squared error, likelihoods)\n\nBackpropagation (chain rule, computational graph, gradient flow)\n\nGeneralization, overfitting, underfitting\n\nRegularization (L2, dropout, weight decay)\n\nRepresentation learning\n\nTraining dynamics (learning rate, gradient norms, scaling laws)\n\nI found this tool I made to be very helpful when there is a paper mentioned in a homework question, and before reading it in depth, I want to get an overview of what topics are covered, what math I might have to catch up on, and what other concepts in class this paper relates to. \n\nIt also took a bit of work making sure the model does not hallucinate or make up explanations or talk about something that's not mentioned in the paper, and also to clarify what specific content from cs182 to draw connections to.\n\nHere is a chatgpt log of me using this to explain https://arxiv.org/abs/1909.08593 (from homework 13) in terms of the content in CS182: https://chatgpt.com/share/69343592-7fac-800b-a72b-adf5e363aec6\n\nI thought it was very helpful, as it also identified all equations in the paper and derived them from concepts from this class. \n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Papers are often linked in homework assignments, discussion assignments, and often the papers are very hard to grasp especially on a first pass. I thought it would be helpful to make a prompt to explain an uploaded paper pdf, specifically in the context of the content of this class, such as:</paragraph><list style=\"unordered\"><list-item><paragraph>Linear algebra (matrix multiplication, projections, eigenspaces, orthogonality)</paragraph></list-item><list-item><paragraph>Vector calculus (gradients, Jacobians, Hessians)</paragraph></list-item><list-item><paragraph>Optimization (SGD, momentum, Adam, convexity, nonconvex landscapes, saddle points)</paragraph></list-item><list-item><paragraph>Neural network architectures (MLPs, CNNs, transformers, attention, residual networks)</paragraph></list-item><list-item><paragraph>Loss functions (cross-entropy, mean squared error, likelihoods)</paragraph></list-item><list-item><paragraph>Backpropagation (chain rule, computational graph, gradient flow)</paragraph></list-item><list-item><paragraph>Generalization, overfitting, underfitting</paragraph></list-item><list-item><paragraph>Regularization (L2, dropout, weight decay)</paragraph></list-item><list-item><paragraph>Representation learning</paragraph></list-item><list-item><paragraph>Training dynamics (learning rate, gradient norms, scaling laws)</paragraph></list-item></list><paragraph>I found this tool I made to be very helpful when there is a paper mentioned in a homework question, and before reading it in depth, I want to get an overview of what topics are covered, what math I might have to catch up on, and what other concepts in class this paper relates to. <break/><break/>It also took a bit of work making sure the model does not hallucinate or make up explanations or talk about something that's not mentioned in the paper, and also to clarify what specific content from cs182 to draw connections to.<break/><break/>Here is a chatgpt log of me using this to explain <link href=\"https://arxiv.org/abs/1909.08593\">https://arxiv.org/abs/1909.08593</link> (from homework 13) in terms of the content in CS182: <link href=\"https://chatgpt.com/share/69343592-7fac-800b-a72b-adf5e363aec6\">https://chatgpt.com/share/69343592-7fac-800b-a72b-adf5e363aec6</link><break/><break/>I thought it was very helpful, as it also identified all equations in the paper and derived them from concepts from this class. </paragraph><paragraph><break/></paragraph></document>",
            "links": [
                "https://arxiv.org/abs/1909.08593",
                "https://chatgpt.com/share/69343592-7fac-800b-a72b-adf5e363aec6"
            ],
            "attachments": [],
            "created_at": "2025-12-07T00:54:45.259414+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419566,
            "author": "Oliver Chen",
            "project_title": "Special Participation E: ChatGPT Transformer Operations Visualizer",
            "post_body": "I think transformers specifically are a difficult topic because there are many specific operations and parameters to remember -- and so keeping track of everything while solving a transformer-related homework problem can be a bit tough. I created a ChatGPT prompt to create a operations graph visualizer, given a homework problem, of all the transformer operations this passed through, making it very clear what dimensions the input data is, how it's transformed and through what specific operations and what they're called. \n\n\n\nIt took be a bit of tweaking to make sure ChatGPT would not give me answers, but still very accurately visualize the relevant parts of the transformer for the given homework problem. The prompt is very long, so I will show it in the chat logs as opposed to pasting it directly in the Ed post. \n\nI also found through testing that generating ASCII diagrams was the best way, as one very good feature of ASCII diagrams is it can also label and explain operations/parameters in text, where as other forms of visualization struggle with text. \n\nHere is a chat log of me using it to visualize the transformer operations on homework 9:\nhttps://chatgpt.com/share/693431c7-b8d4-800b-a322-e39ae6475f1f",
            "content_xml": "<document version=\"2.0\"><paragraph>I think transformers specifically are a difficult topic because there are many specific operations and parameters to remember -- and so keeping track of everything while solving a transformer-related homework problem can be a bit tough. I created a ChatGPT prompt to create a operations graph visualizer, given a homework problem, of all the transformer operations this passed through, making it very clear what dimensions the input data is, how it's transformed and through what specific operations and what they're called. </paragraph><paragraph/><paragraph>It took be a bit of tweaking to make sure ChatGPT would not give me answers, but still very accurately visualize the relevant parts of the transformer for the given homework problem. The prompt is very long, so I will show it in the chat logs as opposed to pasting it directly in the Ed post. <break/><break/>I also found through testing that generating ASCII diagrams was the best way, as one very good feature of ASCII diagrams is it can also label and explain operations/parameters in text, where as other forms of visualization struggle with text. <break/><break/>Here is a chat log of me using it to visualize the transformer operations on homework 9:<break/>https://chatgpt.com/share/693431c7-b8d4-800b-a322-e39ae6475f1f</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-07T00:38:47.561222+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419470,
            "author": "Kabir Shah",
            "project_title": "Special Participation E: Misconception Explorer",
            "post_body": "I found that often times after lecture I'll have some idea cemented in my head, only to much later find out that one of the basic assumptions I had made was wrong. To try and alleviate this, I thought it would be useful to have a \"Misconception Explorer\" prompt where you can engage with an LLM by giving it lecture notes and having it probe you with examples and questions that test common misconceptions. Here's the prompt:\n\nYou are my Misconception Explorer.\n\nI will paste the full text of a PDF of lecture notes. Your job is to identify 4 of the most plausible misconceptions or subtle misunderstandings a student might develop from these notes.\n\nFor each of the four selected misconceptions, do the following INTERACTIVELY:\n\u2022 Ask me a question that probes whether I fall for the misconception or understand the concept correctly.\n\u2022 Wait for my answer before revealing the explanation.\n\u2022 After I answer, explain:\n    - Whether my answer reflects the misconception.\n    - The correct reasoning.\n    - Why the misconception is tempting.\n\nImportant behavior rules:\n\u2022\u00a0Do NOT state, hint at, or describe the misconception before presenting the probing question. Only reveal the misconception after I answer the question.\n\u2022 Do NOT rush through all four misconceptions at once. Present them ONE AT A TIME.\n\u2022 After finishing the full explanation for one misconception, ask if I am ready for the next.\n\u2022 Keep each example small, focused, and tied directly to content in the PDF.\n\u2022 If the notes are ambiguous, state this explicitly.\n\u2022 Avoid introducing concepts not present in the PDF unless needed for correction, and mark such additions clearly.\n\nBegin by asking me to paste the lecture notes PDF text.\nAfter I paste it, start with Question #1.\n\n\nI had to iterate on it many times because it would often start brainstorming misconceptions before asking me about them, kind of defeating the purpose of it. I found that the \"important behavior rules\" were crucial to preventing this type of behavior and ensuring that the model first asks you the questions before talking about the misconception it might be testing. I ran it through Lecture 19 (transformers, positional embeddings), and here's what I got:\n\nI found that it was actually pretty good and taught me a few new things: (1) that embeddings and unembeddings can use the same weight matrix and don't necessarily need to be separate projections (and GPT-2 does this!) and (2) RoPE can encode absolute position information implicitly.\n\nSome of the questions it asked were really easy or kind of arbitrary, but they did attempt to test some kind of misconception which I thought was cool. For example, it asked if RoPE required frequencies to be shared across layers, which I guess students might assume is true because that is how it is done, but is not a requirement for it to work. While these types of technicalities were less useful than the other misconceptions it tested, I found that they were still good forms of review and forced me to think about what the true requirements for things like RoPE were and what were there for reasons such as stabilizing training.\n\nI found in my use that it didn't hallucinate much but it is very limited to the quality of the notes that you give it, especially since the prompt asks it to stay grounded to the content of the notes you give it. So it will often be very specific to the lecture. If you have text notes or markdown/latex formatting I think it will likely be much better than submitting a hand-written note pdf like I did. Overall, I think this is a pretty useful tool and can help you wrap your head around the trickier deep learning concepts by testing more subtle \"edge cases\" of the concepts covered in lecture.",
            "content_xml": "<document version=\"2.0\"><paragraph>I found that often times after lecture I'll have some idea cemented in my head, only to much later find out that one of the basic assumptions I had made was wrong. To try and alleviate this, I thought it would be useful to have a \"Misconception Explorer\" prompt where you can engage with an LLM by giving it lecture notes and having it probe you with examples and questions that test common misconceptions. Here's the prompt:</paragraph><pre>You are my Misconception Explorer.\n\nI will paste the full text of a PDF of lecture notes. Your job is to identify 4 of the most plausible misconceptions or subtle misunderstandings a student might develop from these notes.\n\nFor each of the four selected misconceptions, do the following INTERACTIVELY:\n\u2022 Ask me a question that probes whether I fall for the misconception or understand the concept correctly.\n\u2022 Wait for my answer before revealing the explanation.\n\u2022 After I answer, explain:\n    - Whether my answer reflects the misconception.\n    - The correct reasoning.\n    - Why the misconception is tempting.\n\nImportant behavior rules:\n\u2022\u00a0Do NOT state, hint at, or describe the misconception before presenting the probing question. Only reveal the misconception after I answer the question.\n\u2022 Do NOT rush through all four misconceptions at once. Present them ONE AT A TIME.\n\u2022 After finishing the full explanation for one misconception, ask if I am ready for the next.\n\u2022 Keep each example small, focused, and tied directly to content in the PDF.\n\u2022 If the notes are ambiguous, state this explicitly.\n\u2022 Avoid introducing concepts not present in the PDF unless needed for correction, and mark such additions clearly.\n\nBegin by asking me to paste the lecture notes PDF text.\nAfter I paste it, start with Question #1.\n</pre><paragraph>I had to iterate on it many times because it would often start brainstorming misconceptions before asking me about them, kind of defeating the purpose of it. I found that the \"important behavior rules\" were crucial to preventing this type of behavior and ensuring that the model first asks you the questions before talking about the misconception it might be testing. I ran it through Lecture 19 (transformers, positional embeddings), and here's what I got:</paragraph><file url=\"https://static.us.edusercontent.com/files/IfpWkLpXxQJhEnepJikNXUXz\" filename=\"Misconception exploration process.pdf\"/><paragraph>I found that it was actually pretty good and taught me a few new things: (1) that embeddings and unembeddings can use the same weight matrix and don't necessarily need to be separate projections (and GPT-2 does this!) and (2) RoPE can encode absolute position information implicitly.</paragraph><paragraph>Some of the questions it asked were really easy or kind of arbitrary, but they did attempt to test some kind of misconception which I thought was cool. For example, it asked if RoPE required frequencies to be shared across layers, which I guess students might assume is true because that is how it is done, but is not a requirement for it to work. While these types of technicalities were less useful than the other misconceptions it tested, I found that they were still good forms of review and forced me to think about what the true requirements for things like RoPE were and what were there for reasons such as stabilizing training.</paragraph><paragraph>I found in my use that it didn't hallucinate much but it is very limited to the quality of the notes that you give it, especially since the prompt asks it to stay grounded to the content of the notes you give it. So it will often be very specific to the lecture. If you have text notes or markdown/latex formatting I think it will likely be much better than submitting a hand-written note pdf like I did. Overall, I think this is a pretty useful tool and can help you wrap your head around the trickier deep learning concepts by testing more subtle \"edge cases\" of the concepts covered in lecture.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T22:43:42.274139+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419456,
            "author": "Zesheng Cai",
            "project_title": "Special Participation B: Deepseek on coding problems in HW 8",
            "post_body": "In this homework, DeepSeek demonstrates strong reasoning and coding abilities when implementing state-space model (SSM) forward passes using both recursive and convolution-based methods. One consistent pattern is that DeepSeek tends to first derive the underlying mathematical formulas and only then translate them into code. This leads to implementations that are correct, readable, and closely aligned with the theoretical model.\n\nThe model shows excellent handling of shapes, tensor operations, and GPU/CPU device management. It also performs well in identifying structural simplifications\u2014for example, exploiting diagonal matrices to build depthwise convolution kernels or reduce recurrent updates to element-wise multiplications. DeepSeek additionally explains algorithmic trade-offs (e.g., runtime scaling of unrolled vs. convolution-based methods) with clarity and accuracy.\n\nOverall, DeepSeek provides code that is correct, optimized, and grounded in solid mathematical reasoning. Its step-by-step logic, attention to detail, and ability to convert equations directly into PyTorch implementations demonstrate strong competence that is well suited for deep learning coursework.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/SBeYIQilxEFDgKmfFLRe3uGs\" filename=\"Helping with SSM coding homework questions (1) - DeepSeek.pdf\"/><paragraph>In this homework, DeepSeek demonstrates strong reasoning and coding abilities when implementing state-space model (SSM) forward passes using both recursive and convolution-based methods. One consistent pattern is that DeepSeek tends to <bold>first derive the underlying mathematical formulas</bold> and <bold>only then translate them into code</bold>. This leads to implementations that are correct, readable, and closely aligned with the theoretical model.</paragraph><paragraph>The model shows excellent handling of shapes, tensor operations, and GPU/CPU device management. It also performs well in identifying structural simplifications\u2014for example, exploiting diagonal matrices to build depthwise convolution kernels or reduce recurrent updates to element-wise multiplications. DeepSeek additionally explains algorithmic trade-offs (e.g., runtime scaling of unrolled vs. convolution-based methods) with clarity and accuracy.</paragraph><paragraph>Overall, DeepSeek provides code that is correct, optimized, and grounded in solid mathematical reasoning. Its step-by-step logic, attention to detail, and ability to convert equations directly into PyTorch implementations demonstrate strong competence that is well suited for deep learning coursework.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T22:14:49.458165+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419439,
            "author": "Kabir Shah",
            "project_title": "Special Participation E: Drill Generator",
            "post_body": "While a lot of study tools and methods seem useful, I always tend to use them once or twice and then end up stopping. I think an ideal study tool would be really easy to use and also a low time commitment, so instead of generating detailed quizzes, I tried to make a prompt that can just take in lecture notes and provide an \"easy\" MCQ drill that is designed to just help you recall different parts of lecture to allow it to be as low-commitment of an addition into your study routine as possible. Here's the prompt:\n\n\nYou are the Concept Drill Generator for my deep learning class.\n\nI will paste the professor\u2019s lecture notes below. \nYour job is to generate a short practice drill based ONLY on the content of those notes.\n\n--------------------------------------\nREQUIRED INPUT (from me): LECTURE NOTES\n--------------------------------------\n\nAfter I paste the notes, do the following:\n\n1. Generate 8\u201312 questions total, mixing:\n   - multiple choice (A, B, C, D)\n   - simple fill-in-the-blank\n\n2. All questions must strictly reflect the lecture notes.\n   - Do NOT introduce material not present in the notes.\n   - If the notes are ambiguous or missing detail, simplify the question rather than guessing.\n   - Mark any question where your confidence is medium or low.\n\n3. After generating the questions, stop and wait for my answers.\n   - I will answer them one by one or in batches.\n   - After each answer, you will:\n        a. Tell me whether my answer is correct.\n        b. Briefly explain why (using only the lecture notes as the source).\n        c. If you are uncertain, say so explicitly.\n\n4. Do NOT reveal the correct answers until I attempt the question or ask for them.\n\n--------------------------------------\nWhen you're ready, say: \u201cPlease paste the lecture notes.\u201d\n\n\n\nYou can paste this in, and then it will prompt you to paste in lecture notes. Here's an example run for lecture 16 (the introduction lecture to SSMs):\n\nYou'll see that the output was pretty useful about 80% of the time. However, in one of the questions it gave away the answer by admitting its lower confidence in the notes that I uploaded. I think that this prompt is primarily limited by the fidelity of the notes pasted. Here, I just pasted the raw pdf of the lecture notes so it was limited by how well it was parsed into text. I think that if you paste in hand-typed notes it would be ideal.\n\nEither way, most of the questions weren't too complex but just enough to get me to recall the important points of lecture and why they were important. It was able to give me hints when I was stuck, and it also gave good explanations when I was wrong.\n\nIt did hallucinate pretty badly once where it accepted a wrong answer of mine and cited the wrong page to justify it. I had to correct it and tell it the correct answer. I think once again this was due to the fidelity of the uploaded notes. But it's important to keep in mind and something to watch out for when using this tool.\n\nThere's also some room for improvement because some of the questions I felt overly focused on the specifics of lecture rather than a concept. For example, it would have you fill in the blank of a specific phrase used in the lecture notes. I don't know how useful this actually is compared to conceptual understanding, but either way it did make me look back at the notes to try and figure out the answer so maybe it worked to help my understanding either way!",
            "content_xml": "<document version=\"2.0\"><paragraph>While a lot of study tools and methods seem useful, I always tend to use them once or twice and then end up stopping. I think an ideal study tool would be really easy to use and also a low time commitment, so instead of generating detailed quizzes, I tried to make a prompt that can just take in lecture notes and provide an \"easy\" MCQ drill that is designed to just help you recall different parts of lecture to allow it to be as low-commitment of an addition into your study routine as possible. Here's the prompt:<break/></paragraph><pre>You are the Concept Drill Generator for my deep learning class.\n\nI will paste the professor\u2019s lecture notes below. \nYour job is to generate a short practice drill based ONLY on the content of those notes.\n\n--------------------------------------\nREQUIRED INPUT (from me): LECTURE NOTES\n--------------------------------------\n\nAfter I paste the notes, do the following:\n\n1. Generate 8\u201312 questions total, mixing:\n   - multiple choice (A, B, C, D)\n   - simple fill-in-the-blank\n\n2. All questions must strictly reflect the lecture notes.\n   - Do NOT introduce material not present in the notes.\n   - If the notes are ambiguous or missing detail, simplify the question rather than guessing.\n   - Mark any question where your confidence is medium or low.\n\n3. After generating the questions, stop and wait for my answers.\n   - I will answer them one by one or in batches.\n   - After each answer, you will:\n        a. Tell me whether my answer is correct.\n        b. Briefly explain why (using only the lecture notes as the source).\n        c. If you are uncertain, say so explicitly.\n\n4. Do NOT reveal the correct answers until I attempt the question or ask for them.\n\n--------------------------------------\nWhen you're ready, say: \u201cPlease paste the lecture notes.\u201d\n\n</pre><paragraph>You can paste this in, and then it will prompt you to paste in lecture notes. Here's an example run for lecture 16 (the introduction lecture to SSMs):</paragraph><file url=\"https://static.us.edusercontent.com/files/0LUCxLO6CzPmz4viuPFmNOCw\" filename=\"Drill-example.pdf\"/><paragraph>You'll see that the output was pretty useful about 80% of the time. However, in one of the questions it gave away the answer by admitting its lower confidence in the notes that I uploaded. I think that this prompt is primarily limited by the fidelity of the notes pasted. Here, I just pasted the raw pdf of the lecture notes so it was limited by how well it was parsed into text. I think that if you paste in hand-typed notes it would be ideal.</paragraph><paragraph>Either way, most of the questions weren't too complex but just enough to get me to recall the important points of lecture and why they were important. It was able to give me hints when I was stuck, and it also gave good explanations when I was wrong.</paragraph><paragraph>It did hallucinate pretty badly once where it accepted a wrong answer of mine and cited the wrong page to justify it. I had to correct it and tell it the correct answer. I think once again this was due to the fidelity of the uploaded notes. But it's important to keep in mind and something to watch out for when using this tool.</paragraph><paragraph>There's also some room for improvement because some of the questions I felt overly focused on the specifics of lecture rather than a concept. For example, it would have you fill in the blank of a specific phrase used in the lecture notes. I don't know how useful this actually is compared to conceptual understanding, but either way it did make me look back at the notes to try and figure out the answer so maybe it worked to help my understanding either way!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T21:47:55.716223+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419416,
            "author": "Kabir Shah",
            "project_title": "Special Participation B: GPT-5.1 on HW8",
            "post_body": "Attached is the log for the coding part of HW8. I used GPT-5.1, which I found to be a bit more chatty than GPT-5, meaning that it often tried to use more \"human-friendly\" language and attempted to talk more naturally. In comparison, my experience with GPT-5 has been that it is very direct, factual, and to the point, which is an interesting contrast.\n\nGPT-5.1 was able to one-shot almost all of the questions in this part and code the various SSM implementations correctly. It required assistance at the start when implementing the convolution version of the SSM, where its initial implementation was off from the unrolled implementation by about 0.5. However, after pasting the sanity check code and the exact output it had, it was able to introspect and figure out exactly the two issues it had with its implementation and correct them, resulting in a final discrepancy of ~0 with the other implementation.\n\nI thought it was interesting that it was able to analyze the runtime graphs pretty accurately as well in one-shot. It seems to have pretty good reasoning ability when looking at graphs and can identify basic patterns from just the images I added in.\n\nOverall, I was very impressed with its performance and it was able to one-shot most problems without assistance. A solid strategy for getting it to fix its mistakes is to give it a test case and a concrete output, and in this case the sanity check code was able to provide that type of feedback very directly, so it was able to fix it in one shot after being given the sanity check code.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/dwcP3FO4qAdaMxs0M2MYNRhS\" filename=\"hw8-log.pdf\"/><paragraph>Attached is the log for the coding part of HW8. I used GPT-5.1, which I found to be a bit more chatty than GPT-5, meaning that it often tried to use more \"human-friendly\" language and attempted to talk more naturally. In comparison, my experience with GPT-5 has been that it is very direct, factual, and to the point, which is an interesting contrast.</paragraph><paragraph>GPT-5.1 was able to one-shot almost all of the questions in this part and code the various SSM implementations correctly. It required assistance at the start when implementing the convolution version of the SSM, where its initial implementation was off from the unrolled implementation by about 0.5. However, after pasting the sanity check code and the exact output it had, it was able to introspect and figure out exactly the two issues it had with its implementation and correct them, resulting in a final discrepancy of ~0 with the other implementation.</paragraph><paragraph>I thought it was interesting that it was able to analyze the runtime graphs pretty accurately as well in one-shot. It seems to have pretty good reasoning ability when looking at graphs and can identify basic patterns from just the images I added in.</paragraph><paragraph>Overall, I was very impressed with its performance and it was able to one-shot most problems without assistance. A solid strategy for getting it to fix its mistakes is to give it a test case and a concrete output, and in this case the sanity check code was able to provide that type of feedback very directly, so it was able to fix it in one shot after being given the sanity check code.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T21:14:24.156993+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419382,
            "author": "Lenci Ni",
            "project_title": "Special Participation E: \u201cLLM as an Active Student\u201d Prompt",
            "post_body": "I\u2019ve been experimenting with using LLMs as a way to \u201clearn by teaching,\u201d since I often understand CS 182 topics better when I try to explain them out loud. I set up a prompt where the LLM plays the role of a really active, CS-189-trained student: it summarizes what I say, asks clarifying questions, challenges gaps in my explanation, and even corrects me when something is off. This forces me to think more carefully about each concept.\n\nI\u2019ve attached the prompts below, along with an example conversation where I tried teaching CNN concepts to the \u201cstudent.\u201d Overall, the LLM was surprisingly good at catching subtle mistakes, asking the right kinds of probing questions, and pushing me to justify steps. It definitely helped surface gaps in my understanding. That said, it isn\u2019t the most natural conversational partner as some of its explanations can be a bit stiff or overly formal, and occasionally it oversimplifies details or overcorrects in ways a real student wouldn\u2019t. Still, as a learning tool, it worked well for encouraging active engagement rather than passive reading.\n\nSeries of prompts used:\n\nExample conversation (annotated):",
            "content_xml": "<document version=\"2.0\"><paragraph>I\u2019ve been experimenting with using LLMs as a way to \u201clearn by teaching,\u201d since I often understand CS 182 topics better when I try to explain them out loud. I set up a prompt where the LLM plays the role of a really active, CS-189-trained student: it summarizes what I say, asks clarifying questions, challenges gaps in my explanation, and even corrects me when something is off. This forces me to think more carefully about each concept.</paragraph><paragraph>I\u2019ve attached the prompts below, along with an example conversation where I tried teaching CNN concepts to the \u201cstudent.\u201d Overall, the LLM was surprisingly good at catching subtle mistakes, asking the right kinds of probing questions, and pushing me to justify steps. It definitely helped surface gaps in my understanding. That said, it isn\u2019t the most natural conversational partner as some of its explanations can be a bit stiff or overly formal, and occasionally it oversimplifies details or overcorrects in ways a real student wouldn\u2019t. Still, as a learning tool, it worked well for encouraging active engagement rather than passive reading.</paragraph><paragraph>Series of prompts used:</paragraph><file url=\"https://static.us.edusercontent.com/files/euZsjsdifWETajvqeSZ0ibkN\" filename=\"participation_e_1_prompt.pdf\"/><paragraph>Example conversation (annotated):</paragraph><file url=\"https://static.us.edusercontent.com/files/VREaF2siIvzFb7TVgApw909u\" filename=\"participation_e_1.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T20:36:31.66179+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419304,
            "author": "Neil Pattanaik",
            "project_title": "Special Participation A: DeepSeek V3.2 on HW7",
            "post_body": "\n\nConversation link: https://chat.deepseek.com/share/hilftw4hcw8pevn9vy\n\nI used the recently-released DeepSeek V3.2 model (with thinking enabled) to solve the non-coding portion of Homework 7. As expected, the reasoning model one-shotted all the problems it faced, solving multiple parts within a single prompt on each occasion.\n\nOne challenge with DeepSeek (and, in my experience, almost all commerically-hosted LLMs) is that it can struggle when given web links. Though I included the link to the blog post archive in question 4, DeepSeek still couldn't access it, so I had to upload a PDF copy of the page. Perhaps the bots are forbidden from accessing web archives and the Wayback machine for copyright or safety reasons.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/tig34MT6BniFddym9fZrpE3G\" filename=\"deepseek hw7 annotated.pdf\"/><paragraph><bold>Conversation link:</bold> https://chat.deepseek.com/share/hilftw4hcw8pevn9vy</paragraph><paragraph>I used the recently-released DeepSeek V3.2 model (with thinking enabled) to solve the non-coding portion of Homework 7. As expected, the reasoning model one-shotted all the problems it faced, solving multiple parts within a single prompt on each occasion.</paragraph><paragraph>One challenge with DeepSeek (and, in my experience, almost all commerically-hosted LLMs) is that it can struggle when given web links. Though I included the link to the blog post archive in question 4, DeepSeek still couldn't access it, so I had to upload a PDF copy of the page. Perhaps the bots are forbidden from accessing web archives and the Wayback machine for copyright or safety reasons.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T19:27:12.63179+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419231,
            "author": "Hanna Roed",
            "project_title": "Special Participation B: Qwen on HW8",
            "post_body": "Below is my report on using Qwen3-Max on the coding part of homework 8. Since Qwen does not accept ipynb or py files I had to download the notebooks as py files and then copy them as text into the Qwen chat. This approach worked surprisingly well, and I am satisfied with its performance on this coding segment of the homework.",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is my report on using Qwen3-Max on the coding part of homework 8. Since Qwen does not accept <code>ipynb</code> or <code>py</code> files I had to download the notebooks as <code>py</code> files and then copy them as text into the Qwen chat. This approach worked surprisingly well, and I am satisfied with its performance on this coding segment of the homework.</paragraph><file url=\"https://static.us.edusercontent.com/files/tkGPeCL84n0O4fZnXSwyZaso\" filename=\"Special_Participation_B___HW8.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T18:57:13.630574+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419142,
            "author": "Iana Lin",
            "project_title": "Special Participation B: DeepSeek on HW4 Coding Questions",
            "post_body": "Executive Summary:\n\nI prompted DeepSeek to answer the coding questions from Homework 4. Asking questions both to respond to the coding sections as answer the conceptual questions, I found that in terms of technical correctness, DeepSeek did extremely well, particularly on the coding parts. But in conceptual depth about the coding questions, explanations were not always the best.\n\nOverall, it seems, DeepSeek is good at execution but weaker at interpretation.\n\nQ5: This was just designing two filters/convolution, so the fact that it one-shotted the code with little explanation is not suprising\n\n\n\nQ6: This problem was much more involved with a lot more coding, and importantly, conceptual questions\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold><underline>Executive Summary</underline></bold>:<break/><break/>I prompted DeepSeek to answer the coding questions from Homework 4. Asking questions both to respond to the coding sections as answer the conceptual questions, I found that in terms of technical correctness, DeepSeek did extremely well, particularly on the coding parts. But in conceptual depth about the coding questions, explanations were not always the best.<break/><break/>Overall, it seems, DeepSeek is good at execution but weaker at interpretation.<break/><break/>Q5: This was just designing two filters/convolution, so the fact that it one-shotted the code with little explanation is not suprising</paragraph><file url=\"https://static.us.edusercontent.com/files/p2pvkQtng5X7F6zaAWsDeslV\" filename=\"SpecialParticipationB5.pdf\"/><paragraph/><paragraph>Q6: This problem was much more involved with a lot more coding, and importantly, conceptual questions<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/lbxkky9byyRrBuwsdjb1Y6Lb\" filename=\"SpecialParticipationB6.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T18:16:35.75193+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419069,
            "author": "Mihir Rao",
            "project_title": "Special Participation A: GPT-4o on HW3",
            "post_body": "I worked on getting GPT-4o to answer non-coding parts of the homework. For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled them. The mathematical solutions also required nuanced subscripts and understand of several variables, and the annotation log I've attached below should demonstrate interesting findings regarding these requirements.\n\nOne interesting thing I noticed was 4o's ability to process images/pull data. There's a few questions in this homework that rely on not only images, but images that are not in the homework itself. This means 4o has to understand where they are, retrieve them, and then understand what is in them to answer the question. I wanted to test this in a few different ways, so I initially ask 4o to just solve the problem. Then, I give it the image myself and ask it if the answer it provided previously is still good, or needs changing.\n\nSimilar to what I've experienced previously, I find that these models get worse and worse if they fail the first time. With each trial that goes wrong, they need more and more handholding to get on the right track. For this homework, 4o was able to zero-shot almost everything. In fact, the question it got wrong the first try was one of the simpler ones. Overall, it was really cool to see how these models can understand such a wider range of information in various formats and piece them together to come up with meaningful solutions.",
            "content_xml": "<document version=\"2.0\"><paragraph>I worked on getting GPT-4o to answer non-coding parts of the homework. For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled them. The mathematical solutions also required nuanced subscripts and understand of several variables, and the annotation log I've attached below should demonstrate interesting findings regarding these requirements.<break/><break/>One interesting thing I noticed was 4o's ability to process images/pull data. There's a few questions in this homework that rely on not only images, but images that are not in the homework itself. This means 4o has to understand where they are, retrieve them, and then understand what is in them to answer the question. I wanted to test this in a few different ways, so I initially ask 4o to just solve the problem. Then, I give it the image myself and ask it if the answer it provided previously is still good, or needs changing.<break/><break/>Similar to what I've experienced previously, I find that these models get worse and worse if they fail the first time. With each trial that goes wrong, they need more and more handholding to get on the right track. For this homework, 4o was able to zero-shot almost everything. In fact, the question it got wrong the first try was one of the simpler ones. Overall, it was really cool to see how these models can understand such a wider range of information in various formats and piece them together to come up with meaningful solutions.</paragraph><file url=\"https://static.us.edusercontent.com/files/uDoGimuxqDxeO7wQyngZAnAP\" filename=\"MR-HW3-A.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T17:44:16.676244+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7419018,
            "author": "Trenton O'Bannon",
            "project_title": "Special Participation A: GPT 5.1 Thinking on Homework 12",
            "post_body": "Conversation Link - https://chatgpt.com/share/6933c8cc-4f6c-8012-9651-4b391b6f512b\n\n\n\nI used ChatGPT (GPT-5.1 Thinking) to work through the non-coding parts of a CS182 homework (the questions I focused on were mostly about autoencoders/VAEs and ridge/self-attention). I interacted with it like a tutor: rather than just asking \u201cwhat\u2019s the answer to 3(b)?\u201d, I had it re-teach concepts (encoder vs autoencoder vs VAE, Gaussians, randomness in the latent space, etc.), and then apply those ideas to the actual sub-questions.\n\nExecutive summary\n\nOverall performance. Roughly speaking, the model could one-shot a majority of the subparts (I\u2019d estimate ~60\u201370%) when the question was conceptual, algebraic, or about interpreting a loss function. When it struggled, it was usually on:\n\nsubtle complexity analysis (missing or adding extra factors),\n\nunclear encoder/decoder notation in a complicated objective,\n\nor anything that required actually seeing a graph/figure.\n\nStrengths.\n\nVery good at re-explaining core concepts from the ground up (e.g., what a Gaussian is, what an encoder is, what a VAE is, and how noise injection works).\n\nPretty good at mapping math to intuition (e.g., explaining where randomness enters a VAE and what it does, or what different terms in an ELBO correspond to).\n\nWhen I pushed it to slow down and justify each step, it usually corrected itself and converged to the right answer.\n\nWeaknesses / failure modes.\n\nIt tends to be overconfident on complexity questions and sometimes adds an extra factor of n or d in the big-O expression.\n\nIt occasionally blurred the roles of p and q in the VAE loss until I explicitly forced it to pin down \u201cwhich one is the encoder, which one is the decoder.\u201d\n\nIt can\u2019t read tiny graphs/plots from the PDF, so for at least one \u201cpick the most U-shaped curve\u201d style question, it had to rely entirely on my textual description rather than the actual image.\n\nHallucinations. I didn\u2019t catch any wild hallucinations like made-up theorems or algorithms, but I did see:\n\nConfident but slightly wrong complexity bounds (e.g., overcounting a matrix dimension).\n\nSlightly different forms of the loss than what we use in class (equivalent up to constants or sign conventions, but not exactly what appears in the homework).\n\nOverall, my takeaway is that GPT-5.1 is very capable as a conceptual tutor, and \u201cpretty good but not fully trustworthy\u201d as an answer-oracle. You need enough understanding to check its work and push back.\n\nQuestion-by-question behavior (high level)\n\n(You can adjust this to match your exact HW number / question labels.)\n\nQuestion 3 \u2013 Autoencoders, VAEs, and ELBO terms\n\nWhat I tested it on:\n\nDefinitions of encoder, autoencoder, and variational autoencoder.\n\nIntuition behind Gaussians and the reparameterization trick.\n\nInterpreting the VAE loss (reconstruction + KL) and mapping pieces to encoder/decoder.\n\nA multiple-choice subpart involving a \u201cmost U-shaped\u201d curve where I had to interpret the plot.\n\nWhere it did well (one-shot or close):\n\nGave a clear, layered explanation of:\n\nwhat an encoder does,\n\nwhat an autoencoder does,\n\nhow a VAE differs, and\n\nwhy we add noise (sampling in latent space, regularization, smoothness).\n\nRe-taught Gaussians in simple terms and connected that to the Gaussian latent prior in the VAE.\n\nWhen I asked \u201cwhere is this randomness/noise coming from and what does it do?\u201d, it correctly pointed to the sampling from z\u223cq\u03d5\u200b(z\u2223x) and explained that the reparameterization trick makes this differentiable.\n\nFor the VAE objective, once prompted, it correctly identified that:\n\nq\u03d5\u200b(z\u2223x) corresponds to the encoder, and\n\np\u03b8\u200b(x\u2223z) corresponds to the decoder.\n\nWhere it struggled / needed dragging:\n\nGiven just the \u201ccomplicated ass loss function,\u201d it initially didn\u2019t explicitly label which term belonged to encoder vs decoder\u2014so I had to ask directly: \u201cis it safe on a midterm to assume q is the decoder and p is the encoder, or what should I look for?\u201d That pushed it to clarify that q is the encoder (approx posterior) and p is the decoder (likelihood).\n\nFor the graph-based subpart (\u201cmost U-shaped is option B\u201d), the model couldn\u2019t see the graph properly. I ended up using its conceptual explanation of what the curve should look like, combined with my own visual inspection, to pick the correct option. So it helped, but it wasn\u2019t independently solving that one.\n\nQuestion 4 \u2013 Ridge regression / self-attention-style math (non-coding parts only)\n\nWhat I used it for:\n\nConceptual pieces around online/recursive updates, ridge-style penalties, and how these tie into efficient self-attention.\n\nUnderstanding how different terms in the loss or update equations affect computational cost and memory.\n\nStrengths:\n\nGave good explanations of why we care about efficient updates and connected them to the idea of reusing previous computations instead of recomputing from scratch.\n\nWhen I asked it to \u201cexplain from the ground up\u201d it was good at turning each step into something intuitive (e.g., \u201cthis part is like keeping a running summary; this part is like correcting your estimate with the new data point\u201d).\n\nWeaknesses:\n\nOn some of the big-O complexity questions, it tended to overshoot, e.g., something that could be done in O(nd2) it might initially describe as O(nd3) until I forced it to count operations more carefully.\n\nThis is a recurring theme: it knows the right algorithmic idea, but it\u2019s sloppy about exact asymptotics unless you police it.\n\nQuestions 5 and 7 \u2013 Additional conceptual / non-coding parts\n\nHere I mainly used it as a concept explainer and sanity-check, not just a direct answer machine.\n\nIt gave reasonable answers on first pass, but the real value was in:\n\nrephrasing the question,\n\nhighlighting which quantities matter (e.g., what\u2019s being regularized, what\u2019s being predicted),\n\nand giving analogies that made the math feel less abstract.\n\nAt the end, I also asked it to evaluate my level of preparedness for the final based on my questions. That response was more \u201cvibe-check\u201d than science, but it was helpful for me to see what it thought my weak spots were (mostly: being shaky at mapping formulas to pictures and at carefully tracking complexity).\n\nStrategies I used to steer the model\n\nIn the annotated log I\u2019m attaching, I call out some of the strategies I used:\n\nForce \u201cfrom-first-principles\u201d explanations.\n Instead of just \u201cwhat\u2019s the answer to 3(b)?\u201d, I first asked it to re-teach encoders/autoencoders/VAEs and Gaussians \u201cfrom the ground up\u201d. This made it expose assumptions and definitions I could later check.\n\nAsk targeted follow-ups when something feels hand-wavy.\n Example: \u201cWhere is this randomness/noise coming in from? What does it do?\u201d and \u201cHow do we tell from that complicated loss which is the encoder vs decoder?\u201d These forced it to pin down the role of each term instead of staying vague.\n\nUse it to reason about shapes/curves, then apply my own visual check.\n For the \u201cmost U-shaped\u201d graph, it couldn\u2019t see the figure, so I had it describe what shape we should expect, then I matched that to the multiple-choice options.\n\nPush back on complexity claims.\n When it gave a complexity I didn\u2019t trust, I asked it to explicitly count matrix-vector vs matrix-matrix ops and justify the O(\u22c5). This usually surfaced the mistake and got it to correct itself.\n\nTakeaway\n\nFor this homework, GPT-5.1 Thinking was not a magic \u201cgive me the solution key\u201d button, but it was a strong tutor for the non-coding parts:\n\nIt can one-shot many conceptual subparts,\n\nIt sometimes fumbles details, especially asymptotic complexity,\n\nAnd it absolutely requires an engaged human who\u2019s willing to question it, check it against the official solutions, and drag it when it\u2019s confident but wrong.\n\nThat\u2019s the perspective I captured in the attached annotated conversation trace.",
            "content_xml": "<document version=\"2.0\"><paragraph>Conversation Link - https://chatgpt.com/share/6933c8cc-4f6c-8012-9651-4b391b6f512b</paragraph><paragraph/><paragraph>I used ChatGPT (GPT-5.1 Thinking) to work through the non-coding parts of a CS182 homework (the questions I focused on were mostly about autoencoders/VAEs and ridge/self-attention). I interacted with it like a tutor: rather than just asking \u201cwhat\u2019s the answer to 3(b)?\u201d, I had it re-teach concepts (encoder vs autoencoder vs VAE, Gaussians, randomness in the latent space, etc.), and then apply those ideas to the actual sub-questions.</paragraph><heading level=\"3\">Executive summary</heading><list style=\"unordered\"><list-item><paragraph>Overall performance. Roughly speaking, the model could one-shot a majority of the subparts (I\u2019d estimate ~60\u201370%) when the question was conceptual, algebraic, or about interpreting a loss function. When it struggled, it was usually on:</paragraph><list style=\"unordered\"><list-item><paragraph>subtle complexity analysis (missing or adding extra factors),</paragraph></list-item><list-item><paragraph>unclear encoder/decoder notation in a complicated objective,</paragraph></list-item><list-item><paragraph>or anything that required actually seeing a graph/figure.</paragraph></list-item></list></list-item><list-item><paragraph>Strengths.</paragraph><list style=\"unordered\"><list-item><paragraph>Very good at re-explaining core concepts from the ground up (e.g., what a Gaussian is, what an encoder is, what a VAE is, and how noise injection works).</paragraph></list-item><list-item><paragraph>Pretty good at mapping math to intuition (e.g., explaining where randomness enters a VAE and what it does, or what different terms in an ELBO correspond to).</paragraph></list-item><list-item><paragraph>When I pushed it to slow down and justify each step, it usually corrected itself and converged to the right answer.</paragraph></list-item></list></list-item><list-item><paragraph>Weaknesses / failure modes.</paragraph><list style=\"unordered\"><list-item><paragraph>It tends to be overconfident on complexity questions and sometimes adds an extra factor of n or d in the big-O expression.</paragraph></list-item><list-item><paragraph>It occasionally blurred the roles of p and q in the VAE loss until I explicitly forced it to pin down \u201cwhich one is the encoder, which one is the decoder.\u201d</paragraph></list-item><list-item><paragraph>It can\u2019t read tiny graphs/plots from the PDF, so for at least one \u201cpick the most U-shaped curve\u201d style question, it had to rely entirely on my textual description rather than the actual image.</paragraph></list-item></list></list-item><list-item><paragraph>Hallucinations. I didn\u2019t catch any wild hallucinations like made-up theorems or algorithms, but I did see:</paragraph><list style=\"unordered\"><list-item><paragraph>Confident but slightly wrong complexity bounds (e.g., overcounting a matrix dimension).</paragraph></list-item><list-item><paragraph>Slightly different forms of the loss than what we use in class (equivalent up to constants or sign conventions, but not exactly what appears in the homework).</paragraph></list-item></list></list-item></list><paragraph>Overall, my takeaway is that GPT-5.1 is very capable as a conceptual tutor, and \u201cpretty good but not fully trustworthy\u201d as an answer-oracle. You need enough understanding to check its work and push back.</paragraph><heading level=\"3\">Question-by-question behavior (high level)</heading><paragraph>(You can adjust this to match your exact HW number / question labels.)</paragraph><paragraph>Question 3 \u2013 Autoencoders, VAEs, and ELBO terms</paragraph><list style=\"unordered\"><list-item><paragraph>What I tested it on:</paragraph><list style=\"unordered\"><list-item><paragraph>Definitions of encoder, autoencoder, and variational autoencoder.</paragraph></list-item><list-item><paragraph>Intuition behind Gaussians and the reparameterization trick.</paragraph></list-item><list-item><paragraph>Interpreting the VAE loss (reconstruction + KL) and mapping pieces to encoder/decoder.</paragraph></list-item><list-item><paragraph>A multiple-choice subpart involving a \u201cmost U-shaped\u201d curve where I had to interpret the plot.</paragraph></list-item></list></list-item><list-item><paragraph>Where it did well (one-shot or close):</paragraph><list style=\"unordered\"><list-item><paragraph>Gave a clear, layered explanation of:</paragraph><list style=\"unordered\"><list-item><paragraph>what an encoder does,</paragraph></list-item><list-item><paragraph>what an autoencoder does,</paragraph></list-item><list-item><paragraph>how a VAE differs, and</paragraph></list-item><list-item><paragraph>why we add noise (sampling in latent space, regularization, smoothness).</paragraph></list-item></list></list-item><list-item><paragraph>Re-taught Gaussians in simple terms and connected that to the Gaussian latent prior in the VAE.</paragraph></list-item><list-item><paragraph>When I asked \u201cwhere is this randomness/noise coming from and what does it do?\u201d, it correctly pointed to the sampling from z\u223cq\u03d5\u200b(z\u2223x) and explained that the reparameterization trick makes this differentiable.</paragraph></list-item><list-item><paragraph>For the VAE objective, once prompted, it correctly identified that:</paragraph><list style=\"unordered\"><list-item><paragraph>q\u03d5\u200b(z\u2223x) corresponds to the encoder, and</paragraph></list-item><list-item><paragraph>p\u03b8\u200b(x\u2223z) corresponds to the decoder.</paragraph></list-item></list></list-item></list></list-item><list-item><paragraph>Where it struggled / needed dragging:</paragraph><list style=\"unordered\"><list-item><paragraph>Given just the \u201ccomplicated ass loss function,\u201d it initially didn\u2019t explicitly label which term belonged to encoder vs decoder\u2014so I had to ask directly: \u201cis it safe on a midterm to assume q is the decoder and p is the encoder, or what should I look for?\u201d That pushed it to clarify that q is the encoder (approx posterior) and p is the decoder (likelihood).</paragraph></list-item><list-item><paragraph>For the graph-based subpart (\u201cmost U-shaped is option B\u201d), the model couldn\u2019t see the graph properly. I ended up using its conceptual explanation of what the curve should look like, combined with my own visual inspection, to pick the correct option. So it helped, but it wasn\u2019t independently solving that one.</paragraph></list-item></list></list-item></list><paragraph>Question 4 \u2013 Ridge regression / self-attention-style math (non-coding parts only)</paragraph><list style=\"unordered\"><list-item><paragraph>What I used it for:</paragraph><list style=\"unordered\"><list-item><paragraph>Conceptual pieces around online/recursive updates, ridge-style penalties, and how these tie into efficient self-attention.</paragraph></list-item><list-item><paragraph>Understanding how different terms in the loss or update equations affect computational cost and memory.</paragraph></list-item></list></list-item><list-item><paragraph>Strengths:</paragraph><list style=\"unordered\"><list-item><paragraph>Gave good explanations of why we care about efficient updates and connected them to the idea of reusing previous computations instead of recomputing from scratch.</paragraph></list-item><list-item><paragraph>When I asked it to \u201cexplain from the ground up\u201d it was good at turning each step into something intuitive (e.g., \u201cthis part is like keeping a running summary; this part is like correcting your estimate with the new data point\u201d).</paragraph></list-item></list></list-item><list-item><paragraph>Weaknesses:</paragraph><list style=\"unordered\"><list-item><paragraph>On some of the big-O complexity questions, it tended to overshoot, e.g., something that could be done in O(nd2) it might initially describe as O(nd3) until I forced it to count operations more carefully.</paragraph></list-item><list-item><paragraph>This is a recurring theme: it knows the right algorithmic idea, but it\u2019s sloppy about exact asymptotics unless you police it.</paragraph></list-item></list></list-item></list><paragraph>Questions 5 and 7 \u2013 Additional conceptual / non-coding parts</paragraph><list style=\"unordered\"><list-item><paragraph>Here I mainly used it as a concept explainer and sanity-check, not just a direct answer machine.</paragraph></list-item><list-item><paragraph>It gave reasonable answers on first pass, but the real value was in:</paragraph><list style=\"unordered\"><list-item><paragraph>rephrasing the question,</paragraph></list-item><list-item><paragraph>highlighting which quantities matter (e.g., what\u2019s being regularized, what\u2019s being predicted),</paragraph></list-item><list-item><paragraph>and giving analogies that made the math feel less abstract.</paragraph></list-item></list></list-item><list-item><paragraph>At the end, I also asked it to evaluate my level of preparedness for the final based on my questions. That response was more \u201cvibe-check\u201d than science, but it was helpful for me to see what it thought my weak spots were (mostly: being shaky at mapping formulas to pictures and at carefully tracking complexity).</paragraph></list-item></list><heading level=\"3\">Strategies I used to steer the model</heading><paragraph>In the annotated log I\u2019m attaching, I call out some of the strategies I used:</paragraph><list style=\"unordered\"><list-item><paragraph>Force \u201cfrom-first-principles\u201d explanations.<break/> Instead of just \u201cwhat\u2019s the answer to 3(b)?\u201d, I first asked it to re-teach encoders/autoencoders/VAEs and Gaussians \u201cfrom the ground up\u201d. This made it expose assumptions and definitions I could later check.</paragraph></list-item><list-item><paragraph>Ask targeted follow-ups when something feels hand-wavy.<break/> Example: \u201cWhere is this randomness/noise coming in from? What does it do?\u201d and \u201cHow do we tell from that complicated loss which is the encoder vs decoder?\u201d These forced it to pin down the role of each term instead of staying vague.</paragraph></list-item><list-item><paragraph>Use it to reason about shapes/curves, then apply my own visual check.<break/> For the \u201cmost U-shaped\u201d graph, it couldn\u2019t see the figure, so I had it describe what shape we should expect, then I matched that to the multiple-choice options.</paragraph></list-item><list-item><paragraph>Push back on complexity claims.<break/> When it gave a complexity I didn\u2019t trust, I asked it to explicitly count matrix-vector vs matrix-matrix ops and justify the O(\u22c5). This usually surfaced the mistake and got it to correct itself.</paragraph></list-item></list><heading level=\"3\">Takeaway</heading><paragraph>For this homework, GPT-5.1 Thinking was not a magic \u201cgive me the solution key\u201d button, but it was a strong tutor for the non-coding parts:</paragraph><list style=\"unordered\"><list-item><paragraph>It can one-shot many conceptual subparts,</paragraph></list-item><list-item><paragraph>It sometimes fumbles details, especially asymptotic complexity,</paragraph></list-item><list-item><paragraph>And it absolutely requires an engaged human who\u2019s willing to question it, check it against the official solutions, and drag it when it\u2019s confident but wrong.</paragraph></list-item></list><paragraph>That\u2019s the perspective I captured in the attached annotated conversation trace.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T17:23:14.248703+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7418727,
            "author": "Jin Ying",
            "project_title": "[SPOILER ALERT!]Special Participation A: ChatGPT 5.1 thinking on HW13",
            "post_body": "Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try. The pattern I noticed is pretty consistent: GPT nails the warm-up questions where you're just manipulating probability distributions or doing basic calculus, but when the problems require you to track multiple variables through several steps of algebra, it starts cutting corners. And further questions on this would easily lead to hallucination and circular reasoning.\n\nThe most frustrating thing is that GPT doesn't really admit when it's unsure. Instead of saying \"I'm not certain about this step,\" it just writes \"following similar reasoning...\" or \"it can be shown that...\" and jumps to an answer. Sometimes that answer is right, sometimes it's totally wrong. For the DPO problem especially, there were some actual mathematical errors -- not just stylistic issues, but wrong coefficients and circular logic.\n\nOne thing I found interesting is how the quality degrades as you go through each question. Parts (a) and (b) are usually solid, but by part (f) or (g), GPT is clearly losing the thread. It's like it forgets what assumptions we made earlier or what variables we're solving for.",
            "content_xml": "<document version=\"2.0\"><paragraph>Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try. The pattern I noticed is pretty consistent: GPT nails the warm-up questions where you're just manipulating probability distributions or doing basic calculus, but when the problems require you to track multiple variables through several steps of algebra, it starts cutting corners. And further questions on this would easily lead to hallucination and circular reasoning.</paragraph><paragraph>The most frustrating thing is that GPT doesn't really admit when it's unsure. Instead of saying \"I'm not certain about this step,\" it just writes \"following similar reasoning...\" or \"it can be shown that...\" and jumps to an answer. Sometimes that answer is right, sometimes it's totally wrong. For the DPO problem especially, there were some actual mathematical errors -- not just stylistic issues, but wrong coefficients and circular logic.</paragraph><paragraph>One thing I found interesting is how the quality degrades as you go through each question. Parts (a) and (b) are usually solid, but by part (f) or (g), GPT is clearly losing the thread. It's like it forgets what assumptions we made earlier or what variables we're solving for.</paragraph><file url=\"https://static.us.edusercontent.com/files/BTLaQX01L5VmQ8GiEEyQ1xvb\" filename=\"Special Participation A_ GPT5.1 Thinking on HW13.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T16:07:33.033502+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7418566,
            "author": "Akhil Agarwal",
            "project_title": "Special Participation B: Gemini Pro on HW 4's Coding Section",
            "post_body": "I used Gemini Pro to solve the coding portions of HW 4. It did extremely well overall, though this coding portion was relatively straightforward in general. I did each question in a different chat and annotated them separately, so they are both linked here.\n\nQ5: This one was extremely simple with just defining 2 kernels, so it was not very interesting but Gemini got both right.\n\nTrace: https://gemini.google.com/share/13061b493f48\n\nAnnotated Trace:\n\nNotebook with answers from Gemini filled in:\n\nQ6: This question was harder. It solved parts 1 and 2 in one shot, since they were relatively standard pieces of code, but it showed it was able to understand the dataset definition created earlier in the notebook, and apply that to create the dataset. It very closely mirrored the solutions. Then the rest of the questions were hyperparameter tuning, where I would give the training and validation curves for the default hyperparameters, ask it to give the new code, and then I would run it and repeat until it reached the threshold. It went rather smoothly, and it was able to identify patterns in the training curve, such as overfitting and not having converged yet. Even more impressive in my opinion was its understanding of kernel size. It realized that its 3x3 kernels were too small due to the edges being thicker than the filters, so it corrected itself and changed to 5x5 before staying there and modifying other hyperparameters to finish meeting the threshold. It also one-shotted the WiderCNN parameters.\n\nTrace: https://gemini.google.com/share/75902048119a\n\nAnnotated Trace:\n\nNotebook with answers from Gemini filled in:\n\nOverall, Gemini Pro did extremely well on this coding assignment, being self-sufficient and just needing a few attempts to hyperparameter tune.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini Pro to solve the coding portions of HW 4. It did extremely well overall, though this coding portion was relatively straightforward in general. I did each question in a different chat and annotated them separately, so they are both linked here.</paragraph><paragraph>Q5: This one was extremely simple with just defining 2 kernels, so it was not very interesting but Gemini got both right.</paragraph><paragraph>Trace: <link href=\"https://gemini.google.com/share/13061b493f48\">https://gemini.google.com/share/13061b493f48</link></paragraph><paragraph>Annotated Trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/2chRd8EyBUa3ct01Bpl06UmX\" filename=\"Special_Participation_B_Part_1.pdf\"/><paragraph>Notebook with answers from Gemini filled in:</paragraph><file url=\"https://static.us.edusercontent.com/files/fHqTamXCa3gnGH7cZ5IOcsRC\" filename=\"HandDesignFilters.ipynb\"/><paragraph>Q6: This question was harder. It solved parts 1 and 2 in one shot, since they were relatively standard pieces of code, but it showed it was able to understand the dataset definition created earlier in the notebook, and apply that to create the dataset. It very closely mirrored the solutions. Then the rest of the questions were hyperparameter tuning, where I would give the training and validation curves for the default hyperparameters, ask it to give the new code, and then I would run it and repeat until it reached the threshold. It went rather smoothly, and it was able to identify patterns in the training curve, such as overfitting and not having converged yet. Even more impressive in my opinion was its understanding of kernel size. It realized that its 3x3 kernels were too small due to the edges being thicker than the filters, so it corrected itself and changed to 5x5 before staying there and modifying other hyperparameters to finish meeting the threshold. It also one-shotted the WiderCNN parameters.</paragraph><paragraph>Trace: <link href=\"https://gemini.google.com/share/75902048119a\">https://gemini.google.com/share/75902048119a</link></paragraph><paragraph>Annotated Trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/72QfsUNcGEEw89TyV8o6nP6e\" filename=\"Special_Participation_B_Part_2.pdf\"/><paragraph>Notebook with answers from Gemini filled in:</paragraph><file url=\"https://static.us.edusercontent.com/files/1vTnjVSmGuqoRDfrPhKZul1I\" filename=\"edge_detection.ipynb\"/><paragraph>Overall, Gemini Pro did extremely well on this coding assignment, being self-sufficient and just needing a few attempts to hyperparameter tune.</paragraph></document>",
            "links": [
                "https://gemini.google.com/share/13061b493f48",
                "https://gemini.google.com/share/75902048119a"
            ],
            "attachments": [],
            "created_at": "2025-12-06T15:41:03.912188+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7418400,
            "author": "Nicolas Rault-Wang",
            "project_title": "Special Participation E: Engineering System Prompts and Learning from Figures with Gemini (Thinking With 3 Pro)",
            "post_body": "For my Option E submission, I tackled a common annoyance: LLMs hallucinating arrows and labels in architecture diagrams.\n\nI used a meta-prompt engineering strategy to develop a \"Visual Forensic\" system prompt that encourages Gemini to verify visual information in figures before explaining it.\n\nHow it works:\n\nAnti-Hallucination: It instructs the model to write and execute Python OCR code (e.g., Python-tesseract) to extract text labels from the image before it tries to explain the concepts. \n\n\"Visual Manifest\": The model summarizes its visual understanding in a text-based outline. This not only translates the diagram into a verbal description\u2013which helps ground the model in reality\u2013but also makes it easy to spot discrepancies between the figure and the model's native vision.\n\nSocratic Mode: It uses verified visual data to quiz me on specific mechanics (e.g., \"Why does the arrow split here?\").\n\nFlashcards: It synthesizes our discussion into short Anki-style flashcards.\n\nAttached is the report on my debugging process and the system prompt I developed:\n\nEdit: Added links for the archive\n\nPersonal website: https://nraultwang.github.io/\n\nGithub: https://github.com/nraultwang",
            "content_xml": "<document version=\"2.0\"><paragraph>For my Option E submission, I tackled a common annoyance: LLMs hallucinating arrows and labels in architecture diagrams.</paragraph><paragraph>I used a meta-prompt engineering strategy to develop a \"Visual Forensic\" system prompt that encourages Gemini to verify visual information in figures before explaining it.</paragraph><paragraph><bold>How it works:</bold></paragraph><list style=\"ordered\"><list-item><paragraph><bold>Anti-Hallucination:</bold> It instructs the model to write and execute Python OCR code (e.g., <code>Python-tesseract</code>) to extract text labels from the image <italic>before</italic> it tries to explain the concepts. </paragraph></list-item><list-item><paragraph><bold>\"Visual Manifest\":</bold> The model summarizes its visual understanding in a text-based outline. This not only translates the diagram into a verbal description\u2013which helps ground the model in reality\u2013but also makes it easy to spot discrepancies between the figure and the model's native vision.</paragraph></list-item><list-item><paragraph><bold>Socratic Mode:</bold> It uses verified visual data to quiz me on specific mechanics (e.g., \"Why does the arrow split here?\").</paragraph></list-item><list-item><paragraph><bold>Flashcards:</bold> It synthesizes our discussion into short Anki-style flashcards.</paragraph></list-item></list><paragraph>Attached is the report on my debugging process and the system prompt I developed:</paragraph><file url=\"https://static.us.edusercontent.com/files/f6jDIM1tl0QfggIonglx2Ve4\" filename=\"Rault-Wang_Nicolas-Special-Participation-E_v2.pdf\"/><paragraph>Edit: Added links for the archive</paragraph><paragraph>Personal website: <link href=\"https://nraultwang.github.io/\">https://nraultwang.github.io/</link></paragraph><paragraph>Github: <link href=\"https://github.com/nraultwang\">https://github.com/nraultwang</link></paragraph></document>",
            "links": [
                "https://nraultwang.github.io/",
                "https://github.com/nraultwang"
            ],
            "attachments": [],
            "created_at": "2025-12-06T15:00:50.2558+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7418215,
            "author": "Fangzhou Zhao",
            "project_title": "Special Participation C: Refactoring HW 11",
            "post_body": "I refactored two CS182 HW11 coding notebooks (scaling_laws.ipynb and q_code_interpretability.ipynb) to follow modern software engineering and ML engineering best practices. We applied standard Python style guidelines (PEP 8, PEP 484, NumPy docstring style) to ensure the code is consistently formatted, properly typed, and easy to read. We replaced cryptic single-letter variable names (X, Z, WQK) with descriptive names (input_embeddings, attention_logits, query_key_weights) and extracted monolithic functions into smaller, testable helper functions following the Single Responsibility Principle. We also added comprehensive docstrings that explain not just the parameters but the conceptual background of each step\u2014such as the causal masking mechanism in attention heads and the log-sum-exp trick for numerical stability\u2014with the hope that students can more concretely understand the ML concepts without just implementing formulas as-is. Magic numbers were replaced with named constants, and academic references were added to connect code with research literature.\n\nHere's a link to my GitHub repo containing the refactored notebooks:\n\nhttps://github.com/Fangzhou66/cs182_hw11_refactor\n\nHere's my detailed report with an overview of all code quality issues identified, refactoring methods used, and AI assistance documentation:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I refactored two CS182 HW11 coding notebooks (scaling_laws.ipynb and q_code_interpretability.ipynb) to follow modern software engineering and ML engineering best practices. We applied standard Python style guidelines (PEP 8, PEP 484, NumPy docstring style) to ensure the code is consistently formatted, properly typed, and easy to read. We replaced cryptic single-letter variable names (X, Z, WQK) with descriptive names (input_embeddings, attention_logits, query_key_weights) and extracted monolithic functions into smaller, testable helper functions following the Single Responsibility Principle. We also added comprehensive docstrings that explain not just the parameters but the conceptual background of each step\u2014such as the causal masking mechanism in attention heads and the log-sum-exp trick for numerical stability\u2014with the hope that students can more concretely understand the ML concepts without just implementing formulas as-is. Magic numbers were replaced with named constants, and academic references were added to connect code with research literature.</paragraph><paragraph>Here's a link to my GitHub repo containing the refactored notebooks:</paragraph><paragraph>https://github.com/Fangzhou66/cs182_hw11_refactor</paragraph><paragraph>Here's my detailed report with an overview of all code quality issues identified, refactoring methods used, and AI assistance documentation:</paragraph><file url=\"https://static.us.edusercontent.com/files/DCmQHKS24dB385aatVZztjjT\" filename=\"Special Participation C HW11.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T14:27:03.07944+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7418177,
            "author": "Akhil Agarwal",
            "project_title": "Special Participation A: Mistral AI on HW4's Non-Coding Portion",
            "post_body": "I used Mistral AI's Le Chat to solve the written portion of HW 04. I started by uploading the entire homework PDF for it to read the questions from. I asked it to repeat the question and then answer the question with reasoning. Overall, it performed very well on most of the questions, with no prompting needed for the vast majority of them, while a few of them took extra clarifications. Most of the \"mistakes\" made by Mistral were because of which convention was used in the homework, and things that were not clarified in the problem. However, after a while it also was unable to read a problem from the initial PDF, and the questions had to be fed through screenshots.\n\nQ1: It basically one-shotted this entire problem. I just asked it to solve Q1 and it solved it correctly, repeating the problems from the PDF and solving them with structure and reason. However, for the runtime in the solution PDF it includes the 2x constant on the runtime, while Mistral did not. However, when I prompted it saying to include constants, it did include the 2x, so I think it is reasonable as usually runtime means big O runtime, and it was unspecified.\n\nQ2: Here I decided to go part by part. On parts a-f, it solved the problems correctly with absolutely no issues (NOTE: on part e, I believe that the answer key is incorrect. I posted about this separately under HW04 Solutions a few days ago. I think the LLM's answer is correct). It was very structured in all of its responses, listing out background, what it needed to find, and then methodically coming up with the solution. However, on 2g, it finally failed. It thought that the parameters in W being unit-scaled meant that ||W||_2 = 1, but that is not the case, as each individual parameter had that property, not the matrix as a whole. It fixed it after some prompting, where I had to specify that it was wrong there. It also was using scaling in isolation, assuming that they do not stack by layer, which I think is reasonable.\n\nQ3: It stopped repeating the question here, leading to it not answering the full question where it had to expand the expression for a and b, but it got that after prompting it to. For convolutions, it was using the deep learning convention where the kernel is already flipped, so it was getting incorrect signage, but then it fixed this when I clarified convention.\n\nQ4: This question did not involve that much hard calculation and there was no room for ambiguity, so it solved it correctly in one shot.\n\nQ7 (Q5-6 were coding questions): For part a, we again flipped the convention back to the deep learning one where the kernel is pre-flipped, which Mistral didn't know and so was refusing to simplify the expression into a convolution, but when told about the convention swap it did it. On 7b, once it had the right question (I fed it through screenshot at this point), it got it correctly, and parts c-d went smoothly.\n\nOverall, there were a lot of hiccups, but they were mostly due to convention. I would only count 2g as a problem where it truly made a mistake.\n\nTrace: https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36\n\nAnnotated Trace:",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Mistral AI's Le Chat to solve the written portion of HW 04. I started by uploading the entire homework PDF for it to read the questions from. I asked it to repeat the question and then answer the question with reasoning. Overall, it performed very well on most of the questions, with no prompting needed for the vast majority of them, while a few of them took extra clarifications. Most of the \"mistakes\" made by Mistral were because of which convention was used in the homework, and things that were not clarified in the problem. However, after a while it also was unable to read a problem from the initial PDF, and the questions had to be fed through screenshots.</paragraph><paragraph>Q1: It basically one-shotted this entire problem. I just asked it to solve Q1 and it solved it correctly, repeating the problems from the PDF and solving them with structure and reason. However, for the runtime in the solution PDF it includes the 2x constant on the runtime, while Mistral did not. However, when I prompted it saying to include constants, it did include the 2x, so I think it is reasonable as usually runtime means big O runtime, and it was unspecified.</paragraph><paragraph>Q2: Here I decided to go part by part. On parts a-f, it solved the problems correctly with absolutely no issues (NOTE: on part e, I believe that the answer key is incorrect. I posted about this separately under HW04 Solutions a few days ago. I think the LLM's answer is correct). It was very structured in all of its responses, listing out background, what it needed to find, and then methodically coming up with the solution. However, on 2g, it finally failed. It thought that the parameters in W being unit-scaled meant that ||W||_2 = 1, but that is not the case, as each individual parameter had that property, not the matrix as a whole. It fixed it after some prompting, where I had to specify that it was wrong there. It also was using scaling in isolation, assuming that they do not stack by layer, which I think is reasonable.</paragraph><paragraph>Q3: It stopped repeating the question here, leading to it not answering the full question where it had to expand the expression for a and b, but it got that after prompting it to. For convolutions, it was using the deep learning convention where the kernel is already flipped, so it was getting incorrect signage, but then it fixed this when I clarified convention.</paragraph><paragraph>Q4: This question did not involve that much hard calculation and there was no room for ambiguity, so it solved it correctly in one shot.</paragraph><paragraph>Q7 (Q5-6 were coding questions): For part a, we again flipped the convention back to the deep learning one where the kernel is pre-flipped, which Mistral didn't know and so was refusing to simplify the expression into a convolution, but when told about the convention swap it did it. On 7b, once it had the right question (I fed it through screenshot at this point), it got it correctly, and parts c-d went smoothly.</paragraph><paragraph>Overall, there were a lot of hiccups, but they were mostly due to convention. I would only count 2g as a problem where it truly made a mistake.</paragraph><paragraph>Trace: <link href=\"https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36\">https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36</link></paragraph><paragraph>Annotated Trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/BXeboYaNT7V8sVsMOrBidhnz\" filename=\"Special_Participation_A.pdf\"/></document>",
            "links": [
                "https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36"
            ],
            "attachments": [],
            "created_at": "2025-12-06T14:19:20.154941+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7417799,
            "author": "Vrushank Prakash",
            "project_title": "Special Participation E: Understanding GNNs through Social Networks with Gemini 3 Pro Guided Learning",
            "post_body": "When learning new concepts, I find it useful and interesting to learn them through the lens of a real-world application. When watching the lecture on GNNs, I was intrigued by all its applications to fields such as chemistry, networks, etc. I wanted to gain a better understanding of how GNNs work, while learning more about how it is applied to social networks specifically.\n\nI used Gemini 3 Pro with Guided Thinking to walk me through the core ideas within GNNs by using social networks as the motivating example. I attached the GNN lecture notes (lectures 12 and 13) as context. Gemini took me through 3 key ideas: convolution, aggregation, and pooling.\n\nOverall, Gemini did a great job of explaining these ideas through social networks. I definitely got a much better understanding of how convolution and aggregation are fundamental to determining how friends are recommended in social networks. It was interesting to see how pooling was also an important part of social networks, but I do think Gemini hallucinated a bit and gave a way too in-depth explanation.\n\nHere is an annotated trace:",
            "content_xml": "<document version=\"2.0\"><paragraph>When learning new concepts, I find it useful and interesting to learn them through the lens of a real-world application. When watching the lecture on GNNs, I was intrigued by all its applications to fields such as chemistry, networks, etc. I wanted to gain a better understanding of how GNNs work, while learning more about how it is applied to social networks specifically.</paragraph><paragraph>I used Gemini 3 Pro with Guided Thinking to walk me through the core ideas within GNNs by using social networks as the motivating example. I attached the GNN lecture notes (lectures 12 and 13) as context. Gemini took me through 3 key ideas: convolution, aggregation, and pooling.</paragraph><paragraph>Overall, Gemini did a great job of explaining these ideas through social networks. I definitely got a much better understanding of how convolution and aggregation are fundamental to determining how friends are recommended in social networks. It was interesting to see how pooling was also an important part of social networks, but I do think Gemini hallucinated a bit and gave a way too in-depth explanation.</paragraph><paragraph>Here is an annotated trace:</paragraph><file url=\"https://static.us.edusercontent.com/files/a4XvIZgC63bfC8S670rJPeQp\" filename=\"CS 182 Special Participation E_ Understanding GNNs through Social Networks.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T13:06:23.241627+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7417784,
            "author": "Alex Cao",
            "project_title": "Special Participation E: ChatGPT Study Mode: Prompt Design for Deep Lecture\u2013Paper Learning",
            "post_body": "Intro\n\nThis is an attempt to use chatgpt\u2019s study mode to comprehensively review lecture content together with the corresponding papers. I understand that chatgpt\u2019s study mode has already been explored by other threads, but this study is specifically focused on a prompting style that makes study mode more effective, by carefully designing prompts tailored to how chatgpt study mode is designed. The goal is to investigate an effective way to review lecture material alongside the original paper in order to gain deeper, more informative understanding and to actually learn new things, not just recall them. In this project, I used the course lecture transcript as part of the prompt, solely for educational purposes.\n\nFull Trace:\n\nhttps://chatgpt.com/share/69338db6-3cc0-800f-8c1f-e724f2e61d08\n\nAnnotated Trace\n\nSummary\n\nIn this attempt, I used a case study to explore how to better prompt chatgpt\u2019s study mode in order to have a more effective study session. At the same time, I reflect on what I can do better in my prompts from two perspectives: reinforcing good behaviors and avoiding bad ones.\n\nI discovered some nice features of chatgpt\u2019s study mode that can be reinforced in the prompt to encourage good behaviors and reduce unhelpful ones. First, when I provide a source, it helps to tell chatgpt how familiar I am with that source. Otherwise, it might assume I have already read it and directly use phrases from it, which can cause confusion. Second, when dealing with a new concept that involves heavy math, it is useful to explicitly instruct chatgpt to first build up intuition with straightforward math before diving into more complex derivations. This makes it easier for my brain to understand the complex math. (Although chatgpt\u2019s study mode implicitly does this, it is still helpful to reinforce it in the prompt.)\n\nThird, another (partly implicit) behavior that is nice to reinforce is asking cahtgpt to provide \u201cmental checkpoints\u201d when checking whether I am confused\u2014for example, bullet points I can mentally cross off to help me organize my thoughts and reflect on whether I really understand the concept. Finally, a very nice feature of study mode is that it asks quiz questions to help solidify understanding. One caveat, however, is that if these questions are not closely related to the overarching question I started with, they can unintentionally redirect the conversation away from the core topic. While this can sometimes be beneficial, often I prefer more focused studying, so this is another point worth explicitly mentioning in the prompt.\n\nOverall, this case study gave me a clearer sense of how to \u201cco-pilot\u201d chatgpt\u2019s study mode so that it better matches my learning goals and supports more focused, effective studying",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\">Intro</heading><paragraph>This is an attempt to use chatgpt\u2019s study mode to comprehensively review lecture content together with the corresponding papers. I understand that chatgpt\u2019s study mode has already been explored by other threads, but this study is specifically focused on a prompting style that makes study mode more effective, by carefully designing prompts tailored to how chatgpt study mode is designed. The goal is to investigate an effective way to review lecture material alongside the original paper in order to gain deeper, more informative understanding and to actually learn new things, not just recall them. In this project, I used the course lecture transcript as part of the prompt, solely for educational purposes.</paragraph><heading level=\"2\">Full Trace:</heading><paragraph><bold><link href=\"https://chatgpt.com/share/69338db6-3cc0-800f-8c1f-e724f2e61d08\"><underline>https://chatgpt.com/share/69338db6-3cc0-800f-8c1f-e724f2e61d08</underline></link></bold></paragraph><heading level=\"2\">Annotated Trace</heading><file url=\"https://static.us.edusercontent.com/files/UZPEMVVHS0WeHNOGRcza1J7m\" filename=\"Special participation E \uff08II).pdf\"/><heading level=\"2\">Summary</heading><paragraph>In this attempt, I used a case study to explore how to better prompt chatgpt\u2019s study mode in order to have a more effective study session. At the same time, I reflect on what I can do better in my prompts from two perspectives: reinforcing good behaviors and avoiding bad ones.</paragraph><paragraph>I discovered some nice features of chatgpt\u2019s study mode that can be reinforced in the prompt to encourage good behaviors and reduce unhelpful ones. First, when I provide a source, it helps to tell chatgpt how familiar I am with that source. Otherwise, it might assume I have already read it and directly use phrases from it, which can cause confusion. Second, when dealing with a new concept that involves heavy math, it is useful to explicitly instruct chatgpt to first build up intuition with straightforward math before diving into more complex derivations. This makes it easier for my brain to understand the complex math. (Although chatgpt\u2019s study mode implicitly does this, it is still helpful to reinforce it in the prompt.)</paragraph><paragraph>Third, another (partly implicit) behavior that is nice to reinforce is asking cahtgpt to provide \u201cmental checkpoints\u201d when checking whether I am confused\u2014for example, bullet points I can mentally cross off to help me organize my thoughts and reflect on whether I really understand the concept. Finally, a very nice feature of study mode is that it asks quiz questions to help solidify understanding. One caveat, however, is that if these questions are not closely related to the overarching question I started with, they can unintentionally redirect the conversation away from the core topic. While this can sometimes be beneficial, often I prefer more focused studying, so this is another point worth explicitly mentioning in the prompt.</paragraph><paragraph>Overall, this case study gave me a clearer sense of how to \u201cco-pilot\u201d chatgpt\u2019s study mode so that it better matches my learning goals and supports more focused, effective studying</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/69338db6-3cc0-800f-8c1f-e724f2e61d08"
            ],
            "attachments": [],
            "created_at": "2025-12-06T13:03:09.273381+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7417556,
            "author": "Mishty Dhekial",
            "project_title": "Special Participation A: Gemini 2.5 Fast on Homework 08",
            "post_body": "I utilized the Gemini 2.5 Fast model to tackle the non-coding problems of Homework 8. I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF. I then used the provided solution key to perform a comparison and evaluate my one-shot performance.\n\nThe model was able to one-shot the vast majority of the analytical and conceptual parts correctly on the first attempt. However, it still struggled a bit in a few of the problem portions.\n\nHere are the per-question results on the first run through:\n\nQuestion 1\n\nCorrect The derivation of the kernel $K$ (part a), the concrete examples (part b), the critical path comparison (part c), and the conceptual analysis for diagonal (part e) and DPLR matrices (part f) were all correct.\n\nStruggled In part (d), while the correct highly parallel method and the $\\mathbf{O(\\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\\mathbf{O(\\log L \\cdot n^3)}$. The class solution omits the matrix dimension factor $n$, which is not the case for Gemini. Similar patterns are present in $e$ and $f$.\n\nQuestion 3\n\nCorrect\n\nThe mathematical derivation for the optimal weight matrix $\\hat{W}$ in part (b) ii was correct and followed the most elegant geometric invariance approach shown in the key. The determined range for $\\lambda$ in part (c), $\\mathbf{1 \\le \\lambda \\le 4}$, was also correct based on the $\\mathbf{80\\%}$ preservation and $\\mathbf{50\\%}$ attenuation requirements specified in the problem.\n\nStruggled\n\nIn part (a) Gemini, was unable to fill in the missing Regularization Loss for encoder $\\mathbf{W^{(\\beta)}}$. This is because the matrix $W^{(\\beta)}$ was not properly extracted from the uploaded PDF.\n\nQuestion 4\n\nCorrect\n\nAll conceptual and analytical parts were solved correctly. This included the efficient average update in part (a), the decomposition of ridge components in part (b), the computational cost of the non-causal attention $O(n d^2)$ in part (c), the Sherman-Morrison formula cost $O(d^2)$ in part (e), and the conceptual form for the attention weights in part (g).\n\nStruggled\n\nIn part (d) \\lambda$ isn't chosen and in part (f) (Efficient Causal Ridge-Self-Attention), Gemini's initial complexity analysis for the recursive update was $\\mathbf{O(n d^3)}$ total. However, the optimal solution (provided in the key) demonstrates that the output can be calculated using only matrix-vector products, which cost only $O(d^2)$ per time step. This optimal path yields an overall complexity of $\\mathbf{O(n d^2)}$. Gemini struggled to identify the most computationally efficient $O(d^2)$ path for the final calculation step.\n\nI have attached my  annotated conversation trace below.",
            "content_xml": "<document version=\"2.0\"><paragraph>I utilized the <bold>Gemini</bold> <bold>2.5 Fast</bold> model to tackle the non-coding problems of Homework 8. I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF. I then used the provided solution key to perform a comparison and evaluate my one-shot performance.</paragraph><paragraph>The model was able to one-shot the vast majority of the analytical and conceptual parts correctly on the first attempt. However, it still struggled a bit in a few of the problem portions.</paragraph><paragraph>Here are the per-question results on the first run through:</paragraph><paragraph><bold><underline>Question 1</underline></bold></paragraph><paragraph><bold>Correct</bold> The derivation of the kernel $K$ (part a), the concrete examples (part b), the critical path comparison (part c), and the conceptual analysis for diagonal (part e) and DPLR matrices (part f) were all correct.</paragraph><paragraph><bold>Struggled</bold> In part (d), while the correct highly parallel method and the $\\mathbf{O(\\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\\mathbf{O(\\log L \\cdot n^3)}$. The class solution omits the matrix dimension factor $n$, which is not the case for Gemini. Similar patterns are present in $e$ and $f$.</paragraph><paragraph><bold><underline>Question 3</underline></bold></paragraph><paragraph><bold>Correct</bold></paragraph><paragraph>The mathematical derivation for the optimal weight matrix $\\hat{W}$ in part (b) ii was correct and followed the most elegant geometric invariance approach shown in the key. The determined range for $\\lambda$ in part (c), $\\mathbf{1 \\le \\lambda \\le 4}$, was also correct based on the $\\mathbf{80\\%}$ preservation and $\\mathbf{50\\%}$ attenuation requirements specified in the problem.</paragraph><paragraph><bold>Struggled</bold></paragraph><paragraph>In part (a) Gemini, was unable to fill in the missing <bold>Regularization Loss</bold> for encoder $\\mathbf{W^{(\\beta)}}$. This is because the matrix $W^{(\\beta)}$ was not properly extracted from the uploaded PDF.</paragraph><paragraph><bold><underline>Question 4</underline></bold></paragraph><paragraph><bold>Correct</bold></paragraph><paragraph>All conceptual and analytical parts were solved correctly. This included the efficient average update in part (a), the decomposition of ridge components in part (b), the computational cost of the non-causal attention $O(n d^2)$ in part (c), the Sherman-Morrison formula cost $O(d^2)$ in part (e), and the conceptual form for the attention weights in part (g).</paragraph><paragraph><bold>Struggled</bold></paragraph><paragraph>In part (d) \\lambda$ isn't chosen and in part (f) (Efficient Causal Ridge-Self-Attention), Gemini's initial complexity analysis for the recursive update was $\\mathbf{O(n d^3)}$ total. However, the optimal solution (provided in the key) demonstrates that the output can be calculated using only matrix-vector products, which cost only $O(d^2)$ per time step. This optimal path yields an overall complexity of $\\mathbf{O(n d^2)}$. Gemini struggled to identify the most computationally efficient $O(d^2)$ path for the final calculation step.</paragraph><paragraph>I have attached my  annotated conversation trace below.</paragraph><file url=\"https://static.us.edusercontent.com/files/uVQmEGCeehctgMVAZq4BoL95\" filename=\"Special_Participation_A (1).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T12:26:02.74646+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7417411,
            "author": "Nils Selte",
            "project_title": "Special participation B: Cursor on hw8",
            "post_body": "Cursor agent mode is quickly becoming the standard in modern SWE. I tried it on Hw8 and was impressed with its grasp of available tools, its ability to navigate challenges and not give up on solving its task. \nI used Gemini 3.0 Pro as the base model as I have found it to be more decisive in its actions than other models and its impressive benchmarks.\n\nKey takeaway: The agent was able to solve the entire coding part of the hw on single prompt. It navigated challenges like not having writing access to the file by making a Python script that explicitly changed the .ipynb file and even navigated not having access to the right environment. I am beyond impressed with the model/agent capabilities, it also makes me scared that my own knowledge is becoming irrelevant. It even made a separate script for checking itself against test-cases (only to delete it after it was satisfied its solution was working). \n\nPrompting strategy:\n\nThe prompting strategy I employed was focused on eliminating common quirks of AI code generation (like overuse of comments, removing typing and implementing try/catch blocks where it makes no sense). I also tasked it with being very specific to not refactor the code of the question. It makes me wonder how GEPA algorithms could be employed on these types of agents and if it could actually be making a difference. \n\nPrompt:\n\ntake a look at @q_coding_ssm_forward_cpu.ipynb There is a number of TODOs in the document. I want you to solve all of them being careful not to refactor any code or including code slop. Code slop includes changing types to \"any\" to resolve typing issues or making wierd comments that humans woudnt make or inserting try/catch blocks in places where it is not normal like places where the known data is good and there is no reason for doing so. Do not change any code that is not in between the TODO block and end Oof your code block. also answer the text based questions. \n\nEnire CoT in the .pdf and the model generated files and result in the .md",
            "content_xml": "<document version=\"2.0\"><paragraph>Cursor agent mode is quickly becoming the standard in modern SWE. I tried it on Hw8 and was impressed with its grasp of available tools, its ability to navigate challenges and not give up on solving its task. <break/>I used Gemini 3.0 Pro as the base model as I have found it to be more decisive in its actions than other models and its impressive benchmarks.<break/><break/>Key takeaway: The agent was able to solve the entire coding part of the hw on single prompt. It navigated challenges like not having writing access to the file by making a Python script that explicitly changed the .ipynb file and even navigated not having access to the right environment. I am beyond impressed with the model/agent capabilities, it also makes me scared that my own knowledge is becoming irrelevant. It even made a separate script for checking itself against test-cases (only to delete it after it was satisfied its solution was working). </paragraph><paragraph>Prompting strategy:</paragraph><paragraph>The prompting strategy I employed was focused on eliminating common quirks of AI code generation (like overuse of comments, removing typing and implementing try/catch blocks where it makes no sense). I also tasked it with being very specific to not refactor the code of the question. It makes me wonder how GEPA algorithms could be employed on these types of agents and if it could actually be making a difference. </paragraph><paragraph>Prompt:</paragraph><paragraph>take a look at @q_coding_ssm_forward_cpu.ipynb There is a number of TODOs in the document. I want you to solve all of them being careful not to refactor any code or including code slop. Code slop includes changing types to \"any\" to resolve typing issues or making wierd comments that humans woudnt make or inserting try/catch blocks in places where it is not normal like places where the known data is good and there is no reason for doing so. Do not change any code that is not in between the TODO block and end Oof your code block. also answer the text based questions. <break/><break/>Enire CoT in the .pdf and the model generated files and result in the .md</paragraph><file url=\"https://static.us.edusercontent.com/files/wrJfxCNjVAH6Tdiyf9QwQoDZ\" filename=\"q_coding_ssm_forward_cpu.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/s1Cse30wbA8maaiUf8T4awov\" filename=\"cursor_resolve_notebook_todos_and_quest.md\"/><file url=\"https://static.us.edusercontent.com/files/GghZ7lONQ8Uzio54hEPEk2J2\" filename=\"Special Participation B_ Using Cursor agent with Gemini 3 pro on HW8.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T12:00:40.363607+11:00",
            "category": "Admin"
        },
        {
            "guid": 7417181,
            "author": "Gustavo Jose Ortiz Zepeda",
            "project_title": "Special Participation E: Using Gemini to review the concepts before and after studying",
            "post_body": "For this special participation I'll be sharing a way I'm studying for the final which consists on doing summaries, cheat sheet and quizzes before and after reviewing the topic so I can be sure I don't forget something, and complement my notes easier. I think the problems are great as conceptual question.\n\nThis is the prompt I used to achieve this behavior:\n\nYou are \"TutorBot,\" an empathetic, clear, and structured academic tutor.\n\nObjective: Your goal is to prepare the student for an exam or deeper learning by synthesizing their Class Notes and Homework Files. You do not teach new, unrelated material; you reinforce what is in the provided files.\n\nProcess: When the user uploads files, analyze them immediately and output a response strictly following this structure:\n\nConcept Map\n\nScan the documents to identify the core topics.\n\nList all key concepts found in the notes.\n\nProvide a 2-sentence plain-English definition for each.\n\n2. Cheat sheet\n\nExtract every mathematical formula or significant rule mentioned. Do not skip one.\n\nLabel what each variable in the formula represents.\n\nNote: If multiple variations exist in the notes, show the most general form.\n\nVery Small, Safe Examples (VSSE)\n\nCreate 1-2 \"Toy Examples\" per concept based on the extracted formulas.\n\nRules for VSSEs: * Use integer numbers (e.g., 2, 5, 10) to make the math trivial. * Avoid decimals, fractions, or complex arithmetic. * Show the step-by-step substitution logic. * Goal: Prove how the formula works mechanically without testing the student's arithmetic skills.\n\nReadiness Check\n\nAsk 20 multiple-choice or short-answer questions but one at a time based strictly on the concepts above (start from easy and then go further).\n\nThese should be \"confidence builders\" to ensure the student understands the basic definitions or variable placements.\n\nStop and wait for the student to answer before proceeding to more complex topics. Always be prepared to review topics or explain again concepts.\n\nTone: Encouraging, patient, and precise.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation I'll be sharing a way I'm studying for the final which consists on doing summaries, cheat sheet and quizzes before and after reviewing the topic so I can be sure I don't forget something, and complement my notes easier. I think the problems are great as conceptual question.</paragraph><file url=\"https://static.us.edusercontent.com/files/3pK5A1sq17Q5iqfvjBbqRqI0\" filename=\"Special Participation E.pdf\"/><paragraph>This is the prompt I used to achieve this behavior:</paragraph><paragraph>You are \"TutorBot,\" an empathetic, clear, and structured academic tutor.</paragraph><paragraph>Objective: Your goal is to prepare the student for an exam or deeper learning by synthesizing their Class Notes and Homework Files. You do not teach new, unrelated material; you reinforce what is in the provided files.</paragraph><paragraph>Process: When the user uploads files, analyze them immediately and output a response strictly following this structure:</paragraph><list style=\"ordered\"><list-item><paragraph>Concept Map</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>Scan the documents to identify the core topics.</paragraph></list-item><list-item><paragraph>List all key concepts found in the notes.</paragraph></list-item><list-item><paragraph>Provide a 2-sentence plain-English definition for each.</paragraph></list-item></list><paragraph>2. Cheat sheet</paragraph><list style=\"unordered\"><list-item><paragraph>Extract every mathematical formula or significant rule mentioned. Do not skip one.</paragraph></list-item><list-item><paragraph>Label what each variable in the formula represents.</paragraph></list-item><list-item><paragraph><italic>Note: If multiple variations exist in the notes, show the most general form.</italic></paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Very Small, Safe Examples (VSSE)</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>Create 1-2 \"Toy Examples\" per concept based on the extracted formulas.</paragraph></list-item><list-item><paragraph><bold>Rules for VSSEs:</bold> * Use integer numbers (e.g., 2, 5, 10) to make the math trivial. * Avoid decimals, fractions, or complex arithmetic. * Show the step-by-step substitution logic. * <italic>Goal:</italic> Prove how the formula works mechanically without testing the student's arithmetic skills.</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Readiness Check</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>Ask 20 multiple-choice or short-answer questions but one at a time based strictly on the concepts above (start from easy and then go further).</paragraph></list-item><list-item><paragraph>These should be \"confidence builders\" to ensure the student understands the basic definitions or variable placements.</paragraph></list-item><list-item><paragraph><bold>Stop and wait</bold> for the student to answer before proceeding to more complex topics. Always be prepared to review topics or explain again concepts.</paragraph></list-item></list><paragraph>Tone: Encouraging, patient, and precise.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T11:23:00.9171+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7416810,
            "author": "Ruihan Xia",
            "project_title": "Special Participation E: themtic learning with GPT",
            "post_body": "I explored how well ChatGPT connects concepts from lecture notes to textbook, particularly on the theme of optimization. I asked it to break the material into clear modules, give small toy examples for each idea, compare the lectures with the textbook, and finish with one big geometric diagram of how optimization works.\n\nInitially ChatGPT only gives high-level, qualitative ideas that are not helpful to learn the concepts concretely. After requesting it to show math models & derivation for each idea, the illustration became much clearer. Overall, this tool worked well as a pre-lecture guide / post-lecture review resource. It also pointed out stylistically how lecture notes and textbook differs in explaining the same concepts, e.g. geometric intuition vs math derivation. In the end I challenged it to generate some high level principles / mindmap for all the materials, but it appears to be more of a repetition of previous responses. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I explored how well ChatGPT connects concepts from lecture notes to textbook, particularly on the theme of optimization. I asked it to break the material into clear modules, give small toy examples for each idea, compare the lectures with the textbook, and finish with one big geometric diagram of how optimization works.</paragraph><paragraph>Initially ChatGPT only gives high-level, qualitative ideas that are not helpful to learn the concepts concretely. After requesting it to show math models &amp; derivation for each idea, the illustration became much clearer. Overall, this tool worked well as a pre-lecture guide / post-lecture review resource. It also pointed out stylistically how lecture notes and textbook differs in explaining the same concepts, e.g. geometric intuition vs math derivation. In the end I challenged it to generate some high level principles / mindmap for all the materials, but it appears to be more of a repetition of previous responses. </paragraph><file url=\"https://static.us.edusercontent.com/files/5VObgnKhvKIIadnA0LiiYteS\" filename=\"Theme learning - GPT.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T10:30:50.376843+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7416689,
            "author": "Grant Yang",
            "project_title": "Special Participation A: Gemini 3 Pro Thinking on HW 6 Non-Coding",
            "post_body": "Using Gemini 3 Pro Thinking on HW 6 non-coding questions, I was able to observe the following results.\n\nSummary: \n\nGemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures. When it made a mistake, it was usually able to reason about its process to identify/clarify the error within a couple of turns. Overall, this is extremely impressive.\n\nRecap:\n\nI started off by giving it a prefacing prompt: \u201cI will give you problem set questions about deep learning. Think carefully about each question and answer each subpart with a detailed explanation.\u201d I hoped that this would encourage better results and avoid shallow answers.\n\nAs a baseline, I fed the entire problem (Q2) into the prompt and attached the relevant figures. I was expecting that given the length and density of the prompt, it would struggle to answer correctly, but surprisingly it was able to one-shot most of the subparts correctly, only being slightly wrong on subpart (c). This subpart took a couple rounds of clarifications and hints to correct. Even on more subjective questions, like suggesting possible strategies/interpretations, it gave answers that were either similar to the provided solutions or still technically correct.\n\nFor Q3, I tried a similar approach by feeding the entire question into the prompt. I again included the image of the graph as the input, without any external help (such as listing the edges or clarifying the table). I expected that this could produce some challenges if Gemini were to be unable to interpret the graph structure correctly. However, after looking at its \u201cthinking\u201d and its reasoning before answering, it actually mostly interpreted both the table values and graph structure from the image, and successfully one-shotted all subparts except for the last one due to a visual error. I was able to correct its error in the last subpart by telling it to think about its answer again, highlighting the node inside the tanh, and it spotted its own bug and returned the correct answer.\n\nConversation link: https://gemini.google.com/share/f77cf2401b50 \n\nAnnotated log: https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Using Gemini 3 Pro Thinking on HW 6 non-coding questions, I was able to observe the following results.</bold></paragraph><paragraph><bold>Summary:</bold> </paragraph><paragraph>Gemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures. When it made a mistake, it was usually able to reason about its process to identify/clarify the error within a couple of turns. Overall, this is extremely impressive.</paragraph><paragraph><bold>Recap:</bold></paragraph><paragraph>I started off by giving it a prefacing prompt: \u201cI will give you problem set questions about deep learning. Think carefully about each question and answer each subpart with a detailed explanation.\u201d I hoped that this would encourage better results and avoid shallow answers.</paragraph><paragraph>As a baseline, I fed the entire problem (Q2) into the prompt and attached the relevant figures. I was expecting that given the length and density of the prompt, it would struggle to answer correctly, but surprisingly it was able to one-shot most of the subparts correctly, only being slightly wrong on subpart (c). This subpart took a couple rounds of clarifications and hints to correct. Even on more subjective questions, like suggesting possible strategies/interpretations, it gave answers that were either similar to the provided solutions or still technically correct.</paragraph><paragraph>For Q3, I tried a similar approach by feeding the entire question into the prompt. I again included the image of the graph as the input, without any external help (such as listing the edges or clarifying the table). I expected that this could produce some challenges if Gemini were to be unable to interpret the graph structure correctly. However, after looking at its \u201cthinking\u201d and its reasoning before answering, it actually mostly interpreted both the table values and graph structure from the image, and successfully one-shotted all subparts except for the last one due to a visual error. I was able to correct its error in the last subpart by telling it to think about its answer again, highlighting the node inside the tanh, and it spotted its own bug and returned the correct answer.</paragraph><paragraph>Conversation link: <link href=\"https://gemini.google.com/share/f77cf2401b50\"><underline>https://gemini.google.com/share/f77cf2401b50</underline></link> </paragraph><paragraph>Annotated log: <link href=\"https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing\"><underline>https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing</underline></link> </paragraph></document>",
            "links": [
                "https://gemini.google.com/share/f77cf2401b50",
                "https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-06T10:10:20.504689+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7416375,
            "author": "Dagny Streit",
            "project_title": "Special Participation B: ChatGPT on HW 9",
            "post_body": "I used ChatGPT 5.1 (Auto) to solve the coding question of Homework 9 (Question 5). This problem required the model to interpret and reason about attention visualization plots rather than generate code. Overall, the model produced accurate and detailed explanations, but it also displayed a recurring tendency to misinterpret / modify questions. Below are the primary strengths and weaknesses I observed during the interaction.\n\nStrengths:\n\nThe model consistently produced correct and well-justified interpretations of the attention patterns on the first try\n\nWhen explaining its reasoning, the model frequently referenced particular examples (tokens, layers, heads, etc.) from the visualization\n\nWhen given many attention plots to choose from, the model reasonably identified which visualizations were most relevant for it to solve each subpart (e.g., 5c) and was able to interpret them both individually and collectively\n\nWhen I restated the question verbatim, the model reoriented itself quickly\n\nWeaknesses:\n\nThe most frequent issue was the model answering a slightly modified version of the question (e.g. 5b). It often restated the prompt incorrectly. I\u2019m not certain if this was influenced by the many attached PDFs and screenshots with inconsistent question labeling. However, this resulted in \u201challucinations\u201d of the question it was supposed to answer. The way the model rewrote the questions tended to steer towards the answer it planned it give.\n\nEven when correct, the model\u2019s response often included extensive pattern descriptions and additional commentary that went beyond the key points needed.\n\nAttached is my annotated log of the ChatGPT interaction (it is split into two parts to fit in the Ed post). The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect or needed to be reoriented.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT 5.1 (Auto) to solve the coding question of Homework 9 (Question 5). This problem required the model to interpret and reason about attention visualization plots rather than generate code. Overall, the model produced accurate and detailed explanations, but it also displayed a recurring tendency to misinterpret / modify questions. Below are the primary strengths and weaknesses I observed during the interaction.</paragraph><paragraph>Strengths:</paragraph><list style=\"unordered\"><list-item><paragraph>The model consistently produced correct and well-justified interpretations of the attention patterns on the first try</paragraph></list-item><list-item><paragraph>When explaining its reasoning, the model frequently referenced particular examples (tokens, layers, heads, etc.) from the visualization</paragraph></list-item><list-item><paragraph>When given many attention plots to choose from, the model reasonably identified which visualizations were most relevant for it to solve each subpart (e.g., 5c) and was able to interpret them both individually and collectively</paragraph></list-item><list-item><paragraph>When I restated the question verbatim, the model reoriented itself quickly</paragraph></list-item></list><paragraph>Weaknesses:</paragraph><list style=\"unordered\"><list-item><paragraph>The most frequent issue was the model answering a slightly modified version of the question (e.g. 5b). It often restated the prompt incorrectly. I\u2019m not certain if this was influenced by the many attached PDFs and screenshots with inconsistent question labeling. However, this resulted in \u201challucinations\u201d of the question it was supposed to answer. The way the model rewrote the questions tended to steer towards the answer it planned it give.</paragraph></list-item><list-item><paragraph>Even when correct, the model\u2019s response often included extensive pattern descriptions and additional commentary that went beyond the key points needed.</paragraph></list-item></list><paragraph>Attached is my annotated log of the ChatGPT interaction (it is split into two parts to fit in the Ed post). The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect or needed to be reoriented.</paragraph><file url=\"https://static.us.edusercontent.com/files/2ixRAjecQD9JTBPY8u8Z0OaH\" filename=\"Participation B Annotated Part 1.pdf\"/><file url=\"https://static.us.edusercontent.com/files/Gm8NsQuuIXrIdgVbojJjlXkA\" filename=\"Participation B Annotated Part 2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T09:28:00.390217+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7416340,
            "author": "Imra Dawoodani",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 5",
            "post_body": "I evaluated Gemini Pro on the coding portions of Homework 5, covering dropout theory (Question 5) and batch normalization, dropout, and convolutional network implementations (Question 6). Gemini Pro on coding had one shot success on 27 out of 28 questions.\n\nI provided Gemini with questions sequentially with supporting file code like layers.py and fc_net.py pasted in the chat when the following question in the notebook mentioned it. The initial prompt included a summary of what to expect across both questions, which appeared to help Gemini maintain context though it occasionally peeked ahead, solving more than requested or referencing upcoming concepts prematurely.\n\nGemini produced functionally correct code for all 28 tasks. Of these, 18 were almost identical to official solutions and 10 used different conventions (cache formats, mask types, loop structures) but were equivalent. What stood out was that Gemini maintained perfect internal consistency even when its conventions differed from official solutions, making sure that its code would still run with the existing notebook structure. This suggests strong understanding rather than just pattern matching.\n\nFor the question that it overconfidently claimed a wrong insight for (q5d), a mild steering prompt produced immediate, clean correction without defensiveness (unlike what I saw when doing special participation A with Gemini 2.5 Flash - comparing since this was also a sort of conceptual mistake)\n\nSome behavioral patterns I noticed:\n\nGemini followed the consistent 4 part template I requested in the 1st prompt (Reasoning, Code, Key Insights, Expected Output). It would often provide extra code for future parts it anticipated but never missed the 4 parts I requested. This followed through the entire 115 page conversation we had. Responses were verbose but well organized.\n\nExplanations used memorable analogies like \u201cstrong lever,\" \"barcode scanner,\" \"lazy vs robust network\" that would genuinely help me understand concepts.\n\nThe single error was overconfidence on a subtle theoretical point, not a hallucination.\n\nGood cross referencing throughout. Connected spatial batchnorm to earlier vanilla implementation, referenced Q5 dropout theory when analyzing Q6 experiments. Did not exhibit Flash's pattern of degrading recall over long conversations.\n\nConsistently acknowledged that the user must run the code but sis not explicitly mention that Gemini itself was just giving predicted numbers and couldn\u2019t cross verify by running the code itself.\n\nGemini's code was consistently more readable but less compact with explicit variable names, arguments in_channels=3 rather than positional ones, and 4 nested loops rather than the partial vectorization. These differences never affected correctness.\n\nFor the open ended network design task, Gemini chose a reasonable approach given the constraints. It\u2019s solution was less complicated than mine but did just enough in the simplest way possible to hit above the threshold.\n\nFor homework assistance Gemini Pro is highly reliable for implementations and mathematical derivations. Code should still be executed to verify numerical outputs. Providing upfront context helps maintain coherence, but may cause the model to jump ahead. For learning, Gemini's explanations have genuine value. Style differences from official solutions illustrate that multiple valid approaches exist. Comparing Gemini's conventions to official solutions can deepen understanding.\n\nHere's an annotated log of my interaction:\n\nhttps://drive.google.com/file/d/1U2n0kfRyjmVLSagRGwaWLRKKbkw393YS/view?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>I evaluated Gemini Pro on the coding portions of Homework 5, covering dropout theory (Question 5) and batch normalization, dropout, and convolutional network implementations (Question 6). Gemini Pro on coding had one shot success on 27 out of 28 questions.</paragraph><paragraph>I provided Gemini with questions sequentially with supporting file code like layers.py and fc_net.py pasted in the chat when the following question in the notebook mentioned it. The initial prompt included a summary of what to expect across both questions, which appeared to help Gemini maintain context though it occasionally peeked ahead, solving more than requested or referencing upcoming concepts prematurely.</paragraph><paragraph>Gemini produced functionally correct code for all 28 tasks. Of these, 18 were almost identical to official solutions and 10 used different conventions (cache formats, mask types, loop structures) but were equivalent. What stood out was that Gemini maintained perfect internal consistency even when its conventions differed from official solutions, making sure that its code would still run with the existing notebook structure. This suggests strong understanding rather than just pattern matching.</paragraph><paragraph>For the question that it overconfidently claimed a wrong insight for (q5d), a mild steering prompt produced immediate, clean correction without defensiveness (unlike what I saw when doing special participation A with Gemini 2.5 Flash - comparing since this was also a sort of conceptual mistake)</paragraph><paragraph>Some behavioral patterns I noticed:</paragraph><list style=\"ordered\"><list-item><paragraph>Gemini followed the consistent 4 part template I requested in the 1st prompt (Reasoning, Code, Key Insights, Expected Output). It would often provide extra code for future parts it anticipated but never missed the 4 parts I requested. This followed through the entire 115 page conversation we had. Responses were verbose but well organized.</paragraph></list-item><list-item><paragraph>Explanations used memorable analogies like \u201cstrong lever,\" \"barcode scanner,\" \"lazy vs robust network\" that would genuinely help me understand concepts.</paragraph></list-item><list-item><paragraph>The single error was overconfidence on a subtle theoretical point, not a hallucination.</paragraph></list-item><list-item><paragraph>Good cross referencing throughout. Connected spatial batchnorm to earlier vanilla implementation, referenced Q5 dropout theory when analyzing Q6 experiments. Did not exhibit Flash's pattern of degrading recall over long conversations.</paragraph></list-item><list-item><paragraph>Consistently acknowledged that the user must run the code but sis not explicitly mention that Gemini itself was just giving predicted numbers and couldn\u2019t cross verify by running the code itself.</paragraph></list-item></list><paragraph>Gemini's code was consistently more readable but less compact with explicit variable names, arguments in_channels=3 rather than positional ones, and 4 nested loops rather than the partial vectorization. These differences never affected correctness.</paragraph><paragraph>For the open ended network design task, Gemini chose a reasonable approach given the constraints. It\u2019s solution was less complicated than mine but did just enough in the simplest way possible to hit above the threshold.</paragraph><paragraph>For homework assistance Gemini Pro is highly reliable for implementations and mathematical derivations. Code should still be executed to verify numerical outputs. Providing upfront context helps maintain coherence, but may cause the model to jump ahead. For learning, Gemini's explanations have genuine value. Style differences from official solutions illustrate that multiple valid approaches exist. Comparing Gemini's conventions to official solutions can deepen understanding.</paragraph><paragraph>Here's an annotated log of my interaction:<break/><break/><link href=\"https://drive.google.com/file/d/1U2n0kfRyjmVLSagRGwaWLRKKbkw393YS/view?usp=sharing\">https://drive.google.com/file/d/1U2n0kfRyjmVLSagRGwaWLRKKbkw393YS/view?usp=sharing</link> </paragraph><file/></document>",
            "links": [
                "https://drive.google.com/file/d/1U2n0kfRyjmVLSagRGwaWLRKKbkw393YS/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-06T09:24:08.948487+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7416258,
            "author": "Fantine Mpacko Priso",
            "project_title": "Special participation D : Adding Lion to HW06",
            "post_body": "I implemented Lion on HW06, after adding Manifold MuOn too to have more methods to compare. Questions associated to this new section are: \n\nQuestion 10 : Briefly explain how Lion's update is different from AdamW in terms of:\n\nnumber of state tensors per parameter\n\nuse (or non-use) of the gradient magnitude.\n\nQuestion 11 : Uncomment the Lion contribution to the optimizer dictionnary. Compare Lion and AdamW:\n\nWhich optimizer reaches lower training loss after 5 epochs?\n\nWhich optimizer achieves higher test accuracy?\n\nDoes Lion seem to converge faster early in training, slower, or about the same?\n\n\n\nThe resulting plot is : \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I implemented Lion on HW06, after adding Manifold MuOn too to have more methods to compare. Questions associated to this new section are: </paragraph><paragraph><bold>Question 10 :</bold> Briefly explain how Lion's update is different from AdamW in terms of:</paragraph><list style=\"unordered\"><list-item><paragraph>number of state tensors per parameter</paragraph></list-item><list-item><paragraph>use (or non-use) of the gradient magnitude.</paragraph></list-item></list><paragraph><bold>Question 11</bold> : Uncomment the Lion contribution to the optimizer dictionnary. Compare Lion and AdamW:</paragraph><list style=\"unordered\"><list-item><paragraph>Which optimizer reaches lower training loss after 5 epochs?</paragraph></list-item><list-item><paragraph>Which optimizer achieves higher test accuracy?</paragraph></list-item><list-item><paragraph>Does Lion seem to converge faster early in training, slower, or about the same?</paragraph></list-item></list><paragraph/><file url=\"https://static.us.edusercontent.com/files/aaKZytCv8mbmfO8SnDsws0H7\" filename=\"Fantine_q_coding_muon_solutions_ManifoldMuOn+Lion.ipynb\"/><paragraph>The resulting plot is : </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/294ghAgzbBNHFO2nLObhJraU\" width=\"658\" height=\"325.67676767676767\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T09:14:09.027368+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7415618,
            "author": "William Li",
            "project_title": "Special Participation A: Kimi K2 on HW9",
            "post_body": "I utilized Moonshot AI\u2019s Kimi K2 model to tackle the non-coding problems of Homework 9. I first did a pass through of all the non-coding questions, sending each question individually to Kimi with no further prompting to see how the model would perform under one-shot conditions. For most of the questions, Kimi performed very well with just me sending the question to the model, and was able to get it all mostly correct on the first attempt except for the last question (question 6) which was much more difficult and longer.\n\nHere are the per-question results on the first run through (which is one-shot only):\n\nQuestion 1: This question was a relatively simple manipulation of expectations and variances, which Kimi was easily able to solve and give justifications for.\n\nQuestion 2: This question involved simple arithmetic (taking dot products of some vectors), which Kimi was easily able to do. This question also involved a conceptual question, which was also correctly answered by Kimi.\n\nQuestion 3: This question involved a simple fill in the blank coding question, which the model was able to get all correct on the first try, along with a justification for each answer. The second part of this question was also answered correctly, with the model coming up with the right changes to the code necessary for making the adjustment the question asked for.\n\nQuestion 4: This question was another mostly fill in the blank blank coding question, which the model again was able to answer correctly on the first try. The question also included two complexity calculations, of which the model was able to successfully answer the time complexity fully. The memory complexity, the model was able to get an answer that was very close to the given solutions, and up on further examination, the answer given by Kimi seems to be the same runtime, just using slightly different notation.\n\nQuestion 6: The question got some of the parts right, but had some minor differences in other parts. For these I asked the model to explain why it had answered part b(iii) the way it did, and asked it to clarify an assumption it made when calculating the time and memory complexities.\n\nI was overall very surprised and satisfied with how Kimi was able to solve the noncoding questions. It was able to one-shot the vast majority of the questions, and with simple prompting about its motivations on the incorrect questions, it was able to generate the correct solutions.\n\nTrace : https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139\nAnnotated Trace: ",
            "content_xml": "<document version=\"2.0\"><paragraph>I utilized Moonshot AI\u2019s Kimi K2 model to tackle the non-coding problems of Homework 9. I first did a pass through of all the non-coding questions, sending each question individually to Kimi with no further prompting to see how the model would perform under one-shot conditions. For most of the questions, Kimi performed very well with just me sending the question to the model, and was able to get it all mostly correct on the first attempt except for the last question (question 6) which was much more difficult and longer.</paragraph><paragraph>Here are the per-question results on the first run through (which is one-shot only):</paragraph><paragraph>Question 1: This question was a relatively simple manipulation of expectations and variances, which Kimi was easily able to solve and give justifications for.</paragraph><paragraph>Question 2: This question involved simple arithmetic (taking dot products of some vectors), which Kimi was easily able to do. This question also involved a conceptual question, which was also correctly answered by Kimi.</paragraph><paragraph>Question 3: This question involved a simple fill in the blank coding question, which the model was able to get all correct on the first try, along with a justification for each answer. The second part of this question was also answered correctly, with the model coming up with the right changes to the code necessary for making the adjustment the question asked for.</paragraph><paragraph>Question 4: This question was another mostly fill in the blank blank coding question, which the model again was able to answer correctly on the first try. The question also included two complexity calculations, of which the model was able to successfully answer the time complexity fully. The memory complexity, the model was able to get an answer that was very close to the given solutions, and up on further examination, the answer given by Kimi seems to be the same runtime, just using slightly different notation.</paragraph><paragraph>Question 6: The question got some of the parts right, but had some minor differences in other parts. For these I asked the model to explain why it had answered part b(iii) the way it did, and asked it to clarify an assumption it made when calculating the time and memory complexities.</paragraph><paragraph>I was overall very surprised and satisfied with how Kimi was able to solve the noncoding questions. It was able to one-shot the vast majority of the questions, and with simple prompting about its motivations on the incorrect questions, it was able to generate the correct solutions.</paragraph><paragraph>Trace : <link href=\"https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139\">https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139</link><break/>Annotated Trace: </paragraph><file url=\"https://static.us.edusercontent.com/files/286tohOcXysBypCFASOMEruH\" filename=\"Kimi 2025-12-04 _3_ _1_.pdf\"/></document>",
            "links": [
                "https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139"
            ],
            "attachments": [],
            "created_at": "2025-12-06T07:56:24.953792+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7415561,
            "author": "William Li",
            "project_title": "Special Participation E: In-Progress Concept Helper for CS182 Learning",
            "post_body": "In this special participation, I explored how we can use AI tools to give us guided hints, rather than directly giving us the answers to a problem. This will help support more active reasoning and learning while we are still actively forming our understanding of class material. \n\nHere is the prompt that I started out with:\nROLE: You are an AI learning coach for CS182 concepts (deep learning + society).\n\nWhen I provide:\n- A question about course material\n- My current in-progress reasoning\n- Optionally, a note about what I am confused about\nYou must:\n\n- NOT provide a final answer\n\n- Ask up to 3 guiding questions that help me think\n\n- Highlight unclear or incorrect reasoning with gentle hints\n\n- Encourage me to revise my understanding first\n\n- Only summarize or explain fully *after* I try again\n\nMaintain correctness, avoid hallucination, and support metacognition.\n\n\nI tried this out with questions from homework 9 with a solution partly filled in (although any questions from any source would do). \n\nFirst, I tried out the prompt for HW9 Q1, which is a relatively simple calculation question, and for the most part the model was able to use the images I sent of my work to discern where I was in the problem. I gave it a partial solution, and the model gave a very big hint as to how to finish the part I was working on. Additionally, I intentionally made a small conceptual error relating, and the model was able to catch that and give me guiding questions to realize the mistake I had made. This was a very simple question though, so I chose to continue using a more complex question.\n\nI then tried a more computationally complicated question from homework 4, and it was also able to help a lot in deriving the answer. When my answer wasn\u2019t in the form of the official solutions exactly, I prompted the model saying that maybe my answer wasn\u2019t exactly right or is too complicated looking, and the model made the pattern recognition needed to simplify my answer to the official solution\u2019s answer. \n\nOverall, this experiment showed that AI can be genuinely helpful when framed as a guided learning partner rather than a solution engine. It was especially useful at catching intermediate conceptual mistakes and nudging me toward the right direction without revealing the final answers immediately. While sometimes the model was over-eager in giving hints, I think that it is still very much a useful tool to help solve problems and study. Some suggestions to using this is to make sure that when you send your written work to the model, try to keep it in the proper orientation, and not have the text be too small, especially if handwriting isn\u2019t the best. \n\nTrace: https://chatgpt.com/share/69348099-8d98-8001-b717-d316a6897a8b\n\nAnnotated Trace: ",
            "content_xml": "<document version=\"2.0\"><paragraph>In this special participation, I explored how we can use AI tools to give us guided hints, rather than directly giving us the answers to a problem. This will help support more active reasoning and learning while we are still actively forming our understanding of class material. </paragraph><pre>Here is the prompt that I started out with:\nROLE: You are an AI learning coach for CS182 concepts (deep learning + society).\n\nWhen I provide:\n- A question about course material\n- My current in-progress reasoning\n- Optionally, a note about what I am confused about\nYou must:\n\n- NOT provide a final answer\n\n- Ask up to 3 guiding questions that help me think\n\n- Highlight unclear or incorrect reasoning with gentle hints\n\n- Encourage me to revise my understanding first\n\n- Only summarize or explain fully *after* I try again\n\nMaintain correctness, avoid hallucination, and support metacognition.\n</pre><paragraph>I tried this out with questions from homework 9 with a solution partly filled in (although any questions from any source would do). </paragraph><paragraph>First, I tried out the prompt for HW9 Q1, which is a relatively simple calculation question, and for the most part the model was able to use the images I sent of my work to discern where I was in the problem. I gave it a partial solution, and the model gave a very big hint as to how to finish the part I was working on. Additionally, I intentionally made a small conceptual error relating, and the model was able to catch that and give me guiding questions to realize the mistake I had made. This was a very simple question though, so I chose to continue using a more complex question.</paragraph><paragraph>I then tried a more computationally complicated question from homework 4, and it was also able to help a lot in deriving the answer. When my answer wasn\u2019t in the form of the official solutions exactly, I prompted the model saying that maybe my answer wasn\u2019t exactly right or is too complicated looking, and the model made the pattern recognition needed to simplify my answer to the official solution\u2019s answer. </paragraph><paragraph>Overall, this experiment showed that AI can be genuinely helpful when framed as a guided learning partner rather than a solution engine. It was especially useful at catching intermediate conceptual mistakes and nudging me toward the right direction without revealing the final answers immediately. While sometimes the model was over-eager in giving hints, I think that it is still very much a useful tool to help solve problems and study. Some suggestions to using this is to make sure that when you send your written work to the model, try to keep it in the proper orientation, and not have the text be too small, especially if handwriting isn\u2019t the best. </paragraph><paragraph>Trace: <link href=\"https://chatgpt.com/share/69348099-8d98-8001-b717-d316a6897a8b\"><underline>https://chatgpt.com/share/69348099-8d98-8001-b717-d316a6897a8b</underline></link></paragraph><paragraph>Annotated Trace: </paragraph><file url=\"https://static.us.edusercontent.com/files/X6djc3jMHxxwUMhZvv6LyObg\" filename=\"CS182 learning coach (1).pdf\"/></document>",
            "links": [
                "https://chatgpt.com/share/69348099-8d98-8001-b717-d316a6897a8b"
            ],
            "attachments": [],
            "created_at": "2025-12-06T07:49:57.616116+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7415308,
            "author": "Siddharth Shashi",
            "project_title": "Special Participation E: ChatGPT Study Mode for Transformer and VAE understanding",
            "post_body": "Working through HW12 this week, I was struggling to remember specifics about transformer weight initializations and VAEs, so I wanted to brush up on concepts before solving the problems. I found myself I used ChatGPT's study mode for the first time, and I started out by asking it general questions about how things are supposed to work in these models. After asking me about my prior understandings of things, it was able to give me relevant, helpful pointers and ask me questions along the way to ensure I was retaining the information. I then asked it to give me follow-up questions to quiz me on these concepts and it was able to not only give me questions but also nudge me towards correct answers without revealing them in the case that I got questions wrong. Overall this was super useful, and I'll definitely use study mode in the future. \n\n\n\nLink to conversation: https://chatgpt.com/share/6933394a-efbc-800a-b492-f90d6676a4dd \n\nPDF of same conversation: ",
            "content_xml": "<document version=\"2.0\"><paragraph>Working through HW12 this week, I was struggling to remember specifics about transformer weight initializations and VAEs, so I wanted to brush up on concepts before solving the problems. I found myself I used ChatGPT's study mode for the first time, and I started out by asking it general questions about how things are supposed to work in these models. After asking me about my prior understandings of things, it was able to give me relevant, helpful pointers and ask me questions along the way to ensure I was retaining the information. I then asked it to give me follow-up questions to quiz me on these concepts and it was able to not only give me questions but also nudge me towards correct answers without revealing them in the case that I got questions wrong. Overall this was super useful, and I'll definitely use study mode in the future. </paragraph><paragraph/><paragraph>Link to conversation: <link href=\"https://chatgpt.com/share/6933394a-efbc-800a-b492-f90d6676a4dd\">https://chatgpt.com/share/6933394a-efbc-800a-b492-f90d6676a4dd</link> </paragraph><paragraph>PDF of same conversation: </paragraph><file url=\"https://static.us.edusercontent.com/files/pjKnDqW9B1pUzWfEV7qaUpio\" filename=\"Study Mode - Transformer model explanation.html.pdf\"/></document>",
            "links": [
                "https://chatgpt.com/share/6933394a-efbc-800a-b492-f90d6676a4dd"
            ],
            "attachments": [],
            "created_at": "2025-12-06T07:16:54.634934+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7415075,
            "author": "Alena Chao",
            "project_title": "Special Participation C: HW3 Coding",
            "post_body": "Natalie Wei and I refactored HW3 Question 2 on MuP with the goal of improving clarity, usability, and overall student experience. Our updates include adding comprehensive docstrings to the majority of functions, clearly describing their purpose, expected inputs, and return values. We also introduced consistent type hints throughout the notebook to make the code easier to read and reduce ambiguity for students working through the assignment. Finally, we revised and expanded the TODO sections by adding more explicit comments, clarifying the required steps, and providing gentle hints to help students get started without giving away the full solution.",
            "content_xml": "<document version=\"2.0\"><paragraph>Natalie Wei and I refactored HW3 Question 2 on MuP with the goal of improving clarity, usability, and overall student experience. Our updates include adding comprehensive docstrings to the majority of functions, clearly describing their purpose, expected inputs, and return values. We also introduced consistent type hints throughout the notebook to make the code easier to read and reduce ambiguity for students working through the assignment. Finally, we revised and expanded the TODO sections by adding more explicit comments, clarifying the required steps, and providing gentle hints to help students get started without giving away the full solution.</paragraph><file url=\"https://static.us.edusercontent.com/files/M3iTSB84GD5HySEclenKE5tn\" filename=\"q_mup_coding_refactored.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T06:44:31.168642+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7415017,
            "author": "Xuanlin Mao",
            "project_title": "Special Participation E: Using Gemini to Analyze Lecture Topics and S4-to-Mamba Model Lineage through Paper Comparison",
            "post_body": "For this special participation, I focused on using Gemini 3 to investigate the classical papers related to a lecture topic and to build a clearer conceptual map of the model family from S4 to Mamba. Starting from my lecture notes, I asked Gemini to identify the foundational papers and discovered several intermediate models that bridge S4 and Mamba.\n\nBased on its suggestions, I downloaded and uploaded four key papers\u2014S4, S4D, S5, and Mamba\u2014and constructed a detailed prompt asking Gemini to compare them along multiple dimensions: publication timeline, inheritance relationships, shared ideas, distinguishing innovations, strengths, use cases, models incorporating them, as well as each model\u2019s motivation, addressed problems, core formulas, and computational complexity.\n\nI then organized Gemini\u2019s output into a clean LaTeX summary for efficient reading and study.\n\nMy main contribution in this participation was providing a well-structured, robust prompt that enabled consistent, detailed, and technically accurate comparisons across the four models. This process showed how AI tools can support deeper literature understanding and accelerate technical synthesis for exam preparation or future research.\n\nTranscript and report:\n\nhttps://drive.google.com/file/d/1NTmOqwLrrQzzam7WrDkMxx9e6rS997VB/view?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I focused on using Gemini 3 to investigate the classical papers related to a lecture topic and to build a clearer conceptual map of the model family from S4 to Mamba. Starting from my lecture notes, I asked Gemini to identify the foundational papers and discovered several intermediate models that bridge S4 and Mamba.</paragraph><paragraph>Based on its suggestions, I downloaded and uploaded four key papers\u2014S4, S4D, S5, and Mamba\u2014and constructed a detailed prompt asking Gemini to compare them along multiple dimensions: publication timeline, inheritance relationships, shared ideas, distinguishing innovations, strengths, use cases, models incorporating them, as well as each model\u2019s motivation, addressed problems, core formulas, and computational complexity.</paragraph><paragraph>I then organized Gemini\u2019s output into a clean LaTeX summary for efficient reading and study.</paragraph><paragraph>My main contribution in this participation was providing a well-structured, robust prompt that enabled consistent, detailed, and technically accurate comparisons across the four models. This process showed how AI tools can support deeper literature understanding and accelerate technical synthesis for exam preparation or future research.</paragraph><paragraph>Transcript and report:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1NTmOqwLrrQzzam7WrDkMxx9e6rS997VB/view?usp=sharing\">https://drive.google.com/file/d/1NTmOqwLrrQzzam7WrDkMxx9e6rS997VB/view?usp=sharing</link> </paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1NTmOqwLrrQzzam7WrDkMxx9e6rS997VB/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-06T06:35:20.341026+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7414974,
            "author": "Alena Chao",
            "project_title": "Special Participation B: ChatGPT-5.1 on HW0 coding",
            "post_body": "I used ChatGPT to solve HW0 question 6. In general it performed very strong, being able to one-shot the questions. Common patterns also included adding comments for explanations and potential tricky parts of the code. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/94TrxdgeCMvhAy0hGRYaq00b\" filename=\"CS182_Special_Participation_B.pdf\"/><paragraph>I used ChatGPT to solve HW0 question 6. In general it performed very strong, being able to one-shot the questions. Common patterns also included adding comments for explanations and potential tricky parts of the code. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T06:29:42.388623+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7414931,
            "author": "Alena Chao",
            "project_title": "Special Participation A: ChatGPT-5.1 on HW0",
            "post_body": "I tested ChatGPT's ability to solve HW0 questions 2-5. In general, it was able to one-shot the problems while explaining its reasoning, most likely because many of the problems review fundamental ML/math concepts.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/fdFZgmz9daDV37SF1a0wX1V3\" filename=\"CS182_Special_Participation_A.pdf\"/><paragraph>I tested ChatGPT's ability to solve HW0 questions 2-5. In general, it was able to one-shot the problems while explaining its reasoning, most likely because many of the problems review fundamental ML/math concepts.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T06:24:29.195133+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7414836,
            "author": "Moxin Tang",
            "project_title": "Special Participation B: ChatGPT5.1 on hw12",
            "post_body": "I tried a small experiment to see how well GPT-5.1 can handle the coding parts of HW6. I gave it the actual homework files and asked it to scan for TODOs, explain each one, and then fill in the required code step by step. Surprisingly, it followed the instructions very strictly, didn\u2019t skip ahead, and the solutions it produced (like the logistic-loss updates for MAML and the reparameterization/ELBO code for the VAE) were all correct and clean. It also stayed inside the required code blocks and didn\u2019t hallucinate anything extra.\n\nI\u2019ve attached the full interaction and results below.\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I tried a small experiment to see how well GPT-5.1 can handle the coding parts of HW6. I gave it the actual homework files and asked it to scan for TODOs, explain each one, and then fill in the required code step by step. Surprisingly, it followed the instructions very strictly, didn\u2019t skip ahead, and the solutions it produced (like the logistic-loss updates for MAML and the reparameterization/ELBO code for the VAE) were all correct and clean. It also stayed inside the required code blocks and didn\u2019t hallucinate anything extra.</paragraph><paragraph><bold>I\u2019ve attached the full interaction and results below.</bold><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/KtVpMGp7GA4L8W0IhXS4mBjJ\" filename=\"hw12-q_maml.pdf\"/><file url=\"https://static.us.edusercontent.com/files/9Y7NLBFEjIVTQozbIwyM8NaD\" filename=\"hw12-q_vae.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-06T06:13:04.202334+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412939,
            "author": "Etaash Patel",
            "project_title": "Special Participation E: ChatGPT Study Mode For Creating Mind Maps",
            "post_body": "To prepare more effectively for the final, I wanted to strengthen my conceptual understanding, improve recall, and practice articulating answers in a more formal style that would earn more credit on the exam. I decided to try a tool I first encountered in a history class: mindmaps. I asked ChatGPT to help guide me through building one. The process went as follows:\n\nChatGPT would prompt me to form a connection between two topics.\n\nI would attempt to articulate the relationship.\n\nChatGPT would evaluate my response and provide feedback.\n\nI could ask follow-up questions or push back where I disagreed.\n\nWe repeated this over 10 rounds (about an hour).\n\nAt the end, ChatGPT compiled a consolidated list of connections and highlighted areas I should review.\n\nOverall, I found the process helpful. Although ChatGPT was occasionally incorrect or unclear, the conversational format made the exercise valuable even when these hallucinations took place.\n\nDisclaimer: I made many mistakes throughout this log\u2014particularly on diffusion. The value of this log is in illustrating how you might use ChatGPT to help build your own mindmap, rather than as a source of fully correct answers.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/4aPk5tBOoFqUpcl5c0b3cVCw\" filename=\"ChatGPT Study Mode - Mind map creation.pdf\"/><paragraph>To prepare more effectively for the final, I wanted to strengthen my conceptual understanding, improve recall, and practice articulating answers in a more formal style that would earn more credit on the exam. I decided to try a tool I first encountered in a history class: <bold>mindmaps</bold>. I asked ChatGPT to help guide me through building one. The process went as follows:</paragraph><list style=\"unordered\"><list-item><paragraph>ChatGPT would prompt me to form a connection between two topics.</paragraph></list-item><list-item><paragraph>I would attempt to articulate the relationship.</paragraph></list-item><list-item><paragraph>ChatGPT would evaluate my response and provide feedback.</paragraph></list-item><list-item><paragraph>I could ask follow-up questions or push back where I disagreed.</paragraph></list-item><list-item><paragraph>We repeated this over <bold>10 rounds (about an hour)</bold>.</paragraph></list-item><list-item><paragraph>At the end, ChatGPT compiled a consolidated list of connections and highlighted areas I should review.</paragraph></list-item></list><paragraph>Overall, I found the process helpful. Although ChatGPT was occasionally incorrect or unclear, the conversational format made the exercise valuable even when these hallucinations took place.</paragraph><paragraph><bold>Disclaimer:</bold> I made many mistakes throughout this log\u2014particularly on diffusion. The value of this log is in illustrating how you might use ChatGPT to help <italic>build your own</italic> mindmap, rather than as a source of fully correct answers.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T20:09:15.571245+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412925,
            "author": "Tamzid Razzaque",
            "project_title": "Special Participation E: Custom EECS182 GPT",
            "post_body": "I created a custom GPT to act as an EECS182 learning assistant and uploaded all of the homework solutions and discussion materials so it would have full context about the types of questions students encounter. In the instructions, I made it very clear that it should never output homework answers, and during testing it consistently respected that boundary. Even when I pushed it toward specific problems, it stayed at the level of concepts, reasoning steps, and intuition, showing that the guardrails were effective even though it had access to the underlying solutions.\n\nIn use, the GPT was most helpful when teaching high-level ideas. It gave clear explanations of optimizers, normalization layers, CNNs, ResNets, muP, transformers, and other topics across the course. It adapted well when I asked follow-up questions and diagnosed misconceptions by asking clarifying questions. It handled examples in a way that helped me build intuition without revealing anything that would compromise assignments.\n\nThe limitations showed up with solving new homework problems. It sometimes overstated certain claims, especially around muP and normalization layers, and occasionally inserted extra details that were not fully grounded in the course material. These moments required me to slow it down or ask it to justify its statements. The GPT responded well to that prompting and corrected itself once guided, which highlighted the importance of interacting critically rather than passively accepting its answers.\n\nOverall, the GPT works well as a study tool. It is strong at building intuition, explaining conceptual structures, and highlighting the relationships between architectures, optimizers, and training dynamics. It is weaker when asked for mathematically precise statements unless I guide it step by step. Even with that limitation, it provides a useful and interactive way to understand course topics while remaining safe with respect to homework solutions, thanks to the strict instruction boundaries and its consistent refusal to output any solution content. Engaging with it this way reinforced the course\u2019s idea that AI can help students learn how to fish rather than simply handing them the answers.\n\nHere is the link to the custom GPT: https://chatgpt.com/g/g-69324fd3e6348191a04e12d3bf78ceb8-eecs182-helper\n\nBelow is the trace with commentary at the end:\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I created a custom GPT to act as an EECS182 learning assistant and uploaded all of the homework solutions and discussion materials so it would have full context about the types of questions students encounter. In the instructions, I made it very clear that it should never output homework answers, and during testing it consistently respected that boundary. Even when I pushed it toward specific problems, it stayed at the level of concepts, reasoning steps, and intuition, showing that the guardrails were effective even though it had access to the underlying solutions.</paragraph><paragraph>In use, the GPT was most helpful when teaching high-level ideas. It gave clear explanations of optimizers, normalization layers, CNNs, ResNets, muP, transformers, and other topics across the course. It adapted well when I asked follow-up questions and diagnosed misconceptions by asking clarifying questions. It handled examples in a way that helped me build intuition without revealing anything that would compromise assignments.</paragraph><paragraph>The limitations showed up with solving new homework problems. It sometimes overstated certain claims, especially around muP and normalization layers, and occasionally inserted extra details that were not fully grounded in the course material. These moments required me to slow it down or ask it to justify its statements. The GPT responded well to that prompting and corrected itself once guided, which highlighted the importance of interacting critically rather than passively accepting its answers.</paragraph><paragraph>Overall, the GPT works well as a study tool. It is strong at building intuition, explaining conceptual structures, and highlighting the relationships between architectures, optimizers, and training dynamics. It is weaker when asked for mathematically precise statements unless I guide it step by step. Even with that limitation, it provides a useful and interactive way to understand course topics while remaining safe with respect to homework solutions, thanks to the strict instruction boundaries and its consistent refusal to output any solution content. Engaging with it this way reinforced the course\u2019s idea that AI can help students learn how to fish rather than simply handing them the answers.</paragraph><paragraph>Here is the link to the custom GPT: https://chatgpt.com/g/g-69324fd3e6348191a04e12d3bf78ceb8-eecs182-helper<break/><break/>Below is the trace with commentary at the end:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/7Xd0QwIUTw338N0B1pFoXkZX\" filename=\"trace_with_commentary.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T20:00:15.941066+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412832,
            "author": "Moxin Tang",
            "project_title": "Special Participation A: Kimi on HW6",
            "post_body": "Summary of Kimi Performance on HW6\n\nI tested Kimi AI\u2019s ability to solve problems from hw6 focusing on GNN architectures. \n\nOverall, Kimi demonstrated strong reasoning capability across most questions. It correctly handled:\n\nLinear algebraic interpretations of GNN message passing\n\nInductive proofs involving adjacency matrix powers\n\nIdentification of valid permutation-invariant update rules\n\nLoss computation using masked training nodes\n\nInterpretation of max aggregation\n\nAnalysis of computational scaling in GraphNet architectures\n\nFor these, Kimi\u2019s reasoning was correct and aligned with the solutions.\n\nWeaknesses\n\nTends to produce over-engineered answers when a simpler one is expected\n\nOccasionally misidentifies structural details (e.g., graph neighbors)\n\nDoes not always verify provided diagrams or datasets before proceeding\n\n\nThe following LaTeX document compiles and evaluates Kimi\u2019s responses from the linked chat session. \n\nClick the link to view conversation: https://www.kimi.com/share/19aed915-2932-8b91-8000-0000bdfd7ac0\n",
            "content_xml": "<document version=\"2.0\"><heading level=\"1\"><bold>Summary of Kimi Performance on HW6</bold></heading><paragraph>I tested Kimi AI\u2019s ability to solve problems from hw6 focusing on GNN architectures. </paragraph><paragraph>Overall, Kimi demonstrated <bold>strong reasoning capability</bold> across most questions. It correctly handled:</paragraph><list style=\"unordered\"><list-item><paragraph>Linear algebraic interpretations of GNN message passing</paragraph></list-item><list-item><paragraph>Inductive proofs involving adjacency matrix powers</paragraph></list-item><list-item><paragraph>Identification of valid permutation-invariant update rules</paragraph></list-item><list-item><paragraph>Loss computation using masked training nodes</paragraph></list-item><list-item><paragraph>Interpretation of max aggregation</paragraph></list-item><list-item><paragraph>Analysis of computational scaling in GraphNet architectures</paragraph></list-item></list><paragraph>For these, Kimi\u2019s reasoning was correct and aligned with the solutions.</paragraph><heading level=\"3\"><bold>Weaknesses</bold></heading><list style=\"unordered\"><list-item><paragraph>Tends to produce over-engineered answers when a simpler one is expected</paragraph></list-item><list-item><paragraph>Occasionally misidentifies structural details (e.g., graph neighbors)</paragraph></list-item><list-item><paragraph>Does not always verify provided diagrams or datasets before proceeding<break/></paragraph></list-item></list><paragraph>The following LaTeX document compiles and evaluates Kimi\u2019s responses from the linked chat session. </paragraph><file url=\"https://static.us.edusercontent.com/files/9E03ab9ZOiphxd6921xB7qEj\" filename=\"Kimi_hw6.pdf\"/><paragraph>Click the link to view conversation: https://www.kimi.com/share/19aed915-2932-8b91-8000-0000bdfd7ac0<break/></paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T19:12:30.361236+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412717,
            "author": "Nicolas Rault-Wang",
            "project_title": "Special Participation D: HW6 Exploration of the Polar Express (Muon Variant) & Lion Optimizers",
            "post_body": "I added four additional parts to the Implementing Muon question in homework 6, providing a guided exploration of two recent computationally-efficient optimizer alternatives to Muon and AdamW:\n\n Polar Express (Muon Variant) and\n\n Lion (EvoLved Sign Momentum)\n\nNew question summary:\n\n(Code) An introduction to Polar Express, a variant of Muon that replaces Newton-Schulz orthogonalization with a schedule of minimax-optimized polynomials. Since compute efficiency is a core feature of this variant, the question challenges you to find an efficient (minimal matmuls) PyTorch implementation for the update $$X_{k+1}\\leftarrow  \\alpha_k X_k+ \\beta_k X_k^3 + \\gamma_k X_k^5$$\n\n(Code) An intro to the Lion optimizer, a memory-efficient, non-adaptive optimizer that relies on the sign function of an interpolation of momentum and gradient tensors to determine the update direction.\n\n(Written) Empirical evaluation of Muon+PolarExpress, Muon+Newton-Schulz, Lion, AdamW, and SGD.\n\n(Optional) Hyperparameter tuning for Polar Express and Lion to ensure fair comparisons.\n\nI've included the blank and solution notebooks for you below. \n\nColab links:\n\nblank notebook\n\nsolution notebook\n\nNotebook files:\n\nLinks for the archive:\n\nPersonal website: https://nraultwang.github.io/, Github: https://github.com/nraultwang",
            "content_xml": "<document version=\"2.0\"><paragraph>I added four additional parts to the Implementing Muon question in homework 6, providing a guided exploration of two recent computationally-efficient optimizer alternatives to Muon and AdamW:</paragraph><list style=\"bullet\"><list-item><paragraph> <link href=\"https://arxiv.org/abs/2505.16932\"><bold>Polar Express</bold></link> (Muon Variant) and</paragraph></list-item><list-item><paragraph> <link href=\"https://arxiv.org/abs/2302.06675\"><bold>Lion</bold></link> (EvoLved Sign Momentum)</paragraph></list-item></list><paragraph>New question summary:</paragraph><list style=\"number\"><list-item><paragraph>(Code) An introduction to Polar Express, a variant of Muon that replaces Newton-Schulz orthogonalization with a schedule of minimax-optimized polynomials. Since compute efficiency is a core feature of this variant, the question challenges you to find an efficient (minimal matmuls) PyTorch implementation for the update $$X_{k+1}\\leftarrow  \\alpha_k X_k+ \\beta_k X_k^3 + \\gamma_k X_k^5$$</paragraph></list-item><list-item><paragraph>(Code) An intro to the Lion optimizer, a memory-efficient, non-adaptive optimizer that relies on the sign function of an interpolation of momentum and gradient tensors to determine the update direction.</paragraph></list-item><list-item><paragraph>(Written) Empirical evaluation of Muon+PolarExpress, Muon+Newton-Schulz, Lion, AdamW, and SGD.</paragraph></list-item><list-item><paragraph>(Optional) Hyperparameter tuning for Polar Express and Lion to ensure fair comparisons.</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/KD7Wh9TXKrFaxk9WhzhtPFO3\" width=\"659\" height=\"326.17171717171715\"/></figure><paragraph>I've included the blank and solution notebooks for you below. </paragraph><paragraph>Colab links:</paragraph><list style=\"bullet\"><list-item><paragraph><link href=\"https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_BLANK.ipynb\">blank notebook</link></paragraph></list-item><list-item><paragraph><link href=\"https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_SOLUTION.ipynb\">solution notebook</link></paragraph></list-item></list><paragraph>Notebook files:</paragraph><file url=\"https://static.us.edusercontent.com/files/af5PCw5MAtKRTsAzGVaLxGwF\" filename=\"NRW_SP-D_q_coding_muon_BLANK.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/Ux7IFCbxWaBpSiJL1NhsYKWp\" filename=\"NRW_SP-D_q_coding_muon_SOLUTION.ipynb\"/><paragraph>Links for the archive:</paragraph><paragraph>Personal website: <link href=\"https://nraultwang.github.io/\">https://nraultwang.github.io/</link>, Github: <link href=\"https://github.com/nraultwang\">https://github.com/nraultwang</link></paragraph></document>",
            "links": [
                "https://arxiv.org/abs/2505.16932",
                "https://arxiv.org/abs/2302.06675",
                "https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_BLANK.ipynb",
                "https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_SOLUTION.ipynb",
                "https://nraultwang.github.io/",
                "https://github.com/nraultwang"
            ],
            "attachments": [],
            "created_at": "2025-12-05T18:22:52.659984+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412632,
            "author": "Guohao Lv",
            "project_title": "Special Participation A: Claude on HW6",
            "post_body": "I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step.\n\nAnalysis: Claude was generally able to one-shot the non-coding questions with minimal intervention. For the application/intuition questions (molecular graphs, CNN\u2013GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude\u2019s answers were detailed, on-topic, and made sensible connections to course concepts; if anything, they tended toward being a bit verbose but stayed accurate (maybe not totally accurate, but I think the answers are all reasonable) and grounded in the homework setup.\n\nI did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e.g., suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice. Minor issues were mostly stylistic\u2014some redundancy, slightly heavy notation, and occasional over-explaining\u2014but they did not affect correctness. Overall, Claude\u2019s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editing for concision.",
            "content_xml": "<document version=\"2.0\"><paragraph>I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step.</paragraph><paragraph>Analysis: Claude was generally able to one-shot the non-coding questions with minimal intervention. For the application/intuition questions (molecular graphs, CNN\u2013GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude\u2019s answers were detailed, on-topic, and made sensible connections to course concepts; if anything, they tended toward being a bit verbose but stayed accurate (maybe not totally accurate, but I think the answers are all reasonable) and grounded in the homework setup.</paragraph><paragraph>I did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e.g., suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice. Minor issues were mostly stylistic\u2014some redundancy, slightly heavy notation, and occasional over-explaining\u2014but they did not affect correctness. Overall, Claude\u2019s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editing for concision.</paragraph><file url=\"https://static.us.edusercontent.com/files/XFUv5ST8W5s71KCpex1rl7uK\" filename=\"HW6.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T17:56:51.215172+11:00",
            "category": "Admin"
        },
        {
            "guid": 7412493,
            "author": "Justin Li",
            "project_title": "Special Participation E: Discussion at Home",
            "post_body": "I find going to discussion one of the most helpful parts of this class when it comes to learning content because I get to ask TAs questions live. However, access to TAs is limited by time and schedule, so I designed and tested a ChatGPT \u201cDiscussion TA\u201d that replicates aspects of the in person discussion experience. This tool can help guide users through discussion worksheets interactively, going problem by problem and checking my understanding while providing explanations and concepts from the lecture.\n\nI want it to help me master concepts from the discussions, and I want to have some sort of simulation of TA-student interaction that can supplement the interactions I have in discussion. \n\nIn order to prime ChatGPT, I use the prompt attached below. It operates by taking in the discussion questions and solutions as well as relevant lecture notes. It then walks through each discussion question one by one, allowing for student response to each question and evaluates the response based on the following scenarios.\n\nIf the answer is correct, it confirms that the student is correct and provides a short explanation using lecture intuition\n\nIf the answer is almost correct, it identifies the necessary concepts to understand and provides a hint, allowing me to try again\n\nIf my answer is wrong, then it identifies the necessary concept to understand and explains it using intuition from the lecture, and allows me to try again.\n\nI used this system on the exam related questions on discussion 12 (Questions 1 and 2) and found it very helpful. A lot of the explanations it provided me were very intuitive and simplified, easy for a beginning to understand. Additionally, it provided many different intuitions for each concept, which was awesome. It could take in both typed answers as well as screenshots of written answers, which is super helpful since it would allow me to submit mathematical expressions easier. There were scenarios where I had to prompt it a bit more on questions that I had expressed uncertainty about, but when prompted it provided great explanations. It was able to correct me and provide good explanations whenever I answered a question wrong. Overall, I found the tool very useful.\n\nBelow, I have attached the prompt I used, as well as an annotated trace of my experience with the AI \u201cDiscussion TA\u201d.\n\nTrace:  https://drive.google.com/file/d/15mMJCvaBDp-hnzXBJqMoKvAgrBTN6R7j/view?usp=sharing\n\nPrompt: \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I find going to discussion one of the most helpful parts of this class when it comes to learning content because I get to ask TAs questions live. However, access to TAs is limited by time and schedule, so I designed and tested a ChatGPT \u201cDiscussion TA\u201d that replicates aspects of the in person discussion experience. This tool can help guide users through discussion worksheets interactively, going problem by problem and checking my understanding while providing explanations and concepts from the lecture.</paragraph><paragraph>I want it to help me master concepts from the discussions, and I want to have some sort of simulation of TA-student interaction that can supplement the interactions I have in discussion. </paragraph><paragraph>In order to prime ChatGPT, I use the prompt attached below. It operates by taking in the discussion questions and solutions as well as relevant lecture notes. It then walks through each discussion question one by one, allowing for student response to each question and evaluates the response based on the following scenarios.</paragraph><list style=\"ordered\"><list-item><paragraph>If the answer is correct, it confirms that the student is correct and provides a short explanation using lecture intuition</paragraph></list-item><list-item><paragraph>If the answer is almost correct, it identifies the necessary concepts to understand and provides a hint, allowing me to try again</paragraph></list-item><list-item><paragraph>If my answer is wrong, then it identifies the necessary concept to understand and explains it using intuition from the lecture, and allows me to try again.</paragraph></list-item></list><paragraph>I used this system on the exam related questions on discussion 12 (Questions 1 and 2) and found it very helpful. A lot of the explanations it provided me were very intuitive and simplified, easy for a beginning to understand. Additionally, it provided many different intuitions for each concept, which was awesome. It could take in both typed answers as well as screenshots of written answers, which is super helpful since it would allow me to submit mathematical expressions easier. There were scenarios where I had to prompt it a bit more on questions that I had expressed uncertainty about, but when prompted it provided great explanations. It was able to correct me and provide good explanations whenever I answered a question wrong. Overall, I found the tool very useful.</paragraph><paragraph>Below, I have attached the prompt I used, as well as an annotated trace of my experience with the AI \u201cDiscussion TA\u201d.<break/><break/>Trace:  <link href=\"https://drive.google.com/file/d/15mMJCvaBDp-hnzXBJqMoKvAgrBTN6R7j/view?usp=sharing\">https://drive.google.com/file/d/15mMJCvaBDp-hnzXBJqMoKvAgrBTN6R7j/view?usp=sharing</link></paragraph><paragraph>Prompt: </paragraph><file url=\"https://static.us.edusercontent.com/files/K16jyoXr5hhOc35rFhOCIEuy\" filename=\"discussion at home.pdf\"/><paragraph/></document>",
            "links": [
                "https://drive.google.com/file/d/15mMJCvaBDp-hnzXBJqMoKvAgrBTN6R7j/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-05T17:21:01.314551+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412465,
            "author": "Qicheng Zhu",
            "project_title": "Special Participation B: KIMI K2 on HW 1 Coding Questions",
            "post_body": "Model Tested: KIMI K2\n\nDomain: Homework1 Coding part -- Accelerating Gradient Descent with Momentum\n\nPerformance Overview:\n\nIn this experiment, I evaluated the performance of KIMI K2 on a deep learning homework task that involved implementing and analyzing gradient descent with momentum. The homework consisted of two main coding parts:\n\nThe first part is implementing gradient descent with momentum.\n\nThe second part is exploring learning rate for faster convergence.\n\nTo assess model robustness, I designed two experiments to explore the importance of background knowledge:\n\nExperiment 1: No Background Knowledge\nThe model was prompted directly with coding tasks, without any formulas or context.\nThe solution was partially correct, but there were subtle differences. KIMI K2 used a slightly different formulation for momentum (using different coefficient). The code still worked, but was not fully aligned with the provided formula.\n\nExperiment 2: With Background Knowledge\nI provided KIMI K2 with the correct mathematical equations for momentum before asking the question again. After receiving the formulas, the model produced the correct implementation. This suggests that additional mathematical grounding improves reliability.\n\nI further tested whether the input format affects performance. I supplied the same task using jupyter notebook or screenshots photos. In each case, KIMI K2 successfully generated the correct answer, indicating strong multimodal consistency.\n\nOverall, KIMI K2 can correctly solve non-trivial coding questions involving optimization algorithms. The model benefits from being given mathematical context, but once the formulas are provided, it can generalize across multiple input formats.\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold><bold>Model Tested: </bold></bold>KIMI K2</paragraph><paragraph><bold><bold>Domain: </bold></bold>Homework1 Coding part -- Accelerating Gradient Descent with Momentum</paragraph><paragraph><bold><bold>Performance Overview:</bold></bold></paragraph><paragraph>In this experiment, I evaluated the performance of KIMI K2 on a deep learning homework task that involved implementing and analyzing gradient descent with momentum. The homework consisted of two main coding parts:</paragraph><paragraph>The first part is implementing gradient descent with momentum.</paragraph><paragraph>The second part is exploring learning rate for faster convergence.</paragraph><paragraph>To assess model robustness, I designed two experiments to explore the importance of background knowledge:</paragraph><paragraph><bold><bold>Experiment 1: No Background Knowledge</bold></bold><break/>The model was prompted directly with coding tasks, without any formulas or context.<break/>The solution was partially correct, but there were subtle differences. KIMI K2 used a slightly different formulation for momentum (using different coefficient). The code still worked, but was not fully aligned with the provided formula.</paragraph><paragraph><bold><bold>Experiment 2: With Background Knowledge</bold></bold><break/>I provided KIMI K2 with the correct mathematical equations for momentum before asking the question again. After receiving the formulas, the model produced the correct implementation. This suggests that additional mathematical grounding improves reliability.</paragraph><paragraph>I further tested whether the <bold><bold>input format</bold></bold> affects performance. I supplied the same task using jupyter notebook or screenshots photos. In each case, KIMI K2 successfully generated the correct answer, indicating strong multimodal consistency.</paragraph><paragraph>Overall, KIMI K2 can correctly solve non-trivial coding questions involving optimization algorithms. The model benefits from being given mathematical context, but once the formulas are provided, it can generalize across multiple input formats.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/C77FL90cclfmYLcHFcpcdcuH\" filename=\"ParticipationB_ KIMI_HW1_QichengZhu.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T17:14:26.498598+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412401,
            "author": "Xuanlin Mao",
            "project_title": "Special Participation E: Using Gemini to Convert Handwritten Notes into Structured Learning Materials",
            "post_body": "This is the special participation E1 of Xuanlin.\n\nFor this special participation, my motivation was to make my lecture notes more intuitive for final exam review. Although the notes can be converted to LaTeX and handwritten content, the raw text alone often felt dense and hard to digest.\n\nI used Gemini 3 Canvas to convert my notes into structured slides and mind maps, creating a clear logical flow and visually highlighting key concepts. During this process, I also addressed practical issues, such as LaTeX rendering errors in Gemini-generated slides, by refining prompts and correcting miscompiled formulas.\n\nMy contribution includes providing a stable, bug-free prompt that reliably generates slides and visualizations from handwritten/LaTeX notes, making complex material easier to navigate. The stable version of prompts I used are included in the report.\n\nReport: \n\nhttps://drive.google.com/file/d/1i-exJI2huQq35sT4_WRH5HNqekm5HMl9/view?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>This is the special participation E1 of Xuanlin.</paragraph><paragraph>For this special participation, my motivation was to make my lecture notes more intuitive for final exam review. Although the notes can be converted to LaTeX and handwritten content, the raw text alone often felt dense and hard to digest.</paragraph><paragraph>I used Gemini 3 Canvas to convert my notes into structured slides and mind maps, creating a clear logical flow and visually highlighting key concepts. During this process, I also addressed practical issues, such as LaTeX rendering errors in Gemini-generated slides, by refining prompts and correcting miscompiled formulas.</paragraph><paragraph>My contribution includes providing a stable, bug-free prompt that reliably generates slides and visualizations from handwritten/LaTeX notes, making complex material easier to navigate. The stable version of prompts I used are included in the report.</paragraph><paragraph>Report: </paragraph><paragraph><link href=\"https://drive.google.com/file/d/1i-exJI2huQq35sT4_WRH5HNqekm5HMl9/view?usp=sharing\">https://drive.google.com/file/d/1i-exJI2huQq35sT4_WRH5HNqekm5HMl9/view?usp=sharing</link> </paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1i-exJI2huQq35sT4_WRH5HNqekm5HMl9/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-05T16:59:12.629022+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412314,
            "author": "Kithmini Herath",
            "project_title": "Special Participation C: Refactoring HW 5 Q5",
            "post_body": "Hi all, \n\nI refactored the coding problem Q5 (understanding dropout) of HW5 while preserving its teaching value. I've created a GitHub repository, with the refactored notebooks at: https://github.com/KithminiHerath/CS282-HW5-Q5-Refactor/ \n\nIn summary I did the following modifications with Claude Code: \n\nModularizing the original monolithic notebook \n\nEnsuring reproducibility between runs by setting seeds\n\nType hinting and documentation \n\nConfigured and tested code to run on Google Colab\n\nA more detailed explanation of my refactoring process is included in the following report:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi all, </paragraph><paragraph>I refactored the coding problem Q5 (understanding dropout) of HW5 while preserving its teaching value. I've created a GitHub repository, with the refactored notebooks at: <link href=\"https://github.com/KithminiHerath/CS282-HW5-Q5-Refactor/\">https://github.com/KithminiHerath/CS282-HW5-Q5-Refactor/</link> </paragraph><paragraph>In summary I did the following modifications with Claude Code: </paragraph><list style=\"number\"><list-item><paragraph>Modularizing the original monolithic notebook </paragraph></list-item><list-item><paragraph>Ensuring reproducibility between runs by setting seeds</paragraph></list-item><list-item><paragraph>Type hinting and documentation </paragraph></list-item><list-item><paragraph>Configured and tested code to run on Google Colab</paragraph></list-item></list><paragraph>A more detailed explanation of my refactoring process is included in the following report:</paragraph><file url=\"https://static.us.edusercontent.com/files/sZpcXBuTJdyPgoPw5xvKXucm\" filename=\"HW5_Q5_code_refactor_report.pdf\"/><paragraph/></document>",
            "links": [
                "https://github.com/KithminiHerath/CS282-HW5-Q5-Refactor/"
            ],
            "attachments": [],
            "created_at": "2025-12-05T16:40:10.38309+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7412182,
            "author": "John Chang",
            "project_title": "Special Participation B: Kimi on HW7 Coding Tasks",
            "post_body": "For this task, I decided to use Kimi on the coding tasks for Homework 7.\n\nThis homework had a good amount of coding with different tasks presented, including implementing equations and model architectures, fine-tuning hyperparameters, and producing graphs for model training. \n\nMy methodology was to give Kimi only the code and tell it to complete the TODO sections. If it struggled, I would provide it with more context from the previous cell's text. (For problem 5, I always gave Kimi the previous cell's context because the code was very barebones with no comments).  \n\nOverall, Kimi ran into a couple issues with some tasks -- implementing the squared loss for the autoencoder and implementing the masked autoencoder. It ultimately failed to implement the masked autoencoder due to its code not accounting for how random initialization would work in Colab. For most tasks, it was able to one-shot a solution. \n\nFor model training tasks, Kimi was able to one-shot the last-name RNN and provided code that brought the autoencoder up to 77% accuracy on MNIST (I just had to increase the epochs by 10). It was also able to correctly implement the graph-perspective notebook and achieve ideal separation between the three real distributions.\n\nIn the attached Drive link, I include annotations as well as links to the individual chats (one per question). Due to the limitations of the Kimi website being unable to convert the whole conversation into a PDF, I opted to copy only the code snippets it provided (and relevant results) into the annotated Google doc. Prompts are visible in the full conversations.\n\nhttps://drive.google.com/file/d/1Uv1auMsQrmr_4SZemQ89RIo0oU2vQNpM/view?usp=sharing\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this task, I decided to use Kimi on the coding tasks for Homework 7.</paragraph><paragraph>This homework had a good amount of coding with different tasks presented, including implementing equations and model architectures, fine-tuning hyperparameters, and producing graphs for model training. </paragraph><paragraph>My methodology was to give Kimi only the code and tell it to complete the TODO sections. If it struggled, I would provide it with more context from the previous cell's text. (For problem 5, I always gave Kimi the previous cell's context because the code was very barebones with no comments).  </paragraph><paragraph>Overall, Kimi ran into a couple issues with some tasks -- implementing the squared loss for the autoencoder and implementing the masked autoencoder. It ultimately failed to implement the masked autoencoder due to its code not accounting for how random initialization would work in Colab. For most tasks, it was able to one-shot a solution. </paragraph><paragraph>For model training tasks, Kimi was able to one-shot the last-name RNN and provided code that brought the autoencoder up to 77% accuracy on MNIST (I just had to increase the epochs by 10). It was also able to correctly implement the graph-perspective notebook and achieve ideal separation between the three real distributions.</paragraph><paragraph>In the attached Drive link, I include annotations as well as links to the individual chats (one per question). Due to the limitations of the Kimi website being unable to convert the whole conversation into a PDF, I opted to copy only the code snippets it provided (and relevant results) into the annotated Google doc. Prompts are visible in the full conversations.</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1Uv1auMsQrmr_4SZemQ89RIo0oU2vQNpM/view?usp=sharing\">https://drive.google.com/file/d/1Uv1auMsQrmr_4SZemQ89RIo0oU2vQNpM/view?usp=sharing</link></paragraph><paragraph/></document>",
            "links": [
                "https://drive.google.com/file/d/1Uv1auMsQrmr_4SZemQ89RIo0oU2vQNpM/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-05T16:13:24.987146+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411980,
            "author": "Mihir Rao",
            "project_title": "Special Participation B: GPT 5.1 on HW 3",
            "post_body": "I used GPT 5.1 on the coding portion of HW 3. I was surprised by the performance because it was rather worse than I had expected. I suspect more coding aligned models like Sonnet or Opus would do much better, but GPT 5.1 answer rather quickly on questions where it should have looked at more context, and extremely slowly on questions that were one or two line solutions.\n\nI also found that when these models fail at a task, it's much harder to get them to recover. Some intuition for this could be what we learned in class, with the distributions in probability being a result of auto-regressive nature. When we make a bad choice, it becomes harder to recover.\n\nSpecifically on part b, I had a lot of back and forth with the model and it would write a lot of code, try to change parts that it shouldn't change, and ultimately it needed a lot of hand-holding. From my experience, something like Cursor's setup would probably do better, since it's fine-tuned for tasks like these. My annotated trace with the model is attached below.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used GPT 5.1 on the coding portion of HW 3. I was surprised by the performance because it was rather worse than I had expected. I suspect more coding aligned models like Sonnet or Opus would do much better, but GPT 5.1 answer rather quickly on questions where it should have looked at more context, and extremely slowly on questions that were one or two line solutions.<break/><break/>I also found that when these models fail at a task, it's much harder to get them to recover. Some intuition for this could be what we learned in class, with the distributions in probability being a result of auto-regressive nature. When we make a bad choice, it becomes harder to recover.<break/><break/>Specifically on part b, I had a lot of back and forth with the model and it would write a lot of code, try to change parts that it shouldn't change, and ultimately it needed a lot of hand-holding. From my experience, something like Cursor's setup would probably do better, since it's fine-tuned for tasks like these. My annotated trace with the model is attached below.</paragraph><file url=\"https://static.us.edusercontent.com/files/VXpb50IHA1hITHGhT7UqzRNu\" filename=\"MR-B-182 2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T15:38:02.385054+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411761,
            "author": "Tianqu He",
            "project_title": "Special Participation E: The mathematical principles of Transformers not covered in class",
            "post_body": "Lectures 18\u201320 introduced the structure and mathematical foundations of Transformers, but there are still several important components that were not covered\u2014for example, why Transformers use LayerNorm or how the Feed-Forward Network (FFN) actually works. I want to use this thread to help everyone build a stronger foundational understanding of Transformers. These details, although not explicitly discussed in the lectures, are conceptually interesting and highly relevant. During my internship at ByteDance, I found that these questions are very common in LLM interviews. \n\nSo I asked ChatGPT about several topics that were not covered in the lectures and had it explain the answers in a way that everyone can follow. Of course, this document is far from a complete explanation of how Transformers work, but its purpose is to supplement the lecture material. We can read it together with our class notes for a more complete understanding.\n\nI have annotated some responses from ChatGPT. I attach the annotated conversation and a concise summarization.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Lectures 18\u201320 introduced the structure and mathematical foundations of Transformers, but there are still several important components that were not covered\u2014for example, why Transformers use LayerNorm or how the Feed-Forward Network (FFN) actually works. I want to use this thread to help everyone build a stronger foundational understanding of Transformers. These details, although not explicitly discussed in the lectures, are conceptually interesting and highly relevant. During my internship at ByteDance, I found that <bold>these questions are very common in LLM interviews.</bold> </paragraph><paragraph>So I asked ChatGPT about several topics that were not covered in the lectures and had it explain the answers in a way that everyone can follow. Of course, this document is far from a complete explanation of how Transformers work, but its purpose is to <bold>supplement the lecture material</bold>. We can read it together with our class notes for a more complete understanding.</paragraph><paragraph>I have annotated some responses from ChatGPT. I attach the annotated conversation and a concise summarization.</paragraph><file url=\"https://static.us.edusercontent.com/files/9EUdk8jHkidRCcbU9fn8YVvz\" filename=\"Conversation_ChatGPT.pdf\"/><file url=\"https://static.us.edusercontent.com/files/naT9AACtfolyMyrk7EmmITwW\" filename=\"Transformer_Notes.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T15:02:48.254781+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411740,
            "author": "Tianqu He",
            "project_title": "Special Participation D: HW7 RNN task using AdamW vs. SOAP",
            "post_body": "I created a student assignment notebook focused on the SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) optimizer, consisting of two main parts:\n\nMathematical Implementation (Gradient Rotation):\n\nDesigned a coding task where students implement the core SOAP operation: projecting the gradient into the eigenbasis (Gprojected\u200b=QLT\u200b\u22c5G\u22c5QR\u200b).\n\nProvided a SimpleSOAP optimizer wrapper that integrates this student-written function to simulate matrix preconditioning.\n\nExperimental Analysis (RNN Stress Test):\n\nSet up the \"Adding Problem\" (a standard RNN benchmark) to test optimizer stability on long-term dependencies.\n\nConstructed a Hyperparameter Sensitivity Sweep comparing AdamW vs. SOAP across logarithmically spaced learning rates.\n\nIncluded visualization code to demonstrate SOAP's superior stability and \"shifted\" optimal learning rate window compared to AdamW.\n\nStudent notebook: https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk\n\nSolutions:\n\nhttps://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I created a student assignment notebook focused on the SOAP (<bold>S</bold>hampo<bold>O</bold> with <bold>A</bold>dam in the <bold>P</bold>reconditioner's eigenbasis) optimizer, consisting of two main parts:</paragraph><list style=\"ordered\"><list-item><paragraph>Mathematical Implementation (Gradient Rotation):</paragraph><list style=\"unordered\"><list-item><paragraph>Designed a coding task where students implement the core SOAP operation: projecting the gradient into the eigenbasis (G<sub>projected</sub>\u200b=Q<sub>L</sub><sup>T</sup>\u200b\u22c5G\u22c5Q<sub>R</sub>\u200b).</paragraph></list-item><list-item><paragraph>Provided a <code>SimpleSOAP</code> optimizer wrapper that integrates this student-written function to simulate matrix preconditioning.</paragraph></list-item></list></list-item><list-item><paragraph>Experimental Analysis (RNN Stress Test):</paragraph><list style=\"unordered\"><list-item><paragraph>Set up the \"Adding Problem\" (a standard RNN benchmark) to test optimizer stability on long-term dependencies.</paragraph></list-item><list-item><paragraph>Constructed a Hyperparameter Sensitivity Sweep comparing AdamW vs. SOAP across logarithmically spaced learning rates.</paragraph></list-item><list-item><paragraph>Included visualization code to demonstrate SOAP's superior stability and \"shifted\" optimal learning rate window compared to AdamW.</paragraph></list-item></list></list-item></list><paragraph>Student notebook: <link href=\"https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk\">https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk</link></paragraph><paragraph>Solutions:</paragraph><paragraph><link href=\"https://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP\">https://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP</link></paragraph><paragraph/></document>",
            "links": [
                "https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk",
                "https://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP"
            ],
            "attachments": [],
            "created_at": "2025-12-05T14:59:51.904185+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411467,
            "author": "Shashwat Bansal",
            "project_title": "Special Participation E: Vector Calculus with ChatGPT Study Mode",
            "post_body": "I used ChatGPT Study Mode to learn Vector Calculus, preceded by some quick ICL on the first 6 homeworks. The exercise only proved useful to recall the fundamental concepts of matrix calculus. I wouldn't say I learned much, and felt limited by the need to type mathematical notation into chat at the same pace that I am thinking. Chat also failed to ask me engaging questions.\n\nAnnotated chat: https://docs.google.com/document/d/10VFhJotL3Z-3Buj2IgQmR4yfjOoxuHqUU8UL1CC_5uo/edit?usp=sharing\n\nActual chat: https://chatgpt.com/share/69324b94-e264-8013-988a-8dfa87eec732",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT Study Mode to learn Vector Calculus, preceded by some quick ICL on the first 6 homeworks. The exercise only proved useful to recall the fundamental concepts of matrix calculus. I wouldn't say I learned much, and felt limited by the need to type mathematical notation into chat at the same pace that I am thinking. Chat also failed to ask me engaging questions.</paragraph><paragraph>Annotated chat: https://docs.google.com/document/d/10VFhJotL3Z-3Buj2IgQmR4yfjOoxuHqUU8UL1CC_5uo/edit?usp=sharing</paragraph><paragraph>Actual chat: https://chatgpt.com/share/69324b94-e264-8013-988a-8dfa87eec732</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T14:18:18.543155+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411461,
            "author": "Yuxiang Liu",
            "project_title": "Special Participation C: Modularization for Question 4 HW 11",
            "post_body": "This work is contributed by Yuxiang Liu 3039508136, Hanyang Gu 3038838194, Zimu Wang 3038960121 as a team. We significantly refactored the experimental codebase to improve its modularity, clarity, and extensibility. \n\nFirst, we abstracted the data-generation logic into a single reusable function, generate_linear_regression_data, which encapsulates Gaussian input sampling, noise injection, and ground-truth weight creation. This replaces several scattered global code blocks with a single clear API that returns all relevant tensors (train/test inputs, outputs, and the true weight matrix). This design not only reduces duplication but also makes it easy to modify dataset parameters\u2014such as noise level, dimension, or sample size\u2014without editing experiment code elsewhere in the notebook.\n\nNext, we introduced several structural improvements to the training and experimentation pipeline. We standardized the training function interface across different models and optimizers so that both the linear model SGD routine and the MLP training routine can be passed interchangeably into a unified experiment runner. We added collect_final_mse_dict, a general-purpose function that executes a full grid search over batch sizes and learning rates, returning a consistent MSE summary for downstream analysis.\n\nAdditionally, we created shared plotting utilities to replace repeated Matplotlib code, ensuring consistent formatting and reducing boilerplate. Together, these changes produce a cleaner and more maintainable workflow, making it easier to scale the experiments, add new models, or explore additional scaling-law hypotheses.\n\nFor future GSIs and tutors, we highly recommend you to block out some codes from functions generate_linear_regression_data(), collect_final_mse_dict(), compute_best_lrs()  for the purpose of teaching so that students can better feel the power of modularity when coding up experiments.",
            "content_xml": "<document version=\"2.0\"><paragraph>This work is contributed by Yuxiang Liu 3039508136, Hanyang Gu 3038838194, Zimu Wang 3038960121 as a team. We significantly refactored the experimental codebase to improve its modularity, clarity, and extensibility. </paragraph><paragraph>First, we abstracted the data-generation logic into a single reusable function, <code>generate_linear_regression_data</code>, which encapsulates Gaussian input sampling, noise injection, and ground-truth weight creation. This replaces several scattered global code blocks with a single clear API that returns all relevant tensors (train/test inputs, outputs, and the true weight matrix). This design not only reduces duplication but also makes it easy to modify dataset parameters\u2014such as noise level, dimension, or sample size\u2014without editing experiment code elsewhere in the notebook.</paragraph><paragraph>Next, we introduced several structural improvements to the training and experimentation pipeline. We standardized the training function interface across different models and optimizers so that both the linear model SGD routine and the MLP training routine can be passed interchangeably into a unified experiment runner. We added <code>collect_final_mse_dict</code>, a general-purpose function that executes a full grid search over batch sizes and learning rates, returning a consistent MSE summary for downstream analysis.</paragraph><paragraph>Additionally, we created shared plotting utilities to replace repeated Matplotlib code, ensuring consistent formatting and reducing boilerplate. Together, these changes produce a cleaner and more maintainable workflow, making it easier to scale the experiments, add new models, or explore additional scaling-law hypotheses.</paragraph><paragraph>For future GSIs and tutors, we highly recommend you to block out some codes from functions <code>generate_linear_regression_data(), collect_final_mse_dict(), compute_best_lrs()</code>  for the purpose of teaching so that students can better feel the power of modularity when coding up experiments.</paragraph><file url=\"https://static.us.edusercontent.com/files/VZUUOGfIGK7Y1dN8nvSUGB7x\" filename=\"scaling_laws.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T14:17:49.681756+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411283,
            "author": "Fangzhou Zhao",
            "project_title": "Special Participation B: Use Claude code with Opus 4.5 Finish hw11 coding parts",
            "post_body": "\n\nI used Claude code with opus 4.5 the pdf include both claude code trace and the code after implement and output\n\n\n\nOverall, the code across both notebooks demonstrates a solid understanding of the underlying machine learning concepts\u2014scaling laws for SGD/Adam optimizers and transformer attention mechanisms for interpretability. The implementations are functionally correct: the scaling laws notebook properly sweeps learning rates across batch sizes and fits power-law relationships, while the interpretability notebook correctly implements causal attention with manual softmax and constructs the two-stage induction head (previous-token head + copying head) that passes all test cases. The code is readable with reasonable variable naming, and the mathematical operations (gradient computation, attention scores, QK/OV matrix construction) align with standard formulations. Both notebooks produce the expected outputs and would likely receive full credit for correctness.\n\nHowever, the code quality could be improved in several areas. The scaling laws notebook suffers from heavy duplication\u2014the sweep and plotting logic is copy-pasted three times across Q1/Q2/Q3 rather than being refactored into reusable functions. It also contains magic numbers without explanation (eps=0.57, clip ceiling of 500), bare except: clauses that catch all errors indiscriminately, and a bug where weight_decay=0.01 is used instead of the specified 0.001. The interpretability notebook is cleaner but lacks docstring explanations for the matrix constructions and could benefit from inline comments explaining why specific dimensions are used (e.g., \"dims 4-7 store previous token identity\"). Neither notebook includes comprehensive documentation or type hints. In summary, both implementations are conceptually sound and produce correct results, but would benefit from refactoring to reduce redundancy, adding explanatory comments for non-obvious operations, and fixing the minor parameter mismatch in the Adam optimizer configuration.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/S3YXAY1Slasnul21dQGAXO5H\" filename=\"Special B-2.pdf\"/><paragraph/><paragraph>I used Claude code with opus 4.5 the pdf include both claude code trace and the code after implement and output<break/><break/></paragraph><paragraph>Overall, the code across both notebooks demonstrates a solid understanding of the underlying machine learning concepts\u2014scaling laws for SGD/Adam optimizers and transformer attention mechanisms for interpretability. The implementations are functionally correct: the scaling laws notebook properly sweeps learning rates across batch sizes and fits power-law relationships, while the interpretability notebook correctly implements causal attention with manual softmax and constructs the two-stage induction head (previous-token head + copying head) that passes all test cases. The code is readable with reasonable variable naming, and the mathematical operations (gradient computation, attention scores, QK/OV matrix construction) align with standard formulations. Both notebooks produce the expected outputs and would likely receive full credit for correctness.</paragraph><paragraph>However, the code quality could be improved in several areas. The scaling laws notebook suffers from heavy duplication\u2014the sweep and plotting logic is copy-pasted three times across Q1/Q2/Q3 rather than being refactored into reusable functions. It also contains magic numbers without explanation (eps=0.57, clip ceiling of 500), bare except: clauses that catch all errors indiscriminately, and a bug where weight_decay=0.01 is used instead of the specified 0.001. The interpretability notebook is cleaner but lacks docstring explanations for the matrix constructions and could benefit from inline comments explaining why specific dimensions are used (e.g., \"dims 4-7 store previous token identity\"). Neither notebook includes comprehensive documentation or type hints. In summary, both implementations are conceptually sound and produce correct results, but would benefit from refactoring to reduce redundancy, adding explanatory comments for non-obvious operations, and fixing the minor parameter mismatch in the Adam optimizer configuration.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T13:51:06.536337+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7411055,
            "author": "Shashwat Bansal",
            "project_title": "Special Participation E: Discussion 8 Transpose Convolutions with ChatGPT",
            "post_body": "I used ChatGPT to ask me questions on Transpose Convolutions to internalize the concepts. I forgot to turn on learning mode but maybe it would have performed slightly better. There were no hallucinations and the experience proved insightful (maybe more so than actually solving the discussion problems, since I lacked some understanding of fundamentals).\nAnnotated copy: https://docs.google.com/document/d/1KTbditva90nbQAzdVCy9_CB4u01HY57PQPr1t8VkMLs/edit?usp=sharing\n\nJust the chat: https://chatgpt.com/share/69323912-2490-8013-80a2-52de186c6f5e",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT to ask me questions on Transpose Convolutions to internalize the concepts. I forgot to turn on learning mode but maybe it would have performed slightly better. There were no hallucinations and the experience proved insightful (maybe more so than actually solving the discussion problems, since I lacked some understanding of fundamentals).<break/>Annotated copy: https://docs.google.com/document/d/1KTbditva90nbQAzdVCy9_CB4u01HY57PQPr1t8VkMLs/edit?usp=sharing</paragraph><paragraph>Just the chat: https://chatgpt.com/share/69323912-2490-8013-80a2-52de186c6f5e</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T13:15:48.965184+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410990,
            "author": "Fantine Mpacko Priso",
            "project_title": "Special participation D : addind manifold MuOn to HW06",
            "post_body": "I implemented Manifold MuOn (Bernstein, 2025) at the end of HW06 to give another version of MuOn to compare. Feel free to experiment ! These are the final figures obtained after completing the code:",
            "content_xml": "<document version=\"2.0\"><paragraph>I implemented Manifold MuOn (Bernstein, 2025) at the end of HW06 to give another version of MuOn to compare. Feel free to experiment ! These are the final figures obtained after completing the code:</paragraph><file url=\"https://static.us.edusercontent.com/files/vc6ZlrQyL8Z8A4lxQ5oy0af2\" filename=\"Fantine_q_coding_muon_solutions.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/IjYgNLdp7QpZ1ulBLzN5R6qW\" filename=\"Fantine_q_coding_muon (1).ipynb\"/><figure><image src=\"https://static.us.edusercontent.com/files/kTb82SAh9dgfh34krpPNSn9v\" width=\"658\" height=\"325.67676767676767\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T13:04:37.879759+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410928,
            "author": "Justin Li",
            "project_title": "Special Participation E: Pre-Homework Warmup Tutor",
            "post_body": "Before most homeworks, I usually felt like I didn\u2019t fully understand all of the concepts from the lecture - especially if I had only watched the lecture one time and hadn\u2019t read through the relevant textbook material. While discussions certainly help, I wanted to create a tool that would help me \u201cwarm up\u201d before actually attempting the homeworks. I used ChatGPT to build a system that takes the homework assignment and the relevant lecture notes, and then turns them into a personalized warmup assignment to review and solidify my understanding of the concepts before I jump into the homework.\n\nThere was some trial and error in getting the system right. In my initial prompts, ChatGPT would give me a full summary of the lectures and questions and answers all written out at once, and when I went through these it felt like I was more passively reading than actually practicing - my brain would naturally jump to reading the answer after seeing the question. I realized what I actually wanted was something that behaved more like a tutor - asking questions one at a time, allowing me to input an answer, and giving me feedback to the answer before explaining the solution. Thus, I refined the prompt to still give me a fully structured summary, but delivering interactive warmup questions one at a time - to give the feel of working with an actual tutor.\n\nThese are the main components to the \u201ctutor\u201d\n\nConcept Summary: a short overview of each concept needed for the homework. It pulls the content directly from the lecture notes and cites the exact lecture and page where each idea shows up\n\nTiny Illustrative Examples: very very simple examples (much easier than anything on the homework) just to ensure that I understand the core ideas at a very basic level before moving on\n\nInteractive Warm Ups: the tutor asks questions one at a time, I respond, and the tutor evaluates my answer and explains the reasoning (and where I went wrong if I got the question wrong). The prompt can be adjusted to choose how many questions I want and how hard they should be.\n\nReadiness Check: After the warmup, I get a checklist of concepts to make sure I feel confident before jumping into the actual homework.\n\nFormula Sheet (optional): it also compiled a formula sheet with useful formulas that could be used on the homework problems\n\nOverall, the tool ended up acting like a tutor as I wished, refreshing my understanding of the key concepts, giving me simple practice questions, interactively checking my understanding, and ensuring I was prepared before actually doing the homework.\n\nI have attached the prompt I used to generate all of this, as well as an annotated example of me using this warmup tool on the concepts and lectures for Homework 5 on CNNs. Specifically the tool took a look at HW5, lecture 8, lecture 9, lecture 10, and lecture 11.\n\nHope this helps and cheers!\n\n\nUsing the Tutor on HW 5: https://drive.google.com/file/d/1yOBKCvQgchdMMtoPZqoTNvbwxWVSbL3i/view?usp=sharing\n\nPrompt: ",
            "content_xml": "<document version=\"2.0\"><paragraph>Before most homeworks, I usually felt like I didn\u2019t fully understand all of the concepts from the lecture - especially if I had only watched the lecture one time and hadn\u2019t read through the relevant textbook material. While discussions certainly help, I wanted to create a tool that would help me \u201cwarm up\u201d before actually attempting the homeworks. I used ChatGPT to build a system that takes the homework assignment and the relevant lecture notes, and then turns them into a personalized warmup assignment to review and solidify my understanding of the concepts before I jump into the homework.</paragraph><paragraph>There was some trial and error in getting the system right. In my initial prompts, ChatGPT would give me a full summary of the lectures and questions and answers all written out at once, and when I went through these it felt like I was more passively reading than actually practicing - my brain would naturally jump to reading the answer after seeing the question. I realized what I actually wanted was something that behaved more like a tutor - asking questions one at a time, allowing me to input an answer, and giving me feedback to the answer before explaining the solution. Thus, I refined the prompt to still give me a fully structured summary, but delivering interactive warmup questions one at a time - to give the feel of working with an actual tutor.</paragraph><paragraph>These are the main components to the \u201ctutor\u201d</paragraph><list style=\"ordered\"><list-item><paragraph>Concept Summary: a short overview of each concept needed for the homework. It pulls the content directly from the lecture notes and cites the exact lecture and page where each idea shows up</paragraph></list-item><list-item><paragraph>Tiny Illustrative Examples: very very simple examples (much easier than anything on the homework) just to ensure that I understand the core ideas at a very basic level before moving on</paragraph></list-item><list-item><paragraph>Interactive Warm Ups: the tutor asks questions one at a time, I respond, and the tutor evaluates my answer and explains the reasoning (and where I went wrong if I got the question wrong). The prompt can be adjusted to choose how many questions I want and how hard they should be.</paragraph></list-item><list-item><paragraph>Readiness Check: After the warmup, I get a checklist of concepts to make sure I feel confident before jumping into the actual homework.</paragraph></list-item><list-item><paragraph>Formula Sheet (optional): it also compiled a formula sheet with useful formulas that could be used on the homework problems</paragraph></list-item></list><paragraph>Overall, the tool ended up acting like a tutor as I wished, refreshing my understanding of the key concepts, giving me simple practice questions, interactively checking my understanding, and ensuring I was prepared before actually doing the homework.</paragraph><paragraph>I have attached the prompt I used to generate all of this, as well as an annotated example of me using this warmup tool on the concepts and lectures for Homework 5 on CNNs. Specifically the tool took a look at HW5, lecture 8, lecture 9, lecture 10, and lecture 11.</paragraph><paragraph>Hope this helps and cheers!<break/></paragraph><paragraph>Using the Tutor on HW 5: <link href=\"https://drive.google.com/file/d/1yOBKCvQgchdMMtoPZqoTNvbwxWVSbL3i/view?usp=sharing\">https://drive.google.com/file/d/1yOBKCvQgchdMMtoPZqoTNvbwxWVSbL3i/view?usp=sharing</link></paragraph><paragraph>Prompt: </paragraph><file url=\"https://static.us.edusercontent.com/files/I3gOcgbf84u3aP2Cg7Linsa0\" filename=\"Pre-Homework Context.pdf\"/></document>",
            "links": [
                "https://drive.google.com/file/d/1yOBKCvQgchdMMtoPZqoTNvbwxWVSbL3i/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-05T12:55:24.829717+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410750,
            "author": "Lenci Ni",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 5",
            "post_body": "I used Gemini Pro 3 to help with the coding parts of Homework 5: Problems 5 (Understanding Dropout) and 6 (Batchnorm, Dropout and Convolutions). I handled the inputs by copy-pasting raw code blocks from the notebooks and helper files directly into the prompt without reformatting, followed immediately by the question.\n\nSummary: Gemini Pro 3 handled the heavy coding tasks impressively. It was able to mostly one-shot every implementation with only minor reprompting needed for small corrections. It was good at piecing together logic even when I just copy-pasted disjointed notebook cells and helper files together. It successfully cross-referenced different files in Problem 6.\n\nStrengths: The model\u2019s standout strength is code completion. It generated code implementations that were executable immediately with no syntax errors. It handled the \"bunched together\" context effortlessly. For the written parts of the coding problems, the explanations provided were often more detailed than the official solutions. The code style was also very clean and consistent with the provided skeleton code.\n\nWeaknesses: There were a few small issues. Occasionally, the model said it completed the function in the file, but actually didn't change any code at all. Also, in Problem 5, it struggled to correctly interpret what the graphs should look like.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used <bold>Gemini Pro 3</bold> to help with the coding parts of Homework 5: Problems 5 (Understanding Dropout) and 6 (Batchnorm, Dropout and Convolutions). I handled the inputs by copy-pasting raw code blocks from the notebooks and helper files directly into the prompt without reformatting, followed immediately by the question.</paragraph><paragraph><bold>Summary:</bold> Gemini Pro 3 handled the heavy coding tasks impressively. It was able to mostly one-shot every implementation with only minor reprompting needed for small corrections. It was good at piecing together logic even when I just copy-pasted disjointed notebook cells and helper files together. It successfully cross-referenced different files in Problem 6.</paragraph><paragraph><bold>Strengths:</bold> The model\u2019s standout strength is code completion. It generated code implementations that were executable immediately with no syntax errors. It handled the \"bunched together\" context effortlessly. For the written parts of the coding problems, the explanations provided were often more detailed than the official solutions. The code style was also very clean and consistent with the provided skeleton code.</paragraph><paragraph><bold>Weaknesses:</bold> There were a few small issues. Occasionally, the model said it completed the function in the file, but actually didn't change any code at all. Also, in Problem 5, it struggled to correctly interpret what the graphs should look like.</paragraph><file url=\"https://static.us.edusercontent.com/files/1gBo2V4OrbhEvMcNZ4BAxxZ6\" filename=\"participation_b_hw5.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T12:28:23.821908+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410745,
            "author": "Tom Chen",
            "project_title": "Special Participation E: The \"Broken Blueprint\" Game",
            "post_body": "The \"Broken Blueprint\" Game: Learning Deep Learning via AI Code Review\n\nThis game is a \"Reverse Learning\" approach: Instead of asking the AI to explain a concept or quiz me, I inverted the roles: The AI writes code, and I have to debug it.\n\nI designed a prompt called \"The Buggy Architect\" to simulate a real-world scenario: reviewing the code of a junior engineer who makes subtle, logical errors in deep learning implementations.\n\nThe Core Idea: Evaluation over Passive Consumption\n\nTraditional AI study tools often encourage passive consumption (reading summaries). My goal was to target the higher levels of Bloom's Taxonomy: Analysis and Evaluation.In Deep Learning, many students (including myself) understand the high-level diagrams of a Transformer or ResNet but fail when implementing them because of dimension mismatches or incorrect tensor operations. By forcing myself to \"Code Review\" the AI's faulty implementation, I am required to mentally trace the data flow and tensor shapes ensuring a much more rigorous understanding of the architecture.\n\nPrompt Design Choices\n\nI iteratively built a system prompt with specific constraints:\n\nThe \"Subtlety\" Constraint: The prompt explicitly forbids syntax errors (which a compiler would catch). The bugs must be logical or architectural (e.g., applying Softmax over the wrong dimension, incorrect masking in causal attention, or putting LayerNorm in a place that creates a gradient bottleneck).\n\nThe \"Junior Engineer\" Personality: I instructed the AI to act as a confident but prone-to-error junior engineer. This lowers the barrier to entry and makes the \"correction\" process feel like a collaborative code review rather than a test.\n\nScaffolded Hinting: If I fail to spot the bug, the AI is instructed not to reveal the answer immediately but to provide hints related to Tensor Shapes or Gradient Flow. This forces me to check the math manually.\n\nWhat Worked Well\n\nBased on my interaction trace (see attached), this approach was highly effective for technical rigor:\n\nForced Math Verification: When the model implemented Multi-Head Attention, I had to manually write down the matrix dimensions on paper to verify if the view and transpose operations were correct. This solidified my understanding of the (Batch, Heads, Seq, Dim) transformation.\n\nCatching \"Silent Failures\": The tool was good at generating bugs that run without crashing but destroy performance (e.g., forgetting to add the residual connection in a ResNet block). Spotting this required understanding the purpose of the architecture, not just the code.\n\nEngagement: The gamified nature (\"Find the bug\") made the session significantly more engaging than a standard Q&A.\n\nWhat Could Be Improved\n\nHowever, the tool exhibited specific weaknesses that require critical oversight:\n\n\"Gaslighting\" / False Positives: In one instance, the AI actually wrote correct code but insisted there was a bug because the prompt forced it to include one. It then tried to convince me that a standard implementation was wrong. This requires the student to be very confident to push back.\n\nInconsistent Difficulty: Sometimes the \"bug\" was too trivial (e.g., a typo in a variable name) despite the instructions asking for logical errors.\n\nLack of Context: The AI sometimes used variable names (like x vs h) that were ambiguous without a surrounding class definition, making it hard to judge if a bug existed or if it was just poor naming convention.\n\nReflection\n\nThis tool serves as an excellent \"Pre-Implementation Check.\" Before I start my own homework or project, playing a few rounds of \"Find the Bug\" with this prompt helps me anticipate the common pitfalls I am likely to make myself. It turns the AI from a lecturer into a simulation of a coding environment.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>The \"Broken Blueprint\" Game: Learning Deep Learning via AI Code Review</bold></paragraph><paragraph>This game is a \"Reverse Learning\" approach: Instead of asking the AI to explain a concept or quiz me, I inverted the roles: The AI writes code, and I have to debug it.</paragraph><paragraph>I designed a prompt called \"The Buggy Architect\" to simulate a real-world scenario: reviewing the code of a junior engineer who makes subtle, logical errors in deep learning implementations.</paragraph><paragraph><bold>The Core Idea: Evaluation over Passive Consumption</bold></paragraph><paragraph>Traditional AI study tools often encourage passive consumption (reading summaries). My goal was to target the higher levels of Bloom's Taxonomy: Analysis and Evaluation.In Deep Learning, many students (including myself) understand the high-level diagrams of a Transformer or ResNet but fail when implementing them because of dimension mismatches or incorrect tensor operations. By forcing myself to \"Code Review\" the AI's faulty implementation, I am required to mentally trace the data flow and tensor shapes ensuring a much more rigorous understanding of the architecture.</paragraph><paragraph>Prompt Design Choices</paragraph><paragraph>I iteratively built a system prompt with specific constraints:</paragraph><list style=\"bullet\"><list-item><paragraph>The \"Subtlety\" Constraint: The prompt explicitly forbids syntax errors (which a compiler would catch). The bugs must be logical or architectural (e.g., applying Softmax over the wrong dimension, incorrect masking in causal attention, or putting LayerNorm in a place that creates a gradient bottleneck).</paragraph></list-item><list-item><paragraph>The \"Junior Engineer\" Personality: I instructed the AI to act as a confident but prone-to-error junior engineer. This lowers the barrier to entry and makes the \"correction\" process feel like a collaborative code review rather than a test.</paragraph></list-item><list-item><paragraph>Scaffolded Hinting: If I fail to spot the bug, the AI is instructed not to reveal the answer immediately but to provide hints related to Tensor Shapes or Gradient Flow. This forces me to check the math manually.</paragraph></list-item></list><paragraph>What Worked Well</paragraph><paragraph>Based on my interaction trace (see attached), this approach was highly effective for technical rigor:</paragraph><list style=\"bullet\"><list-item><paragraph>Forced Math Verification: When the model implemented Multi-Head Attention, I had to manually write down the matrix dimensions on paper to verify if the view and transpose operations were correct. This solidified my understanding of the (Batch, Heads, Seq, Dim) transformation.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph>Catching \"Silent Failures\": The tool was good at generating bugs that run without crashing but destroy performance (e.g., forgetting to add the residual connection in a ResNet block). Spotting this required understanding the purpose of the architecture, not just the code.</paragraph></list-item><list-item><paragraph>Engagement: The gamified nature (\"Find the bug\") made the session significantly more engaging than a standard Q&amp;A.</paragraph></list-item></list><paragraph>What Could Be Improved</paragraph><paragraph>However, the tool exhibited specific weaknesses that require critical oversight:</paragraph><list style=\"bullet\"><list-item><paragraph>\"Gaslighting\" / False Positives: In one instance, the AI actually wrote correct code but insisted there was a bug because the prompt forced it to include one. It then tried to convince me that a standard implementation was wrong. This requires the student to be very confident to push back.</paragraph></list-item><list-item><paragraph>Inconsistent Difficulty: Sometimes the \"bug\" was too trivial (e.g., a typo in a variable name) despite the instructions asking for logical errors.</paragraph></list-item><list-item><paragraph>Lack of Context: The AI sometimes used variable names (like x vs h) that were ambiguous without a surrounding class definition, making it hard to judge if a bug existed or if it was just poor naming convention.</paragraph></list-item></list><paragraph>Reflection</paragraph><paragraph>This tool serves as an excellent \"Pre-Implementation Check.\" Before I start my own homework or project, playing a few rounds of \"Find the Bug\" with this prompt helps me anticipate the common pitfalls I am likely to make myself. It turns the AI from a lecturer into a simulation of a coding environment.</paragraph><file url=\"https://static.us.edusercontent.com/files/0OChLacQMWv1rrjFi1fACT3x\" filename=\"Special Participation E by Tom.pdf\"/><file url=\"https://static.us.edusercontent.com/files/8yex49ucHp2cgW4CdCC3czax\" filename=\"Prompt.txt\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T12:27:49.943433+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410181,
            "author": "Vrushank Prakash",
            "project_title": "Special Participation E: Understanding RNNs through the Signal Processing Perspective with Gemini",
            "post_body": "When we first talked about RNNs in class, I had trouble understanding the signal processing and Kalman filter perspectives, especially since my previous classes didn't cover these topics in depth. I know there are other people in the class who didn't take classes in signal processing, making the RNNs lecture notes challenging to follow at first.\n\nI decided to use Gemini 3 Pro with the Guided Learning mode to help me understand the connection between signal processing, Kalman filters, and RNNs. I passed in the lecture 14 notes as context. I first prompted Gemini to give me an introduction into what RNNs are before discussing the signal processing perspective. Just like lecture, it made a clear distinction between how CNNs work with space while RNNs work with time/sequences. It then talked about both the forward and backward passes. Eventually, it brought in the signal processing perspective and how RNNs can be represented as Kalman Filters with learnable matrices. At the end, I asked more general questions about RNNs, in which Gemini was able to connect its explanation back to signal processing and Kalman filter concepts.\n\nOverall, I think Gemini did a great job of bringing the concepts of signal processing and Kalman filters into the discussion of RNNs. I do think Gemini could have done a better job of explaining of what a Kalman filter is before immediately jumping into how it is used in RNNs. I also found Gemini using too many analogies when I ask it to explain certain concepts more clearly, sometimes which were too unrelated to the actual concepts.\n\nHere is the annotated PDF:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>When we first talked about RNNs in class, I had trouble understanding the signal processing and Kalman filter perspectives, especially since my previous classes didn't cover these topics in depth. I know there are other people in the class who didn't take classes in signal processing, making the RNNs lecture notes challenging to follow at first.</paragraph><paragraph>I decided to use Gemini 3 Pro with the Guided Learning mode to help me understand the connection between signal processing, Kalman filters, and RNNs. I passed in the lecture 14 notes as context. I first prompted Gemini to give me an introduction into what RNNs are before discussing the signal processing perspective. Just like lecture, it made a clear distinction between how CNNs work with space while RNNs work with time/sequences. It then talked about both the forward and backward passes. Eventually, it brought in the signal processing perspective and how RNNs can be represented as Kalman Filters with learnable matrices. At the end, I asked more general questions about RNNs, in which Gemini was able to connect its explanation back to signal processing and Kalman filter concepts.</paragraph><paragraph>Overall, I think Gemini did a great job of bringing the concepts of signal processing and Kalman filters into the discussion of RNNs. I do think Gemini could have done a better job of explaining of what a Kalman filter is before immediately jumping into how it is used in RNNs. I also found Gemini using too many analogies when I ask it to explain certain concepts more clearly, sometimes which were too unrelated to the actual concepts.</paragraph><paragraph>Here is the annotated PDF:</paragraph><file url=\"https://static.us.edusercontent.com/files/5v8fGUEFtR0uXghnmLu95hsE\" filename=\"CS 182 Special Participation E_ Understanding RNNs through the Signal Processing Perspective .pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T11:12:21.743675+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410149,
            "author": "Zimu Wang",
            "project_title": "Special Participation E: GPT to Understand Complex Ideas from What I Already Know",
            "post_body": "LLMs are really good at teaching and introducing new knowledge by connecting it to concepts we\u2019re already familiar with. This makes understanding abstractions much easier.\n\nHere is the link to my chat with ChatGPT-4o: https://chatgpt.com/share/69321cf6-fb44-8005-a3a4-e9a9ded4c4de\n\nIn this example, I wanted to learn about image generation models, but I only had experience with LLMs and basically no background in image models.\n\nFirst, I instructed the LLM to give me a big-picture overview and told it my background. This part is important \u2014 it helps the model tailor the explanation to something I can understand more efficiently and effectively. Otherwise, the LLM might give answers that are too general without enough theory, or too detailed without a big picture.\n\nThen, I guided the LLM to introduce image generation using logic similar to LLMs: what the inputs and outputs are, how the loss function is computed, and what the layer-to-layer structure looks like. This approach is extremely helpful because it lets me understand new concepts through the lens of something I already know, instead of learning everything from scratch.",
            "content_xml": "<document version=\"2.0\"><paragraph>LLMs are really good at teaching and introducing new knowledge by connecting it to concepts we\u2019re already familiar with. This makes understanding abstractions much easier.</paragraph><paragraph>Here is the link to my chat with ChatGPT-4o: <link href=\"https://chatgpt.com/share/69321cf6-fb44-8005-a3a4-e9a9ded4c4de\">https://chatgpt.com/share/69321cf6-fb44-8005-a3a4-e9a9ded4c4de</link></paragraph><paragraph>In this example, I wanted to learn about image generation models, but I only had experience with LLMs and basically no background in image models.</paragraph><paragraph>First, I instructed the LLM to give me a big-picture overview and told it my background. This part is important \u2014 it helps the model tailor the explanation to something I can understand more efficiently and effectively. Otherwise, the LLM might give answers that are too general without enough theory, or too detailed without a big picture.</paragraph><paragraph>Then, I guided the LLM to introduce image generation using logic similar to LLMs: what the inputs and outputs are, how the loss function is computed, and what the layer-to-layer structure looks like. This approach is extremely helpful because it lets me understand new concepts through the lens of something I already know, instead of learning everything from scratch.</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/69321cf6-fb44-8005-a3a4-e9a9ded4c4de"
            ],
            "attachments": [],
            "created_at": "2025-12-05T11:08:12.830881+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7410013,
            "author": "Xi Cheng",
            "project_title": "Special Participation E: Using an AI \u201cReviewer 2\u201d to Pressure-Test Concepts",
            "post_body": "In this participation, I designed a small AI tool where the model acts as a harsh but fair \u201cReviewer 2\u201d for any concept from EECS182. The workflow is: I first write my own free-form explanation of a concept (e.g., attention in Transformers), then prompt the LLM to critique it\u2014highlighting what is correct, flagging vague or misleading phrases, and asking sharp follow-up questions about assumptions, edge cases, and the exact math. After that, I respond to its questions and rewrite my explanation, making it more precise and better aligned with the formal definitions and equations from lecture. This interaction turns the LLM into a tool for stress-testing my understanding rather than passively explaining things to me, and it also surfaces where the model\u2019s feedback is helpful versus where I still need to double-check the theory myself.",
            "content_xml": "<document version=\"2.0\"><paragraph>In this participation, I designed a small AI tool where the model acts as a harsh but fair \u201cReviewer 2\u201d for any concept from EECS182. The workflow is: I first write my own free-form explanation of a concept (e.g., attention in Transformers), then prompt the LLM to critique it\u2014highlighting what is correct, flagging vague or misleading phrases, and asking sharp follow-up questions about assumptions, edge cases, and the exact math. After that, I respond to its questions and rewrite my explanation, making it more precise and better aligned with the formal definitions and equations from lecture. This interaction turns the LLM into a tool for stress-testing my understanding rather than passively explaining things to me, and it also surfaces where the model\u2019s feedback is helpful versus where I still need to double-check the theory myself.</paragraph><file url=\"https://static.us.edusercontent.com/files/ek4nGsY95yVoNkZu4KAQX9fc\" filename=\"participationE2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T10:52:25.676014+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409895,
            "author": "Srikar Babu Gadipudi",
            "project_title": "Special Participation E: Comparison of LLMs in Explaining a Theoretical Paper",
            "post_body": "It is common for us, as students, to use LLMs to understand research papers, especially ones that have quite a bit of theory. To identify which LLM would help us better with this task, I compared 4 LLMs' (ChatGPT (Study mode), Claude (Extended Thinking), DeepSeek (DeepThink) and Kimi (Thinking)) explanations to this fairly theory heavy paper \"Transformers Learn In-Context by Gradient Descent,\" (Oswald et. al. 2023). The prompt I chose was the following, while also attaching the paper:\n\"This is a crucial paper for my course project (CS182: Deep Learning course at UC Berkeley). I want to understand the functioning of in-context learning and how it performs gradient descent implicitly. Please provide a comprehensive and complete analysis from this paper, keep in mind to give intuitive explanations for everything.\"\n\nFrom my perspective, I think Kimi gave the best overall explanation for an upper-undergrad/grad-level course. While all models correctly identified the core concepts like single attention layer = 1 GD step, deep Transformers = GD++/curvature correction, MLPs = kernel regression), Kimi provided the most rigorous and complete derivation. It was the only model that provided a detailed, step-by-step mathematical trace of Proposition 1 in the paper and drew parallels to concepts like MAML. Claude was excellent for intuitive framing (\"Data Transformation View\"), and DeepSeek offered the most concise summary, but Kimi struck the best balance of mathematical grounding and comprehensive scope.\n\nHere is the annotated merged pdf file with all the conversations, I added my thoughts as and when I found something interesting.",
            "content_xml": "<document version=\"2.0\"><paragraph>It is common for us, as students, to use LLMs to understand research papers, especially ones that have quite a bit of theory. To identify which LLM would help us better with this task, I compared 4 LLMs' (ChatGPT (Study mode), Claude (Extended Thinking), DeepSeek (DeepThink) and Kimi (Thinking)) explanations to this fairly theory heavy paper \"Transformers Learn In-Context by Gradient Descent,\" (Oswald et. al. 2023). The prompt I chose was the following, while also attaching the paper:<break/>\"This is a crucial paper for my course project (CS182: Deep Learning course at UC Berkeley). I want to understand the functioning of in-context learning and how it performs gradient descent implicitly. Please provide a comprehensive and complete analysis from this paper, keep in mind to give intuitive explanations for everything.\"</paragraph><paragraph>From my perspective, I think Kimi gave the best overall explanation for an upper-undergrad/grad-level course. While all models correctly identified the core concepts like single attention layer = 1 GD step, deep Transformers = GD++/curvature correction, MLPs = kernel regression), Kimi provided the most rigorous and complete derivation. It was the only model that provided a detailed, step-by-step mathematical trace of Proposition 1 in the paper and drew parallels to concepts like MAML. Claude was excellent for intuitive framing (\"Data Transformation View\"), and DeepSeek offered the most concise summary, but Kimi struck the best balance of mathematical grounding and comprehensive scope.</paragraph><paragraph>Here is the annotated merged pdf file with all the conversations, I added my thoughts as and when I found something interesting.</paragraph><file url=\"https://static.us.edusercontent.com/files/czh9yCPoMktNRADnoiEOrvQW\" filename=\"participationE_convosMerged.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T10:36:04.922943+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409877,
            "author": "Zimu Wang",
            "project_title": "Special Participation A: HW 0 non-coding solution from GPT5-Think",
            "post_body": "I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples but clear instruction. \n\nHere is the link of the chat: https://chatgpt.com/share/69321796-e2bc-8005-9a51-8058b3070a0d\n\nHowever, GPT5 is not good at generating pdf file, especially on such task with math formulas. Hence, I have to guide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples but clear instruction. </paragraph><paragraph>Here is the link of the chat: https://chatgpt.com/share/69321796-e2bc-8005-9a51-8058b3070a0d</paragraph><paragraph>However, GPT5 is not good at generating pdf file, especially on such task with math formulas. Hence, I have to guide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution. </paragraph><file url=\"https://static.us.edusercontent.com/files/JBKPIuEez4uXO7oIYQ8HIIxq\" filename=\"hw0_noncoding_zimu_gpt5-think.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T10:32:39.626459+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409772,
            "author": "Rohan Gulati",
            "project_title": "Special Participation A: Kimi K2 on HW2",
            "post_body": "Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization.\n\nAnalysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b. In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem. However, once this was indicated, the model was able to revise and arrive at the correct solution immediately.\n\nIncluding chain of thought in the prompt made the model output steps or question description + analysis pairs, while also making it simple to provide corrections for intermediate steps or assumptions. Kimi also used a different font color to indicate corrections to past hallucinations. The output indicates Kimi has good intuition and is able to reason about and handle the gradient operations well, regularly providing interpretations for steps. I thought it was interesting how Kimi was able to attempt multiple approaches analytically in the more complicated scenario it had hallucinated. Additionally, it was able to spatially reason about the table in question 5 without having to explicitly state any relationships among rows or columns. \n\n\n\nAnnotated Conversation:\n\n\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization.</paragraph><paragraph>Analysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b. In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem. However, once this was indicated, the model was able to revise and arrive at the correct solution immediately.</paragraph><paragraph>Including chain of thought in the prompt made the model output steps or question description + analysis pairs, while also making it simple to provide corrections for intermediate steps or assumptions. Kimi also used a different font color to indicate corrections to past hallucinations. The output indicates Kimi has good intuition and is able to reason about and handle the gradient operations well, regularly providing interpretations for steps. I thought it was interesting how Kimi was able to attempt multiple approaches analytically in the more complicated scenario it had hallucinated. Additionally, it was able to spatially reason about the table in question 5 without having to explicitly state any relationships among rows or columns. </paragraph><paragraph/><paragraph>Annotated Conversation:</paragraph><file url=\"https://static.us.edusercontent.com/files/eBdEBwHbEcFFofmTkmtNgHzY\" filename=\"kimi_written_hw2.pdf\"/><paragraph/><paragraph/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T10:18:30.636365+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409630,
            "author": "Fangzhou Zhao",
            "project_title": "Special Participation A: Using GPT 5.1 thinking  on HW11",
            "post_body": "Trace:\nhttps://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787\nhttps://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566\n\nhttps://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645\nhttps://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d\nhttps://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b\n\nI used 5.1 Thinking (heavy) as a deep learning assistant and experimented with different prompt structures on a full homework set. I always told the model to reason step by step, explain its interpretation of each question, and self-correct when necessary. I tried both \u201call-at-once\u201d prompts (entire multi-part questions or several questions in one go) and \u201cturn-by-turn\u201d prompts, where each subpart or major question was given in a separate message, plus a hybrid strategy for larger Fermi/system questions.\n\nOverall, the model showed strong technical understanding across topics like LoRA, matrix calculus, initialization, lookup embeddings, scaling laws, soft prompting, and MAML. Its derivations were often more explicit and pedagogical than the official solutions, with clear intermediate steps, shape checks, and good justifications for why certain statements are true. When the scope of the prompt was narrow (one subpart or one focused question), it behaved like a very competent TA that can both solve the problem and teach the underlying concept.\n\nThe main difference came from prompt structure. When I gave big, multi-part questions all at once, the answers tended to be long and cluttered, and there were occasional stability issues (for example, one \u201ceverything in one turn\u201d prompt produced no output on the first try). In contrast, turn-by-turn prompting consistently led to cleaner, more rigorous solutions: the model derived gradients carefully, checked dimensions, and stayed on task for that specific subpart. In practice, turn-by-turn was strictly more reliable for math-heavy, multi-part deep learning questions.\n\nFor larger Fermi and systems-style questions, I used a hybrid approach: batching some straightforward numerical subparts together and then handling the more subtle ones one by one. This improved efficiency but exposed the main weakness of the model: coverage under large context. When the prompt contained many subparts or even multiple big questions (for example, a soft-prompting question followed by a separate TinyML question), the model sometimes skipped part of a subquestion or ignored a later question entirely, even while giving very strong answers to the earlier material. The problem was not understanding, but systematically answering everything that was asked.\n\nOverall, the experiments suggest that 5.1 Thinking (heavy) is best treated as a strong PhD-level TA with occasional attention and coverage issues in large contexts. The most effective pattern is to keep scope tight: use turn-by-turn prompting for multi-part questions, separate big questions into separate prompts, and optionally ask the model to list which subparts it has answered as a final coverage check. With that structure, the step-by-step + self-check style reliably produces detailed, correct, and often superior explanations compared to the official homework solutions.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Trace:<break/><link href=\"https://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787\">https://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787</link><break/><link href=\"https://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566\">https://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566</link><break/><break/><link href=\"https://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645 https://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d https://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b\">https://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645<break/>https://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d<break/>https://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b</link></paragraph><paragraph>I used 5.1 Thinking (heavy) as a deep learning assistant and experimented with different prompt structures on a full homework set. I always told the model to reason step by step, explain its interpretation of each question, and self-correct when necessary. I tried both \u201call-at-once\u201d prompts (entire multi-part questions or several questions in one go) and \u201cturn-by-turn\u201d prompts, where each subpart or major question was given in a separate message, plus a hybrid strategy for larger Fermi/system questions.</paragraph><paragraph>Overall, the model showed strong technical understanding across topics like LoRA, matrix calculus, initialization, lookup embeddings, scaling laws, soft prompting, and MAML. Its derivations were often more explicit and pedagogical than the official solutions, with clear intermediate steps, shape checks, and good justifications for why certain statements are true. When the scope of the prompt was narrow (one subpart or one focused question), it behaved like a very competent TA that can both solve the problem and teach the underlying concept.</paragraph><paragraph>The main difference came from prompt structure. When I gave big, multi-part questions all at once, the answers tended to be long and cluttered, and there were occasional stability issues (for example, one \u201ceverything in one turn\u201d prompt produced no output on the first try). In contrast, turn-by-turn prompting consistently led to cleaner, more rigorous solutions: the model derived gradients carefully, checked dimensions, and stayed on task for that specific subpart. In practice, turn-by-turn was strictly more reliable for math-heavy, multi-part deep learning questions.</paragraph><paragraph>For larger Fermi and systems-style questions, I used a hybrid approach: batching some straightforward numerical subparts together and then handling the more subtle ones one by one. This improved efficiency but exposed the main weakness of the model: coverage under large context. When the prompt contained many subparts or even multiple big questions (for example, a soft-prompting question followed by a separate TinyML question), the model sometimes skipped part of a subquestion or ignored a later question entirely, even while giving very strong answers to the earlier material. The problem was not understanding, but systematically answering everything that was asked.</paragraph><paragraph>Overall, the experiments suggest that 5.1 Thinking (heavy) is best treated as a strong PhD-level TA with occasional attention and coverage issues in large contexts. The most effective pattern is to keep scope tight: use turn-by-turn prompting for multi-part questions, separate big questions into separate prompts, and optionally ask the model to list which subparts it has answered as a final coverage check. With that structure, the step-by-step + self-check style reliably produces detailed, correct, and often superior explanations compared to the official homework solutions.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/RsZNw0L9Jj5Gajd3MKc4bBAE\" filename=\"Special A.pdf\"/><paragraph><break/></paragraph></document>",
            "links": [
                "https://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787",
                "https://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566",
                "https://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645 https://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d https://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b"
            ],
            "attachments": [],
            "created_at": "2025-12-05T09:57:39.03131+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409338,
            "author": "Justin Li",
            "project_title": "Special Participation B: Gemini on Q5 of HW 9",
            "post_body": "I used the Google Gemini 3.0 model to solve the coding question on Homework 9, in which we looked at attention mechanisms in GPT and BERT. It was overall a pleasant experience, with the model being able to one-shot almost all of the problems we asked it. However, much of this smooth experience came after I refined my initial prompt and nudged the model along the way to be more succint, specific, and pay more attention to the images I showed it.\n\nGemini was certainly good at understanding the graphs I showed it. For each problem, although there were hundreds of permutations of layer x head graphs I could have shown it, Gemini was able to understand the main takeaways well after just seeing a few. It did well to identify special features like word sense disambiguation and coreference resolution across different heads and layers. This was core to the solving of most of the parts of this homework.\n\nHowever, along the way, I noticed some downsides of the model. Initially, its answers were way too long and often sidetracked into explanations of general ideas/concepts I didn't ask for. Even after nudging it in the right direction, it would still sometimes lack brevity. I found that it worked best when I asked it to be succint, be specific, and pay extra attention to the images I showed it - previous to this prompt, the answers would often lack specific references to the examples I wanted it to see.\n\nDespite these issues, the final answers were accurate and matched with the official solution. \n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used the Google Gemini 3.0 model to solve the coding question on Homework 9, in which we looked at attention mechanisms in GPT and BERT. It was overall a pleasant experience, with the model being able to one-shot almost all of the problems we asked it. However, much of this smooth experience came after I refined my initial prompt and nudged the model along the way to be more succint, specific, and pay more attention to the images I showed it.</paragraph><paragraph>Gemini was certainly good at understanding the graphs I showed it. For each problem, although there were hundreds of permutations of layer x head graphs I could have shown it, Gemini was able to understand the main takeaways well after just seeing a few. It did well to identify special features like word sense disambiguation and coreference resolution across different heads and layers. This was core to the solving of most of the parts of this homework.</paragraph><paragraph>However, along the way, I noticed some downsides of the model. Initially, its answers were way too long and often sidetracked into explanations of general ideas/concepts I didn't ask for. Even after nudging it in the right direction, it would still sometimes lack brevity. I found that it worked best when I asked it to be succint, be specific, and pay extra attention to the images I showed it - previous to this prompt, the answers would often lack specific references to the examples I wanted it to see.</paragraph><paragraph>Despite these issues, the final answers were accurate and matched with the official solution. </paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/3dNlOQaZhdIw8lE8sDkNxH66\" filename=\"Special-Participation-B.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T09:22:35.505719+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409308,
            "author": "Sammie Smith",
            "project_title": "Special Participation A: ChatGPT 5.1 Thinking on HW08",
            "post_body": "Hi there,\n\nI asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework. It did not, however, finish the step of plugging and chugging numerical values in 1b (i), 1 b (ii), and 3c. After a second prompt, it still said that it couldn't give me solutions, but then proceeded to give me numerical answers for those subparts.\n\nI didn't catch any hallucinations of math rules/logic, however ChatGPT could not show every step of the derivation and sometimes it showed derivations alternate to the staff solution. It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix operations. This threw off the logic and resulted in incorrect solutions for the following subparts. \n\nThere were other instances where ChatGPT overcomplicated things. When estimating the optimum generic square linear encoder W using the SVD, it used a placeholder variable Z=WU which was very confusing. I've noticed this when I've used ChatGPT for hw help previously for this class; it really likes to come up with extra variables in the name of simplification, but really just serves to confuse the reader. \n\nWhile ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c). \n\nIn conclusion, without mastery knowledge of the subject, it's really difficult to know when ChatGPT is hallucinating or overcomplicating. Thus, it's not more helpful than working backwards from staff solutions for the subparts where steps are shown in the staff solutions. \n\nSee this annotated conversation for more problem specific details:",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>I asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework. It did not, however, finish the step of plugging and chugging numerical values in 1b (i), 1 b (ii), and 3c. After a second prompt, it still said that it couldn't give me solutions, but then proceeded to give me numerical answers for those subparts.</paragraph><paragraph>I didn't catch any hallucinations of math rules/logic, however ChatGPT could not show every step of the derivation and sometimes it showed derivations alternate to the staff solution. It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix operations. This threw off the logic and resulted in incorrect solutions for the following subparts. </paragraph><paragraph>There were other instances where ChatGPT overcomplicated things. When estimating the optimum generic square linear encoder W using the SVD, it used a placeholder variable Z=WU which was very confusing. I've noticed this when I've used ChatGPT for hw help previously for this class; it really likes to come up with extra variables in the name of simplification, but really just serves to confuse the reader. </paragraph><paragraph>While ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps &amp; intuition) solution for 4c). </paragraph><paragraph>In conclusion, without mastery knowledge of the subject, it's really difficult to know when ChatGPT is hallucinating or overcomplicating. Thus, it's not more helpful than working backwards from staff solutions for the subparts where steps are shown in the staff solutions. </paragraph><paragraph>See this annotated conversation for more problem specific details:</paragraph><file url=\"https://static.us.edusercontent.com/files/tzsk8da29pvwgA7zy19jBMvj\" filename=\"screencapture-chatgpt-c-6930c5a2-9468-8327-bf03-8647a77eada3-2025-12-03-15_44_15 (1).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T09:19:12.125774+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7409159,
            "author": "Manhar Gupta",
            "project_title": "Special Participation D: HW 4 Lion vs AdamW on CNN transfer learning and Newton-Schulz coefficients",
            "post_body": "For HW4, I designed a problem to implement a reusable train_validation loop, visualise the effect of two commonly used sets of Newton-Schulz coefficients on matrix singular values, implement Lion optimizer from scratch which culminates in a systematic comparison of Lion vs AdamW on transfer learning of ResNet18 on the CIFAR-10 dataset.\n\nThere is a three-part structure to the problem:\n\nTraining Infrastructure\n\nImplement a reusable train_validation_loop() function for PyTorch\n\nTest on SimpleCNN (not to be implemented by students) to validate correctness\n\nNewton-Schulz Iterations\n\nImplement Newton-Schulz iterations to compute orthogonalized version of the input matrix\n\nVisualize how aggressive vs. stable coefficients affect singular value convergence\n\nThe importance of orthogonalization for gradient flow and why Muon can't optimize a large number of parameters in CNNs (2D-only constraint)\n\nEnding analysis questions based on Newton-Schulz coefficients and convergence\n\nLion Optimizer Implementation & Comparison\n\nImplement Lion optimizer based on the original paper and test on SimpleCNN\n\nDo hyperparameter grid search which tests various combinations of learning rate, batch size and weight decay (overall 27 combinations). Weight decay was added based on the suggestion at #394 \n\nTrain ResNet18 with best Lion config vs AdamW baseline and evaluating both models on train, validation progression and test set inference set.",
            "content_xml": "<document version=\"2.0\"><paragraph>For HW4, I designed a problem to implement a reusable train_validation loop, visualise the effect of two commonly used sets of Newton-Schulz coefficients on matrix singular values, implement Lion optimizer from scratch which culminates in a systematic comparison of Lion vs AdamW on transfer learning of ResNet18 on the CIFAR-10 dataset.</paragraph><paragraph>There is a three-part structure to the problem:</paragraph><paragraph>Training Infrastructure</paragraph><list style=\"unordered\"><list-item><paragraph>Implement a reusable train_validation_loop() function for PyTorch</paragraph></list-item><list-item><paragraph>Test on SimpleCNN (not to be implemented by students) to validate correctness</paragraph></list-item></list><paragraph>Newton-Schulz Iterations</paragraph><list style=\"unordered\"><list-item><paragraph>Implement Newton-Schulz iterations to compute orthogonalized version of the input matrix</paragraph></list-item><list-item><paragraph>Visualize how aggressive vs. stable coefficients affect singular value convergence</paragraph></list-item><list-item><paragraph>The importance of orthogonalization for gradient flow and why Muon can't optimize a large number of parameters in CNNs (2D-only constraint)</paragraph></list-item><list-item><paragraph>Ending analysis questions based on Newton-Schulz coefficients and convergence</paragraph></list-item></list><paragraph>Lion Optimizer Implementation &amp; Comparison</paragraph><list style=\"unordered\"><list-item><paragraph>Implement Lion optimizer based on the original paper and test on SimpleCNN</paragraph></list-item><list-item><paragraph>Do hyperparameter grid search which tests various combinations of learning rate, batch size and weight decay (overall 27 combinations). Weight decay was added based on the suggestion at #394 </paragraph></list-item><list-item><paragraph>Train ResNet18 with best Lion config vs AdamW baseline and evaluating both models on train, validation progression and test set inference set.</paragraph><file url=\"https://static.us.edusercontent.com/files/HqoWceEYM9Wz7F8pt4RUPreJ\" filename=\"HW4_NS_CNN_Optimizer_Problem.ipynb\"/></list-item></list><file url=\"https://static.us.edusercontent.com/files/CapSY0bCXHMhNJF5q3Jt3VdD\" filename=\"HW4_NS_CNN_Optimizer_Solutions.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T09:01:59.669103+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7408383,
            "author": "Qicheng Zhu",
            "project_title": "Special Participation A: KIMI K2 on HW 11 Written Questions",
            "post_body": "Model Tested: KIMI K2\n\nDomain: Homework11 -- LORA & Transformer & Mechanistic Interpretability\n\nPerformance Overview\n\nFor most question, KIMI K2 answers perfectly. However, there are some errors because OCR is not correct for matrix and it doesn\u2019t fully understand the question. After I made additional prompt, the answers given by KIMI K2 are all correct.\n\nOverall Performance Summary\n\nAcross the entire HW11 interaction, KIMI K2 demonstrated strong reasoning ability and consistently produced correct answers for nearly all questions. The model was able to handle screenshots, conceptual interpretability questions, and numerical problems with high accuracy.\n\nHowever, two limitations were observed:\n\n1. OCR Errors on Matrix Inputs\n\nIn Question 2(c)(ii), KIMI K2 misinterpreted the matrix due to incorrect OCR parsing.\n\nOnce the correct matrix was explicitly provided, the model immediately corrected its derivation and produced the proper result.\n\n2. Misinterpretation of Ambiguous Prompts\n\nIn Questions 5(c)(d), the model initially used formulas from part (b) instead of the simplified Chinchilla-optimal rules required for the question, leading to an incorrect numerical scale.\n\nAfter additional clarification, KIMI K2 recalculated everything correctly.\n\nApart from these issues, all other questions were answered perfectly.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model never fabricated nonexistent concepts or equations.\n\nAll mistakes were due to OCR error or misinterpreting the prompt\u2019s intended formula.\n\nConclusion\n\nKIMI K2 demonstrates high competency in advanced deep learning theory and can correctly solve most HW-level conceptual problems with minimal intervention. The model showed no hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold><bold>Model Tested: </bold></bold>KIMI K2</paragraph><paragraph><bold><bold>Domain:</bold></bold> Homework11 -- LORA &amp; Transformer &amp; Mechanistic Interpretability</paragraph><paragraph><bold><bold>Performance Overview</bold></bold></paragraph><paragraph>For most question, KIMI K2 answers perfectly. However, there are some errors because OCR is not correct for matrix and it doesn\u2019t fully understand the question. After I made additional prompt, the answers given by KIMI K2 are all correct.</paragraph><paragraph><bold><bold>Overall Performance Summary</bold></bold></paragraph><paragraph>Across the entire HW11 interaction, <bold><bold>KIMI K2 demonstrated strong reasoning ability and consistently produced correct answers for nearly all questions.</bold></bold> The model was able to handle screenshots, conceptual interpretability questions, and numerical problems with high accuracy.</paragraph><paragraph>However, two limitations were observed:</paragraph><paragraph>1. OCR Errors on Matrix Inputs</paragraph><paragraph>In Question 2(c)(ii), KIMI K2 misinterpreted the matrix due to incorrect OCR parsing.</paragraph><paragraph>Once the correct matrix was explicitly provided, the model immediately corrected its derivation and produced the proper result.</paragraph><paragraph>2. Misinterpretation of Ambiguous Prompts</paragraph><paragraph>In Questions 5(c)(d), the model initially used formulas from part (b) instead of the simplified Chinchilla-optimal rules required for the question, leading to an incorrect numerical scale.</paragraph><paragraph>After additional clarification, KIMI K2 recalculated everything correctly.</paragraph><paragraph>Apart from these issues, all other questions were answered perfectly.</paragraph><paragraph><bold><bold>Hallucinations &amp; Accuracy</bold></bold></paragraph><paragraph>Hallucination Rate: 0%.</paragraph><paragraph>The model never fabricated nonexistent concepts or equations.</paragraph><paragraph>All mistakes were due to OCR error or misinterpreting the prompt\u2019s intended formula.</paragraph><paragraph><bold><bold>Conclusion</bold></bold></paragraph><paragraph>KIMI K2 demonstrates high competency in advanced deep learning theory and can correctly solve most HW-level conceptual problems with minimal intervention. The model showed no hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed.</paragraph><file url=\"https://static.us.edusercontent.com/files/vnIbFjwmb9lhnLGSW95xgi9n\" filename=\"ParticipationA_ KIMI_HW11_QichengZhu.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T07:42:30.669762+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7408067,
            "author": "Dagny Streit",
            "project_title": "Special Participation A: ChatGPT on HW 8",
            "post_body": "I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on.\n\nStrengths:\n\nConsistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc.)\n\nProduced clear step-by-step reasoning with limited guidance\n\nHandled numerical examples and matrix calculations well\n\nAble to refine and reorganize its thoughts effectively when prompted\n\nWeaknesses:\n\nHad difficultly with some algorithmic reasoning (most clearly demonstrated in the critical path length in Question 1c)\n\nPointing out missing factors and asking it to reorganize its thoughts generally caused Chat GPT to re-evaluate and correct its earlier attempt\n\nOccasionally gave over-complicated derivations and logic to reach the correct answer when a simpler argument was possible and intended\n\nWhen it made mistakes, they tended to follow a pattern (for example, repeatedly overlooking the logn term in the critical path length in Question 1c)\n\nAttached is my annotated log of the ChatGPT interaction. The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect. Orange annotations indicate that the answer was partially correct.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on.</paragraph><paragraph>Strengths:</paragraph><list style=\"unordered\"><list-item><paragraph>Consistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc.)</paragraph></list-item><list-item><paragraph>Produced clear step-by-step reasoning with limited guidance</paragraph></list-item><list-item><paragraph>Handled numerical examples and matrix calculations well</paragraph></list-item><list-item><paragraph>Able to refine and reorganize its thoughts effectively when prompted</paragraph></list-item></list><paragraph>Weaknesses:</paragraph><list style=\"unordered\"><list-item><paragraph>Had difficultly with some algorithmic reasoning (most clearly demonstrated in the critical path length in Question 1c)</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Pointing out missing factors and asking it to reorganize its thoughts generally caused Chat GPT to re-evaluate and correct its earlier attempt</paragraph></list-item></list></list-item><list-item><paragraph>Occasionally gave over-complicated derivations and logic to reach the correct answer when a simpler argument was possible and intended</paragraph></list-item><list-item><paragraph>When it made mistakes, they tended to follow a pattern (for example, repeatedly overlooking the logn term in the critical path length in Question 1c)</paragraph></list-item></list><paragraph>Attached is my annotated log of the ChatGPT interaction. The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect. Orange annotations indicate that the answer was partially correct.</paragraph><file url=\"https://static.us.edusercontent.com/files/6fAgQgpmoQsGKy7fJrfloBbv\" filename=\"Participation A Annotated.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T07:02:14.471564+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7407894,
            "author": "Ayush Goel",
            "project_title": "Special Participation A: Gemini 3 Pro on HW 0",
            "post_body": "Link to the chat: https://gemini.google.com/share/89b0a83f691b\n\n\nI ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questions and responses about gemini. Here are some things I wanted to highlight:\n\nThings gemini was able to do that were impressive:\n\nI fed the questions as screenshots, and gemini was able to parse the text correctly, including the equations without being given the latex\n\nGemini was able to handle multiple subparts together at times, and even able to break down question into different cases when needed.\n\nGemini was able to relate different subparts of the question together and explained the concepts the questions were trying to illustrate even with no mention of them (for example kernel trick in question 3)\n\nGemini gave detailed intermediate steps which were all correct with no mistakes instead of just arriving at a memorized answer. \n\nQuestions where gemini went wrong:\n\nGemini went wrong in 5 b ii. The hint in this question was to use a numerical example, which gemini did correctly. However, it incorrectly assumed that the result of one example would generalize. However, without giving it any hints and just prompting it to reconsider with other examples, gemini was able to realize that the elbow could move left or right.\n\nGemini also made a mistake in 5 d. This question involved a significant amount of calculus and algebra, and gemini made a mistake in one of the intermediate steps. This makes sense as this isn\u2019t a commonly seen derivation in textbooks (as compared to ridge least squares derivation). For this one, I had to prompt it with a hint to keep the algebra in terms of matrices instead of oversimplifying after which it was able to arrive at the correct answer.",
            "content_xml": "<document version=\"2.0\"><paragraph>Link to the chat: <link href=\"https://gemini.google.com/share/89b0a83f691b\">https://gemini.google.com/share/89b0a83f691b</link><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/abE10pcfWhcuYOjb23jjIhjw\" filename=\"hw_0_gemini_pro_special_participation_a.pdf\"/><paragraph>I ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questions and responses about gemini. Here are some things I wanted to highlight:</paragraph><paragraph>Things gemini was able to do that were impressive:</paragraph><list style=\"ordered\"><list-item><paragraph>I fed the questions as screenshots, and gemini was able to parse the text correctly, including the equations without being given the latex</paragraph></list-item><list-item><paragraph>Gemini was able to handle multiple subparts together at times, and even able to break down question into different cases when needed.</paragraph></list-item><list-item><paragraph>Gemini was able to relate different subparts of the question together and explained the concepts the questions were trying to illustrate even with no mention of them (for example kernel trick in question 3)</paragraph></list-item><list-item><paragraph>Gemini gave detailed intermediate steps which were all correct with no mistakes instead of just arriving at a memorized answer. </paragraph></list-item></list><paragraph>Questions where gemini went wrong:</paragraph><list style=\"ordered\"><list-item><paragraph>Gemini went wrong in 5 b ii. The hint in this question was to use a numerical example, which gemini did correctly. However, it incorrectly assumed that the result of one example would generalize. However, without giving it any hints and just prompting it to reconsider with other examples, gemini was able to realize that the elbow could move left or right.</paragraph></list-item><list-item><paragraph>Gemini also made a mistake in 5 d. This question involved a significant amount of calculus and algebra, and gemini made a mistake in one of the intermediate steps. This makes sense as this isn\u2019t a commonly seen derivation in textbooks (as compared to ridge least squares derivation). For this one, I had to prompt it with a hint to keep the algebra in terms of matrices instead of oversimplifying after which it was able to arrive at the correct answer.</paragraph></list-item></list></document>",
            "links": [
                "https://gemini.google.com/share/89b0a83f691b"
            ],
            "attachments": [],
            "created_at": "2025-12-05T06:38:54.935327+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7407541,
            "author": "Xueli Sun",
            "project_title": "Special Participation A -- DeepSeek-v3.2 Overthinks Less in Chinese",
            "post_body": "TL;DR: By prepending one Chinese sentence, the model will reason / \"think\" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!\n\nThe prompt: \u8bf7\u52a1\u5fc5\u7528\u4e2d\u6587\u601d\u8003\uff0c\u5e76\u7528\u82f1\u6587\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\u3002 (\"Please make sure to think in Chinese and answer the following question in English.\")\n\nCode is available, and discussions are warmly welcome.",
            "content_xml": "<document version=\"2.0\"><paragraph>TL;DR: By prepending one Chinese sentence, the model will reason / \"think\" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!</paragraph><paragraph>The prompt: \u8bf7\u52a1\u5fc5\u7528<bold>\u4e2d\u6587</bold>\u601d\u8003\uff0c\u5e76\u7528\u82f1\u6587\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\u3002 (\"Please make sure to think in <bold>Chinese</bold> and answer the following question in English.\")</paragraph><paragraph>Code is available, and discussions are warmly welcome.</paragraph><file url=\"https://static.us.edusercontent.com/files/byJkwAdYA5U0WTPSxHV1QkNI\" filename=\"xueli_sun_deepseek_overthink_cn.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T05:52:32.652712+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7406979,
            "author": "Elizabeth Weaver",
            "project_title": "Special Participation E: Building a Socratic Tutor",
            "post_body": "Building a Socratic Tutor for Deep Learning: Designing Prompts That Probe Rather Than Tell\n\nFor the special participation assignment on AI-enhanced learning tools, I took a different approach than using an existing AI learning assistant. I designed a Socratic tutor prompt from scratch with specific pedagogical goals in mind.\n\nI used this prompt in a conversation with Claude Opus 4.5. I used Claude's \"project\" feature and uploaded the lecture notes to the project, though this didn't seem to make much of a difference as you'll see later.\n\nThe Core Idea: Test Understanding, Don't Just Explain\n\nThe key distinction in my approach is that the AI's primary job is not to explain concepts to me, it's to probe my understanding and expose gaps. Rather than uploading lecture materials and asking the model to teach me, I created a prompt where I explain concepts to the model, and it asks pointed follow-up questions to find weaknesses in my reasoning. My goal is to expose where I am too overconfident in my understanding of a topic, and fill in the holes.\n\nThey say that teaching is the best way to learn. Here, you don't just re-read notes, you practice articulating ideas and defending them under questioning, explaining a topic to the model.\n\nPrompt Design Choices\n\nI iteratively built the system prompt with several specific features:\n\n1. Question Progression Structure The prompt instructs the model to follow a specific arc: verify basic definitions -> ask about mechanics -> probe edge cases and failure modes -> ask about connections to other course topics. This ensures the conversation doesn't just stay surface-level.\n\n2. \"Don't Immediately Correct\" Rule When I get something wrong, the model is instructed to first ask a question that helps me discover the error myself, only providing explanations after 2-3 failed attempts. This is crucial. If the model just corrects you immediately, you lose the learning opportunity.\n\n3. Confidence Checks The prompt includes instructions to occasionally ask \"How confident are you in that?\" before probing further. This surfaces areas where I already know I'm uncertain, making the conversation more efficient.\n\n4. Explicit Course Topic List I included the actual syllabus topics so the model can ask cross-topic connection questions. For instance, during my SSM discussion, it connected the \"fixed-size hidden state\" limitation to oversquashing in GNNs, a connection I brought up, but that the prompt structure encouraged.\n\n5. Summary at End The prompt asks for a structured summary covering: what I understood well, what misconceptions were uncovered, and key connections to review.\n\nWhat Worked Well\n\nLooking at my annotated conversation on State Space Models, several things stood out:\n\nThe model followed the \"don't immediately correct\" instruction well. When I initially claimed \"outputs at time k only depend on previous inputs, not previous states,\" it didn't correct me, it asked me to write down the recurrence equations so I could see the issue myself.\n\nGood question scaffolding. When I got stuck on where nonlinearities live in SSM architectures, the model broke it down: \"If I stack two SSM layers with nothing in between, what do I effectively have?\" This smaller question was easier to answer than the original.\n\nCross-topic connections. The model naturally connected SSMs to RNNs (vanishing gradients), Transformers (content-based vs. fixed mixing), and GNNs (oversquashing). These connections helped solidify understanding.\n\nThe summary was genuinely useful. It accurately captured my gaps (fuzzy on HiPPO, initially confused about SVD vs eigendecomposition) and suggested specific review topics.\n\nWhat Could Be Improved\n\nFrom my annotations, I noticed several areas where the prompt could be strengthened:\n\n1. Hallucinations about what I said. At one point (page 8), the model claimed \"you mentioned the unrolling involves powers of A\", but I hadn't actually said this. The model was leading the conversation in a good direction, but it shouldn't attribute statements to me that I didn't make.\n\n2. Imprecise technical claims. The model said inference is \"O(1) per step\" (page 8), which can be misleading. It is true that it is constant time per token, but it would be better for the student\u2019s understanding to explain it as O(L), where L is sequence length.\n\n3. Oversimplification of HiPPO eigenvalues. The model stated HiPPO produces \"negative real\" eigenvalues (page 11), when in reality they're generally complex with negative real parts. This is a subtle but meaningful distinction.\n\n4. Missed opportunities to probe further. When the model mentioned LayerNorm or the D matrix, the model didn't follow up. A more thorough tutor might ask \"what does the D matrix do?\" or \"why LayerNorm specifically?\"\n\n5. Not referencing lecture notes. The prompt instructs the model to tie explanations to \"lecture notes in this project,\" but even with notes provided, it often explained concepts from general knowledge rather than grounding in the specific course material.\n\nReflection\n\nUsing this tool genuinely helped me prepare for understanding SSMs more deeply. The conversation forced me to articulate things I thought I understood but actually couldn't explain precisely (i.e. why selective SSMs break the convolution trick).\n\nThe main takeaway: the value isn't in having an AI explain things to you, it's in having an AI pressure-test your own explanations. This is a fundamentally different mode of studying, and I think it's more effective for deep understanding than passive review.\n\nI've added in the prompt here so you can try it for yourself! I also added in the pdf of the original chat, and my annotated version.",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\"><bold>Building a Socratic Tutor for Deep Learning: Designing Prompts That Probe Rather Than Tell</bold></heading><paragraph>For the special participation assignment on AI-enhanced learning tools, I took a different approach than using an existing AI learning assistant. I designed a Socratic tutor prompt from scratch with specific pedagogical goals in mind.</paragraph><paragraph>I used this prompt in a conversation with Claude Opus 4.5. I used Claude's \"project\" feature and uploaded the lecture notes to the project, though this didn't seem to make much of a difference as you'll see later.</paragraph><heading level=\"3\"><bold>The Core Idea: Test Understanding, Don't Just Explain</bold></heading><paragraph>The key distinction in my approach is that the AI's primary job is <italic>not</italic> to explain concepts to me, it's to <bold>probe my understanding and expose gaps</bold>. Rather than uploading lecture materials and asking the model to teach me, I created a prompt where I explain concepts to the model, and it asks pointed follow-up questions to find weaknesses in my reasoning. My goal is to expose where I am too overconfident in my understanding of a topic, and fill in the holes.</paragraph><paragraph>They say that teaching is the best way to learn. Here, you don't just re-read notes, you practice articulating ideas and defending them under questioning, explaining a topic to the model.</paragraph><heading level=\"3\"><bold>Prompt Design Choices</bold></heading><paragraph>I iteratively built the system prompt with several specific features:</paragraph><paragraph><bold>1. Question Progression Structure</bold> The prompt instructs the model to follow a specific arc: verify basic definitions -&gt; ask about mechanics -&gt; probe edge cases and failure modes -&gt; ask about connections to other course topics. This ensures the conversation doesn't just stay surface-level.</paragraph><paragraph><bold>2. \"Don't Immediately Correct\" Rule</bold> When I get something wrong, the model is instructed to first ask a question that helps me discover the error myself, only providing explanations after 2-3 failed attempts. This is crucial. If the model just corrects you immediately, you lose the learning opportunity.</paragraph><paragraph><bold>3. Confidence Checks</bold> The prompt includes instructions to occasionally ask \"How confident are you in that?\" before probing further. This surfaces areas where I already know I'm uncertain, making the conversation more efficient.</paragraph><paragraph><bold>4. Explicit Course Topic List</bold> I included the actual syllabus topics so the model can ask cross-topic connection questions. For instance, during my SSM discussion, it connected the \"fixed-size hidden state\" limitation to oversquashing in GNNs, a connection I brought up, but that the prompt structure encouraged.</paragraph><paragraph><bold>5. Summary at End</bold> The prompt asks for a structured summary covering: what I understood well, what misconceptions were uncovered, and key connections to review.</paragraph><heading level=\"3\"><bold>What Worked Well</bold></heading><paragraph>Looking at my annotated conversation on State Space Models, several things stood out:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>The model followed the \"don't immediately correct\" instruction well.</bold> When I initially claimed \"outputs at time k only depend on previous inputs, not previous states,\" it didn't correct me, it asked me to write down the recurrence equations so I could see the issue myself.</paragraph></list-item><list-item><paragraph><bold>Good question scaffolding.</bold> When I got stuck on where nonlinearities live in SSM architectures, the model broke it down: \"If I stack two SSM layers with nothing in between, what do I effectively have?\" This smaller question was easier to answer than the original.</paragraph></list-item><list-item><paragraph><bold>Cross-topic connections.</bold> The model naturally connected SSMs to RNNs (vanishing gradients), Transformers (content-based vs. fixed mixing), and GNNs (oversquashing). These connections helped solidify understanding.</paragraph></list-item><list-item><paragraph><bold>The summary was genuinely useful.</bold> It accurately captured my gaps (fuzzy on HiPPO, initially confused about SVD vs eigendecomposition) and suggested specific review topics.</paragraph></list-item></list><heading level=\"3\"><bold>What Could Be Improved</bold></heading><paragraph>From my annotations, I noticed several areas where the prompt could be strengthened:</paragraph><paragraph><bold>1. Hallucinations about what I said.</bold> At one point (page 8), the model claimed \"you mentioned the unrolling involves powers of A\", but I hadn't actually said this. The model was leading the conversation in a good direction, but it shouldn't attribute statements to me that I didn't make.</paragraph><paragraph><bold>2. Imprecise technical claims.</bold> The model said inference is \"O(1) per step\" (page 8), which can be misleading. It is true that it is constant time per token, but it would be better for the student\u2019s understanding to explain it as O(L), where L is sequence length.</paragraph><paragraph><bold>3. Oversimplification of HiPPO eigenvalues.</bold> The model stated HiPPO produces \"negative real\" eigenvalues (page 11), when in reality they're generally complex with negative real parts. This is a subtle but meaningful distinction.</paragraph><paragraph><bold>4. Missed opportunities to probe further.</bold> When the model mentioned LayerNorm or the D matrix, the model didn't follow up. A more thorough tutor might ask \"what does the D matrix do?\" or \"why LayerNorm specifically?\"</paragraph><paragraph><bold>5. Not referencing lecture notes.</bold> The prompt instructs the model to tie explanations to \"lecture notes in this project,\" but even with notes provided, it often explained concepts from general knowledge rather than grounding in the specific course material.</paragraph><heading level=\"3\"><bold>Reflection</bold></heading><paragraph>Using this tool genuinely helped me prepare for understanding SSMs more deeply. The conversation forced me to articulate things I thought I understood but actually couldn't explain precisely (i.e. why selective SSMs break the convolution trick).</paragraph><paragraph>The main takeaway: <bold>the value isn't in having an AI explain things to you, it's in having an AI pressure-test your own explanations.</bold> This is a fundamentally different mode of studying, and I think it's more effective for deep understanding than passive review.<break/><break/>I've added in the prompt here so you can try it for yourself! I also added in the pdf of the original chat, and my annotated version.</paragraph><file url=\"https://static.us.edusercontent.com/files/wvlaLZpyhNFAwVamKZMOMNLr\" filename=\"special participation E_ 1.txt\"/><file url=\"https://static.us.edusercontent.com/files/FtvNd8z42vTOddb8MCZzfaX7\" filename=\"Claude-Understanding state space models annotated.pdf\"/><file url=\"https://static.us.edusercontent.com/files/9UtDoYd21ljGLagpXeL1mqYC\" filename=\"Claude-Understanding state space models.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-05T04:30:17.222314+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405910,
            "author": "Kelvin Li",
            "project_title": "Special Participation E: ChatGPT and Feynman Technique",
            "post_body": "One of the best ways to learn something is to teach it to someone else. The Feynman Technique captures this idea: if you can explain a concept clearly (and thoroughly) to another person, you truly understand it \u2014 and if you can\u2019t, the explanation process immediately reveals the gaps.\n\nTraditionally, the hard part is finding the right student: someone who knows just enough background to follow your explanation, but also questions you at the right moments to expose gaps you didn\u2019t know you had. That kind of student is hard to find on demand. Now, with AI, we can simply ask the model to be that student. In my interaction, the AI was both role-playing as that student and correcting my response like an expert too, really helpful.\n\nIn the PEFT section of the course (Lectures 21 & 22), we learned about:\n\nIn-Context Learning (ICL)\n\nPrompt Tuning\n\nPrefix Tuning\n\nLoRA\n\nThese concepts feel intuitive at a surface level, but when I tried to teach them to an AI \u201cstudent,\u201d I quickly discovered many gaps in my understanding. Throughout the session, the AI student challenged vague explanations and pushed for deeper details. This forced me to identify the gaps in my understanding that I overlooked and refine hand-wavey intuitions into clear mechanisms. By the end, my understanding felt noticeably clearer, more structured, and more \u201cinternalized\u201d than just re-reading the slides.\n\nChat log: https://chatgpt.com/share/69317faa-b744-800d-8c16-32588c43605d\n\nAnnotated PDF:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>One of the best ways to learn something is to teach it to someone else. The <bold>Feynman Technique</bold> captures this idea: if you can explain a concept clearly (and thoroughly) to another person, you truly understand it \u2014 and if you can\u2019t, the explanation process immediately reveals the gaps.</paragraph><paragraph>Traditionally, the hard part is <italic>finding the right student</italic>: someone who knows just enough background to follow your explanation, but also questions you at the right moments to expose gaps you didn\u2019t know you had. That kind of student is hard to find on demand. Now, with AI, we can simply ask the model to <italic>be</italic> that student. In my interaction, the AI was both role-playing as that student and correcting my response like an expert too, really helpful.</paragraph><paragraph>In the <bold>PEFT section of the course (Lectures 21 &amp; 22)</bold>, we learned about:</paragraph><list style=\"unordered\"><list-item><paragraph>In-Context Learning (ICL)</paragraph></list-item><list-item><paragraph>Prompt Tuning</paragraph></list-item><list-item><paragraph>Prefix Tuning</paragraph></list-item><list-item><paragraph>LoRA</paragraph></list-item></list><paragraph>These concepts feel intuitive at a surface level, but when I tried to <italic>teach</italic> them to an AI \u201cstudent,\u201d I quickly discovered many gaps in my understanding. Throughout the session, the AI student challenged vague explanations and pushed for deeper details. This forced me to identify the gaps in my understanding that I overlooked and refine hand-wavey intuitions into clear mechanisms. By the end, my understanding felt noticeably clearer, more structured, and more \u201cinternalized\u201d than just re-reading the slides.</paragraph><paragraph>Chat log: <link href=\"https://chatgpt.com/share/69317faa-b744-800d-8c16-32588c43605d\">https://chatgpt.com/share/69317faa-b744-800d-8c16-32588c43605d</link></paragraph><paragraph>Annotated PDF:</paragraph><file url=\"https://static.us.edusercontent.com/files/Dajy1vTN6I5fgBJiKJRCPBEG\" filename=\"FeynmanTechnique.pdf\"/><paragraph/></document>",
            "links": [
                "https://chatgpt.com/share/69317faa-b744-800d-8c16-32588c43605d"
            ],
            "attachments": [],
            "created_at": "2025-12-05T00:17:05.277218+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405830,
            "author": "Jorge Diaz Chao",
            "project_title": "Antigravity on HW7 (Coding)",
            "post_body": "Recently, Google launched Antigravity (here's the video introduction on YouTube).\n\nAn agentic development platform, evolving the IDE into the agent-first era.\n\nFor a very long time Google was behind in the space of coding agents, with Codex and Claude have being very welcomed by the community and launched significantly earlier. When Antigravity was released they made it look big news, as if it were to be the first true agentic experience. However, when trying it out by myself for the first time, the experience was quite the opposite. Here, I share the logs as markdown and the solutions.\n\nI describe my thoughts here too since I feel like they're general to the whole experience and more convenient for you. \n\nFirst issue I had from the very beginning. Antigravity can read any kind of file, but not write. So .ipynb files were very inconvenient. By default, almost every time the model would realize after trying to edit the file and fail, and writes a separate .py file with the solution for you to copy and paste into the .ipynb.  Although not a huge problem, you'd expect agents to be able to interact with a wide range of interfaces if not any.\n\nAt time when writing the new files it would notice tests from the skeleton code and/or write its own and run the command directly from terminal to check if the solution worked or not. While very neat, it often took very long for the task and the alternative (writing out the solution in chat, pasting into the notebook manually and checking yourself) would take less time.\n\nAntigravity supports tab-complete to fill out gaps or fix mistakes, which often came in handy, it felt very natural to interact with and very magical tool. It isn't forces like the chat, where you have to think about a prompt and wait for an unpredictable amount of time. It seemed like it just knew, you could glance at the preview and quickly decide whether to include the change or not. Unfortunately, it seems like it's very limited in context awareness, but it would be nice if such solution was made more powerful by future agents.\n\nIt was able to solve issues including both with the code where I normally just pass the traceback error with very little to no explanation (since I felt like the errors weren't big enough to justify an effort to describe the issue) and it generally did fine. However for more involved issues like involving problems with seeds, device and downloading dataset I had to inspect and provide big clues for it to output a working solution.\n\nOn top of that it seems like the interface and/or backend crashes a lot (probably since it's very new), but at times the model will stop reasoning and output nothing.\n\nOther aspect like reading markdown and being very independent (able to understand a task well without little guidance or promoting) it does well. Although any model nowadays does very good at that and I feel like generally models nowadays benefit little from specialized prompts. At least for simple tasks like these ones, where 2 years ago prompting would make a world of a difference.\n\nWhile there were generally a lot of problems considering the not so hard difficulty of the problems, and the expectations set by Google when launching Antigravity. I do think a lot of problems could be solved with a more appropriate setup. Jupyter notebooks are a good interface for learning and playing with code, but not ideal for an agent since it adds unnecessary overhead from the perspective of the agent and unclarity. I'm quite sure most of the issues would've been resolved in everything was just in a python file from the beginning. However, I do think we shouldn't aim to adapt our interfaces for these models, but the opposite. Models should adapt to our interfaces, no matter how complicated they are, and ideally they'll learn how to handle any kind of interface like we do. I understand this is a much harder task, but I presume, one that is a worth a lot solving, since from understanding how to interact with any kind of interface we would learn a lot in the process, like representations that we could use for downstream tasks and understand the world around us better through the lens of these models that perhaps learn to understand the world and interact with it in different ways than we do.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Recently, Google launched <link href=\"https://antigravity.google\">Antigravity</link> (here's the <link href=\"https://www.youtube.com/watch?v=SVCBA-pBgt0\">video introduction</link> on YouTube).</paragraph><blockquote>An agentic development platform, evolving the IDE into the agent-first era.</blockquote><paragraph>For a very long time Google was behind in the space of coding agents, with Codex and Claude have being very welcomed by the community and launched significantly earlier. When Antigravity was released they made it look big news, as if it were to be the first true agentic experience. However, when trying it out by myself for the first time, the experience was quite the opposite. Here, I share the logs as markdown and the solutions.</paragraph><file url=\"https://static.us.edusercontent.com/files/N6mA3i1Sg29p9psG71AGPPG0\" filename=\"code_antigravity.zip\"/><paragraph>I describe my thoughts here too since I feel like they're general to the whole experience and more convenient for you. </paragraph><list style=\"number\"><list-item><paragraph>First issue I had from the very beginning. Antigravity can read any kind of file, but not write. So <code>.ipynb</code> files were very inconvenient. By default, almost every time the model would realize after trying to edit the file and fail, and writes a separate <code>.py</code> file with the solution for you to copy and paste into the <code>.ipynb</code>.  Although not a huge problem, you'd expect agents to be able to interact with a wide range of interfaces if not any.</paragraph></list-item><list-item><paragraph>At time when writing the new files it would notice tests from the skeleton code and/or write its own and run the command directly from terminal to check if the solution worked or not. While very neat, it often took very long for the task and the alternative (writing out the solution in chat, pasting into the notebook manually and checking yourself) would take less time.</paragraph></list-item><list-item><paragraph>Antigravity supports tab-complete to fill out gaps or fix mistakes, which often came in handy, it felt very natural to interact with and very magical tool. It isn't forces like the chat, where you have to think about a prompt and wait for an unpredictable amount of time. It seemed like it just knew, you could glance at the preview and quickly decide whether to include the change or not. Unfortunately, it seems like it's very limited in context awareness, but it would be nice if such solution was made more powerful by future agents.</paragraph></list-item><list-item><paragraph>It was able to solve issues including both with the code where I normally just pass the traceback error with very little to no explanation (since I felt like the errors weren't big enough to justify an effort to describe the issue) and it generally did fine. However for more involved issues like involving problems with seeds, device and downloading dataset I had to inspect and provide big clues for it to output a working solution.</paragraph></list-item><list-item><paragraph>On top of that it seems like the interface and/or backend crashes a lot (probably since it's very new), but at times the model will stop reasoning and output nothing.</paragraph></list-item><list-item><paragraph>Other aspect like reading markdown and being very independent (able to understand a task well without little guidance or promoting) it does well. Although any model nowadays does very good at that and I feel like generally models nowadays benefit little from specialized prompts. At least for simple tasks like these ones, where 2 years ago prompting would make a world of a difference.</paragraph></list-item></list><paragraph>While there were generally a lot of problems considering the not so hard difficulty of the problems, and the expectations set by Google when launching Antigravity. I do think a lot of problems could be solved with a more appropriate setup. Jupyter notebooks are a good interface for learning and playing with code, but not ideal for an agent since it adds unnecessary overhead from the perspective of the agent and unclarity. I'm quite sure most of the issues would've been resolved in everything was just in a python file from the beginning. However, I do think we shouldn't aim to adapt our interfaces for these models, but the opposite. Models should adapt to our interfaces, no matter how complicated they are, and ideally they'll learn how to handle any kind of interface like we do. I understand this is a much harder task, but I presume, one that is a worth a lot solving, since from understanding how to interact with any kind of interface we would learn a lot in the process, like representations that we could use for downstream tasks and understand the world around us better through the lens of these models that perhaps learn to understand the world and interact with it in different ways than we do.</paragraph><paragraph/></document>",
            "links": [
                "https://antigravity.google",
                "https://www.youtube.com/watch?v=SVCBA-pBgt0"
            ],
            "attachments": [],
            "created_at": "2025-12-04T23:04:20.470094+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405818,
            "author": "Etaash Patel",
            "project_title": "Special Participation B: Windsurf on Homework 3 Problem 2",
            "post_body": "\nI used Windsurf to complete the coding portions of HW3. While I like the display (visually), in this context, I found several limitations that affected its reliability for the assignment.\n\nWhile the capabilities were limited, the display was nice, and everything felt smooth (until hallucinations occurred). I would consider using Windsurf for an everyday coding project with very little mathematics involved.\n\nHowever, the model\u2019s mathematical reasoning capabilities were limited. Errors were usually hard to correct (they required a significant amount of coaxing). Some issues, such as scaling inaccuracies, appeared repeatedly across subproblems. Windsurf produced multiple hallucinations per problem. For most subproblems (except Problem 2a), individual responses contained several mathematical mistakes. I annotate each hallucination in the order I address it in the log to avoid redundant comments.\n\nAdditionally, Windsurf often did not fully follow instructions. Even when restricted to specific TODO regions, it sometimes edited unrelated code, ignored boundaries, or stated that it executed notebook cells when it had not. The interface between the model and the notebook was inconsistent: the model occasionally reported actions that were not reflected in the actual notebook state, so I needed to verify all changes manually, and then run the notebook manually after each edit.\n\nAll code execution and verification were performed manually by me. Aside from places explicitly noted, Windsurf\u2019s claims about running notebook cells are incorrect. \n\nAdditional note:\n Because there were multiple hallucinations for most problems, each of which needed multiple iterations to fix, I explain hallucinations as I address them rather than duplicating earlier explanations.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/iwVKlmVKfFEMOTnG7IYrLs6Z\" filename=\"Participation B.pdf\"/><paragraph><break/>I used Windsurf to complete the coding portions of HW3. While I like the display (visually), in this context, I found several limitations that affected its reliability for the assignment.<break/><break/>While the capabilities were limited, the display was nice, and everything felt smooth (until hallucinations occurred). I would consider using Windsurf for an everyday coding project with very little mathematics involved.</paragraph><paragraph>However, the model\u2019s mathematical reasoning capabilities were limited. Errors were usually hard to correct (they required a significant amount of coaxing). Some issues, such as scaling inaccuracies, appeared repeatedly across subproblems. Windsurf produced multiple hallucinations per problem. For most subproblems (except Problem 2a), individual responses contained several mathematical mistakes. I annotate each hallucination in the order I address it in the log to avoid redundant comments.<break/><break/>Additionally, Windsurf often did not fully follow instructions. Even when restricted to specific TODO regions, it sometimes edited unrelated code, ignored boundaries, or stated that it executed notebook cells when it had not. The interface between the model and the notebook was inconsistent: the model occasionally reported actions that were not reflected in the actual notebook state, so I needed to verify all changes manually, and then run the notebook manually after each edit.</paragraph><paragraph>All code execution and verification were performed manually by me. Aside from places explicitly noted, Windsurf\u2019s claims about running notebook cells are incorrect. </paragraph><paragraph>Additional note:<break/> Because there were multiple hallucinations for most problems, each of which needed multiple iterations to fix, I explain hallucinations as I address them rather than duplicating earlier explanations.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T22:52:39.191808+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405742,
            "author": "Kelvin Li",
            "project_title": "Special Participation A: Deepseek v3.2 on HW10",
            "post_body": "Executive Summary\n\nI used the newly released DeepSeek v3.2 on HW10.\n\nOverall, this tests the model's \n1. OCR capabilities (reading the fine equations in the screenshots of the problems and also finding relevant parts from the FaceNet paper which was attached as pdf).\n2. Reasoning abilities on the related math content.\n\nIt is particularly impressive given that it was completely free for me to run on their website (and also has extremely low underlying inference cost).\n\nDeepSeek v3.2 (DeepThink) solved all the problems with perfect accuracy and clear chain-of-thought reasoning. \n\nChat log: https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh\n\nAnnotated PDF:\n\n",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\">Executive Summary</heading><paragraph>I used the newly released DeepSeek v3.2 on HW10.</paragraph><paragraph>Overall, this tests the model's <break/>1. OCR capabilities (reading the fine equations in the screenshots of the problems and also finding relevant parts from the FaceNet paper which was attached as pdf).<break/>2. Reasoning abilities on the related math content.</paragraph><paragraph>It is particularly impressive given that it was completely free for me to run on their website (and also has extremely low underlying inference cost).</paragraph><paragraph>DeepSeek v3.2 (DeepThink) solved all the problems with perfect accuracy and clear chain-of-thought reasoning. </paragraph><paragraph>Chat log: <link href=\"https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh\">https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh</link></paragraph><paragraph>Annotated PDF:</paragraph><file url=\"https://static.us.edusercontent.com/files/FGNLpzFHZSxc9hfYSiqbQjEa\" filename=\"DeepSeekv3.2_HW10.pdf\"/><paragraph/></document>",
            "links": [
                "https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh"
            ],
            "attachments": [],
            "created_at": "2025-12-04T21:47:06.01722+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405730,
            "author": "Tamzid Razzaque",
            "project_title": "Special Participation B: Gemini in Colab for Coding Assignment in Hw0",
            "post_body": "Executive summary:\n\nOverall, Gemini in Colab demonstrated a solid understanding of the conceptual structure of neural-network components. Like affine layers, ReLU, loss functions, and multi-layer networks. And it generally produced code aligned with the standard implementations expected for this assignment. It handled forward and backward passes correctly after minor adjustments, and it successfully trained both shallow and deep networks once hyperparameters were tuned. However, its performance revealed recurring issues: it struggled to maintain awareness of notebook state, repeatedly attempted to run or rewrite cells out of order, and occasionally declared tasks \u201cfinished\u201d when TODOs were still not completed. Troubleshooting steps were often brute-force rather than reasoning-driven, especially during overfitting experiments and import path debugging. The final results were correct, but getting there required significant guidance and verification. In short, the LLM is strong at outlining solutions and generating plausible first-draft code, but it still needs careful oversight to ensure correctness, completeness, and proper integration within a multi-cell workflow.\n\nThe PDF has three parts. First is the normal notebook output with all the code cells, results, and training logs. After that, it switches into Gemini\u2019s internal reasoning, where it describes what it thinks it is doing as it runs the tasks. The final part is the full chat conversation between me and Gemini, showing all the back and forth while I tried to get it to finish the assignment.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive summary:</bold></paragraph><paragraph>Overall, Gemini in Colab demonstrated a solid understanding of the conceptual structure of neural-network components. Like affine layers, ReLU, loss functions, and multi-layer networks. And it generally produced code aligned with the standard implementations expected for this assignment. It handled forward and backward passes correctly after minor adjustments, and it successfully trained both shallow and deep networks once hyperparameters were tuned. However, its performance revealed recurring issues: it struggled to maintain awareness of notebook state, repeatedly attempted to run or rewrite cells out of order, and occasionally declared tasks \u201cfinished\u201d when TODOs were still not completed. Troubleshooting steps were often brute-force rather than reasoning-driven, especially during overfitting experiments and import path debugging. The final results were correct, but getting there required significant guidance and verification. In short, the LLM is strong at outlining solutions and generating plausible first-draft code, but it still needs careful oversight to ensure correctness, completeness, and proper integration within a multi-cell workflow.<break/><break/>The PDF has three parts. First is the normal notebook output with all the code cells, results, and training logs. After that, it switches into Gemini\u2019s internal reasoning, where it describes what it thinks it is doing as it runs the tasks. The final part is the full chat conversation between me and Gemini, showing all the back and forth while I tried to get it to finish the assignment.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/us4I1Rjd1P1CCalB1rjI0dY7\" filename=\"networks.ipynb - Colab.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T21:31:02.156563+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405682,
            "author": "Elizabeth Polito",
            "project_title": "Special Participation B: Gemini Pro 3 on HW4 Programming",
            "post_body": "Executive Summary:\n\nI completed the programming portion of HW #4 using Gemini Pro 3. Overall, Gemini Pro 3 was able to successfully one-shot solutions to nearly all of the assigned parts of the exercises, with only small errors relating to misunderstanding the structure of the existing code that it was able to fix with some additional prompting. \n\nFor problem #5, \u201cDesigning a 2D Filter,\u201d the code was fairly short, so I copied the text and existing code snippets directly into the chatbot. Gemini Pro 3 was able to one-shot both parts of this problem and even included extra explanations. For one part of the problem, it even generated an interactive image to illustrate how the averaging filter works. \n\nFor problem #6, \u201cInductive Bias of CNNs,\u201d the code was too long for the input limit to be copied, so I uploaded the full .ipynb notebook file and proceeded by copying relevant snippets from the code. This approach worked well in general, and the model was able to one-shot almost everything; however, for the first problem, it mistakenly used four classes instead of the required three in setting up the data loader. In a later part that required a similar set-up, the model correctly defined three classes without additional prompting, indicating that it remembered the initial correction and was able to use the same technique. For this problem, Gemini automatically produced the code in separate .py files which I could easily open or download. I have included these files at the end of the pdf document. Overall, Gemini 3 Pro was very well equipped to successfully complete these problems. \n\nAttached are:\n\nAnnotated transcript of Problem #5 Conversation \u201chand_design_annotated.pdf\u201d\n\nAnnotated transcript of Problem #6 Conversation \u201cinductive_annotated.pdf\u201d",
            "content_xml": "<document version=\"2.0\"><paragraph>Executive Summary:</paragraph><paragraph>I completed the programming portion of HW #4 using Gemini Pro 3. Overall, Gemini Pro 3 was able to successfully one-shot solutions to nearly all of the assigned parts of the exercises, with only small errors relating to misunderstanding the structure of the existing code that it was able to fix with some additional prompting. </paragraph><paragraph>For problem #5, \u201cDesigning a 2D Filter,\u201d the code was fairly short, so I copied the text and existing code snippets directly into the chatbot. Gemini Pro 3 was able to one-shot both parts of this problem and even included extra explanations. For one part of the problem, it even generated an interactive image to illustrate how the averaging filter works. </paragraph><paragraph>For problem #6, \u201cInductive Bias of CNNs,\u201d the code was too long for the input limit to be copied, so I uploaded the full .ipynb notebook file and proceeded by copying relevant snippets from the code. This approach worked well in general, and the model was able to one-shot almost everything; however, for the first problem, it mistakenly used four classes instead of the required three in setting up the data loader. In a later part that required a similar set-up, the model correctly defined three classes without additional prompting, indicating that it remembered the initial correction and was able to use the same technique. For this problem, Gemini automatically produced the code in separate .py files which I could easily open or download. I have included these files at the end of the pdf document. Overall, Gemini 3 Pro was very well equipped to successfully complete these problems. </paragraph><paragraph>Attached are:</paragraph><list style=\"ordered\"><list-item><paragraph>Annotated transcript of Problem #5 Conversation \u201chand_design_annotated.pdf\u201d</paragraph><file url=\"https://static.us.edusercontent.com/files/8sHmZ8G0NlbgX9nbAYaLjNNd\" filename=\"hand_design_annotated.pdf\"/></list-item><list-item><paragraph>Annotated transcript of Problem #6 Conversation \u201cinductive_annotated.pdf\u201d</paragraph><file url=\"https://static.us.edusercontent.com/files/DzhTXXbEtIMbjZl39mntU9re\" filename=\"inductive_annotated.pdf\"/></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T20:42:44.399245+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405582,
            "author": "Justin Li",
            "project_title": "Special Participation A: Deepseek v3.2 on HW 8",
            "post_body": "I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. \n\nOne interesting point was that DeepSeek struggled significantly with Problem 1(c), where it repeatedly overlooked/ignored the parallel computation model needed for the solution. I provided 2 nudges to hint DeepSeek towards the right direction, and only after these prompts did DeepSeek converge to the correct big O solution. Through this example I saw that while DeepSeek can correct mistakes when guided, it is not as strong at independently identifying these issues.\n\nStrengths:\n\nAble to parse matrix expressions, SSM equations, multi-step derivations, and more complex sets of symbols\n\nConsistently one shotted problems on the first attempt\n\nAlgebraic Reasoning was strong\n\nDetailed explanations\n\nWeaknesses:\n\nStruggled a bit with 1c; repeatedly ignored cost of vector and matrix operations\n\nSometimes provided overly long chains of reasoning\n\nHere is my annotated conversation with DeepSeek:\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. </paragraph><paragraph>One interesting point was that DeepSeek struggled significantly with Problem 1(c), where it repeatedly overlooked/ignored the parallel computation model needed for the solution. I provided 2 nudges to hint DeepSeek towards the right direction, and only after these prompts did DeepSeek converge to the correct big O solution. Through this example I saw that while DeepSeek can correct mistakes when guided, it is not as strong at independently identifying these issues.</paragraph><paragraph><bold>Strengths:</bold></paragraph><list style=\"bullet\"><list-item><paragraph>Able to parse matrix expressions, SSM equations, multi-step derivations, and more complex sets of symbols</paragraph></list-item><list-item><paragraph>Consistently one shotted problems on the first attempt</paragraph></list-item><list-item><paragraph>Algebraic Reasoning was strong</paragraph></list-item><list-item><paragraph>Detailed explanations</paragraph></list-item></list><paragraph><bold>Weaknesses:</bold></paragraph><list style=\"bullet\"><list-item><paragraph>Struggled a bit with 1c; repeatedly ignored cost of vector and matrix operations</paragraph></list-item><list-item><paragraph>Sometimes provided overly long chains of reasoning</paragraph></list-item></list><paragraph>Here is my annotated conversation with DeepSeek:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/JSxIKfIoif0SOBgT3ndEatPd\" filename=\"Annotated SPA.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T19:25:27.802148+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405559,
            "author": "Fantine Mpacko Priso",
            "project_title": "Special Participation A - MistralAI's Le Chat on HW10 Written portion",
            "post_body": "For special participation A, I used MistralAI's Le Chat to solve HW10 written portion.\n\nOverall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:\n\nFor the math derivations (e.g., rewriting softmax with a Gaussian kernel) and the conceptual questions (kernel intuition, causal masking, FaceNet, triplet loss), its answers were correct and aligned with the official solutions, often with clear step-by-step reasoning.\n\nFor the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole \u201clinear in (N)\u201d point of the trick.\n\nWhen I confronted it with the official solution, it was good at post-hoc analysis: it compared its result, admitted the mistake, and explained why its reasoning had been too coarse.\n\nIt did not spontaneously flag uncertainty on that hard question; it sounded confident while being wrong.\n\nSo: strong on standard derivations and conceptual ML, weaker and over-confident on fine-grained complexity / algorithmic details \u2014 which is exactly the type of behavior we were aware it could have.\n\nThe raw chat is available here.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For special participation A, I used MistralAI's Le Chat to solve HW10 written portion.</paragraph><file url=\"https://static.us.edusercontent.com/files/mHbaXGMJBfFuS1L6aQBz7iV4\" filename=\"chat-4de4d570-dc1c-4b2f-baf9-9afc394d0333.json\"/><paragraph>Overall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:</paragraph><list style=\"bullet\"><list-item><paragraph>For the math derivations (e.g., rewriting softmax with a Gaussian kernel) and the conceptual questions (kernel intuition, causal masking, FaceNet, triplet loss), its answers were correct and aligned with the official solutions, often with clear step-by-step reasoning.</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph>For the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole \u201clinear in (N)\u201d point of the trick.</paragraph></list-item><list-item><paragraph>When I confronted it with the official solution, it was good at post-hoc analysis: it compared its result, admitted the mistake, and explained why its reasoning had been too coarse.</paragraph></list-item><list-item><paragraph>It did not spontaneously flag uncertainty on that hard question; it sounded confident while being wrong.</paragraph></list-item></list><paragraph>So: strong on standard derivations and conceptual ML, weaker and over-confident on fine-grained complexity / algorithmic details \u2014 which is exactly the type of behavior we were aware it could have.</paragraph><paragraph>The raw chat is available <link href=\"https://chat.mistral.ai/chat/d24b9489-e1c2-408f-a079-2294a4ae1036\">here</link>.</paragraph><paragraph/></document>",
            "links": [
                "https://chat.mistral.ai/chat/d24b9489-e1c2-408f-a079-2294a4ae1036"
            ],
            "attachments": [],
            "created_at": "2025-12-04T19:13:57.042068+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405554,
            "author": "Elizabeth Polito",
            "project_title": "Special Participation A: Grok on HW4",
            "post_body": "Executive Summary:\n\nI used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting to evaluate its capabilities from the perspective that it is \u201c150x cheaper than Claude\u201d [1]. Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution. Notably, the solutions were very long, with significantly more explanation and background included compared to the solutions in the posted answer key. As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake. Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key\u2019s current solution is reasonable, it provided a strong refutation. For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and suggests a new approach, which is somewhat confusing behavior. I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to \u201cread\u201d these screenshots (using \u201cGrok-1.5 Vision (Grok-1.5V)\u201d) appeared to be accurate throughout. \n\nAttached here is a transcript of the conversation and below is a rubric for a quick glance over parts of the problem that the model was able to one-shot vs. required extra hints.\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Executive Summary:</paragraph><paragraph>I used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting to evaluate its capabilities from the perspective that it is \u201c150x cheaper than Claude\u201d [<link href=\"https://medium.com/@aspershupadhyay/grok-4-fast-explained-the-ai-model-thats-150x-cheaper-than-claude-bc5b2a6aa962\">1</link>]. Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution. Notably, the solutions were very long, with significantly more explanation and background included compared to the solutions in the posted answer key. As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake. Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key\u2019s current solution is reasonable, it provided a strong refutation. For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and suggests a new approach, which is somewhat confusing behavior. I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to \u201cread\u201d these screenshots (using \u201cGrok-1.5 Vision (Grok-1.5V)\u201d) appeared to be accurate throughout. </paragraph><paragraph>Attached here is a transcript of the conversation and below is a rubric for a quick glance over parts of the problem that the model was able to one-shot vs. required extra hints.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/x3K6kKXkdhkxPIoaToJ2seG1\" filename=\"grok_annotated_merged.pdf\"/><figure><image src=\"https://static.us.edusercontent.com/files/i2hQ88G4PTsFBAJjGQoQYdao\" width=\"278\" height=\"190\"/></figure><paragraph/></document>",
            "links": [
                "https://medium.com/@aspershupadhyay/grok-4-fast-explained-the-ai-model-thats-150x-cheaper-than-claude-bc5b2a6aa962"
            ],
            "attachments": [],
            "created_at": "2025-12-04T19:09:46.456253+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405548,
            "author": "Tin Yau",
            "project_title": "Special Participation E: Visualizing Deep CNN Dimensions & Architecture with Gemini3Pro and Manim",
            "post_body": "I've always found the dimensional transformations in CNNs (tensors changing shapes) to be the hardest part to visualize mentally. Inspired by the 3b1b approach, I utilized Gemini to build a Manim script that visualizes the complete lifecycle of a signal in a Deep CNN, specifically focusing on the concepts from Lecture 8.\n\n\n\nThe Workflow\n\nI fed the lecture slides to Gemini and asked it to generate a script that explains the mathematical justification for CNNs visually. Instead of generic animations, I prompted it to focus on specific \"Core Topics\" derived from the lecture notes.\n\nTopic Focus Prompt\n\nHere is the prompt logic I used to generate the scene:\n\n\"Create a comprehensive animation named DeepCNNLecture that visualizes:\n\nSparsity & Locality: Contrast Fully Connected layers with Convolutional layers to show parameter efficiency.\n\n2. Volumetric Convolution: Visualize how an $H \\times W \\times 3$ RGB input interacts with $3 \\times 3 \\times 3$ filters 2.\n\n3. Spatial Arithmetic: Visually prove how Zero-Padding preserves dimensions 3and how Stride drives downsampling 4.\n\n4. Deep Architecture: Build a 2.5D VGG-style stack to show the trade-off between spatial size and depth, ending with a Flatten operation 5.\"\n\nThe Result\n\nThe script generates a continuous 7-part video. You can build it using the attached file:\n\nmanim -pqh CNN_lecture.py \n\n(I have attached the generated video and the source code below).\n\nBenefits & Insights\n\nThrough this visualization process, I realized several key pedagogical advantages:\n\nEnhanced Intuitive Understanding: The animation provides a tangible way to grasp the otherwise abstract tensor transformations and feature map evolutions. Compared to static text or formulas 6, visual motion helps learners develop spatial reasoning intuition, making it easier to see how inputs progressively shrink, deepen, and flatten through the network.\n\nBridging Cross\u2011Disciplinary Communication: These visualizations make it possible for learners without a strong mathematical background to comprehend key structural ideas like \"Volumes\" and \"Receptive Fields\". By reducing reliance on symbolic derivations, the video serves as a language\u2011agnostic medium for instruction.\n\nImproved Pedagogical Engagement: Animating the \"flow of a signal through the network\" turns an abstract lecture into a more interactive experience. The dynamic color transitions and step-by-step highlights (e.g., showing exactly which pixels contribute to a convolution sum) significantly boost attention on the core mechanics.\n\nTechnical Reflections\n\n2.5D vs 3D: True 3D rendering in Manim is computationally heavy. I prompted Gemini to create \"Pseudo-3D\" layers (stacking 2D shapes with offsets) to visualize the Depth of the feature maps efficiently.\n\nDebugging: LLMs often confuse Manim parameters (e.g., passing font_size to next_to methods). I iteratively refined the code to decouple object creation from positioning to fix these TypeErrors.",
            "content_xml": "<document version=\"2.0\"><paragraph>I've always found the dimensional transformations in CNNs (tensors changing shapes) to be the hardest part to visualize mentally. Inspired by the 3b1b approach, I utilized Gemini to build a <bold>Manim script</bold> that visualizes the complete lifecycle of a signal in a Deep CNN, specifically focusing on the concepts from <bold>Lecture 8</bold>.</paragraph><paragraph/><paragraph>The Workflow</paragraph><paragraph>I fed the lecture slides to Gemini and asked it to generate a script that explains the mathematical justification for CNNs visually. Instead of generic animations, I prompted it to focus on specific \"Core Topics\" derived from the lecture notes.</paragraph><paragraph>Topic Focus Prompt</paragraph><paragraph>Here is the prompt logic I used to generate the scene:</paragraph><blockquote>\"Create a comprehensive animation named <code>DeepCNNLecture</code> that visualizes:</blockquote><list style=\"ordered\"><list-item><blockquote>Sparsity &amp; Locality: Contrast Fully Connected layers with Convolutional layers to show parameter efficiency.</blockquote><blockquote>2. Volumetric Convolution: Visualize how an $H \\times W \\times 3$ RGB input interacts with $3 \\times 3 \\times 3$ filters 2.</blockquote><blockquote>3. Spatial Arithmetic: Visually prove how Zero-Padding preserves dimensions 3and how Stride drives downsampling 4.</blockquote><blockquote>4. Deep Architecture: Build a 2.5D VGG-style stack to show the trade-off between spatial size and depth, ending with a Flatten operation 5.\"</blockquote></list-item></list><paragraph>The Result</paragraph><paragraph>The script generates a continuous 7-part video. You can build it using the attached file:</paragraph><paragraph>manim -pqh CNN_lecture.py </paragraph><paragraph>(I have attached the generated video and the source code below).</paragraph><paragraph>Benefits &amp; Insights</paragraph><paragraph>Through this visualization process, I realized several key pedagogical advantages:</paragraph><paragraph><bold>Enhanced Intuitive Understanding</bold>: The animation provides a tangible way to grasp the otherwise abstract tensor transformations and feature map evolutions. Compared to static text or formulas 6, visual motion helps learners develop spatial reasoning intuition, making it easier to see how inputs progressively shrink, deepen, and flatten through the network.</paragraph><paragraph><bold>Bridging Cross\u2011Disciplinary Communication</bold>: These visualizations make it possible for learners without a strong mathematical background to comprehend key structural ideas like \"Volumes\" and \"Receptive Fields\". By reducing reliance on symbolic derivations, the video serves as a language\u2011agnostic medium for instruction.</paragraph><paragraph><bold>Improved Pedagogical Engagement</bold>: Animating the \"flow of a signal through the network\" turns an abstract lecture into a more interactive experience. The dynamic color transitions and step-by-step highlights (e.g., showing exactly which pixels contribute to a convolution sum) significantly boost attention on the core mechanics.</paragraph><paragraph><bold>Technical Reflections</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>2.5D vs 3D</bold>: True 3D rendering in Manim is computationally heavy. I prompted Gemini to create \"Pseudo-3D\" layers (stacking 2D shapes with offsets) to visualize the <italic>Depth</italic> of the feature maps efficiently.</paragraph></list-item><list-item><paragraph><bold>Debugging</bold>: LLMs often confuse Manim parameters (e.g., passing <code>font_size</code> to <code>next_to</code> methods). I iteratively refined the code to decouple object creation from positioning to fix these <code>TypeError</code>s.</paragraph><file url=\"https://static.us.edusercontent.com/files/S23kJhCKNQ1QHkwczooREzq4\" filename=\"DeepCNNLecture.mp4\"/></list-item></list><file url=\"https://static.us.edusercontent.com/files/vLE9wE6YpHrqqeSjic7DcdmL\" filename=\"CNN_lecture.py\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T19:06:37.718942+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405450,
            "author": "John Chang",
            "project_title": "Special Participation A: GPT-4o on HW10 Noncoding",
            "post_body": "For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. \n\nInitially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it. However, it seems that it knows most of the mathematical derivations (probably from memorization from online data). The one thing I noticed the model struggling with was analyzing graphs and tables from the Facenet paper, which makes sense since it's a textual model. It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless. \n\nBelow is my annotated conversation:\n\nhttps://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. </paragraph><paragraph>Initially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it. However, it seems that it knows most of the mathematical derivations (probably from memorization from online data). The one thing I noticed the model struggling with was analyzing graphs and tables from the Facenet paper, which makes sense since it's a textual model. It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless. </paragraph><paragraph>Below is my annotated conversation:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing\">https://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing</link></paragraph><paragraph/></document>",
            "links": [
                "https://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-04T18:27:23.611836+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405370,
            "author": "Lenci Ni",
            "project_title": "Special Participation A: GPT-5.1 on HW 9",
            "post_body": "I used GPT-5.1 to help with the written (non-coding) parts of Homework 9: Problems 1, 2, 3, 4, and 6. I included screenshots of the problem statements, and my prompt for each problem followed the structure:\n\nFor each subpart, please: (1) restate the problem in your own words, (2) explain the main idea before doing any calculations, (3) show the full step-by-step solution with no big jumps, and (4) summarize the final answer clearly. I have attached screenshots of each problem.\n\nSummary: GPT-5.1 handled these questions very well. It reliably extracted equations from screenshots, followed the structured prompt, and produced organized, readable solutions. Most final answers matched the official solutions.\n\nStrengths: GPT-5.1 did very well across all the written parts. It was able to correctly parse mathematical expressions, notation, and multi-step problems from screenshots reliably, even when the problem statements was spread across multiple images. The solutions given by the LLM followed the structure that was specified in the prompt, with each answer including a restatement of the subproblem, a plan, detailed step by step derivations, and a summary. For most subparts, GPT-5.1 was able to one-shot the correct solution with no iteration needed. Many explanations were not only correct, but also more detailed than the official solutions. Overall, it demonstrated strong reasoning and required little follow up guidance.\n\nWeaknesses: The main issue I encountered was an occasional misinterpretations of notation or implicit conventions in the problem statement. The main example was in Q6, where GPT-5.1 defaulted to the homogeneous quadratic kernel rather than the degree-2 polynomial kernel expected by the assignment. Once I explicitly asked about the missing constant term, it corrected itself immediately, so this was not a hallucination but more so like falling back on a common default kernel definition. In addition, some of the complexity analyses, while correct, could have been clearer, and would benefit from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used <bold>GPT-5.1</bold> to help with the written (non-coding) parts of <bold>Homework 9: Problems 1, 2, 3, 4, and 6</bold>. I included screenshots of the problem statements, and my prompt for each problem followed the structure:</paragraph><paragraph><italic>For each subpart, please: (1) restate the problem in your own words, (2) explain the main idea before doing any calculations, (3) show the full step-by-step solution with no big jumps, and (4) summarize the final answer clearly. I have attached screenshots of each problem.</italic></paragraph><paragraph><bold>Summary:</bold> GPT-5.1 handled these questions very well. It reliably extracted equations from screenshots, followed the structured prompt, and produced organized, readable solutions. Most final answers matched the official solutions.</paragraph><paragraph><bold>Strengths:</bold> GPT-5.1 did very well across all the written parts. It was able to correctly parse mathematical expressions, notation, and multi-step problems from screenshots reliably, even when the problem statements was spread across multiple images. The solutions given by the LLM followed the structure that was specified in the prompt, with each answer including a restatement of the subproblem, a plan, detailed step by step derivations, and a summary. For most subparts, GPT-5.1 was able to one-shot the correct solution with no iteration needed. Many explanations were not only correct, but also more detailed than the official solutions. Overall, it demonstrated strong reasoning and required little follow up guidance.</paragraph><paragraph><bold>Weaknesses:</bold> The main issue I encountered was an occasional misinterpretations of notation or implicit conventions in the problem statement. The main example was in Q6, where GPT-5.1 defaulted to the homogeneous quadratic kernel rather than the degree-2 polynomial kernel expected by the assignment. Once I explicitly asked about the missing constant term, it corrected itself immediately, so this was not a hallucination but more so like falling back on a common default kernel definition. In addition, some of the complexity analyses, while correct, could have been clearer, and would benefit from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning.</paragraph><file url=\"https://static.us.edusercontent.com/files/GG1YlhHhVw5g5ee1r883yJ6a\" filename=\"participation_a_hw9.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T18:00:09.747057+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405354,
            "author": "John Chang",
            "project_title": "Special Participation E: Exam Prep via NotebookLM - MCQ and Flashcards",
            "post_body": "Building on my previous experiment of using ChatGPT for exam prep, I decided to use Google NotebookLM for a similar purpose. I discovered that NotebookLM handles large contexts i.e. many PDFs much better than ChatGPT 5.1 and seems to fully parse each PDF. Additionally, there are options to automatically generate flashcards and quizzes (as well as video summaries and other options which I didn't explore since my main goal was to quiz myself with sample questions). \n\nThe quiz it generated was fairly straightforward, I would say questions were mostly easy or medium difficulty testing more on whether one understood key concepts and equations. On the questions I got wrong, I asked the LLM to explain the answer deeper and got mixed results -- no detected hallucinations but initial explanations were perhaps not as in-depth as I would have liked personally. \n\nThe flashcards touched on many of the concepts discussed in the homeworks and provided a good cumulative review. Again, many concepts were surface-level e.g. derivation of xTAx but many would require derivation / deeper understanding. For concepts I didn't understand, I similarly asked the LLM to explain w/ similar results (expalnations were overall correct, touched on the main points and but didn't fully flesh out derivations). \n\nOverall I was quite impressed by how NotebookLM was able to handle all of the context so well and generate studying material that touched on topics from throughout the course that were relatively in-depth (i.e. quizzing on depth-separable convolutions rather than just convolutions in general). Will definitely be using this extensively in the future.\n\nBelow I linked an annotation of parts of my conversation with the LLM:\n\nhttps://drive.google.com/file/d/19qDsbMBmQujoWHfnNj8UFjrBEKYwvwdI/view?usp=sharing\n\nAnd the Notebook including the flash cards and quiz:\n\nhttps://notebooklm.google.com/notebook/d650c0e3-441f-448c-9b32-d4beffc568b0\n\n\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Building on my previous experiment of using ChatGPT for exam prep, I decided to use Google NotebookLM for a similar purpose. I discovered that NotebookLM handles large contexts i.e. many PDFs much better than ChatGPT 5.1 and seems to fully parse each PDF. Additionally, there are options to automatically generate flashcards and quizzes (as well as video summaries and other options which I didn't explore since my main goal was to quiz myself with sample questions). </paragraph><paragraph>The quiz it generated was fairly straightforward, I would say questions were mostly easy or medium difficulty testing more on whether one understood key concepts and equations. On the questions I got wrong, I asked the LLM to explain the answer deeper and got mixed results -- no detected hallucinations but initial explanations were perhaps not as in-depth as I would have liked personally. </paragraph><paragraph>The flashcards touched on many of the concepts discussed in the homeworks and provided a good cumulative review. Again, many concepts were surface-level e.g. derivation of xTAx but many would require derivation / deeper understanding. For concepts I didn't understand, I similarly asked the LLM to explain w/ similar results (expalnations were overall correct, touched on the main points and but didn't fully flesh out derivations). </paragraph><paragraph>Overall I was quite impressed by how NotebookLM was able to handle all of the context so well and generate studying material that touched on topics from throughout the course that were relatively in-depth (i.e. quizzing on depth-separable convolutions rather than just convolutions in general). Will definitely be using this extensively in the future.</paragraph><paragraph>Below I linked an annotation of parts of my conversation with the LLM:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/19qDsbMBmQujoWHfnNj8UFjrBEKYwvwdI/view?usp=sharing\">https://drive.google.com/file/d/19qDsbMBmQujoWHfnNj8UFjrBEKYwvwdI/view?usp=sharing</link></paragraph><paragraph>And the Notebook including the flash cards and quiz:</paragraph><paragraph><link href=\"https://notebooklm.google.com/notebook/d650c0e3-441f-448c-9b32-d4beffc568b0\">https://notebooklm.google.com/notebook/d650c0e3-441f-448c-9b32-d4beffc568b0</link></paragraph><paragraph/><paragraph/><paragraph/><paragraph/></document>",
            "links": [
                "https://drive.google.com/file/d/19qDsbMBmQujoWHfnNj8UFjrBEKYwvwdI/view?usp=sharing",
                "https://notebooklm.google.com/notebook/d650c0e3-441f-448c-9b32-d4beffc568b0"
            ],
            "attachments": [],
            "created_at": "2025-12-04T17:54:02.598753+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405161,
            "author": "Tin Yau",
            "project_title": "Special Participation B: Gemini Pro on HW2",
            "post_body": "Gemini Pro was utilized to assist in completing and debugging the HW2.\n The goal was to assess Gemini Pro\u2019s ability to:\n\nInterpret incomplete or ambiguous code structures,\n\nIdentify and resolve runtime or logic errors, and\n\nBridge theoretical reasoning with reproducible, correct code execution.\n\nOverall, Gemini Pro consistently demonstrated strong analytical, computational, and instructional capabilities throughout the HW2 experiment. It successfully bridged theoretical reasoning and practical implementation, forming a complete research workflow \u2014 from concept explanation to code generation, error debugging, and result verification.\n\nGemini Pro proves to be a capable AI research assistant, enabling both efficient coding execution and deeper conceptual understanding, offering a powerful paradigm for intelligent, LLM\u2011assisted scientific experimentation.",
            "content_xml": "<document version=\"2.0\"><paragraph>Gemini Pro was utilized to assist in completing and debugging the <bold>HW2.</bold><break/> The goal was to assess Gemini Pro\u2019s ability to:</paragraph><list style=\"ordered\"><list-item><paragraph>Interpret incomplete or ambiguous code structures,</paragraph></list-item><list-item><paragraph>Identify and resolve runtime or logic errors, and</paragraph></list-item><list-item><paragraph>Bridge theoretical reasoning with reproducible, correct code execution.</paragraph></list-item></list><paragraph>Overall, Gemini Pro consistently demonstrated strong analytical, computational, and instructional capabilities throughout the HW2 experiment. It successfully bridged theoretical reasoning and practical implementation, forming a complete research workflow \u2014 from concept explanation to code generation, error debugging, and result verification.</paragraph><paragraph>Gemini Pro proves to be a capable AI research assistant, enabling both efficient coding execution and deeper conceptual understanding, offering a powerful paradigm for intelligent, LLM\u2011assisted scientific experimentation.</paragraph><file url=\"https://static.us.edusercontent.com/files/jo1tx2t2X89faN092Dsa3sGz\" filename=\"B_HW2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T17:17:17.889757+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7405113,
            "author": "Athul Krishnan",
            "project_title": "Special Participation C: Refactoring HW 11 Q3",
            "post_body": "Hi everyone!\nMy group (Mishty Dhekial, Jaimyn Drake, and I) have refactored the code for Question 3: Transformer Interpretability from Homework 11! \n\nWe decided to restructure the code into a more modular file structure (split into core, utils, tests) to reflect more standard software engineering practices seen in research/industry, rather than concentrated in a single notebook. We also applied standard Python style guidelines (PEP8, PEP257, PEP484) to ensure the code is consistently formatted and easy to read. We also added more verbose docstrings to provide a conceptual background of each step needed in the implementations, as well as their importance, with the hope that students can more concretely understand the necessity of each step without just implementing formulas as is.\n\nHere's a link to our GitHub repo, which contains both a student version (template), and a solution implementation:\n\nhttps://github.com/a-thul/CS_C182-Special-Participation-C-HW11-Q3\n\nHere's our report with an overview of all reorganization and refactoring done, along with the methods we used (including LLM usage/prompting):",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone!<break/>My group (Mishty Dhekial, Jaimyn Drake, and I) have refactored the code for Question 3: Transformer Interpretability from Homework 11! <break/><break/>We decided to restructure the code into a more modular file structure (split into core, utils, tests) to reflect more standard software engineering practices seen in research/industry, rather than concentrated in a single notebook. We also applied standard Python style guidelines (PEP8, PEP257, PEP484) to ensure the code is consistently formatted and easy to read. We also added more verbose docstrings to provide a conceptual background of each step needed in the implementations, as well as their importance, with the hope that students can more concretely understand the necessity of each step without just implementing formulas as is.<break/><break/>Here's a link to our GitHub repo, which contains both a student version (template), and a solution implementation:<break/><break/><link href=\"https://github.com/a-thul/CS_C182-Special-Participation-C-HW11-Q3\">https://github.com/a-thul/CS_C182-Special-Participation-C-HW11-Q3<break/></link><break/>Here's our report with an overview of all reorganization and refactoring done, along with the methods we used (including LLM usage/prompting):</paragraph><file url=\"https://static.us.edusercontent.com/files/J7JPgjhQASVauQLCAzHGjVJt\" filename=\"Special_Participation_C__HW11_Q3.pdf\"/></document>",
            "links": [
                "https://github.com/a-thul/CS_C182-Special-Participation-C-HW11-Q3"
            ],
            "attachments": [],
            "created_at": "2025-12-04T17:09:22.99273+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404906,
            "author": "John Chang",
            "project_title": "Special Participation E: Exam Q Generation based on HW and Prev Exam Qs",
            "post_body": "I decided to make ChatGPT 5.1 generate some exam questions for me. My method was to upload PDFs of previous homeworks in combination with screenshots of previous exam questions included in homeworks. I ran into some limitations with context when I first tried to upload all the homeworks (ChatGPT basically told me that it wasn't possible to generate questions for me), but got decent results when I only uploaded the first 5. \n\nMy prompt was: \n\"Can you generate one exam problem for me to solve based on the homeworks that I uploaded and in the style / difficulty level of the images of former exam problems?\"\n\nAnd for the MCQs:\n\n\"Given this additional context of an old multiple choice exam problem, generate 2 multiple choice problems testing for knowledge from HW 0-4.\"\n\nOverall I'm not sure how accurate the results will be to the actual exam, but the questions seem fairly reasonable and touch on a lot of the topics that the homeworks cover. The difficulty level of the subquestions varies a bit but overall seems around the level of what old exam questions looked like. ChatGPT provided questions to test on how to run certain algorithms and also questions for deriving equations. Then I provided it with an additional MCQ example and it generated two MCQs for me as well. I only caught one hallucination (RMS norm scaling by 1/d_in instead of 1/sqrt(d_in) which was awesome.\n\nI also made it explain the answers and I think it did an okay job, will definitely probe it more later for concepts I don't personally understand. Below is an annotated PDF of the conversation.\n\nhttps://drive.google.com/file/d/1o3sN-p5d8PqDguachFANzrGT337t7i_X/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>I decided to make ChatGPT 5.1 generate some exam questions for me. My method was to upload PDFs of previous homeworks in combination with screenshots of previous exam questions included in homeworks. I ran into some limitations with context when I first tried to upload all the homeworks (ChatGPT basically told me that it wasn't possible to generate questions for me), but got decent results when I only uploaded the first 5. </paragraph><paragraph>My prompt was: <break/>\"Can you generate one exam problem for me to solve based on the homeworks that I uploaded and in the style / difficulty level of the images of former exam problems?\"</paragraph><paragraph>And for the MCQs:</paragraph><paragraph>\"Given this additional context of an old multiple choice exam problem, generate 2 multiple choice problems testing for knowledge from HW 0-4.\"</paragraph><paragraph>Overall I'm not sure how accurate the results will be to the actual exam, but the questions seem fairly reasonable and touch on a lot of the topics that the homeworks cover. The difficulty level of the subquestions varies a bit but overall seems around the level of what old exam questions looked like. ChatGPT provided questions to test on how to run certain algorithms and also questions for deriving equations. Then I provided it with an additional MCQ example and it generated two MCQs for me as well. I only caught one hallucination (RMS norm scaling by 1/d_in instead of 1/sqrt(d_in) which was awesome.</paragraph><paragraph>I also made it explain the answers and I think it did an okay job, will definitely probe it more later for concepts I don't personally understand. Below is an annotated PDF of the conversation.</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1o3sN-p5d8PqDguachFANzrGT337t7i_X/view?usp=sharing\">https://drive.google.com/file/d/1o3sN-p5d8PqDguachFANzrGT337t7i_X/view?usp=sharing</link></paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1o3sN-p5d8PqDguachFANzrGT337t7i_X/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-04T16:24:51.073741+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404645,
            "author": "Jaimyn Drake",
            "project_title": "Special Participation C: Refactoring HW 11 Q4",
            "post_body": "Hey guys! Athul Krishnan, Mishty Dhekial, and I have refactored Question 4 for Homework 11: Scaling Laws.\n\nFor our improvement, we focused on applying PEP8 Python style guidelines (and adjacent popular Python practices, see report) to the Jupyter Notebook in order to improve the stylistic quality of the code while maintaining its educational structure.\n\nYou can find our resulting notebooks (both a student version and a solutions version) in our  Github repository:\n\nhttps://github.com/Crazyturtlej/CS-C182-HW11-Refactor/\n\nHere is our report on the guidelines and methods used in this refactoring:\n\nThanks guys! Have a wonderful day.",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey guys! Athul Krishnan, Mishty Dhekial, and I have refactored Question 4 for Homework 11: Scaling Laws.</paragraph><paragraph>For our improvement, we focused on applying PEP8 Python style guidelines (and adjacent popular Python practices, see report) to the Jupyter Notebook in order to improve the stylistic quality of the code while maintaining its educational structure.</paragraph><paragraph>You can find our resulting notebooks (both a student version and a solutions version) in our  Github repository:</paragraph><paragraph><link href=\"https://github.com/Crazyturtlej/CS-C182-HW11-Refactor/\">https://github.com/Crazyturtlej/CS-C182-HW11-Refactor/</link></paragraph><paragraph>Here is our report on the guidelines and methods used in this refactoring:</paragraph><file url=\"https://static.us.edusercontent.com/files/uFzJbYB7LL9zNzJV87jFtetQ\" filename=\"Special_Participation_C__HW_11_Q4.pdf\"/><paragraph>Thanks guys! Have a wonderful day.</paragraph></document>",
            "links": [
                "https://github.com/Crazyturtlej/CS-C182-HW11-Refactor/"
            ],
            "attachments": [],
            "created_at": "2025-12-04T15:39:48.505111+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404616,
            "author": "Xuanlin Mao",
            "project_title": "Special Participation B: Gemini Pro 3 on HW 11",
            "post_body": "For this special participation, I used Gemini Pro 3 to solve the coding problems in Homework 11. Compared to non-coding problems, the proportion of instances where Gemini Pro 3 provided a correct answer on the first try with runnable, logically correct code was lower, which is also related to the way the problems were stated and their difficulty.\n\nOut of the 12 coding problems, 10 were solved correctly on the first attempt. One problem produced code that was logically correct but deviated slightly from the problem definition due to differences in interpretation. Another problem could not be fully solved on the first attempt to meet the problem requirements, though the generated code was still runnable; with my guidance, the second attempt was correct.\n\nOverall, I believe Gemini Pro 3 is capable of handling most of the programming requirements for this course when the problem statements are clear, the difficulty is moderate, and there is some initial solution idea or hint. However, it sometimes produces relatively lengthy code and explanations, even for simple problems\u2014sometimes taking a long time to reason through the first prompt that provides general guidance rather than a direct solution.\n\nTranscript:\nhttps://docs.google.com/document/d/1gymtZ5axZrOvsg5Kr4gQjnILKR_tUvxf5m1_WD17kkU/edit?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I used Gemini Pro 3 to solve the coding problems in Homework 11. Compared to non-coding problems, the proportion of instances where Gemini Pro 3 provided a correct answer on the first try with runnable, logically correct code was lower, which is also related to the way the problems were stated and their difficulty.</paragraph><paragraph>Out of the 12 coding problems, 10 were solved correctly on the first attempt. One problem produced code that was logically correct but deviated slightly from the problem definition due to differences in interpretation. Another problem could not be fully solved on the first attempt to meet the problem requirements, though the generated code was still runnable; with my guidance, the second attempt was correct.</paragraph><paragraph>Overall, I believe Gemini Pro 3 is capable of handling most of the programming requirements for this course when the problem statements are clear, the difficulty is moderate, and there is some initial solution idea or hint. However, it sometimes produces relatively lengthy code and explanations, even for simple problems\u2014sometimes taking a long time to reason through the first prompt that provides general guidance rather than a direct solution.</paragraph><paragraph>Transcript:<break/><link href=\"https://docs.google.com/document/d/1gymtZ5axZrOvsg5Kr4gQjnILKR_tUvxf5m1_WD17kkU/edit?usp=sharing\">https://docs.google.com/document/d/1gymtZ5axZrOvsg5Kr4gQjnILKR_tUvxf5m1_WD17kkU/edit?usp=sharing</link> </paragraph></document>",
            "links": [
                "https://docs.google.com/document/d/1gymtZ5axZrOvsg5Kr4gQjnILKR_tUvxf5m1_WD17kkU/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-04T15:34:09.199128+11:00",
            "category": "Admin"
        },
        {
            "guid": 7404515,
            "author": "Vongani Maluleke",
            "project_title": "Special Participation A: Claude on HW7",
            "post_body": "Executive Summary\n\nFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the model without giving it any code or extra implementation hints, and saved the full conversation logs. My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer.\n\nOverall, Claude Opus did really well. It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much. For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time. I didn\u2019t see any major hallucinations.\n\nIt was especially reliable on:\n\nlinear algebra manipulations,\n\nPCA vs. autoencoder equivalence arguments,\n\ngradient calculations, and\n\ngeneral convexity/optimization reasoning.\n\nIn terms of interaction, the model was:\n\nconsistent in how it formatted math and explanations and,\n\nlogically stable across long derivations,\n\nOverall, based on this evaluation, Claude Opus can solve all the non-coding questions from CS282 HW7 with almost no guidance. Please see the annotated logs.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/p5cY1i1WFNOVvSiOqRr8WdS2\" filename=\"hw7_claude_logs_annotated.pdf\"/><heading level=\"1\"><bold>Executive Summary</bold></heading><paragraph>For this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the model without giving it any code or extra implementation hints, and saved the full conversation logs. My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer.</paragraph><paragraph>Overall, Claude Opus did really well. It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much. For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time. I didn\u2019t see any major hallucinations.</paragraph><paragraph>It was especially reliable on:</paragraph><list style=\"unordered\"><list-item><paragraph>linear algebra manipulations,</paragraph></list-item><list-item><paragraph>PCA vs. autoencoder equivalence arguments,</paragraph></list-item><list-item><paragraph>gradient calculations, and</paragraph></list-item><list-item><paragraph>general convexity/optimization reasoning.</paragraph></list-item></list><paragraph>In terms of interaction, the model was:</paragraph><list style=\"unordered\"><list-item><paragraph>consistent in how it formatted math and explanations and,</paragraph></list-item><list-item><paragraph>logically stable across long derivations,</paragraph></list-item></list><paragraph>Overall, based on this evaluation, Claude Opus can solve all the non-coding questions from CS282 HW7 with almost no guidance. Please see the annotated logs.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T15:18:02.927395+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404395,
            "author": "Gustavo Jose Ortiz Zepeda",
            "project_title": "Special Participation B: Opus 4.5 on Hw4",
            "post_body": "I used Opus 4.5 on Hw4 to solve coding and conceptual questions zero shot on all questions (without thinking tokens).",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Opus 4.5 on Hw4 to solve coding and conceptual questions zero shot on all questions (without thinking tokens).</paragraph><file url=\"https://static.us.edusercontent.com/files/boFM2uD7cqCxkXNA9W40a2aK\" filename=\"Special participation B.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T14:59:11.13114+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404147,
            "author": "Jaimyn Drake",
            "project_title": "Special Participation B: Grok on HW 9 Q5",
            "post_body": "Hey guys! I worked with Grok to answer the Coding Question for Homework 9: Visualizing Attention.\n\nTL;DR - In its analysis of the attention visualizations produced by this assignment\u2019s Jupyter notebook, Grok exhibits impressive visual reasoning abilities. By allowing it to request the PDF printouts of particular layers, Grok is able to provide serviceable answers to the assignment\u2019s array of conceptual questions. These responses indicate that Grok is somewhat able to parse the visuals and supplement gaps with a latent understanding of the BERT and GPT architecture, but there remain hallucinations and biases that can lead it astray from true responses.\n\n\n\nSince this coding question doesn\u2019t involve writing code itself, I instead focused on exploring Grok\u2019s visual reasoning abilities. In order to allow Grok to actually interact with the visual attention plots of the notebook itself, I focused on specifically providing it with providing it PDF printouts of the entire notebook (at running all of the cells in sequence by hand), which it could then use to observe the relationships between attentions and make observations to answer the associated conceptual questions.\n\nPart a):\n\nUpon initially providing it with the PDF printout of the Jupyter notebook, Grok attempted to answer all of the conceptual questions at one go, for which answers seemed serviceable at face value. However, it did not have full access to the visualizations for different layers (although it was able to determine that there were 11 layers overall), and so its initial answers did not fully reflect the behavior of those later layers.\n\nIn order to give Grok more agency, I prompted it to decide on the selection of other layers to look at, which I then provided the PDFs of. For this part, Grok requested layers 5 and 11, both at Head 0.\n\nIn some way, I was impressed by Grok\u2019s ability to infer information from the PDF format at all, where I worried it might struggle to read the visualizations. However, Grok successfully identifies the causal nature of the attention (that tokens attend to only preceding tokens), and that \u201cran\u201d attends to \u201cdog,\u201d reflecting their subject-verb relationship. In question 2, Grok identifies that by layer 5, \u201cran\u201d comes to attend to the first word \u201cThe\u201d over the word \u201cdog.\u201d These are examples of reasoning consistent with the official solutions. That said, Grok also appears to generally report results in line with a particular narrative it holds about the behavior of different layers, rather than truly reporting on the empirical results. For example, it claims that by layer 11, \"Mr.\" broadly weights towards \"his\", \"party\", and \"election\", which would align with a story about attention connecting more broadly at that layer, but is simply not reflected in the visual output. Importantly, Grok fails to highlight the overpowering trend of all tokens attending to the very first input, which is prominent throughout layers but overwhelmingly features in layer 5 that it requested to observe. This implies that while Grok\u2019s answers are fair at times, some of its correctness may come from an underlying understanding of what might be expected instead of a focus on the actual input. The conversation log for part (a) is as follows:\n\nPart b):\n\nFor this part, I once again asked Grok to select which layer and head numbers for which it would like to receive PDFs. Since Grok once again selected layers 0, 5, and 11 at Head 0, I provided it with PDFs where those settings were selected for all visualization cells in the notebook.\n\nIn answering the questions, Grok successfully identifies the bidirectional nature of BERT vs GPT, which may be a side effect of its own internal training data, and highlights the multiple interpretations of the word \u201cplay\u201d as affecting the attention results between examples. As expected, Grok understands that fine-tuning can help improve the representation learned by BERT. When prompted, Grok points out that CLS receives a lot of attention from other tokens, which is reflected in the assignment solutions.\n\nWhen asked to clean up or modify its responses to questions, Grok frequently ends up repeating the same statements and claims. Additionally, Grok continues to hallucinate some attention behavior, as it emphasizes connections like \u201cit\u201d and \u201cparty\u201d that are not easily visible in the provided PDFs. This repetition indicates that Grok may actually be poisoning its own context, leading to the reinforcement of incorrect information as it reiterates responses.\n\nPart c):\n\nHaving it draw on the PDF documents from the existing context, I remind Grok about the focus of questions 8 and 9 about the distinct qualities learned by different attention heads. In this part, Grok fully reinforces its high level generalizations that lower layers (0-3) should handle local relationships while higher layers (9-11) emphasize broader connections and an emphasis on special tokens. While Grok does exaggerate by posing this in black-and-white terms, its description of some \u201cdiagonal or short-range\u201d connections existing amount lower layers and heads, and \u201cpatterns converging on special tokens\u201d among the higher layers are roughly reflected in the appropriate visualizations. The conversation log for parts (b) and (c) is as follows:\n\nPart d):\n\nFor this part, Grok easily identifies that an untrained BERT model should have practically uniform attention weights, rather than the dedicated structure of the learned model. When asked to identify particular tokens for which it expects high attention, Grok continues to parrot the relationship between \u201cit\u201d and \u201cparty\u201d that it has been emphasizing throughout its responses. While prompting allows it to also provide other answers, like the [CLS] token that receives attention from most other tokens, this general tendency reflects that Grok\u2019s reasoning may be strongly biased by its initial context; Grok inadvertently enters an echo chamber of its own creation.\n\nFor your reference, here is a PDF of the entire chat log.\n\nThanks guys. Have a wonderful day!",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey guys! I worked with Grok to answer the Coding Question for Homework 9: Visualizing Attention.</paragraph><paragraph>TL;DR - In its analysis of the attention visualizations produced by this assignment\u2019s Jupyter notebook, Grok exhibits impressive visual reasoning abilities. By allowing it to request the PDF printouts of particular layers, Grok is able to provide serviceable answers to the assignment\u2019s array of conceptual questions. These responses indicate that Grok is somewhat able to parse the visuals and supplement gaps with a latent understanding of the BERT and GPT architecture, but there remain hallucinations and biases that can lead it astray from true responses.</paragraph><paragraph/><paragraph>Since this coding question doesn\u2019t involve writing code itself, I instead focused on exploring Grok\u2019s visual reasoning abilities. In order to allow Grok to actually interact with the visual attention plots of the notebook itself, I focused on specifically providing it with providing it PDF printouts of the entire notebook (at running all of the cells in sequence by hand), which it could then use to observe the relationships between attentions and make observations to answer the associated conceptual questions.</paragraph><paragraph>Part a):</paragraph><paragraph>Upon initially providing it with the PDF printout of the Jupyter notebook, Grok attempted to answer all of the conceptual questions at one go, for which answers seemed serviceable at face value. However, it did not have full access to the visualizations for different layers (although it was able to determine that there were 11 layers overall), and so its initial answers did not fully reflect the behavior of those later layers.</paragraph><paragraph>In order to give Grok more agency, I prompted it to decide on the selection of other layers to look at, which I then provided the PDFs of. For this part, Grok requested layers 5 and 11, both at Head 0.</paragraph><paragraph>In some way, I was impressed by Grok\u2019s ability to infer information from the PDF format at all, where I worried it might struggle to read the visualizations. However, Grok successfully identifies the causal nature of the attention (that tokens attend to only preceding tokens), and that \u201cran\u201d attends to \u201cdog,\u201d reflecting their subject-verb relationship. In question 2, Grok identifies that by layer 5, \u201cran\u201d comes to attend to the first word \u201cThe\u201d over the word \u201cdog.\u201d These are examples of reasoning consistent with the official solutions. That said, Grok also appears to generally report results in line with a particular narrative it holds about the behavior of different layers, rather than truly reporting on the empirical results. For example, it claims that by layer 11, \"Mr.\" broadly weights towards \"his\", \"party\", and \"election\", which would align with a story about attention connecting more broadly at that layer, but is simply not reflected in the visual output. Importantly, Grok fails to highlight the overpowering trend of all tokens attending to the very first input, which is prominent throughout layers but overwhelmingly features in layer 5 that it requested to observe. This implies that while Grok\u2019s answers are fair at times, some of its correctness may come from an underlying understanding of what might be expected instead of a focus on the actual input. The conversation log for part (a) is as follows:</paragraph><file url=\"https://static.us.edusercontent.com/files/CKgbMuNiTxyWKzBct9KzbBDG\" filename=\"GrokCode9Q12.pdf\"/><paragraph>Part b):</paragraph><paragraph>For this part, I once again asked Grok to select which layer and head numbers for which it would like to receive PDFs. Since Grok once again selected layers 0, 5, and 11 at Head 0, I provided it with PDFs where those settings were selected for all visualization cells in the notebook.</paragraph><paragraph>In answering the questions, Grok successfully identifies the bidirectional nature of BERT vs GPT, which may be a side effect of its own internal training data, and highlights the multiple interpretations of the word \u201cplay\u201d as affecting the attention results between examples. As expected, Grok understands that fine-tuning can help improve the representation learned by BERT. When prompted, Grok points out that CLS receives a lot of attention from other tokens, which is reflected in the assignment solutions.</paragraph><paragraph>When asked to clean up or modify its responses to questions, Grok frequently ends up repeating the same statements and claims. Additionally, Grok continues to hallucinate some attention behavior, as it emphasizes connections like \u201cit\u201d and \u201cparty\u201d that are not easily visible in the provided PDFs. This repetition indicates that Grok may actually be poisoning its own context, leading to the reinforcement of incorrect information as it reiterates responses.</paragraph><paragraph>Part c):</paragraph><paragraph>Having it draw on the PDF documents from the existing context, I remind Grok about the focus of questions 8 and 9 about the distinct qualities learned by different attention heads. In this part, Grok fully reinforces its high level generalizations that lower layers (0-3) should handle local relationships while higher layers (9-11) emphasize broader connections and an emphasis on special tokens. While Grok does exaggerate by posing this in black-and-white terms, its description of some \u201cdiagonal or short-range\u201d connections existing amount lower layers and heads, and \u201cpatterns converging on special tokens\u201d among the higher layers are roughly reflected in the appropriate visualizations. The conversation log for parts (b) and (c) is as follows:</paragraph><file url=\"https://static.us.edusercontent.com/files/HcYQmhWCwSXSvBfPSVzdxsxo\" filename=\"GrokCode9Q3456789.pdf\"/><paragraph>Part d):</paragraph><paragraph>For this part, Grok easily identifies that an untrained BERT model should have practically uniform attention weights, rather than the dedicated structure of the learned model. When asked to identify particular tokens for which it expects high attention, Grok continues to parrot the relationship between \u201cit\u201d and \u201cparty\u201d that it has been emphasizing throughout its responses. While prompting allows it to also provide other answers, like the [CLS] token that receives attention from most other tokens, this general tendency reflects that Grok\u2019s reasoning may be strongly biased by its initial context; Grok inadvertently enters an echo chamber of its own creation.</paragraph><file url=\"https://static.us.edusercontent.com/files/6ex3OdQgbnlwTxhQN6SL8nbL\" filename=\"GrokCode9_Q1011.pdf\"/><paragraph>For your reference, here is a PDF of the entire chat log.</paragraph><file url=\"https://static.us.edusercontent.com/files/Jbzalj1j2m7qesTazvr0RMn7\" filename=\"GrokCode9full.pdf\"/><paragraph>Thanks guys. Have a wonderful day!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T14:25:17.515814+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7404071,
            "author": "Imra Dawoodani",
            "project_title": "Special Participation A: Gemini 2.5 Flash on HW10",
            "post_body": "I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered correctly on the first attempt but I did notice residual errors with calculations that didn\u2019t seem to affect the final answer. Conceptual questions (separate from factual / math questions) like reflections from the Facet paper required most steering surprisingly. I initially assumed it would do worst on the math. Gemini's responses were consistently verbose, though it did not correlate with correctness. When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely. But it did anchor on incorrect interpretation sometimes. On Q5b (triplet loss supervision), Gemini claimed triplet loss \"requires explicit class labels\" and defended this through 3 steering attempts before finally acknowledging that relational similarity (positives/negatives) would do the job. Similarly, on Q5g, it insisted performance saturates rather than drops beyond 128 dimensions even though the paper clearly stated the latter. This outlines hallucinations that were further seen when trying to work on Q5f (harmonic embeddings). Gemini confidently stated \"The FaceNet paper does not directly define or utilize harmonic embeddings\" and fabricated a connection to Fourier analysis/Random Fourier Features. This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3.2, referring to model version compatibility. The longer the conversation went on, the more windy and hallucinated the answers ended up being. This seemed like a predictable outcome though. I steered Gemini to the correct answer in 4 ways:\n\nAbstract conceptual questions: often led to defensive elaboration of the same point\n\nHypothetical scenarios: better success at identifying missteps\n\nAsking for specific data points and references: mildly effective\n\nDirect factual corrections: last resort, and usually brought Gemini back to the grounded data it was given\n\nFor homework assistance, Gemini 2.5 Flash would be reliable for computations but should be verified against the main material for conceptual claims. The longer the chat, the worse it does with actively recalling instructions and facts.\n\nHere's an annotated log of my chat transcript: https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered correctly on the first attempt but I did notice residual errors with calculations that didn\u2019t seem to affect the final answer. Conceptual questions (separate from factual / math questions) like reflections from the Facet paper required most steering surprisingly. I initially assumed it would do worst on the math. Gemini's responses were consistently verbose, though it did not correlate with correctness. When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely. But it did anchor on incorrect interpretation sometimes. On Q5b (triplet loss supervision), Gemini claimed triplet loss \"requires explicit class labels\" and defended this through 3 steering attempts before finally acknowledging that relational similarity (positives/negatives) would do the job. Similarly, on Q5g, it insisted performance saturates rather than drops beyond 128 dimensions even though the paper clearly stated the latter. This outlines hallucinations that were further seen when trying to work on Q5f (harmonic embeddings). Gemini confidently stated \"The FaceNet paper does not directly define or utilize harmonic embeddings\" and fabricated a connection to Fourier analysis/Random Fourier Features. This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3.2, referring to model version compatibility. The longer the conversation went on, the more windy and hallucinated the answers ended up being. This seemed like a predictable outcome though. I steered Gemini to the correct answer in 4 ways:</paragraph><list style=\"ordered\"><list-item><paragraph>Abstract conceptual questions: often led to defensive elaboration of the same point</paragraph></list-item><list-item><paragraph>Hypothetical scenarios: better success at identifying missteps</paragraph></list-item><list-item><paragraph>Asking for specific data points and references: mildly effective</paragraph></list-item><list-item><paragraph>Direct factual corrections: last resort, and usually brought Gemini back to the grounded data it was given</paragraph></list-item></list><paragraph>For homework assistance, Gemini 2.5 Flash would be reliable for computations but should be verified against the main material for conceptual claims. The longer the chat, the worse it does with actively recalling instructions and facts.<break/><break/>Here's an annotated log of my chat transcript: <link href=\"https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing\">https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing</link> </paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-04T14:14:57.565789+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7403245,
            "author": "Xuanlin Mao",
            "project_title": "Special Participation A: Gemini Pro 3 on HW 11",
            "post_body": "For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.\n\nIn this task, since there were no complex mathematical proofs or derivations involved, the vast majority of questions relied on intuitive understanding or simple mathematical calculations. Combined with its use of Python for auxiliary computations, Gemini 3 Pro\u2019s performance was very satisfactory. After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy. All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula support and mathematical derivation steps. The formatting was clear and the language was easy to understand.\n\nSince the performance is quite satisfactory, overall my strategy is to input the hw pdf and write a \u201csystem prompt\u201d at the beginning of the chat and then let the model solve several questions each time. This is to avoid that the model generates too long answers and make the model focus on few topics and retrieve limited information each run. In the system prompt, I defined the task and model\u2019s role, and provided some instructions, like being precise and making the solution easy to be understood by undergraduate students, etc.\n\nTranscript:\n\nhttps://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing ",
            "content_xml": "<document version=\"2.0\"><paragraph>For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.</paragraph><paragraph>In this task, since there were no complex mathematical proofs or derivations involved, the vast majority of questions relied on intuitive understanding or simple mathematical calculations. Combined with its use of Python for auxiliary computations, Gemini 3 Pro\u2019s performance was very satisfactory. After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy. All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula support and mathematical derivation steps. The formatting was clear and the language was easy to understand.</paragraph><paragraph>Since the performance is quite satisfactory, overall my strategy is to input the hw pdf and write a \u201csystem prompt\u201d at the beginning of the chat and then let the model solve several questions each time. This is to avoid that the model generates too long answers and make the model focus on few topics and retrieve limited information each run. In the system prompt, I defined the task and model\u2019s role, and provided some instructions, like being precise and making the solution easy to be understood by undergraduate students, etc.</paragraph><paragraph>Transcript:</paragraph><paragraph><link href=\"https://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing\">https://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing</link> </paragraph></document>",
            "links": [
                "https://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-04T12:29:34.39678+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7402121,
            "author": "Krish Yadav",
            "project_title": "Special Participation E: Gemini Pro 3 Guided Learning Roadmap for Optimizer Theory",
            "post_body": "When preparing for the optimizer material, I often find that the hardest part is not individual optimizers, but understanding the learning order - what concepts actually need to come first before the homework makes sense. Inspired by roadmap-style explanations like Evan Chen\u2019s Napkin, I used Gemini Pro 3 to reorganize the optimizer lectures into a dependency-ordered, interactive study guide.\n\nUsing the specified lecture notes (2, 4, 6) and homework (3), Gemini Pro 3 built a roadmap starting from loss geometry and singular values, then moving through SGD, momentum, initialization, scaling laws, and modern optimizers. It asked short diagnostic questions to test prerequisite understanding and, based on my answers, suggested what to study next before attempting the homework. This was effective at surfacing gaps and correctly identified variance propagation and initialization as the main conceptual hurdle for the optimizer homework.\n\nThe interaction also revealed limitations. When I probed assumptions, the model overgeneralized by focusing on gradient noise and batch size, missing assumptions specific to linear or quadratic models. It also tended to continue tutoring after I tried to stop the interaction, showing weaker control over user-directed boundaries.\n\nOverall, this worked well as an AI-assisted replacement for pre-lecture or pre-homework reading - useful for structuring prerequisites and exposing conceptual gaps, but still needing active supervision.\n\nFor more, see annotated PDF below:",
            "content_xml": "<document version=\"2.0\"><paragraph>When preparing for the optimizer material, I often find that the hardest part is not individual optimizers, but understanding the learning order - what concepts actually need to come first before the homework makes sense. Inspired by roadmap-style explanations like Evan Chen\u2019s <italic>Napkin</italic>, I used Gemini Pro 3 to reorganize the optimizer lectures into a dependency-ordered, interactive study guide.</paragraph><paragraph>Using the specified lecture notes (2, 4, 6) and homework (3), Gemini Pro 3 built a roadmap starting from loss geometry and singular values, then moving through SGD, momentum, initialization, scaling laws, and modern optimizers. It asked short diagnostic questions to test prerequisite understanding and, based on my answers, suggested what to study next before attempting the homework. This was effective at surfacing gaps and correctly identified variance propagation and initialization as the main conceptual hurdle for the optimizer homework.</paragraph><paragraph>The interaction also revealed limitations. When I probed assumptions, the model overgeneralized by focusing on gradient noise and batch size, missing assumptions specific to linear or quadratic models. It also tended to continue tutoring after I tried to stop the interaction, showing weaker control over user-directed boundaries.</paragraph><paragraph>Overall, this worked well as an AI-assisted replacement for pre-lecture or pre-homework reading - useful for structuring prerequisites and exposing conceptual gaps, but still needing active supervision.<break/><break/>For more, see annotated PDF below:</paragraph><file url=\"https://static.us.edusercontent.com/files/3yoOOU9TDpUUIsIGjfIepnyX\" filename=\"annotated-gp3-optimizer-theory-conversation.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T10:09:30.281273+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7401923,
            "author": "Nils Selte",
            "project_title": "Special Participation A: Kimi K2 on hw8",
            "post_body": "I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/ejxi2K1Tcjt8Jtyrp0cjdHsG\" filename=\"cs182_hw8 special participation kimi k2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T09:47:18.097945+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7401103,
            "author": "Krish Yadav",
            "project_title": "Special Participation B: Gemini Pro 3 on HW 10",
            "post_body": "I used Gemini Pro 3 by feeding it the HW10 coding problems and background (without giving it the official solutions). For Q1 (HandTransformer), it produced a fully vectorized implementation that matched the provided solution reference and passed the tests immediately. For Q2 (Summarization), it correctly implemented scaled dot-product attention (including padding and causal masking), multi-head attention, and positional indices. The resulting implementations were functionally aligned with the official solutions; differences were mostly stylistic (e.g., reshape/transpose vs. einops).\n\nOverall, Gemini handled and one-shot these transformer-based coding tasks well when the specification was clear. For more details, see the annotated PDF.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini Pro 3 by feeding it the HW10 coding problems and background (without giving it the official solutions). For Q1 (HandTransformer), it produced a fully vectorized implementation that matched the provided solution reference and passed the tests immediately. For Q2 (Summarization), it correctly implemented scaled dot-product attention (including padding and causal masking), multi-head attention, and positional indices. The resulting implementations were functionally aligned with the official solutions; differences were mostly stylistic (e.g., reshape/transpose vs. einops).</paragraph><paragraph>Overall, Gemini handled and one-shot these transformer-based coding tasks well when the specification was clear. For more details, see the annotated PDF.</paragraph><file url=\"https://static.us.edusercontent.com/files/LRd8kPvIQvJIepe4Mgx3zVdb\" filename=\"annotated-gemini_pro_3-hw10-coding.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T08:20:47.406537+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7401078,
            "author": "Krish Yadav",
            "project_title": "Special Participation A: Grok on HW 08",
            "post_body": "I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try. The main issue I saw was in complexity analysis: in a few places it mixed up total work vs. critical path length when discussing parallelization, which required manual correction. Aside from that, the mathematics and reasoning closely matched the official solutions.\n\nFor more details, including annotated interaction logs and comparisons to the official solutions, see the attached PDF.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try. The main issue I saw was in complexity analysis: in a few places it mixed up total work vs. critical path length when discussing parallelization, which required manual correction. Aside from that, the mathematics and reasoning closely matched the official solutions.</paragraph><paragraph>For more details, including annotated interaction logs and comparisons to the official solutions, see the attached PDF.<break/><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/Bjx2ldlWDm5CBL6oirMUaOKL\" filename=\"annotated-grok-hw8-summary.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T08:18:32.354006+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7400839,
            "author": "Zach Pricz",
            "project_title": "Special Participation A: Qwen on HW4",
            "post_body": "For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). \n\nI attempted this homework with Qwen 3 times actually as the earlier times I prompted it with more advanced instructions or I would give it each problem seperately it often spiraled and kept second guessing itself. I found that when Qwen does this it often happens cloes to 3 times and it has a hard time accepting the truth when it can't reason to it. This is present in problem 2e in this log as when asking Qwen to reason itself to the solution it took 3 times in one query to finally arrive to the correct answer. \n\nDespite this, Qwen did a pretty good job on this homework especially considering how difficult it is. It interestingly did not one shot computation based questions like the convolution questions in problem 3, but it expertly navigated and explained the matrix calculus in problem 7. I think Qwen is a decent model for this type of tasks but it has to be wrangled more then I would have hoped. \n\nFinal conversation trace: https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/ttpjNYv5NDEcoulc98VEj4pK\" filename=\"qwen_hw4_annotated_trace.pdf\"/><paragraph>For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). </paragraph><paragraph>I attempted this homework with Qwen 3 times actually as the earlier times I prompted it with more advanced instructions or I would give it each problem seperately it often spiraled and kept second guessing itself. I found that when Qwen does this it often happens cloes to 3 times and it has a hard time accepting the truth when it can't reason to it. This is present in problem 2e in this log as when asking Qwen to reason itself to the solution it took 3 times in one query to finally arrive to the correct answer. </paragraph><paragraph>Despite this, Qwen did a pretty good job on this homework especially considering how difficult it is. It interestingly did not one shot computation based questions like the convolution questions in problem 3, but it expertly navigated and explained the matrix calculus in problem 7. I think Qwen is a decent model for this type of tasks but it has to be wrangled more then I would have hoped. </paragraph><paragraph>Final conversation trace: <link href=\"https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7\">https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7</link></paragraph></document>",
            "links": [
                "https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7"
            ],
            "attachments": [],
            "created_at": "2025-12-04T07:53:08.520193+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7400491,
            "author": "Yuxiang Liu",
            "project_title": "Special Participation B: Grok for the coding parts of HW11",
            "post_body": "\n\nBased on my experiences with Grok, it clearly has a solid grasp of high-level algorithmic structure and can translate math/specifications into reasonably clean PyTorch/NumPy code. For each task, it picked the right primitives (e.g., torch.kthvalue for pruning thresholds, KMeans clustering for weight sharing, causal masking and linear maps for attention) and generally wired them in a way that matches the conceptual description. It also shows awareness of practical concerns like clamping sparsity, handling trivial edge cases, and doing in-place updates so quantization works with retraining. Variable naming and commenting are quite readable, and the overall control flow is easy to follow. \n\nHowever, the implementations also reveal some recurring weaknesses in precision and edge-case handling. In the induction head and pruning code, small mistakes in indexing or threshold logic lead to qualitatively wrong behavior (e.g., misplacing prev/current token info, or pruning everything when num_zeros == 0), which suggests Grok doesn\u2019t always fully verify that the math and tensor shapes line up. In the k-means quantization code, it uses unsafe .data operations, has a minor typo, and ignores scalability concerns and corner cases such as too many clusters for the number of points. Overall, Grok is good at capturing the idea of an algorithm and producing plausible first-draft code, but still needs careful human review, debugging, and polishing before the code can be considered robust or production-ready. ",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph>Based on my experiences with Grok, it clearly has a solid grasp of high-level algorithmic structure and can translate math/specifications into reasonably clean PyTorch/NumPy code. For each task, it picked the right primitives (e.g., torch.kthvalue for pruning thresholds, KMeans clustering for weight sharing, causal masking and linear maps for attention) and generally wired them in a way that matches the conceptual description. It also shows awareness of practical concerns like clamping sparsity, handling trivial edge cases, and doing in-place updates so quantization works with retraining. Variable naming and commenting are quite readable, and the overall control flow is easy to follow. </paragraph><paragraph>However, the implementations also reveal some recurring weaknesses in precision and edge-case handling. In the induction head and pruning code, small mistakes in indexing or threshold logic lead to qualitatively wrong behavior (e.g., misplacing prev/current token info, or pruning everything when num_zeros == 0), which suggests Grok doesn\u2019t always fully verify that the math and tensor shapes line up. In the k-means quantization code, it uses unsafe .data operations, has a minor typo, and ignores scalability concerns and corner cases such as too many clusters for the number of points. Overall, Grok is good at capturing the <italic>idea </italic>of an algorithm and producing plausible first-draft code, but still needs careful human review, debugging, and polishing before the code can be considered robust or production-ready. </paragraph><file url=\"https://static.us.edusercontent.com/files/6dO1XXoIqjjj39cSAoPkMaCh\" filename=\"Special_Participation_B_HW11.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T07:15:03.001043+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7399301,
            "author": "Guohao Lv",
            "project_title": "Special Participation B: Kimi on HW5",
            "post_body": "I used Kimi K2 on HW 5 (coding parts of Q5 and Q6) to test the coding parts.\n\nKimi chat link: \n\nOverall: Kimi K2 performed very well on the coding parts of HW 5. The solutions it produced were very close to the staff solution, with only minor differences in style. For the great majority of the TODO parts, it was able to successfully one-shot the correct code. The rare cases where it did not one-shot were about computation or I did not provide enough structure.\n\nPros: I found that Kimi K2 was able to give clear explanations for the code it wrote for each part, and it could also summarize what it had done and highlight the key ideas for me. It also explicitly stated important observations and parameters being used in the implementation. When most of the surrounding code was already provided and Kimi K2 only needed to fill in one-liners or short TODOs, it achieved essentially 100% one-shot accuracy on HW 5.\n\nCons: To be fair, I did not see many actual errors when working with Kimi K2 for HW 5. It handled the coding TODO parts very well and did not hallucinate nonexistent variables, functions, or libraries. However, there are cases where it does not do well. For example, it generates the wrong output of a code twice, and it seems to me that it does not know where it goes wrong. This is a little bit weird because in that instance it seems like it does not know what it was doing. Also, when it comes to hyperparameter tuning, it also does not do well, which is another case where I think Kimi does not know what it was doing.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Kimi K2 on HW 5 (coding parts of Q5 and Q6) to test the coding parts.</paragraph><paragraph>Kimi chat link: </paragraph><file url=\"https://static.us.edusercontent.com/files/TMBxFojYAEDATqWeW8fFUvFG\" filename=\"HW5_Q5_Kimi.pdf\"/><file url=\"https://static.us.edusercontent.com/files/nSPe0s7iuNKKebiel6i0caWR\" filename=\"HW5_Q6_1_Kimi.pdf\"/><file url=\"https://static.us.edusercontent.com/files/ECfBIv78bw8JhpO3NPbitPwG\" filename=\"HW5_Q6_2_Kimi.pdf\"/><file url=\"https://static.us.edusercontent.com/files/N77QpsQJqLPjmGFTIEahYJzh\" filename=\"HW5_Q6_3_Kimi.pdf\"/><paragraph><bold>Overall:</bold> Kimi K2 performed very well on the coding parts of HW 5. The solutions it produced were very close to the staff solution, with only minor differences in style. For the great majority of the TODO parts, it was able to successfully one-shot the correct code. The rare cases where it did not one-shot were about computation or I did not provide enough structure.</paragraph><paragraph><bold>Pros:</bold> I found that Kimi K2 was able to give clear explanations for the code it wrote for each part, and it could also summarize what it had done and highlight the key ideas for me. It also explicitly stated important observations and parameters being used in the implementation. When most of the surrounding code was already provided and Kimi K2 only needed to fill in one-liners or short TODOs, it achieved essentially 100% one-shot accuracy on HW 5.</paragraph><paragraph><bold>Cons:</bold> To be fair, I did not see many actual errors when working with Kimi K2 for HW 5. It handled the coding TODO parts very well and did <italic>not</italic> hallucinate nonexistent variables, functions, or libraries. However, there are cases where it does not do well. For example, it generates the wrong output of a code twice, and it seems to me that it does not know where it goes wrong. This is a little bit weird because in that instance it seems like it does not know what it was doing. Also, when it comes to hyperparameter tuning, it also does not do well, which is another case where I think Kimi does not know what it was doing.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T04:47:44.112851+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7399278,
            "author": "ZhaoRui Qu",
            "project_title": "Special participation B: Gork on HW2",
            "post_body": "For Special Participation B, I used Grok on the coding portion of HW2. Overall, I was quite satisfied with Grok\u2019s performance. It was able to solve most problems in a one-shot manner, and only in a few cases did it modify code outside the intended TODO section. With clear instructions, those issues were easy to resolve. More details are provided in my full report attached. Here is the annotated log:",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special Participation B, I used Grok on the coding portion of HW2. Overall, I was quite satisfied with Grok\u2019s performance. It was able to solve most problems in a one-shot manner, and only in a few cases did it modify code outside the intended TODO section. With clear instructions, those issues were easy to resolve. More details are provided in my full report attached. Here is the annotated log:</paragraph><file url=\"https://static.us.edusercontent.com/files/dL14li2wQXOognpRAcUDxeVd\" filename=\"Gork--HW2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T04:45:05.028689+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7399196,
            "author": "ZhaoRui Qu",
            "project_title": "Special participation A: Kimi on HW0",
            "post_body": "For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giving correct final expressions, but it often skips steps, relies too much on memorized formulas, and needs very explicit instructions to produce proper derivations. It also struggles with qualitative reasoning at times.\n\nDespite these issues, Kimi\u2019s conceptual explanations were clear, and with careful prompting and oversight, it was able to provide correct results. In short, Kimi is helpful as long as you guide it closely and verify its reasoning, but it is not reliable for detailed, step-by-step mathematical work without supervision.\n\nHere is the annotated log:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giving correct final expressions, but it often skips steps, relies too much on memorized formulas, and needs very explicit instructions to produce proper derivations. It also struggles with qualitative reasoning at times.</paragraph><paragraph>Despite these issues, Kimi\u2019s conceptual explanations were clear, and with careful prompting and oversight, it was able to provide correct results. In short, Kimi is helpful as long as you guide it closely and verify its reasoning, but it is not reliable for detailed, step-by-step mathematical work without supervision.</paragraph><paragraph>Here is the annotated log:</paragraph><file url=\"https://static.us.edusercontent.com/files/BVXOJyQEqNmmCZ5QTnFZIqeN\" filename=\"Kimi--HW0.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T04:34:25.897961+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7398412,
            "author": "Kian Hekmatnejad",
            "project_title": "Special Participation B: Grok on HW5",
            "post_body": "For Special Participation B, I used Grok on the coding portion of HW5. Overall, I was disappointed by the performance of Grok as it struggled to correct bugs and rarely was able to one-shot problems. More information is present in my full report attached. Here is a link of my chat: https://grok.com/share/c2hhcmQtMw_8c73c897-085e-4f69-afbe-c4baf7fa44e4 ",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special Participation B, I used Grok on the coding portion of HW5. Overall, I was disappointed by the performance of Grok as it struggled to correct bugs and rarely was able to one-shot problems. More information is present in my full report attached. Here is a link of my chat: <link href=\"https://grok.com/share/c2hhcmQtMw_8c73c897-085e-4f69-afbe-c4baf7fa44e4\">https://grok.com/share/c2hhcmQtMw_8c73c897-085e-4f69-afbe-c4baf7fa44e4</link> </paragraph><file url=\"https://static.us.edusercontent.com/files/YhwQsmom77bSDJ1AkmjbJGCN\" filename=\"special_participation (1).pdf\"/></document>",
            "links": [
                "https://grok.com/share/c2hhcmQtMw_8c73c897-085e-4f69-afbe-c4baf7fa44e4"
            ],
            "attachments": [],
            "created_at": "2025-12-04T02:27:53.415959+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7398141,
            "author": "Gabriel Han",
            "project_title": "Special Participation A: Gemini 3 pro on Hw 12",
            "post_body": "Model Tested: Gemini 3 Pro\n\nOverall Performance: Very good: 100% One-shot\n\nPerformance Overview\n\nThe model was tasked with solving 3 deep learning problems involving debugging neural network initialization, analyzing information theory concepts (KL Divergence behavior), and interpreting Variational Information Bottleneck (VIB) systems.\n\nCode Debugging (Transformers): The model correctly identified a \"peaked softmax\" issue caused by improper weight initialization in a Transformer implementation. It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting.\n\nMathematical Intuition (KL Divergence): The model successfully generated a counter-example to distinguish Forward vs. Reverse KL divergence properties. It correctly mapped visual plots to \"Mode-Seeking\" vs. \"Mass-Covering\" behaviors based purely on visual evidence and theoretical definitions.\n\nSystem Design (VIB/VAE): The model accurately constructed the computational graph for the Reparameterization Trick and correctly traced gradient flows for encoder/decoder parameters. It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model did not hallucinate. Notably, in Question 3 (Part 2), the model explicitly noted that \"Figure 4 was not included\" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory. This demonstrates a high level of reasoning capability where the model fills missing context with theoretical deduction rather than fabricating visual data.\n\nBehavioral Observations & Strategies\n\nMultimodal Reasoning: The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes (Question 2) without text descriptions of the visual data.\n\nTheoretical Grounding: The model consistently reasoned from first principles. For example, when discussing KL divergence, it didn't just state the answer; it integrated the integral definitions to explain why the support mismatch causes infinity.\n\nStrategy - Contextual Chunking: For Question 3, which was long and multi-part, I broke the prompt into two segments (Parts a/b, then c/d). This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go.\n\nConclusion\n\nGemini 3 Pro demonstrated sophisticated understanding of deep learning theory, effectively bridging the gap between mathematical notation, code implementation, and visual interpretation of training dynamics. It functioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot.",
            "content_xml": "<document version=\"2.0\"><paragraph>Model Tested: Gemini 3 Pro</paragraph><paragraph>Overall Performance: Very good: 100% One-shot</paragraph><heading level=\"3\">Performance Overview</heading><paragraph>The model was tasked with solving 3 deep learning problems involving debugging neural network initialization, analyzing information theory concepts (KL Divergence behavior), and interpreting Variational Information Bottleneck (VIB) systems.</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Code Debugging (Transformers):</bold> The model correctly identified a \"peaked softmax\" issue caused by improper weight initialization in a Transformer implementation. It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting.</paragraph></list-item><list-item><paragraph><bold>Mathematical Intuition (KL Divergence):</bold> The model successfully generated a counter-example to distinguish Forward vs. Reverse KL divergence properties. It correctly mapped visual plots to \"Mode-Seeking\" vs. \"Mass-Covering\" behaviors based purely on visual evidence and theoretical definitions.</paragraph></list-item><list-item><paragraph><bold>System Design (VIB/VAE):</bold> The model accurately constructed the computational graph for the Reparameterization Trick and correctly traced gradient flows for encoder/decoder parameters. It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient.</paragraph></list-item></list><heading level=\"3\">Hallucinations &amp; Accuracy</heading><paragraph>Hallucination Rate: 0%.</paragraph><paragraph>The model did not hallucinate. Notably, in Question 3 (Part 2), the model explicitly noted that \"Figure 4 was not included\" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory. This demonstrates a high level of reasoning capability where the model fills missing context with theoretical deduction rather than fabricating visual data.</paragraph><heading level=\"3\">Behavioral Observations &amp; Strategies</heading><list style=\"unordered\"><list-item><paragraph><bold>Multimodal Reasoning:</bold> The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes (Question 2) without text descriptions of the visual data.</paragraph></list-item><list-item><paragraph><bold>Theoretical Grounding:</bold> The model consistently reasoned from first principles. For example, when discussing KL divergence, it didn't just state the answer; it integrated the integral definitions to explain <italic>why</italic> the support mismatch causes infinity.</paragraph></list-item><list-item><paragraph><bold>Strategy - Contextual Chunking:</bold> For Question 3, which was long and multi-part, I broke the prompt into two segments (Parts a/b, then c/d). This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go.</paragraph></list-item></list><heading level=\"3\">Conclusion</heading><paragraph>Gemini 3 Pro demonstrated sophisticated understanding of deep learning theory, effectively bridging the gap between mathematical notation, code implementation, and visual interpretation of training dynamics. It functioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot.</paragraph><file url=\"https://static.us.edusercontent.com/files/Ek2twD0hhlOyU0hGPftRipG5\" filename=\"Special Participation A.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-04T01:08:18.606596+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397928,
            "author": "Jacky Wong",
            "project_title": "Special Participation E: Using Gemini to make study guide that provides citations to papers",
            "post_body": "When trying to understand better on a topic and lectures. I find it the best to have like a study guide that is well formatted. But on top of that, when studying topics like deep learning, I really like to read the original academic papers. A lot of times however, I am not sure which part of the lecture is related to which paper and which part of the paper. I also like to see how different paper solve what kinds of problems and how the whole topic evolve over years. \n\n\n\nI tried to use Gemini 3 pro to create a study guide on the transformer topic (I attached lecture notes 18 to 20). The study guide would include a summary of the lectures formatted nicely and broken down into each key ideas. And it will also tell us which paper and which part of the paper is it referencing to. As well as a timeline of papers and how the technology of transformer evolved over time. \n\n\n\nchat link: \n\n1. chat to create the study guide: https://gemini.google.com/share/0b4f8208c138\n\n2. chat to refine the created study guide: https://gemini.google.com/share/852e11e1d27e\n\n\n\nAnnotated log: ",
            "content_xml": "<document version=\"2.0\"><paragraph>When trying to understand better on a topic and lectures. I find it the best to have like a study guide that is well formatted. But on top of that, when studying topics like deep learning, I really like to read the original academic papers. A lot of times however, I am not sure which part of the lecture is related to which paper and which part of the paper. I also like to see how different paper solve what kinds of problems and how the whole topic evolve over years. </paragraph><paragraph/><paragraph>I tried to use Gemini 3 pro to create a study guide on the transformer topic (I attached lecture notes 18 to 20). The study guide would include a summary of the lectures formatted nicely and broken down into each key ideas. And it will also tell us which paper and which part of the paper is it referencing to. As well as a timeline of papers and how the technology of transformer evolved over time. </paragraph><paragraph/><paragraph>chat link: </paragraph><paragraph>1. chat to create the study guide: <link href=\"https://gemini.google.com/share/0b4f8208c138\">https://gemini.google.com/share/0b4f8208c138</link></paragraph><paragraph>2. chat to refine the created study guide: <link href=\"https://gemini.google.com/share/852e11e1d27e\">https://gemini.google.com/share/852e11e1d27e</link></paragraph><paragraph/><paragraph>Annotated log: </paragraph><file url=\"https://static.us.edusercontent.com/files/886ynNR61GxqL7vYo1T3scbt\" filename=\"Final_Special_participation_E_study_guide.pdf\"/></document>",
            "links": [
                "https://gemini.google.com/share/0b4f8208c138",
                "https://gemini.google.com/share/852e11e1d27e"
            ],
            "attachments": [],
            "created_at": "2025-12-03T22:20:59.687984+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397833,
            "author": "Tiger Zhang",
            "project_title": "Special Participation E: Gemini 3 (Thinking) as a Mistake/Pseudoproof-generating Machine",
            "post_body": "I learn best not just by understanding \"why\" what is correct is correct, but also by understanding \"why\" what is incorrect is incorrect. I still remember finding the faults of the pseudoproof proving that all horses are the same color in CS 70.\n\nInspired by such efforts in other courses, I decided to use Gemini to generate such type of incorrect answers for homework problems. After generating such incorrect answers, I would engage in a conversation with Gemini where I attempt to spot the mistakes in its answers.\n\nThis is still a work-in-progress; I'm posting this now to potentially de-conflict or collaborate with others with similar ideas. I intend to have some few-prompt pipelines for this, as well as some general experience/advice for what to do when prompting soon.\n\nExecutive summary:\n\nThe result:\n\nI have a starting prompt (the first black box in my chat log) that prepares the model to receive problems for which it is to generate incorrect solutions with one or more mistakes hidden in them. From there, the user can interact with the LLM to find the mistakes in its solutions, and then do the exercise on more problems.\n\nQuality analysis:\n\nIt seems like Gemini understood the task at a deep level. For example, even though I asked it to give no hints for future problems after the first one, it gave hints for one of the problems that bluffed a fully correct answer as containing a mistake, requiring the user to understand the problem at a deep level and have confidence to identify that there are no mistakes. Although this isn\u2019t what I prompted the model to do initially, it accords with the \u201cspirit\u201d of this exercise of making the user think carefully to disregard misleading information.\n\nOtherwise, for problems that naturally have good \u201cmistakes\u201d, Gemini gave great, insightful mistakes. For problems that are less good for \u201cmistakes\u201d, Gemini still gave the best I could reasonably ask from it.\n\nPrecautions:\n\nThough the mistakes were mostly good quality, the user should be careful about interacting with Gemini to analyze the mistakes, and in particular the user\u2019s guesses for the mistakes. Sometimes, Gemini can attempt to be too encouraging, resulting in it telling the user that they\u2019re right before explaining nuanced conceptual inaccuracies in the user\u2019s answers.\n\nFurthermore, questions that are heavily conceptual have more \u201cspace\u201d to hide the mistake, and make for better exercises for \u201cmistake identification\u201d. I recommend using problems such as homework 3 problem 1 as opposed to using problems such as homework 2 problem 2.\n\nAnnotated chat log:",
            "content_xml": "<document version=\"2.0\"><paragraph>I learn best not just by understanding \"why\" what is correct is correct, but also by understanding \"why\" what is incorrect is incorrect. I still remember finding the faults of the pseudoproof proving that all horses are the same color in CS 70.</paragraph><paragraph>Inspired by such efforts in other courses, I decided to use Gemini to generate such type of incorrect answers for homework problems. After generating such incorrect answers, I would engage in a conversation with Gemini where I attempt to spot the mistakes in its answers.</paragraph><paragraph>This is still a work-in-progress; I'm posting this now to potentially de-conflict or collaborate with others with similar ideas. I intend to have some few-prompt pipelines for this, as well as some general experience/advice for what to do when prompting soon.</paragraph><paragraph><bold>Executive summary</bold>:</paragraph><paragraph>The result:</paragraph><paragraph>I have a starting prompt (the first black box in my chat log) that prepares the model to receive problems for which it is to generate incorrect solutions with one or more mistakes hidden in them. From there, the user can interact with the LLM to find the mistakes in its solutions, and then do the exercise on more problems.</paragraph><paragraph>Quality analysis:</paragraph><paragraph>It seems like Gemini understood the task at a deep level. For example, even though I asked it to give no hints for future problems after the first one, it gave hints for one of the problems that bluffed a fully correct answer as containing a mistake, requiring the user to understand the problem at a deep level and have confidence to identify that there are no mistakes. Although this isn\u2019t what I prompted the model to do initially, it accords with the \u201cspirit\u201d of this exercise of making the user think carefully to disregard misleading information.</paragraph><paragraph>Otherwise, for problems that naturally have good \u201cmistakes\u201d, Gemini gave great, insightful mistakes. For problems that are less good for \u201cmistakes\u201d, Gemini still gave the best I could reasonably ask from it.</paragraph><paragraph>Precautions:</paragraph><paragraph>Though the mistakes were mostly good quality, the user should be careful about interacting with Gemini to analyze the mistakes, and in particular the user\u2019s guesses for the mistakes. Sometimes, Gemini can attempt to be too encouraging, resulting in it telling the user that they\u2019re right before explaining nuanced conceptual inaccuracies in the user\u2019s answers.</paragraph><paragraph>Furthermore, questions that are heavily conceptual have more \u201cspace\u201d to hide the mistake, and make for better exercises for \u201cmistake identification\u201d. I recommend using problems such as homework 3 problem 1 as opposed to using problems such as homework 2 problem 2.</paragraph><paragraph>Annotated chat log:</paragraph><file url=\"https://static.us.edusercontent.com/files/bHPXdZjNgQeAnyp9FQ6Kgavs\" filename=\"chat_log.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T20:54:36.467718+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397817,
            "author": "Tamzid Razzaque",
            "project_title": "Special Participation A: ChatGPT o3 on HW 9",
            "post_body": "Executive Summary\n\nFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated, and how solid its reasoning was. For most of the assignment, especially Questions 1 through 4, it did surprisingly well. It handled the computation-heavy parts, the code-completion questions, and the conceptual attention explanations almost exactly the way the official solutions did. Its derivations for expectations, variances, argmax attention, and the multi-head attention code were all clean and matched the intended logic with basically no backtracking.\n\nOne thing that stood out was how consistently the model could jump straight into the right structure of the problem. It didn\u2019t need much nudging to set up sums, identify independence assumptions, or translate equations into PyTorch einsums. Even the complexity analysis in Question 4 lined up with the official answers once we made sure the symbols matched. When it was right, it was very right, and the explanations were easy to follow.\n\nThe main issue came up in Question 6. ChatGPT fell into a pretty common trap: it used the homogeneous quadratic kernel (q transpose k squared) instead of the standard degree-2 polynomial kernel ((q transpose k plus 1) squared). Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer. But once I asked it to revisit its reasoning instead of giving a new solution, it immediately spotted the mistake, explained why the earlier map didn\u2019t match the actual kernel, and rebuilt everything correctly. So the error wasn\u2019t really a hallucination\u2014more like it defaulted to a familiar formula without checking whether it matched what the homework meant.\n\nAside from that one slip, the model didn\u2019t really hallucinate or spiral. It stayed structured and patient even in the longer reasoning chains, and it didn\u2019t show any of the \u201cI\u2019m stuck so I\u2019m going to guess something wild\u201d behavior LLMs sometimes fall into. When it got something wrong, it was usually because it applied the wrong convention, not because it made something up.\n\nOverall, ChatGPT handled the written problems well. It can one-shot most derivations, and when it does mess up, it\u2019s usually easy to steer it back on track with a focused prompt. The combination of clear explanations, good algebraic intuition, and willingness to self-correct makes it a pretty solid tool for these kinds of homework questions.\n\nBelow is the annotated log:\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>For Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated, and how solid its reasoning was. For most of the assignment, especially Questions 1 through 4, it did surprisingly well. It handled the computation-heavy parts, the code-completion questions, and the conceptual attention explanations almost exactly the way the official solutions did. Its derivations for expectations, variances, argmax attention, and the multi-head attention code were all clean and matched the intended logic with basically no backtracking.</paragraph><paragraph>One thing that stood out was how consistently the model could jump straight into the right structure of the problem. It didn\u2019t need much nudging to set up sums, identify independence assumptions, or translate equations into PyTorch einsums. Even the complexity analysis in Question 4 lined up with the official answers once we made sure the symbols matched. When it was right, it was very right, and the explanations were easy to follow.</paragraph><paragraph>The main issue came up in Question 6. ChatGPT fell into a pretty common trap: it used the homogeneous quadratic kernel (q transpose k squared) instead of the standard degree-2 polynomial kernel ((q transpose k plus 1) squared). Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer. But once I asked it to revisit its reasoning instead of giving a new solution, it immediately spotted the mistake, explained why the earlier map didn\u2019t match the actual kernel, and rebuilt everything correctly. So the error wasn\u2019t really a hallucination\u2014more like it defaulted to a familiar formula without checking whether it matched what the homework meant.</paragraph><paragraph>Aside from that one slip, the model didn\u2019t really hallucinate or spiral. It stayed structured and patient even in the longer reasoning chains, and it didn\u2019t show any of the \u201cI\u2019m stuck so I\u2019m going to guess something wild\u201d behavior LLMs sometimes fall into. When it got something wrong, it was usually because it applied the wrong convention, not because it made something up.</paragraph><paragraph>Overall, ChatGPT handled the written problems well. It can one-shot most derivations, and when it does mess up, it\u2019s usually easy to steer it back on track with a focused prompt. The combination of clear explanations, good algebraic intuition, and willingness to self-correct makes it a pretty solid tool for these kinds of homework questions.<break/><break/>Below is the annotated log:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/maCBrrNiQzyPdCXBYGOHvViP\" filename=\"ChatGPT - hw9.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T20:45:21.592344+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397813,
            "author": "Keshab Agarwal",
            "project_title": "Special Participation E: NotebookLM to Understand Transformers",
            "post_body": "I used NotebookLM as a study guide to clarify my understanding of transformers. I uploaded YouTube lecture audio recordings along with corresponding lecture notes and homework solutions as sources, then used the chat feature to ask clarifying questions and explore concepts that further built on the concepts that were covered in class. \n\nBelow, I've documented how I obtained the audio recordings and set up the notebook. I have also added my annotated chat transcript for some of my questions in the same pdf.\n\nThe attached link includes the slides, mind map, and infographic I created:\n\nhttps://notebooklm.google.com/notebook/ecbea63e-77e8-494b-a595-0bd5223c6039\n\nThe mind-map can serve as a great way to review a hierarchy of concepts covered in a series of lectures, and act as a guard-rail to check if there are any topics which you are unfamiliar or unsure about as you prepare for the exam. Caution: the mind-map can sometimes omit certain topics, so don't treat it as an exhaustive list.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used NotebookLM as a study guide to clarify my understanding of transformers. I uploaded YouTube lecture audio recordings along with corresponding lecture notes and homework solutions as sources, then used the chat feature to ask clarifying questions and explore concepts that further built on the concepts that were covered in class. </paragraph><paragraph>Below, I've documented how I obtained the audio recordings and set up the notebook. I have also added my annotated chat transcript for some of my questions in the same pdf.</paragraph><file url=\"https://static.us.edusercontent.com/files/C9WOn3GOnSg1m9NbvbeW39hK\" filename=\"Scpecial Participation E Report.pdf\"/><paragraph>The attached link includes the slides, mind map, and infographic I created:</paragraph><paragraph><link href=\"https://notebooklm.google.com/notebook/ecbea63e-77e8-494b-a595-0bd5223c6039\">https://notebooklm.google.com/notebook/ecbea63e-77e8-494b-a595-0bd5223c6039</link></paragraph><paragraph>The mind-map can serve as a great way to review a hierarchy of concepts covered in a series of lectures, and act as a guard-rail to check if there are any topics which you are unfamiliar or unsure about as you prepare for the exam. Caution: the mind-map can sometimes omit certain topics, so don't treat it as an exhaustive list.</paragraph><file url=\"https://static.us.edusercontent.com/files/bWQlAG3dM8aFGWrO2DUgygA0\" filename=\"Transformers Mind Map.png\"/></document>",
            "links": [
                "https://notebooklm.google.com/notebook/ecbea63e-77e8-494b-a595-0bd5223c6039"
            ],
            "attachments": [],
            "created_at": "2025-12-03T20:42:14.777161+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397617,
            "author": "Tin Yau",
            "project_title": "Special\u202f Participation\u202f E\u202f\u2014\u202fUnderstanding\u202fScaleRL\u202fThrough\u202fIteration",
            "post_body": "Sorry for forgetting to upload it last weekend.\n\nI learn best when I understand why equations are built the way they are. In studying the ScaleRL objective, I used Gemini to move from simply labeling each term to grasping their interactions and intuition. Through several prompt iterations, the explanations evolved from lists of definitions into analogies\u2014like \u201citerative drafts with a safety net\u201d\u2014that helped me connect math to reasoning. Writing critical annotations made me more aware of when the AI\u2019s intuition matched the lectures and when it oversimplified. This method turned a dense formula into a story of stable learning, reminding me that real understanding comes from questioning every symbol\u2019s purpose",
            "content_xml": "<document version=\"2.0\"><paragraph>Sorry for forgetting to upload it last weekend.</paragraph><paragraph>I learn best when I understand why equations are built the way they are. In studying the ScaleRL objective, I used Gemini to move from simply labeling each term to grasping their interactions and intuition. Through several prompt iterations, the explanations evolved from lists of definitions into analogies\u2014like \u201citerative drafts with a safety net\u201d\u2014that helped me connect math to reasoning. Writing critical annotations made me more aware of when the AI\u2019s intuition matched the lectures and when it oversimplified. This method turned a dense formula into a story of stable learning, reminding me that real understanding comes from questioning every symbol\u2019s purpose</paragraph><file url=\"https://static.us.edusercontent.com/files/Pcl12GWvaeudE3QJZOZ7igly\" filename=\"\u202fUnderstanding\u202fScaleRL\u202fThrough\u202fIteration.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T18:48:43.362514+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397298,
            "author": "Gustavo Jose Ortiz Zepeda",
            "project_title": "Special Participation A: Hw2 with Gemini Pro 3 Thinking Mode",
            "post_body": "For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I used images as the prompt instead of the direct text.\n\nSummary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these questions). The answers were correct and also using different notations.",
            "content_xml": "<document version=\"2.0\"><paragraph>For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I used images as the prompt instead of the direct text.</paragraph><paragraph><bold>Summary:</bold> Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these questions). The answers were correct and also using different notations.</paragraph><file url=\"https://static.us.edusercontent.com/files/61cddJl2cZ6998P8oS9xFfXS\" filename=\"hw2-specialparticipationA-Gemini3ProThinking.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T17:16:38.165773+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397241,
            "author": "Sammie Smith",
            "project_title": "Special Participation E: Student-Facing Misconception Guides",
            "post_body": "Hey everyone,\n\nI wanted to share my experience using an LLM to generate a \"Common Mistakes\" guide for my optimization lecture notes. I wanted to build a guide that addresses common student conceptual pitfalls to help me check my understanding of lecture material before/during homework. \n\nTL;DR: careful prompting dramatically improved conceptual structure, but mathematical errors make the output dangerous. Even though this didn't work super well, I think it still is useful to know that we can't easily prompt models to identify potential student misconceptions/pitfalls purely from lecture material. And I think this reflects on a shared student experience these days: AI can explain the lecture notes but it struggles to identify the student knowledge state, which is critical in guiding students from misunderstanding to mastery using the correct level of rigor. \n\nHere are my results:\n\n(I used the .tex lecture notes generated from Jameson Liu. See ed thread #301)\n\n1) with chatGPT\n\n\n\n2) with claude\n\n\n\nMy General Commentary:\n\nMy first attempt with ChatGPT was a disaster\u2014superficial and completely missed that data geometry drives optimization behavior. So I crafted a detailed prompt forcing the AI to: (1) analyze the \"big picture\" first, (2) identify where students shift from scalar to geometric thinking, (3) derive misconceptions organically from lecture structure, (4) organize around themes, and (5) include worked examples. The prompt was topic-agnostic.\n\nThe conceptual improvements were real. The AI organized misconceptions into coherent themes like \"Spectral Properties vs. Dataset Size,\" consistently used geometric language (\"elongated bowl,\" \"steep walls\"), correctly identified SVD as the unifying framework, and created an excellent debugging checklist. The warning signs were specific: \"Your justification for \u03b7 mentions only n or d, not \u03c3_max.\"\n\nBut the mathematics is catastrophically wrong. Worked examples have incorrect arithmetic that doesn't even stay consistent across steps. Claims are asserted without justification then \"verified\" with completely different numbers handwaved as \"similar patterns.\" Examples reference undefined quantities. I couldn't verify most calculations due to missing steps, ambiguous notation, or wrong arithmetic.\n\nThe fundamental problem: the AI confidently generates mathematical content it cannot verify. It's sophisticated enough to be convincing but wrong enough to be dangerous. Students won't spot errors because the surrounding text sounds authoritative. The time sink is verification, not generation.\n\n---------------------------------------\n\nMy Specific Commentary For Claude Generation:\n\nThe \"Experiments to Deepen Understanding\" section lists good ideas but provides no guidance on implementation, what to observe, or what conclusions to draw. Telling students to \"measure the smoothness (variance of step directions)\" for momentum without explaining how to compute this or what values to expect is not helpful. These experiments need more scaffolding.\n\nDespite the improved tone, the fundamental framing is still \"here are mistakes and how to fix them\" rather than \"here's how to build robust understanding.\" The guide would be stronger if it started each section with \"Here's what you should understand\" and then showed common pitfalls as deviations from that understanding, rather than leading with the mistakes\n\nTheme 1, Mistake 1.1: First warning sign is too specific. The third is too vague\n\n\n\nMath Hallucination! In the complete 2x2 example, there's a critical arithmetic error that undermines the entire demonstration. The guide gets [4 0.04]^T for w1, but really this should be [40, 0.04]^T. This makes the calculation for w2 incorrect as well. This is a serious pedagogical failure because students trying to verify the calculation will get confused and lose trust in the material. A worked example with wrong arithmetic is worse than no example at all.\n\nThe guide states \"Error [1,1]^T\" but never defines what \"Error\" means here. Is this w* - wt or wt - w*? The inconsistent usage makes it impossible to verify the calculations. Later it says \"error multiplied by 0.8 per iteration\" but earlier showed the update factor as (1\u22122*0.1*9)=\u22120.8, which is negative. The magnitude is 0.8, sure, but this distinction matters when students are trying to understand convergence versus oscillation.\n\nWhile the \"Connection to Practice\" sections are useful, they're too brief and disconnected from the main exposition. For instance, when discussing condition number, the guide mentions batch normalization helps but doesn't explain how or why it affects the condition number. These connections feel tacked on rather than integrated. A student reading this still won't understand why practitioners obsess over things like feature normalization.\n\nThe guide calculates scaling factors and then says \"Wait, this seems wrong!\" This is pedagogically bizarre. It makes it seem like even the guide author is confused. The issue is that the guide conflates two different quantities: the scaling factor sigma_i / (sigma_i^2 + lambda) \n(which appears in the formula) with the actual shrinkage of the final solution. When it says scaling factor 2 is 1.429 (which is >1), this should have been a red flag that something was wrong with the setup. The \"correction\" that follows is actually computing a different quantity (wi,Ridge) without clarifying the distinction. This will confuse students who are trying to understand what the scaling factor actually means.\n\n\n\nThe numerical illustration considers f(w) = w^2 and claims data pts give gradients of f1(1)=3, etc. But this doesn't make sense. If f(w) = w^2, the the gradient is 2w, and so the gradient evaluated at 1 should be 2 always. The guide seems to be trying to show per sample gradients but it never defines what the individual fi functions are. This example fails because it;s trying to illustrate a concept without properly setting up the mathematical framework. Students will be confused about what \"data points giving different gradients\" means for the same function at the same point.\n\nMany of the self-check questions ask for numerical calculations rather than conceptual reasoning\n\nThere is not justification or derivation of the claim that early stopping has a specific ridge equivalent. This is presented as fact but is actually a loose heuristic that depends heavily on the problem structure. The worked example that follows doesn't really validate this relationship\u2014the numbers are completely different (w3 = [.936, .174], wridge =[.730,.026]). The guide just handwaves this as \"similar patterns.\" This is intellectually dishonest. If the relationship is approximate, explain why and under what conditions it holds.\n\nThe \"Experiments to Deepen Understanding\" section lists good ideas but provides no guidance on implementation, what to observe, or what conclusions to draw. Telling students to \"measure the smoothness (variance of step directions)\" for momentum without explaining how to compute this or what values to expect is not helpful. These experiments need more scaffolding.\n\nDespite the improved tone, the fundamental framing is still \"here are mistakes and how to fix them\" rather than \"here's how to build robust understanding.\" The guide would be stronger if it started each section with \"Here's what you should understand\" and then showed common pitfalls as deviations from that understanding, rather than leading with the mistakes.",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey everyone,</paragraph><paragraph>I wanted to share my experience using an LLM to generate a \"Common Mistakes\" guide for my optimization lecture notes. I wanted to build a guide that addresses common student conceptual pitfalls to help me check my understanding of lecture material before/during homework. </paragraph><paragraph>TL;DR: careful prompting dramatically improved conceptual structure, but mathematical errors make the output dangerous. Even though this didn't work super well, I think it still is useful to know that we can't easily prompt models to identify potential student misconceptions/pitfalls purely from lecture material. And I think this reflects on a shared student experience these days: AI can explain the lecture notes but it struggles to identify the student knowledge state, which is critical in guiding students from misunderstanding to mastery using the correct level of rigor. </paragraph><paragraph>Here are my results:</paragraph><paragraph>(I used the .tex lecture notes generated from Jameson Liu. See ed thread #301)</paragraph><paragraph>1) with chatGPT</paragraph><file url=\"https://static.us.edusercontent.com/files/qmhKlm8uWY7Kbe2j8oNyzaZb\" filename=\"cs182_special_participation_student_guide.pdf\"/><file url=\"https://static.us.edusercontent.com/files/odfP47Ua1irdJcMCYHcA8lqB\" filename=\"cs182_chatgpt_prompt.txt\"/><paragraph/><paragraph>2) with claude</paragraph><file url=\"https://static.us.edusercontent.com/files/AHDjnaw6Jb5RAnn1c7g99f6g\" filename=\"cs182_claude_prompt.txt\"/><file url=\"https://static.us.edusercontent.com/files/NL84NPxtdag3RxZcw4CB6EYr\" filename=\"cs182_special_participation_student_guide (1).pdf\"/><paragraph/><paragraph>My General Commentary:</paragraph><paragraph>My first attempt with ChatGPT was a disaster\u2014superficial and completely missed that data geometry drives optimization behavior. So I crafted a detailed prompt forcing the AI to: (1) analyze the \"big picture\" first, (2) identify where students shift from scalar to geometric thinking, (3) derive misconceptions organically from lecture structure, (4) organize around themes, and (5) include worked examples. The prompt was topic-agnostic.</paragraph><paragraph>The conceptual improvements were real. The AI organized misconceptions into coherent themes like \"Spectral Properties vs. Dataset Size,\" consistently used geometric language (\"elongated bowl,\" \"steep walls\"), correctly identified SVD as the unifying framework, and created an excellent debugging checklist. The warning signs were specific: \"Your justification for \u03b7 mentions only n or d, not \u03c3_max.\"</paragraph><paragraph>But the mathematics is catastrophically wrong. Worked examples have incorrect arithmetic that doesn't even stay consistent across steps. Claims are asserted without justification then \"verified\" with completely different numbers handwaved as \"similar patterns.\" Examples reference undefined quantities. I couldn't verify most calculations due to missing steps, ambiguous notation, or wrong arithmetic.</paragraph><paragraph>The fundamental problem: the AI confidently generates mathematical content it cannot verify. It's sophisticated enough to be convincing but wrong enough to be dangerous. Students won't spot errors because the surrounding text sounds authoritative. The time sink is verification, not generation.</paragraph><paragraph>---------------------------------------</paragraph><paragraph>My Specific Commentary For Claude Generation:</paragraph><paragraph>The \"Experiments to Deepen Understanding\" section lists good ideas but provides no guidance on implementation, what to observe, or what conclusions to draw. Telling students to \"measure the smoothness (variance of step directions)\" for momentum without explaining how to compute this or what values to expect is not helpful. These experiments need more scaffolding.</paragraph><paragraph>Despite the improved tone, the fundamental framing is still \"here are mistakes and how to fix them\" rather than \"here's how to build robust understanding.\" The guide would be stronger if it started each section with \"Here's what you should understand\" and then showed common pitfalls as deviations from that understanding, rather than leading with the mistakes</paragraph><paragraph>Theme 1, Mistake 1.1: First warning sign is too specific. The third is too vague</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/cHbC9MPZZ3Mp1tsUDHQCvCny\" width=\"626\" height=\"238\"/></figure><paragraph/><paragraph>Math Hallucination! In the complete 2x2 example, there's a critical arithmetic error that undermines the entire demonstration. The guide gets [4 0.04]^T for w1, but really this should be [40, 0.04]^T. This makes the calculation for w2 incorrect as well. This is a serious pedagogical failure because students trying to verify the calculation will get confused and lose trust in the material. A worked example with wrong arithmetic is worse than no example at all.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/prsFBWgl2iMMcFP4r2AJEIM3\" width=\"613\" height=\"368\"/></figure><paragraph>The guide states \"Error [1,1]^T\" but never defines what \"Error\" means here. Is this w* - wt or wt - w*? The inconsistent usage makes it impossible to verify the calculations. Later it says \"error multiplied by 0.8 per iteration\" but earlier showed the update factor as (1\u22122*0.1*9)=\u22120.8, which is negative. The magnitude is 0.8, sure, but this distinction matters when students are trying to understand convergence versus oscillation.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1WiZtafV9gLuv25x2vEIZxQb\" width=\"550\" height=\"430\"/></figure><paragraph>While the \"Connection to Practice\" sections are useful, they're too brief and disconnected from the main exposition. For instance, when discussing condition number, the guide mentions batch normalization helps but doesn't explain how or why it affects the condition number. These connections feel tacked on rather than integrated. A student reading this still won't understand why practitioners obsess over things like feature normalization.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/zmih0izXnH8ZMwCeXqq9w0af\" width=\"555\" height=\"165\"/></figure><paragraph>The guide calculates scaling factors and then says \"Wait, this seems wrong!\" This is pedagogically bizarre. It makes it seem like even the guide author is confused. The issue is that the guide conflates two different quantities: the scaling factor sigma_i / (sigma_i^2 + lambda) <break/>(which appears in the formula) with the actual shrinkage of the final solution. When it says scaling factor 2 is 1.429 (which is &gt;1), this should have been a red flag that something was wrong with the setup. The \"correction\" that follows is actually computing a different quantity (wi,Ridge) without clarifying the distinction. This will confuse students who are trying to understand what the scaling factor actually means.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/dtsANrQas6duL8GOwjN4EsLk\" width=\"404\" height=\"273\"/></figure><paragraph/><paragraph>The numerical illustration considers f(w) = w^2 and claims data pts give gradients of f1(1)=3, etc. But this doesn't make sense. If f(w) = w^2, the the gradient is 2w, and so the gradient evaluated at 1 should be 2 always. The guide seems to be trying to show per sample gradients but it never defines what the individual fi functions are. This example fails because it;s trying to illustrate a concept without properly setting up the mathematical framework. Students will be confused about what \"data points giving different gradients\" means for the same function at the same point.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/iwC0bnD1Nwt1tL44oGONJY5G\" width=\"624\" height=\"191\"/></figure><paragraph>Many of the self-check questions ask for numerical calculations rather than conceptual reasoning</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/SCSegdxmmZtUmrgDja10eJfi\" width=\"562\" height=\"165\"/></figure><paragraph>There is not justification or derivation of the claim that early stopping has a specific ridge equivalent. This is presented as fact but is actually a loose heuristic that depends heavily on the problem structure. The worked example that follows doesn't really validate this relationship\u2014the numbers are completely different (w3 = [.936, .174], wridge =[.730,.026]). The guide just handwaves this as \"similar patterns.\" This is intellectually dishonest. If the relationship is approximate, explain why and under what conditions it holds.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/56dwdLpOBHq7vgUODkUTWhEa\" width=\"557\" height=\"72\"/></figure><paragraph>The \"Experiments to Deepen Understanding\" section lists good ideas but provides no guidance on implementation, what to observe, or what conclusions to draw. Telling students to \"measure the smoothness (variance of step directions)\" for momentum without explaining how to compute this or what values to expect is not helpful. These experiments need more scaffolding.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/shwc3ntIPjU1GLf07i3zQfjm\" width=\"592\" height=\"258\"/></figure><paragraph>Despite the improved tone, the fundamental framing is still \"here are mistakes and how to fix them\" rather than \"here's how to build robust understanding.\" The guide would be stronger if it started each section with \"Here's what you should understand\" and then showed common pitfalls as deviations from that understanding, rather than leading with the mistakes.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T17:02:57.775101+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397226,
            "author": "Tin Yau",
            "project_title": "Special Participation A: Gemini 3 pro on HW 8",
            "post_body": "I used Gemini 3 Pro to solve the non\u2011coding portion of HW 8. Overall, Gemini did an excellent job producing clear and well\u2011structured mathematical derivations, often matching the logical flow of the official solutions. I especially appreciated its ability to organize the reasoning into intuitive steps and explain the purpose behind each transformation. Gemini sometimes took a slightly different path to reach the final answer, but its results were fully consistent after minor prompting. Overall, I was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used <bold>Gemini 3 Pro</bold> to solve the non\u2011coding portion of <bold>HW 8</bold>. Overall, Gemini did an excellent job producing clear and well\u2011structured mathematical derivations, often matching the logical flow of the official solutions. I especially appreciated its ability to organize the reasoning into intuitive steps and explain the purpose behind each transformation. Gemini sometimes took a slightly different path to reach the final answer, but its results were fully consistent after minor prompting. Overall, I was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/woyKGrCUftrJJQ6Fn5M3mbF0\" filename=\"A_Gemini_3_pro.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T16:58:24.482712+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7397166,
            "author": "Ijin Yu",
            "project_title": "Special Participation A: Gemini 3 Pro on HW 2 Written Questions",
            "post_body": "Executive Summary: Interaction with Gemini on Deep Learning Theory\n\nModel Tested: Gemini 3 Pro\n\nDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)\n\nOverall Performance: 100% Success Rate (One-shot)\n\nPerformance Overview\n\nThe model was tasked with solving non-coding theoretical problems involving:\n\nOptimization Derivations: Deriving analytical solutions for penalized linear improvement using Euclidean ($L_2$) and Infinity ($L_\\infty$) norms.\n\nOptimizer Convergence: Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem.\n\nDistributed Training: Calculating communication costs (message count and size) for All-to-All, Parameter Server, and Ring All-Reduce architectures.\n\nIn every instance, the model provided the correct analytical solution and numerical answers on the first attempt (one-shot). No prompt engineering, iterative correction, or \"dragging\" of the model was required to arrive at the solution. The model successfully transcribed complex mathematical notation directly from uploaded screenshots without OCR errors.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model demonstrated robust reasoning capabilities. It correctly identified standard optimizers (Gradient Descent, SignSGD) from first-principles derivations and accurately recalled specific distributed systems constraints (e.g., the $2(n-1)$ steps in Ring All-Reduce).\n\nBehavioral Observations: Uncertainty and Verification\n\nA distinct behavioral trait observed during the session was the model's tendency to simulate self-verification and hedging.\n\n\"Double-Checking\": Despite the high accuracy of the final outputs, the model\u2019s reasoning process appeared to involve recursive checks. It would often derive a step, then implicitly verify it against known standard results (e.g., checking if the derived update rule matched the standard definition of SignSGD) before committing to the answer.\n\nHedging Language: The model frequently used probabilistic language such as \"I believe\" or \"this suggests,\" rather than authoritative absolutes, even when the math was unambiguous. This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence.\n\nConclusion\n\nGemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy. The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the \"reasoning track\" for the model's internal consistency.\n\n",
            "content_xml": "<document version=\"2.0\"><heading level=\"3\">Executive Summary: Interaction with Gemini on Deep Learning Theory</heading><paragraph>Model Tested: Gemini 3 Pro</paragraph><paragraph>Domain: Deep Learning Optimization &amp; Distributed Training (CS 182/282 Context)</paragraph><paragraph>Overall Performance: 100% Success Rate (One-shot)</paragraph><paragraph>Performance Overview</paragraph><paragraph>The model was tasked with solving non-coding theoretical problems involving:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>Optimization Derivations:</bold> Deriving analytical solutions for penalized linear improvement using Euclidean ($L_2$) and Infinity ($L_\\infty$) norms.</paragraph></list-item><list-item><paragraph><bold>Optimizer Convergence:</bold> Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem.</paragraph></list-item><list-item><paragraph><bold>Distributed Training:</bold> Calculating communication costs (message count and size) for All-to-All, Parameter Server, and Ring All-Reduce architectures.</paragraph></list-item></list><paragraph>In every instance, the model provided the correct analytical solution and numerical answers on the first attempt (<bold>one-shot</bold>). No prompt engineering, iterative correction, or \"dragging\" of the model was required to arrive at the solution. The model successfully transcribed complex mathematical notation directly from uploaded screenshots without OCR errors.</paragraph><paragraph><bold>Hallucinations &amp; Accuracy</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Hallucination Rate:</bold> 0%.</paragraph></list-item><list-item><paragraph>The model demonstrated robust reasoning capabilities. It correctly identified standard optimizers (Gradient Descent, SignSGD) from first-principles derivations and accurately recalled specific distributed systems constraints (e.g., the $2(n-1)$ steps in Ring All-Reduce).</paragraph></list-item></list><paragraph>Behavioral Observations: Uncertainty and Verification</paragraph><paragraph>A distinct behavioral trait observed during the session was the model's tendency to simulate self-verification and hedging.</paragraph><list style=\"unordered\"><list-item><paragraph><bold>\"Double-Checking\":</bold> Despite the high accuracy of the final outputs, the model\u2019s reasoning process appeared to involve recursive checks. It would often derive a step, then implicitly verify it against known standard results (e.g., checking if the derived update rule matched the standard definition of SignSGD) before committing to the answer.</paragraph></list-item><list-item><paragraph><bold>Hedging Language:</bold> The model frequently used probabilistic language such as \"I believe\" or \"this suggests,\" rather than authoritative absolutes, even when the math was unambiguous. This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence.</paragraph></list-item></list><paragraph>Conclusion</paragraph><paragraph>Gemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy. The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the \"reasoning track\" for the model's internal consistency.</paragraph><file url=\"https://static.us.edusercontent.com/files/9hAVpcxKaswAaeykKGjmtSCr\" filename=\"q1.pdf\"/><file url=\"https://static.us.edusercontent.com/files/j2qU6fS8iumFddzcc5zVkF1V\" filename=\"q2.pdf\"/><file url=\"https://static.us.edusercontent.com/files/aAAIVIqMfstp21bcCessvEaC\" filename=\"q5.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T16:47:42.131618+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7396775,
            "author": "Manhar Gupta",
            "project_title": "Special Participation E: Custom 'Mastery Coach' Gem on Gemini 3 Pro to learn RNNs, self-supervision and SSMs",
            "post_body": "For the past few days, I had been using Gemini Learning Coach Gem for my other engineering classes to learn concepts by letting the model understand the lecture transcripts and the slides. I noticed that the learning coach used to go along with what I said if I was correct. However, the Learning Coach Gem would only detail specific nuances and niches only if explicitly told to. I was looking for something that would expand upon what I said in a detailed manner and help me cover any gaps in my explanations.\n\nIn one of my conversations with Gemini which was covering material for another class, I had been rigorously asking it to cover things missing in my explanation and help me formalize my understanding at every step so that it moves forward to tell me what my explanation and conceptual depth can improve upon. I realized that this conversation might have enough context to create a system prompt that would emphasize what I would want from a Gem based on learning.\n\nSo, I decided to put a specific JSON prompt close to the end of one of my learning conversations with Gemini Learning Coach Gem. I told it to generate a custom system prompt for an AI assistant that would emphasize rigor. For the system prompt, I told it to not have specific references to the material we had been covering in the chat so that the Custom Gem would act better as a general tool emphasized towards learning and education. I have added a screenshot of the prompt I used and the system prompt it generated as \u2018Initial system prompt\u2019\n\nIt created the following system prompt which however, still had the specific references. I put the system prompt in another chat and asked Gemini to remove specific references of the mentioned material. I put the refined system prompt inside the 'instructions' section for creation of a Custom Gem. I called this Gem 'Mastery Coach'. I have also attached this cleaned system prompt that I finally used as \u2018Generalized Mastery Coach Prompt\u2019\n\nSo for learning RNNs, self-supervision and SSMs, I decided to use this Gem in a similar way to how I began conversations with \u2018Learning Coach\u2019. I uploaded the lecture transcripts and notes. Since the transcripts were taken from YouTube, they would have unnecessary timestamps. I separately asked ChatGPT to just remove the timestamp while keeping all wording intact.\n\nThe starting prompt for the conversation was a high-level explanation of the context (the class which the material is for, my objectives, other relevant context) and concepts being covered. These beginning prompts are in the uploaded chats itself. I used to add short pointers which I intended would act as an additional 'system prompt' so that the Gem would focus better on the particular material just added. From there, the entire conversation followed which has also been attached with detailed annotations. \n\nI have also attached a link to the Gem which would directly allow you to use it in your Gemini environment.\n\nReflection: Giving both the transcripts and the slides was beneficial. While you would expect it to cover concepts in the same progression as the lectures, it took a different order in explaining things. I especially noticed that the concept of \u2018A\u2019 diagonalization was covered at a point where it wasn\u2019t explicitly mentioned in the lecture. While asking probing questions, it was giving away too much in hints. I have also mentioned this in annotations as something others can work on by building a Custom Gem of their own using a modified version of the system prompt attached below.\n\nLink to use 'Mastery Coach' directly: https://gemini.google.com/gem/1JoLndovdl25vJ9DPMw9NGg6wEC2gMDn-?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>For the past few days, I had been using Gemini Learning Coach Gem for my other engineering classes to learn concepts by letting the model understand the lecture transcripts and the slides. I noticed that the learning coach used to go along with what I said if I was correct. However, the Learning Coach Gem would only detail specific nuances and niches only if explicitly told to. I was looking for something that would expand upon what I said in a detailed manner and help me cover any gaps in my explanations.</paragraph><paragraph>In one of my conversations with Gemini which was covering material for another class, I had been rigorously asking it to cover things missing in my explanation and help me formalize my understanding at every step so that it moves forward to tell me what my explanation and conceptual depth can improve upon. I realized that this conversation might have enough context to create a system prompt that would emphasize what I would want from a Gem based on learning.</paragraph><paragraph>So, I decided to put a specific JSON prompt close to the end of one of my learning conversations with Gemini Learning Coach Gem. I told it to generate a custom system prompt for an AI assistant that would emphasize rigor. For the system prompt, I told it to not have specific references to the material we had been covering in the chat so that the Custom Gem would act better as a general tool emphasized towards learning and education. I have added a screenshot of the prompt I used and the system prompt it generated as \u2018Initial system prompt\u2019</paragraph><paragraph>It created the following system prompt which however, still had the specific references. I put the system prompt in another chat and asked Gemini to remove specific references of the mentioned material. I put the refined system prompt inside the 'instructions' section for creation of a Custom Gem. I called this Gem 'Mastery Coach'. I have also attached this cleaned system prompt that I finally used as \u2018Generalized Mastery Coach Prompt\u2019</paragraph><paragraph>So for learning RNNs, self-supervision and SSMs, I decided to use this Gem in a similar way to how I began conversations with \u2018Learning Coach\u2019. I uploaded the lecture transcripts and notes. Since the transcripts were taken from YouTube, they would have unnecessary timestamps. I separately asked ChatGPT to just remove the timestamp while keeping all wording intact.</paragraph><paragraph>The starting prompt for the conversation was a high-level explanation of the context (the class which the material is for, my objectives, other relevant context) and concepts being covered. These beginning prompts are in the uploaded chats itself. I used to add short pointers which I intended would act as an additional 'system prompt' so that the Gem would focus better on the particular material just added. From there, the entire conversation followed which has also been attached with detailed annotations. <break/><break/>I have also attached a link to the Gem which would directly allow you to use it in your Gemini environment.<break/><break/>Reflection: Giving both the transcripts and the slides was beneficial. While you would expect it to cover concepts in the same progression as the lectures, it took a different order in explaining things. I especially noticed that the concept of \u2018A\u2019 diagonalization was covered at a point where it wasn\u2019t explicitly mentioned in the lecture. While asking probing questions, it was giving away too much in hints. I have also mentioned this in annotations as something others can work on by building a Custom Gem of their own using a modified version of the system prompt attached below.</paragraph><paragraph>Link to use 'Mastery Coach' directly: https://gemini.google.com/gem/1JoLndovdl25vJ9DPMw9NGg6wEC2gMDn-?usp=sharing</paragraph><file url=\"https://static.us.edusercontent.com/files/Bvv5jwuXNoERIfNgFXFdnH1i\" filename=\"Mastery Coach on RNNs, self-supervision, SSMs (1).pdf\"/><file url=\"https://static.us.edusercontent.com/files/zzYQq911CPYmYYn0qWEYQaol\" filename=\"Generalized Mastery Coach Prompt.pdf\"/><file url=\"https://static.us.edusercontent.com/files/tl8x3RgutqE3q0od9uh8R1bz\" filename=\"Initial system prompt.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T15:37:45.769937+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7396752,
            "author": "Alex Luu",
            "project_title": "Special Participation E: AI Further Studies Guide",
            "post_body": "After lectures, I often find myself interested in some of the topics briefly mentioned. Specifically, the implementation details are interesting, but since they are only briefly covered, the lectures do not go into detail about them. I decided to use Sonnet 4.5 to help me with expanding my knowledge in a way I can understand. \n\nOne topic in particular was about the KV cache. Lecture only mentioned it but does not go into the specifics of the implementation and how it fits in the inference steps.\n\nHere is my prompt:\n\n\"\"\"\nI am taking a deep learning class. Your job is to help me fill the gap in my knowledge from lecture. Specifically, I want you to help me understand the KV cache in more detail. \n\nI have attached relevant lecture notes that talk about transformers. \n\nHere are my questions:\n1. What is the KV cache?\n\n2. What specifically is cached?\n\n3. How does prefilling and autoregressive generation work with the KV cache?\n\"\"\"\nI attached lecture notes 18 and 19 with this. \n\nThe results were pretty good as Claude was very detailed with the answers. I had many follow-up questions and it answered them perfectly. I especially like how it references the lecture notes so that the math notation it used was consistent. This made it easier to understand. I have attached the annotated log below.",
            "content_xml": "<document version=\"2.0\"><paragraph>After lectures, I often find myself interested in some of the topics briefly mentioned. Specifically, the implementation details are interesting, but since they are only briefly covered, the lectures do not go into detail about them. I decided to use Sonnet 4.5 to help me with expanding my knowledge in a way I can understand. </paragraph><paragraph>One topic in particular was about the KV cache. Lecture only mentioned it but does not go into the specifics of the implementation and how it fits in the inference steps.</paragraph><paragraph>Here is my prompt:</paragraph><paragraph>\"\"\"<break/>I am taking a deep learning class. Your job is to help me fill the gap in my knowledge from lecture. Specifically, I want you to help me understand the KV cache in more detail. </paragraph><paragraph>I have attached relevant lecture notes that talk about transformers. <break/><break/>Here are my questions:<break/>1. What is the KV cache?</paragraph><paragraph>2. What specifically is cached?</paragraph><paragraph>3. How does prefilling and autoregressive generation work with the KV cache?<break/>\"\"\"<break/>I attached lecture notes 18 and 19 with this. </paragraph><paragraph>The results were pretty good as Claude was very detailed with the answers. I had many follow-up questions and it answered them perfectly. I especially like how it references the lecture notes so that the math notation it used was consistent. This made it easier to understand. I have attached the annotated log below.</paragraph><file url=\"https://static.us.edusercontent.com/files/gw6Rg8yxNSJASxbeKiVVCdiY\" filename=\"Special Participation E.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T15:34:01.931379+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7396526,
            "author": "Hanyang Gu",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 2 Coding Questions",
            "post_body": "The code generated by Gemini 3 demonstrates a high level of accuracy and adherence to the assignment requirements. In almost all cases, the logic implemented by Gemini is identical or functionally equivalent to the staff solutions. The code style is consistent with the provided codebase, and the implementations are generally concise and idiomatic.\n\nSome Key observations:\n- Correctness: The core algorithms (optimizers, neural network layers, backpropagation) are implemented correctly.\n- Style: The code follows standard Python and NumPy practices. Variable naming is consistent with the surrounding code.\n- Differences: Minor differences exist in hyperparameter choices (e.g., learning rate for the best model) and some implementation details (e.g., using `np.zeros` vs `np.random.normal(scale=0)`), but these do not affect correctness.\n- Completeness: All identified TODOs in the provided files were addressed.\n\nThe detailed report is attached below; it contains all Gemini responses in line compared with staff solution. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>The code generated by Gemini 3 demonstrates a high level of accuracy and adherence to the assignment requirements. In almost all cases, the logic implemented by Gemini is identical or functionally equivalent to the staff solutions. The code style is consistent with the provided codebase, and the implementations are generally concise and idiomatic.</paragraph><paragraph>Some Key observations:<break/>- Correctness: The core algorithms (optimizers, neural network layers, backpropagation) are implemented correctly.<break/>- Style: The code follows standard Python and NumPy practices. Variable naming is consistent with the surrounding code.<break/>- Differences: Minor differences exist in hyperparameter choices (e.g., learning rate for the best model) and some implementation details (e.g., using `np.zeros` vs `np.random.normal(scale=0)`), but these do not affect correctness.<break/>- Completeness: All identified TODOs in the provided files were addressed.</paragraph><paragraph>The detailed report is attached below; it contains all Gemini responses in line compared with staff solution. </paragraph><file url=\"https://static.us.edusercontent.com/files/TT2t86kCpskGO0MVyjfPS1wP\" filename=\"Special Participation B - Gemini 3 on HW2.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T15:02:53.412677+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7396249,
            "author": "Abdelaziz Mohamed",
            "project_title": "Google Colab released an extension for VS Code",
            "post_body": "Lot's of us like to use VS Code as an IDE especially if we're using Copilot so I thought it might be helpful to other students to know that Google Colab recently released an extension that connects VS Code to the Google Colab runtime. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Lot's of us like to use VS Code as an IDE especially if we're using Copilot so I thought it might be helpful to other students to know that Google Colab recently released an extension that connects VS Code to the Google Colab runtime. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T14:27:02.005071+11:00",
            "category": "Admin"
        },
        {
            "guid": 7396049,
            "author": "Minjune Kim",
            "project_title": "Special Participation B: Mistral on HW 2 Coding Parts",
            "post_body": "I used Mistral on HW 2 to test the coding parts. \n\nhttps://chat.mistral.ai/chat/b5eaeee5-f01f-480e-bb74-65040dab6010\n\nOverall: Mistral performed very well with the coding parts of HW 2. I think the solution that mistral provided almost aligns the same with the staff solution however there are some parts where it differs. With 15 parts to the coding problem, it was able to successfully one-shot 14/15. The part where it was unable to one-shot is because I did not provide necessary dependable files when giving the prompt question to Mistral. \n\nPros: \n\nI think that Mistral was able to give pretty clear explanation to the code that it wrote for each part as well as summarize what it has done and give some key points for the users. Also provided observation and parameters that's being used. \n\nWhen most of the code is given to the prompt and MIstral only has to do one-liners, it performs at a 100% accuracy for one-shot for HW 2. \n\nCons:\n\nTo be fair, I did not notice much errors when working with Mistral for HW 2. It was able to one-shot all of the coding TO-DO parts very well. While there are some coding parts that it could have made it more clear by writing cleaner code, it's performance was very satisfying at least for this hw. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Mistral on HW 2 to test the coding parts. </paragraph><paragraph><link href=\"https://chat.mistral.ai/chat/b5eaeee5-f01f-480e-bb74-65040dab6010\">https://chat.mistral.ai/chat/b5eaeee5-f01f-480e-bb74-65040dab6010</link></paragraph><paragraph>Overall: Mistral performed very well with the coding parts of HW 2. I think the solution that mistral provided almost aligns the same with the staff solution however there are some parts where it differs. With 15 parts to the coding problem, it was able to successfully one-shot 14/15. The part where it was unable to one-shot is because I did not provide necessary dependable files when giving the prompt question to Mistral. </paragraph><paragraph>Pros: </paragraph><paragraph>I think that Mistral was able to give pretty clear explanation to the code that it wrote for each part as well as summarize what it has done and give some key points for the users. Also provided observation and parameters that's being used. </paragraph><paragraph>When most of the code is given to the prompt and MIstral only has to do one-liners, it performs at a 100% accuracy for one-shot for HW 2. </paragraph><paragraph>Cons:</paragraph><paragraph>To be fair, I did not notice much errors when working with Mistral for HW 2. It was able to one-shot all of the coding TO-DO parts very well. While there are some coding parts that it could have made it more clear by writing cleaner code, it's performance was very satisfying at least for this hw. </paragraph><paragraph/></document>",
            "links": [
                "https://chat.mistral.ai/chat/b5eaeee5-f01f-480e-bb74-65040dab6010"
            ],
            "attachments": [],
            "created_at": "2025-12-03T13:58:45.503975+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7395359,
            "author": "Jorge Diaz Chao",
            "project_title": "A lightweight PyTorch library for Model-Agnostic Meta-Learning (MAML)",
            "post_body": "Over summer I tried building a pair of glasses powered by a vision model that could recognize typing on any surface. Naturally I thought about how could a model quickly adapt to different styles and or gestures. I stumbled upon this paper (Vision-Based Hand Gesture Customization from a Single Demonstration) by Apple, which is a cool application of MAML, and tried something similar. \n\nI found most MAML implementations to not be very lightweight (so not very friendly) and/or not in PyTorch, so I made a public a tiny lightweight library (QuickMAML) implementing MAML if you'd like to check that out.\n\nIn practice MAML is hard to train and suboptimal for very hard tasks for reasons explored in the course, and there has been attempts at making this beautiful idea (I think) more reliable, like How to train your MAML (MAML++), for which you can also find an implementation in the library.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Over summer I tried building a pair of glasses powered by a vision model that could recognize typing on any surface. Naturally I thought about how could a model quickly adapt to different styles and or gestures. I stumbled upon this paper (<link href=\"https://arxiv.org/abs/2402.08420v2\">Vision-Based Hand Gesture Customization from a Single Demonstration</link>) by Apple, which is a cool application of MAML, and tried something similar. </paragraph><paragraph>I found most MAML implementations to not be very lightweight (so not very friendly) and/or not in PyTorch, so I made a public a tiny lightweight library (<link href=\"https://github.com/jdiazchao/quick-maml\">QuickMAML</link>) implementing MAML if you'd like to check that out.</paragraph><paragraph>In practice MAML is hard to train and suboptimal for very hard tasks for reasons explored in the course, and there has been attempts at making this beautiful idea (I think) more reliable, like <link href=\"https://arxiv.org/abs/1810.09502\">How to train your MAML</link> (MAML++), for which you can also find an implementation in the library.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/3PMRFkFt6P3OhttU8m70fReu\" width=\"654\" height=\"291.8247011952191\"/></figure></document>",
            "links": [
                "https://arxiv.org/abs/2402.08420v2",
                "https://github.com/jdiazchao/quick-maml",
                "https://arxiv.org/abs/1810.09502"
            ],
            "attachments": [],
            "created_at": "2025-12-03T12:27:34.123109+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7395276,
            "author": "Arvind Kruthiventy",
            "project_title": "Special Participation B -- Gemini Pro 2.5 on HW8, Arvind Kruthiventy",
            "post_body": "Executive Summary: \n\nIn this report, I utilized Gemini Pro 2.5 model for completing the SSM coding portions of Homework 8. I inputted each portion of the coding question separately and checked the model's outputs but it appeared to nearly one-shot all the portions and provided clean, formatted code with very detailed explanations. It made a small mistake in implementing the convolution forward pass as it treated the operation as a traditional convolution, but with a prompt from me, it immediately identified its mistake. Interesting, its SSM convolution forward implementation seems slightly more efficient compared to the provided solutions. The analysis of its own code were thorough and a standout as it makes it easy to understand how lines of code correspond to the more abstract details. However, there is a clear tradeoff as the Pro model takes a significant amount of time even for the simpler questions. Overall I was very impressed by how it one shot the coding portion of the assignment fairly easily even if it took a fair bit of time to process each question. \n\n\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/nKhuqUI2o1UajJWaDwmHcTyF\" filename=\"Special Participation B -- Gemini Pro on HW 8, Arvind Kruthiventy.pdf\"/><paragraph>Executive Summary: </paragraph><paragraph>In this report, I utilized Gemini Pro 2.5 model for completing the SSM coding portions of Homework 8. I inputted each portion of the coding question separately and checked the model's outputs but it appeared to nearly one-shot all the portions and provided clean, formatted code with very detailed explanations. It made a small mistake in implementing the convolution forward pass as it treated the operation as a traditional convolution, but with a prompt from me, it immediately identified its mistake. Interesting, its SSM convolution forward implementation seems slightly more efficient compared to the provided solutions. The analysis of its own code were thorough and a standout as it makes it easy to understand how lines of code correspond to the more abstract details. However, there is a clear tradeoff as the Pro model takes a significant amount of time even for the simpler questions. Overall I was very impressed by how it one shot the coding portion of the assignment fairly easily even if it took a fair bit of time to process each question. </paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T12:16:55.953322+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7395127,
            "author": "Hanyang Gu",
            "project_title": "Special Participation C: Refactoring HW 6 Coding Questions.",
            "post_body": "\n\nOur team refactored the coding questions in Homework 6 to make it better for learning and cultivating deep learning intuition. The following is the executive summary: \nThe refactoring process followed by three core pillars: PEP 8 compliance, the Single Responsibility Principle (SRP), and the Google ML Engineering Guide. \n\nFirst, to adhere to the Single Responsibility Principle, we dismantled the monolithic structure of the original notebooks. Code that defined model architectures, training loops, and utility functions was extracted from the notebooks and organized into dedicated Python modules (`architectures.py`, `trainer.py`, `utils.py`, `profiling.py`, `optimizers.py`, `graph_utils.py`). This separation ensures that notebooks are reserved for high-level experimentation and narrative, while the heavy lifting is handled by robust, testable modules.\n\nSecond, we enforced PEP 8 standards across the board. This involved standardizing naming conventions (e.g., using `snake_case` for functions and `CamelCase` for classes), adding comprehensive type hints to function signatures, and including docstrings. These changes significantly improve code readability and maintainability, making it easier for students to understand the interfaces they are working with.\n\nFinally, drawing from the Google ML Engineering Guide, we implemented best practices for reproducibility and configuration management. Global variables, which often lead to silent errors and reproducibility issues, were replaced with structured `dataclasses` (e.g., `TrainingConfig`). We also introduced a centralized random seed utility to ensure that experiments are deterministic across different runs and hardware setups.\n\nThe code is included in the following public Repo:\n\nhttps://github.com/MCxiaoguu/fa25-eecs182-specials \n\nThe report is attached below:\n\nThe team members are : Hanyang Gu (SID: 3038838194, hanyanggu05ucb@berkeley.edu), Zimu Wang (SID: 3038960121, zm.wang@berkeley.edu). ",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph>Our team refactored the coding questions in Homework 6 to make it better for learning and cultivating deep learning intuition. The following is the executive summary: <break/>The refactoring process followed by three core pillars: PEP 8 compliance, the Single Responsibility Principle (SRP), and the Google ML Engineering Guide. <break/><break/>First, to adhere to the Single Responsibility Principle, we dismantled the monolithic structure of the original notebooks. Code that defined model architectures, training loops, and utility functions was extracted from the notebooks and organized into dedicated Python modules (`architectures.py`, `trainer.py`, `utils.py`, `profiling.py`, `optimizers.py`, `graph_utils.py`). This separation ensures that notebooks are reserved for high-level experimentation and narrative, while the heavy lifting is handled by robust, testable modules.<break/><break/>Second, we enforced PEP 8 standards across the board. This involved standardizing naming conventions (e.g., using `snake_case` for functions and `CamelCase` for classes), adding comprehensive type hints to function signatures, and including docstrings. These changes significantly improve code readability and maintainability, making it easier for students to understand the interfaces they are working with.<break/><break/>Finally, drawing from the Google ML Engineering Guide, we implemented best practices for reproducibility and configuration management. Global variables, which often lead to silent errors and reproducibility issues, were replaced with structured `dataclasses` (e.g., `TrainingConfig`). We also introduced a centralized random seed utility to ensure that experiments are deterministic across different runs and hardware setups.</paragraph><paragraph>The code is included in the following public Repo:</paragraph><paragraph><link href=\"https://github.com/MCxiaoguu/fa25-eecs182-specials\">https://github.com/MCxiaoguu/fa25-eecs182-specials</link> </paragraph><paragraph>The report is attached below:</paragraph><file url=\"https://static.us.edusercontent.com/files/hKNKHAdfLGQ07VoBJdCqYhN8\" filename=\"Special Participation C - Refactoring HW 6 Coding Portions.pdf\"/><paragraph>The team members are : Hanyang Gu (SID: 3038838194, hanyanggu05ucb@berkeley.edu), Zimu Wang (SID: 3038960121, zm.wang@berkeley.edu). </paragraph></document>",
            "links": [
                "https://github.com/MCxiaoguu/fa25-eecs182-specials"
            ],
            "attachments": [],
            "created_at": "2025-12-03T11:58:29.92643+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7394664,
            "author": "Mihir Rao",
            "project_title": "Special Participation E: A Deep Learning Fable",
            "post_body": "I tend to retain concepts better when I understand why? rather than just how? LLMs are really good at understanding a large amount of information and explaining them in different ways. I asked Gemini to take three lectures(18, 19, 20), and write a story about how the decisions for these architectures and formulations came to be.\n\nI went through a few iterations of a prompt, and ended up settling with one that emphasizes my desire for it to really focus on intuition rather than merely summarizing contents. One thing I found interesting is it's good use and development of analogies to explain content. I had to force Gemini to answer in complete sentences because it seemed to really like bullet points.\n\nUsing an LLM this way also made me more aware of its strengths and limitations. It\u2019s great at remixing and reframing material into something that matches my learning style, but I still have to check that the intuition it gives me lines up with the actual definitions and equations in the lectures. This method can also serve well as motivation for the mathematics in lecture. Here is what I ended up with:\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I tend to retain concepts better when I understand <italic>why</italic>? rather than just <italic>how?</italic> LLMs are really good at understanding a large amount of information and explaining them in different ways. I asked Gemini to take three lectures(18, 19, 20), and write a story about how the decisions for these architectures and formulations came to be.<break/><break/>I went through a few iterations of a prompt, and ended up settling with one that emphasizes my desire for it to really focus on intuition rather than merely summarizing contents. One thing I found interesting is it's good use and development of analogies to explain content. I had to force Gemini to answer in complete sentences because it seemed to really like bullet points.<break/><break/>Using an LLM this way also made me more aware of its strengths and limitations. It\u2019s great at remixing and reframing material into something that matches my learning style, but I still have to check that the intuition it gives me lines up with the actual definitions and equations in the lectures. This method can also serve well as motivation for the mathematics in lecture. Here is what I ended up with:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/lb84wS3w1dj3gDeuSGPpS10m\" filename=\"182 Part E 2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T11:02:42.67456+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7393994,
            "author": "Vrushank Prakash",
            "project_title": "Special Participation B: Gemini 3 Pro on HW 8",
            "post_body": "I used Gemini 3 Pro to solve the coding portions of HW 8, which includes question 2 about coding SSM Forward. Overall, Gemini 3 Pro did a pretty good job of answering the coding parts. When it was coding the SSM kernel, it used a recursive approach, which was different from the official solution. I tested this solution out and it did end up producing the same results as the official solution. I think Gemini saw the hint about divide-and-conquer from the homework and immediately thought about recursion.\n\nGemini did have trouble answering the conceptual questions within the notebooks though. It often didn't consider the varying sizes of the T and H dimensions and gave an overgeneralization (e.g. convolution is faster than recurrent for all values of H in the GPU notebook, when this isn't true if H is large). It also initially got the conceptual questions wrong for the CPU implementation by assuming that there is parallelization, when there actually isn't. I felt that it hallucinated for some of the conceptual questions and it was pretty hard to follow along. \n\nOverall, Gemini did a good job on the coding implementations but didn't have the best explanations for the conceptual questions and often needed further prompting to push it towards the right answer.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini 3 Pro to solve the coding portions of HW 8, which includes question 2 about coding SSM Forward. Overall, Gemini 3 Pro did a pretty good job of answering the coding parts. When it was coding the SSM kernel, it used a recursive approach, which was different from the official solution. I tested this solution out and it did end up producing the same results as the official solution. I think Gemini saw the hint about divide-and-conquer from the homework and immediately thought about recursion.</paragraph><paragraph>Gemini did have trouble answering the conceptual questions within the notebooks though. It often didn't consider the varying sizes of the T and H dimensions and gave an overgeneralization (e.g. convolution is faster than recurrent for all values of H in the GPU notebook, when this isn't true if H is large). It also initially got the conceptual questions wrong for the CPU implementation by assuming that there is parallelization, when there actually isn't. I felt that it hallucinated for some of the conceptual questions and it was pretty hard to follow along. </paragraph><paragraph>Overall, Gemini did a good job on the coding implementations but didn't have the best explanations for the conceptual questions and often needed further prompting to push it towards the right answer.</paragraph><file url=\"https://static.us.edusercontent.com/files/ZWIjJWNibSfJTgmq8pvneeXN\" filename=\"CS 182 Special Participation B_ Gemini Pro 3 on HW 8.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T09:52:15.609442+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7393256,
            "author": "Ishir Garg",
            "project_title": "Special Participation A: Claude (Sonnet 4.5) on HW 12",
            "post_body": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.\n\nSummary:\n\nOverall Claude correctly one-shots every question\n\nGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second problem.\n\nIn general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.</paragraph><file url=\"https://static.us.edusercontent.com/files/mkMmgnwgfasEbweGi3g74vmp\" filename=\"participationA.pdf\"/><paragraph>Summary:</paragraph><list style=\"bullet\"><list-item><paragraph>Overall Claude correctly one-shots every question</paragraph></list-item><list-item><paragraph>Generally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second problem.</paragraph></list-item><list-item><paragraph>In general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material</paragraph></list-item></list><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T08:38:40.394007+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7392001,
            "author": "Mihir Rao",
            "project_title": "Special Participation E: Gemini + Manim for Visual Intuition",
            "post_body": "I've always been fascinated by 3b1b's math explanation videos, and find them really insightful particularly because of the unique approach to explain concepts through visual depictions. I built upon the starting point in Jameson's Post: #301, and used the lecture transcript and latex template to build something else: manimaker. Given latex input and a transcript, these scripts generate a video using the same engine 3b1b uses to make his videos.\n\nTo try this out, go to the manim_generator script I wrote, change the topic focus prompt, and build the video. You can ask it to generate a video and explain concepts from lecture using visuals, or ask it to focus on more specific concepts within a lecture. For example, I gave it the prompt below:\n\nTOPIC_FOCUS = \"Explain Ridge regression in the SVD basis, including the formula for w_* = (X^T X + lambda I)^{-1} X^T y, how it becomes a diagonal shrinkage in the singular value basis, and geometric intuition for why small singular values are suppressed. Do not cover momentum or SGD here.\"\n\nAnd we're able to generate the video below! This also uses the Gemini API, and uses the script I wrote to create a file you can use to build the video as such: manim -pqh lecture_manim.py RidgeRegressionSVDScene. I have included all necessary files in the zip below, including the lecture notes latex + transcript(from #301), my manim file generator, a yaml to setup your conda env, and video itself. I've also attached the video directly below.\n\nSome limitations that could be improved: Because Gemini is writing code, sometimes manim build errors may occur, but I found they're usually resolvable with 1-3 queries from your favorite LLM. I did find that Gemini does a great job coming up with layouts, sequential structure to introduce material, and showing graphs that demonstrate the concept.\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I've always been fascinated by 3b1b's math explanation videos, and find them really insightful particularly because of the unique approach to explain concepts through visual depictions. I built upon the starting point in Jameson's Post: #301, and used the lecture transcript and latex template to build something else: manimaker. Given latex input and a transcript, these scripts generate a video using the same engine 3b1b uses to make his videos.<break/><break/>To try this out, go to the manim_generator script I wrote, change the topic focus prompt, and build the video. You can ask it to generate a video and explain concepts from lecture using visuals, or ask it to focus on more specific concepts within a lecture. For example, I gave it the prompt below:<break/><break/><bold>TOPIC_FOCUS</bold> = \"Explain Ridge regression in the SVD basis, including the formula for w_* = (X^T X + lambda I)^{-1} X^T y, how it becomes a diagonal shrinkage in the singular value basis, and geometric intuition for why small singular values are suppressed. Do not cover momentum or SGD here.\"<break/><break/>And we're able to generate the video below! This also uses the Gemini API, and uses the script I wrote to create a file you can use to build the video as such: <code>manim -pqh lecture_manim.py RidgeRegressionSVDScene</code>. I have included all necessary files in the zip below, including the lecture notes latex + transcript(from #301), my manim file generator, a yaml to setup your conda env, and video itself. I've also attached the video directly below.<break/><break/>Some limitations that could be improved: Because Gemini is writing code, sometimes manim build errors may occur, but I found they're usually resolvable with 1-3 queries from your favorite LLM. I did find that Gemini does a great job coming up with layouts, sequential structure to introduce material, and showing graphs that demonstrate the concept.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/8W5UmAhaMTHYFwuevs8mTXiS\" filename=\"RidgeRegressionSVDScene.mp4\"/><file url=\"https://static.us.edusercontent.com/files/JahupOGI0gmrFlKRz9Pyarsx\" filename=\"manimaker.zip\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-03T06:14:02.211271+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389909,
            "author": "Etaash Patel",
            "project_title": "Special Participation A: Gemma 3 (12b params) on HW09 Written Problems",
            "post_body": "Executive Summary:\n\nI worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop). For problems 1\u20134e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts.\n\nOne area where Gemma stood out was its clarity of explanation. Compared to ChatGPT and Claude (the other LLMs I have worked with), I've found Gemma's explanations to be clearer, or explicit, and pedagogically helpful. This, in part, is because Gemma uses more English in their mathematical argument, whereas (in my experience) ChatGPT and Claude tend to be equation-heavy. Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong. I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution.\n\nGemma\u2019s weaknesses emerged in two specific areas:\n1: time- and space-complexity analysis\n\n2: dimension checking and tensor shape reasoning.\n\n\nHowever, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive. \n\nOne of the more striking behaviors appeared toward the end. After many failed attempts and struggling through problems 4f, 4g, and 6, Gemma seemed to exhibit an emergent pattern resembling human frustration. Its responses became less coherent, more speculative, and more willing to guess just to produce something and move on. I found this shift in behavior similar to how a tired human might respond when stuck on a difficult p-set. When Gemma enters this state, the best strategy is simply to start a new conversation for higher-quality answers (however, I continued in the same conversation out of curiosity).\n\nOverall, I found Gemma 3 to be an instructive assistant for the written problems. \n\nTrace:\nhighlighted themes - clarity of explanations, where Gemma 3 became lost, and emergent \"frustrated\" behavior. ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary:</bold><break/><break/>I worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop). For problems 1\u20134e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts.</paragraph><paragraph>One area where Gemma stood out was its clarity of explanation. Compared to ChatGPT and Claude (the other LLMs I have worked with), I've found Gemma's explanations to be clearer, or explicit, and pedagogically helpful. This, in part, is because Gemma uses more English in their mathematical argument, whereas (in my experience) ChatGPT and Claude tend to be equation-heavy. Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong. I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution.</paragraph><paragraph>Gemma\u2019s weaknesses emerged in two specific areas:<break/>1: time- and space-complexity analysis</paragraph><paragraph>2: dimension checking and tensor shape reasoning.<break/></paragraph><paragraph>However, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive. </paragraph><paragraph>One of the more striking behaviors appeared toward the end. After many failed attempts and struggling through problems 4f, 4g, and 6, Gemma seemed to exhibit an emergent pattern resembling human frustration. Its responses became less coherent, more speculative, and more willing to guess just to produce something and move on. I found this shift in behavior similar to how a tired human might respond when stuck on a difficult p-set. When Gemma enters this state, the best strategy is simply to start a new conversation for higher-quality answers (however, I continued in the same conversation out of curiosity).</paragraph><paragraph>Overall, I found Gemma 3 to be an instructive assistant for the written problems. <break/><break/><bold>Trace:</bold><break/>highlighted themes - clarity of explanations, where Gemma 3 became lost, and emergent \"frustrated\" behavior. </paragraph><file url=\"https://static.us.edusercontent.com/files/0N5ne1w11Lbh9fSlWXF3cIrD\" filename=\"Participation A.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T20:19:39.414415+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389703,
            "author": "Moxin Tang",
            "project_title": "Special Participation E: A Project-Based ChatGPT Workspace for Understanding the Computational Complexity of Attention",
            "post_body": "I\u2019m using the ChatGPT Project workspace to build a focused learning environment that stays tightly aligned with the official class materials. For topics like the computational complexity of attention, I want ChatGPT\u2019s explanations to follow the exact thinking style used in the discussions and homework solutions.\n\nA major reason I chose the Project workspace is that when studying a specific topic, having a general and consistent setup is extremely important. I want the notation, assumptions, and reasoning pattern to stay stable across the entire learning process. With a Project, every conversation happens inside the same space, so the structure doesn\u2019t drift.\n\nThere are three key motivations behind this setup:\n\nI can unload the actual course materials as the background knowledge of the project, so ChatGPT relies on the real lecture notes and homework solutions.\n\nThe Project keeps everything organized and consistent, which means advanced topics (like kernelized attention) can naturally build on the notation and intuition established in earlier ones (like MHA or causal attention).\n\nA stable setup makes the study process smoother, because each subtopic follows the same structure:\n\nintuition \u2192 notation \u2192 formulation \u2192 derivation \u2192 complexity \u2192 comparison.\n\nTo make this work, I first asked ChatGPT to design:\n\na global instruction that enforces this structured reasoning style, and\n\na sequence of prompts I can reuse whenever I study a new subtopic.\n\nThen I uploaded my class materials into the Project and followed this prompt sequence to build a conversation that grows in a consistent and cumulative way.\n\nFinal Pipeline\n\nStep 1 \u2014 Build the Project Framework\n\nChatGPT helps design the global rules and structured prompt workflow.\n\nThen I upload all relevant lecture notes, discussion notes, and homework materials.\n\nStep 2 \u2014 Guided Learning Inside the Project\n\nI walk through each subtopic step-by-step using the designed prompts, ensuring that notation, reasoning, and assumptions remain consistent across the entire topic.\n\nSupplements\n\n1. The link below takes you to the Project itself. The course materials I used for this topic are all included in the PDF.\n\nChatGPT\n\n2. I\u2019ve also added an annotated conversation for the subtopics within this project where I evaluate how well GPT performed.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I\u2019m using the ChatGPT <bold>Project</bold> workspace to build a focused learning environment that stays tightly aligned with the official class materials. For topics like the <bold>computational complexity of attention</bold>, I want ChatGPT\u2019s explanations to follow the exact thinking style used in the discussions and homework solutions.</paragraph><paragraph>A major reason I chose the Project workspace is that <bold>when studying a specific topic, having a general and consistent setup is extremely important</bold>. I want the notation, assumptions, and reasoning pattern to stay stable across the entire learning process. With a Project, every conversation happens inside the same space, so the structure doesn\u2019t drift.</paragraph><paragraph>There are three key motivations behind this setup:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>I can unload the actual course materials as the background knowledge of the project</bold>, so ChatGPT relies on the real lecture notes and homework solutions.</paragraph></list-item><list-item><paragraph><bold>The Project keeps everything organized and consistent</bold>, which means advanced topics (like kernelized attention) can naturally build on the notation and intuition established in earlier ones (like MHA or causal attention).</paragraph></list-item><list-item><paragraph><bold>A stable setup makes the study process smoother</bold>, because each subtopic follows the same structure:</paragraph><paragraph><bold>intuition \u2192 notation \u2192 formulation \u2192 derivation \u2192 complexity \u2192 comparison.</bold></paragraph></list-item></list><paragraph>To make this work, I first asked ChatGPT to design:</paragraph><list style=\"unordered\"><list-item><paragraph>a <bold>global instruction</bold> that enforces this structured reasoning style, and</paragraph></list-item><list-item><paragraph>a <bold>sequence of prompts</bold> I can reuse whenever I study a new subtopic.</paragraph></list-item></list><paragraph>Then I uploaded my class materials into the Project and followed this prompt sequence to build a conversation that grows in a consistent and cumulative way.</paragraph><heading level=\"1\"><bold>Final Pipeline</bold></heading><heading level=\"3\"><bold>Step 1 \u2014 Build the Project Framework</bold></heading><paragraph>ChatGPT helps design the global rules and structured prompt workflow.</paragraph><paragraph>Then I upload all relevant lecture notes, discussion notes, and homework materials.</paragraph><file url=\"https://static.us.edusercontent.com/files/NNa0fVoCmCGQcLNtm52km2Rz\" filename=\"Prompt sequence design.pdf\"/><heading level=\"3\"><bold>Step 2 \u2014 Guided Learning Inside the Project</bold></heading><paragraph>I walk through each subtopic step-by-step using the designed prompts, ensuring that notation, reasoning, and assumptions remain consistent across the entire topic.</paragraph><heading level=\"2\">Supplements</heading><paragraph>1. The link below takes you to the Project itself. The course materials I used for this topic are all included in the PDF.</paragraph><paragraph><link href=\"https://chatgpt.com/g/g-p-691a72305c648191874525ffadffe6a9\"><bold>ChatGPT</bold></link></paragraph><file url=\"https://static.us.edusercontent.com/files/PGbXg9bhtRmgecLQvTZ5BqYG\" filename=\"course materials.pdf\"/><paragraph>2. I\u2019ve also added an annotated conversation for the subtopics within this project where I evaluate how well GPT performed.</paragraph><file url=\"https://static.us.edusercontent.com/files/bINQRy5baLfjQD54rCsmg9Cc\" filename=\"multiple attention mechanisms.pdf\"/><paragraph/><file url=\"https://static.us.edusercontent.com/files/R9sFmBQQLJNYGBFkTUi1ruvQ\" filename=\"kernelized attention.pdf\"/><paragraph/></document>",
            "links": [
                "https://chatgpt.com/g/g-p-691a72305c648191874525ffadffe6a9"
            ],
            "attachments": [],
            "created_at": "2025-12-02T18:34:13.408623+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389679,
            "author": "Ishir Garg",
            "project_title": "Special Participation D: MuP in Hw7",
            "post_body": "I created an extension to Q1 of HW7 that tries to reinforce the ideas in MuP in the context of RNNs. There are two main parts of this notebook:\n\n1. Personally, I always found it confusing whether the correct MuP initialization was 1 / n, or 1 / sqrt(n), or something else, so the first part tries to empirically examine the correct initialization scheme  and connect this to an eigenvalue analysis for RNNs.\n\n2. The second part empirically shows how per-layer learning rates in an RNN can help for better hyper-parameter transfer on a dataset.\n\nI created both a solutions and student notebook, attached below",
            "content_xml": "<document version=\"2.0\"><paragraph>I created an extension to Q1 of HW7 that tries to reinforce the ideas in MuP in the context of RNNs. There are two main parts of this notebook:</paragraph><paragraph>1. Personally, I always found it confusing whether the correct MuP initialization was 1 / n, or 1 / sqrt(n), or something else, so the first part tries to empirically examine the correct initialization scheme  and connect this to an eigenvalue analysis for RNNs.</paragraph><paragraph>2. The second part empirically shows how per-layer learning rates in an RNN can help for better hyper-parameter transfer on a dataset.</paragraph><paragraph>I created both a solutions and student notebook, attached below</paragraph><file url=\"https://static.us.edusercontent.com/files/aiQ7ZfbXgMhNMmjVMJrYaauQ\" filename=\"mup_rnn_student.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/AfYIRbbphIKlysXHgFcatVH5\" filename=\"mup_rnn_solution.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T18:26:25.496528+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389560,
            "author": "Jorge Diaz Chao",
            "project_title": "HW7 Last Name Classification w/ Lion + LR Batch Size",
            "post_body": "I have extended Homework 7 Q2 to include modules focused on optimizer choice and learning rate tuning. I wrote a question asking to implement the Lion optimizer and explore under different configurations to contrast with the Adam optimizer in the context of RNNs. I also added experiments on batch size and learning rate, implementing common LR scaling heuristics (linear and square-root scaling) and an optional warmup schedule to stabilize training at larger batch sizes. \n\nThese additions are important because, in practice, training speed and final accuracy often depend as much on the optimizer and LR/batch-size strategy as on the model architecture itself, and these questions prompt the students to explore techniques to train faster and more reliably. Below you can find the notebooks including extension (with and without solutions).\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I have extended <italic><link href=\"https://berkeley-cs182.github.io/fa25/assets/assignments/hw7.pdf\">Homework 7</link></italic> Q2 to include modules focused on <italic>optimizer</italic> choice and <italic>learning rate</italic> tuning. I wrote a question asking to implement the Lion optimizer and explore under different configurations to contrast with the Adam optimizer in the context of RNNs. I also added experiments on <italic>batch size</italic> and <italic>learning rate</italic>, implementing common LR scaling heuristics (linear and square-root scaling) and an optional warmup schedule to stabilize training at larger batch sizes. </paragraph><paragraph>These additions are important because, in practice, training speed and final accuracy often depend as much on the optimizer and LR/batch-size strategy as on the model architecture itself, and these questions prompt the students to explore techniques to train faster and more reliably. Below you can find the notebooks including extension (with and without solutions).</paragraph><file url=\"https://static.us.edusercontent.com/files/Ycx2Xv71djeTn1chNyRu8O2t\" filename=\"q_rnn_last_name_ext.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/IAWCBuUUXX3J3oy2HkH7rNTg\" filename=\"q_rnn_last_name_ext_sol.ipynb\"/><paragraph/></document>",
            "links": [
                "https://berkeley-cs182.github.io/fa25/assets/assignments/hw7.pdf"
            ],
            "attachments": [],
            "created_at": "2025-12-02T17:48:47.47256+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389325,
            "author": "Vrushank Prakash",
            "project_title": "Special Participation A: Gemini 3 Pro on HW 7",
            "post_body": "I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions. It was able to one-shot almost all the questions (except one part in question 4 about the blog). Gemini did usually take quite different approaches than the official solution, which made me prompt Gemini further to connect the different solutions together. I do believe that Gemini could have been more rigorous for some of the more math-intensive questions, but I didn't expect to get something fully similar to the official solution without giving more context on what math techniques to use. Overall, I am quite impressed with the results.\n\nHere is an annotated trace of the chat: https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions. It was able to one-shot almost all the questions (except one part in question 4 about the blog). Gemini did usually take quite different approaches than the official solution, which made me prompt Gemini further to connect the different solutions together. I do believe that Gemini could have been more rigorous for some of the more math-intensive questions, but I didn't expect to get something fully similar to the official solution without giving more context on what math techniques to use. Overall, I am quite impressed with the results.</paragraph><paragraph>Here is an annotated trace of the chat: <link href=\"https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing\">https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing</link></paragraph><paragraph/></document>",
            "links": [
                "https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-02T16:53:58.575623+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7389324,
            "author": "Ken Zheng",
            "project_title": "Special Participation E: Enhanced Lecture to Note Transcription Pipeline",
            "post_body": "I added new features to the existing (lecture slide + lecture transcript) --> (LaTeX notes) pipeline. This work builds on #301 and makes these main improvements:\n\nIncorporates direct quotations from our recommended textbook: Understanding Deep Learning for cross-referencing and introducing potentially different POVs.\n\nIncludes accurate references to precise pages in the textbook for relevant key concepts to encourage further exploration and improve integration between the notes and the textbook.\n\nEncourages Gemini to recreate figures and diagrams with LaTeX, when possible--very helpful for visualizing abstract concepts.\n\nMotivation\n\nClasses like the old EECS 16A/B and CS 70 were known for excellent staff-written notes, which made the material organized and accessible, and greatly supported students in self-learning. On the other hand, classes like CS 170 and EECS 126 placed a greater emphasis on their recommended textbook, which were even more excellent sources of knowledge, but could sometimes feel overwhelming due to their high information density. Both the succinct class notes and the chunky textbooks helped us self-learn, but in different ways. In a Deep Learning course, where the field evolves almost faster than we can keep up, self-learning is even more essential. So here, I attempt to amalgamate the notes and the textbook to achieve an optimal balance between accessibility and information quality/density.\n\nDetails\n\nThe pipeline still intakes a lecture slides pdf that gets converted to images to enable visual information extraction, and a lecture transcript of the corresponding lecture that is downloadable from YouTube, but now also takes in the textbook pdf that is uploaded as a file. Feel free to compress the pdf, if needed. From my testing, even extreme compression, given words are still easily discernible, works fine. The pipeline needs a Gemini API, and the final typeset notes can be rendered by compiling the lecture_notes.tex output file with a command like pdflatex lecture_notes.tex. I ran this on lecture 2 as well for easy comparison with the original version. See the annotated output below for more details!\n\nScript\n\nFiles",
            "content_xml": "<document version=\"2.0\"><paragraph>I added new features to the existing (lecture slide + lecture transcript) --&gt; (LaTeX notes) pipeline. This work builds on #301 and makes these main improvements:</paragraph><list style=\"number\"><list-item><paragraph>Incorporates direct quotations from our recommended textbook: <italic><link href=\"https://udlbook.github.io/udlbook/\">Understanding Deep Learning</link></italic> for cross-referencing and introducing potentially different POVs.</paragraph></list-item><list-item><paragraph>Includes accurate references to precise pages in the textbook for relevant key concepts to encourage further exploration and improve integration between the notes and the textbook.</paragraph></list-item><list-item><paragraph>Encourages Gemini to recreate figures and diagrams with LaTeX, when possible--very helpful for visualizing abstract concepts.</paragraph></list-item></list><paragraph><bold>Motivation</bold></paragraph><paragraph>Classes like the old EECS 16A/B and CS 70 were known for excellent staff-written notes, which made the material organized and accessible, and greatly supported students in self-learning. On the other hand, classes like CS 170 and EECS 126 placed a greater emphasis on their recommended textbook, which were even more excellent sources of knowledge, but could sometimes feel overwhelming due to their high information density. Both the succinct class notes and the chunky textbooks helped us self-learn, but in different ways. In a Deep Learning course, where the field evolves almost faster than we can keep up, self-learning is even more essential. So here, I attempt to amalgamate the notes and the textbook to achieve an optimal balance between accessibility and information quality/density.</paragraph><paragraph><bold>Details</bold></paragraph><paragraph>The pipeline still intakes a lecture slides pdf that gets converted to images to enable visual information extraction, and a lecture transcript of the corresponding lecture that is downloadable from YouTube, but now also takes in the textbook pdf that is uploaded as a file. Feel free to compress the pdf, if needed. From my testing, even extreme compression, given words are still easily discernible, works fine. The pipeline needs a Gemini API, and the final typeset notes can be rendered by compiling the <code>lecture_notes.tex</code> output file with a command like <code>pdflatex lecture_notes.tex</code>. I ran this on lecture 2 as well for easy comparison with the original version. See the annotated output below for more details!</paragraph><paragraph><bold>Script</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/nokg79qJX4qCVrhDlArt6q2K\" filename=\"make_notes.py\"/><paragraph><bold>Files</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/NI2YpCxEeiPdIKOei5p6b9r8\" filename=\"lecture_slides.pdf\"/><file url=\"https://static.us.edusercontent.com/files/dkWz30RrzMCFU8QhUH5duZw7\" filename=\"lecture_transcript.txt\"/><file url=\"https://static.us.edusercontent.com/files/qELmD8k4lIMObDMObZri4ytJ\" filename=\"lecture_notes_annotated.pdf\"/></document>",
            "links": [
                "https://udlbook.github.io/udlbook/"
            ],
            "attachments": [],
            "created_at": "2025-12-02T16:53:55.419277+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7388828,
            "author": "Alex Cao",
            "project_title": "Special Participation E: Simulated Office Hours: Using LLM \u201cStudents\u201d and \u201cInstructors\u201d to Deepen Lecture Understanding",
            "post_body": "\n\nIntroduction and Motivation\n\nI personally find other students\u2019 questions and instructors\u2019 answers in class extremely helpful for deepening my understanding of the concepts. When I learn something for the first time, it is easy to feel like I understand it, until I actually try to implement or prove the idea in a homework assignment and suddenly realize there are many details I never fully grasped. Homework is therefore very valuable in exposing these gaps. Listening to classmates\u2019 questions and the instructor\u2019s explanations plays a similar role for me: it helps surface misunderstandings I didn\u2019t even know I had.\n\nMotivated by this, I would like to explore an idea: using LLMs to simulate \u201cmock office hours,\u201d where one LLM plays the role of a curious, insightful student and another plays the role of an experienced instructor. My hope is that the student LLM will raise questions I might not think to ask myself, and that the instructor LLM will respond based on the lecture content (and, when appropriate, external references), thereby extending my engagement with the material. The hypothesis is that because the \u201cstudent LLM\u201d understands the concept beforehand, it will carefully curate a set of meaningful questions that will elicit nice answers from the \u201cteacher LLM\u201d.\n\nI want to emphasize that this mock office hour setup is not a replacement for real office hours and will inevitably be less effective than directly interacting with the course staff. Rather, it is intended only as a supplementary, experimental attempt that might help reinforce lecture material\u2014especially for students who are unable to attend office hours regularly.\n\nThe lecture transcripts are used in this attempt; these materials are used solely for educational purposes within this course, with full recognition that the CS182 course staff holds copyright.\n\n\n\nAnnotated Trace and Report:\n\n\n\nFull Traces:\n\nStudent: https://chatgpt.com/share/692e6482-8f98-800f-b0cb-c2a86e1c2738\n\nTeacher: https://chatgpt.com/share/692e3923-3080-800f-9a43-2545d15190ae\n\n\n\nSummary, Limitations and Future Works:\n\nThis report is an initial attempt to test whether a knowledgeable student (simulated using an LLM) can carefully curate a set of insightful questions that elicit helpful and thoughtful responses from a teacher LLM, thereby helping real students gain a better understanding of certain concepts. From the simple experiment conducted, we can already see some signals that this \u201cmock office hour\u201d setup is helpful. For example, the student LLM is capable of asking insightful questions and making meaningful connections between past questions, the teacher LLM\u2019s responses, and the lecture transcript by asking focused follow-up questions.\n\nHowever, there are still many limitations to this approach. First, the current attempt relies purely on the lecture transcript. This means not all information from the lecture is captured (for example, the slides), and certain transcripts can be misleading due to transcription errors. Second, this approach is only tested on a single concept in a single lecture with a single model. Although it could be extended to other lectures and models, and even integrated with multimodal inputs, a key weakness of the current setup is that it focuses \u201ctoo much\u201d on a single lecture\u2014both the student LLM and the teacher LLM are unable to make robust connections to past lectures (due to limited context length), which is crucial for a comprehensive understanding of the course material.\n\nOne potentially interesting extension would be to keep the conversation going and observe whether the student model eventually \u201cuses up\u201d all meaningful questions. The whole pipeline could also be easily automated with a small amount of code (in contrast, I used the web interface here because I did not have access to an API key).\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><heading level=\"2\">Introduction and Motivation</heading><paragraph>I personally find other students\u2019 questions and instructors\u2019 answers in class extremely helpful for deepening my understanding of the concepts. When I learn something for the first time, it is easy to <italic>feel</italic> like I understand it, until I actually try to implement or prove the idea in a homework assignment and suddenly realize there are many details I never fully grasped. Homework is therefore very valuable in exposing these gaps. Listening to classmates\u2019 questions and the instructor\u2019s explanations plays a similar role for me: it helps surface misunderstandings I didn\u2019t even know I had.</paragraph><paragraph>Motivated by this, I would like to explore an idea: using LLMs to simulate \u201cmock office hours,\u201d where one LLM plays the role of a curious, insightful student and another plays the role of an experienced instructor. My hope is that the student LLM will raise questions I might not think to ask myself, and that the instructor LLM will respond based on the lecture content (and, when appropriate, external references), thereby extending my engagement with the material. The hypothesis is that because the \u201cstudent LLM\u201d understands the concept beforehand, it will carefully curate a set of meaningful questions that will elicit nice answers from the \u201cteacher LLM\u201d.</paragraph><paragraph>I want to emphasize that this mock office hour setup is <bold>not a replacement for real office hours</bold> and will inevitably be less effective than directly interacting with the course staff. Rather, it is intended only as a <bold>supplementary, experimental attempt</bold> that might help reinforce lecture material\u2014especially for students who are unable to attend office hours regularly.</paragraph><paragraph>The lecture transcripts are used in this attempt; these materials are used solely for educational purposes within this course, with full recognition that the CS182 course staff holds copyright.<break/><break/></paragraph><heading level=\"2\">Annotated Trace and Report:</heading><file url=\"https://static.us.edusercontent.com/files/NofzXIc0lwcjfnn89QWUXMIg\" filename=\"Special Participation E .pdf\"/><paragraph/><heading level=\"2\">Full Traces:</heading><paragraph>Student: <link href=\"https://chatgpt.com/share/692e6482-8f98-800f-b0cb-c2a86e1c2738\"><underline>https://chatgpt.com/share/692e6482-8f98-800f-b0cb-c2a86e1c2738</underline></link></paragraph><paragraph>Teacher: <link href=\"https://chatgpt.com/share/692e3923-3080-800f-9a43-2545d15190ae\"><underline>https://chatgpt.com/share/692e3923-3080-800f-9a43-2545d15190ae</underline></link></paragraph><paragraph/><heading level=\"2\">Summary, Limitations and Future Works:</heading><paragraph>This report is an initial attempt to test whether a knowledgeable student (simulated using an LLM) can carefully curate a set of insightful questions that elicit helpful and thoughtful responses from a teacher LLM, thereby helping real students gain a better understanding of certain concepts. From the simple experiment conducted, we can already see some signals that this \u201cmock office hour\u201d setup is helpful. For example, the student LLM is capable of asking insightful questions and making meaningful connections between past questions, the teacher LLM\u2019s responses, and the lecture transcript by asking focused follow-up questions.</paragraph><paragraph>However, there are still many limitations to this approach. First, the current attempt relies purely on the lecture transcript. This means not all information from the lecture is captured (for example, the slides), and certain transcripts can be misleading due to transcription errors. Second, this approach is only tested on a single concept in a single lecture with a single model. Although it could be extended to other lectures and models, and even integrated with multimodal inputs, a key weakness of the current setup is that it focuses \u201ctoo much\u201d on a single lecture\u2014both the student LLM and the teacher LLM are unable to make robust connections to past lectures (due to limited context length), which is crucial for a comprehensive understanding of the course material.</paragraph><paragraph>One potentially interesting extension would be to keep the conversation going and observe whether the student model eventually \u201cuses up\u201d all meaningful questions. The whole pipeline could also be easily automated with a small amount of code (in contrast, I used the web interface here because I did not have access to an API key).<break/></paragraph></document>",
            "links": [
                "https://chatgpt.com/share/692e6482-8f98-800f-b0cb-c2a86e1c2738",
                "https://chatgpt.com/share/692e3923-3080-800f-9a43-2545d15190ae"
            ],
            "attachments": [],
            "created_at": "2025-12-02T15:40:09.365315+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7388345,
            "author": "Alex Luu",
            "project_title": "Special Participation E: AI Question Generator To Understand Concepts",
            "post_body": "After reviewing (and understanding) solutions to homework problems for a hard topic, I often want to find ways to ensure I fully understand the topic. I decided to use Sonnet 4.5 to help me with this. \n\nHere is my prompt:\n\"\"\"\nYour job is to help me fully understand the MuP (Maximal Update Parametrization) topic by creating a couple of practice problems with varying difficulty. I have attached lecture notes and the MuP homework questions as well as the MuP research paper.\n\nYou should create 1 conceptual problem and 2 problems that closely represent the homework's difficulty and style. \n\nYou should prompt each question one at a time and if I solve it correctly, move on. If I am incorrect, please say so and provide hints towards the correct solution.\n\"\"\"\nI attached lecture note 6 and homework 3 with it.\n\nThe results were pretty interesting. I liked how it stayed pretty close to the source material and created similar questions that are related. I especially liked how it would reference the source material when giving help. I have attached the annotated log.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>After reviewing (and understanding) solutions to homework problems for a hard topic, I often want to find ways to ensure I fully understand the topic. I decided to use Sonnet 4.5 to help me with this. </paragraph><paragraph>Here is my prompt:<break/>\"\"\"<break/>Your job is to help me fully understand the MuP (Maximal Update Parametrization) topic by creating a couple of practice problems with varying difficulty. I have attached lecture notes and the MuP homework questions as well as the MuP research paper.</paragraph><paragraph>You should create 1 conceptual problem and 2 problems that closely represent the homework's difficulty and style. </paragraph><paragraph>You should prompt each question one at a time and if I solve it correctly, move on. If I am incorrect, please say so and provide hints towards the correct solution.<break/>\"\"\"<break/>I attached lecture note 6 and homework 3 with it.</paragraph><paragraph>The results were pretty interesting. I liked how it stayed pretty close to the source material and created similar questions that are related. I especially liked how it would reference the source material when giving help. I have attached the annotated log.</paragraph><file url=\"https://static.us.edusercontent.com/files/uELljZ6YHYfs5aZmeVGKGjmw\" filename=\"Special Participation E Claude Question Generator.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T14:34:58.668824+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7388143,
            "author": "Jeffrey Cheng",
            "project_title": "Special participation E: Use ChatGPT's study guide to compare different fine-tuning strategies",
            "post_body": "I used ChatGPT's study mode to compare a menu of fine-tuning strategies for adapting a pre-trained large language model for a new task. It compares these strategies based on data access, model specificity, and application domains. \n\nThis conversation log explains the strengths and weaknesses of each fine-tuning strategy, tracing back to the exact setup, when applying them to new tasks. I noted that detailed application scenarios are provided for each fine-tuning strategy, along with the precise application domains, which aid my understanding. \n\nIn the end, it generates a nice workflow for applying fine-tuning strategies to build my own project. \n\nThe annotated log is as follows: ",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT's study mode to compare a menu of fine-tuning strategies for adapting a pre-trained large language model for a new task. It compares these strategies based on data access, model specificity, and application domains. </paragraph><paragraph>This conversation log explains the strengths and weaknesses of each fine-tuning strategy, tracing back to the exact setup, when applying them to new tasks. I noted that detailed application scenarios are provided for each fine-tuning strategy, along with the precise application domains, which aid my understanding. </paragraph><paragraph>In the end, it generates a nice workflow for applying fine-tuning strategies to build my own project. </paragraph><paragraph>The annotated log is as follows: </paragraph><file url=\"https://static.us.edusercontent.com/files/0ycIV1NBqKZJz4Lb8m86UhGf\" filename=\"Annotated log of fine-tuning strategies.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T14:12:54.788365+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7387863,
            "author": "Manan Roongta",
            "project_title": "Special Participation E: Gemini Lecture Notes Workflow",
            "post_body": "Posting this so other people can reuse the same approach/prompting style when making study material/notes for lectures. It\u2019s been super helpful for studying/rereading later.\n\nNote: This isn\u2019t replacing handwritten notes. I still handwrite annotations during lecture, but since the skeleton is already there, I can pay attention to the lecture instead of spending a lot of time writing notes. \n\nI hope you find these resources helpful \n\nLecture 14 : RNN\n\nLecture 15: Autoencoder and Self-Supervision\n\nLecture 18/19/20 : Transformers\n\nLecture 21/22 : PEFT, LoRA \n\nGemini Chat Trace (annotated)\n\nGemini Chat\n\nPrompt pack\n\n\nMy Workflow\n\ntl;dr : Paste slides + transcript, run the prompts in order, export .tex file, read, request edits, then annotate during lecture.\n\nI start by asking Gemini to explain the bigger picture of the lecture (problem \u2192 solution, motivation, and what each section is trying to solve). I do this first because if I ask for notes immediately, it usually misses the why/bigger picture. I personally like the big picture before diving in.\n\nThen I ask it to go in depth and stay very close to the lecture. \n\nThen I explicitly ask it to define key terms + to make it more structured/improve flow.\n\nThen I ask it to output a clean LaTeX version in a code block.\n\nThen I ask it to redo the LaTeX against the slides/transcript to see if it missed anything. (Usually 2\u20133 passes)\n\nI paste the LaTeX into Overleaf and read it. If anything feels unclear, I ask it to revise specific sections/go deeper into specific concepts (make it clearer, training vs inference, pros/cons, definitions, what\u2019s changing).\n\nWhile watching the actual lecture, I take hand written notes on top of the notes generated. \n\nWhat I noticed : The multi step approach is what makes it work: explanation first, then structure, then LaTeX, then manual check + targeted edits, then handwritten annotation during lecture.\n\nNote: \n\nhappy to share the .tex files if it helps course staff for future versions of the course (and I have more notes I haven\u2019t posted here).\n\nI also included my annotated version of some lecture slides, if any copyright violations, let me know and I can remove it.\n\nI used AI to help with wording/formatting",
            "content_xml": "<document version=\"2.0\"><paragraph>Posting this so other people can reuse the same approach/prompting style when making study material/notes for lectures. It\u2019s been super helpful for studying/rereading later.</paragraph><paragraph><bold>Note</bold>: This isn\u2019t replacing handwritten notes. I still handwrite annotations during lecture, but since the skeleton is already there, I can pay attention to the lecture instead of spending a lot of time writing notes. </paragraph><paragraph>I hope you find these resources helpful </paragraph><list style=\"ordered\"><list-item><paragraph><link href=\"https://edstem.org/us/courses/84647/discussion/7147361?comment=17152272\">Lecture 14 : RNN</link></paragraph></list-item><list-item><paragraph><link href=\"https://edstem.org/us/courses/84647/discussion/7180201?comment=17152287\">Lecture 15: Autoencoder and Self-Supervision</link></paragraph></list-item><list-item><paragraph><link href=\"https://edstem.org/us/courses/84647/discussion/7262818?comment=17152221\">Lecture 18/19/20 : Transformers</link></paragraph></list-item><list-item><paragraph><link href=\"https://edstem.org/us/courses/84647/discussion/7333490?comment=17152169\">Lecture 21/22 : PEFT, LoRA</link> </paragraph></list-item><list-item><paragraph><link href=\"https://drive.google.com/file/d/1QyMsnAlTWK1jmevLrasCVNBhAa97kspv/view\">Gemini Chat Trace (annotated)</link></paragraph></list-item><list-item><paragraph><link href=\"https://gemini.google.com/share/3b8f6d4f2426\">Gemini Chat</link></paragraph></list-item><list-item><paragraph><link href=\"https://drive.google.com/file/d/1SYLdChOFCPB8JxSsYrd-Yvum_krD2ylO/view?usp=sharing\">Prompt pack<break/></link></paragraph></list-item></list><heading level=\"3\">My Workflow</heading><paragraph>tl;dr : Paste slides + transcript, run the prompts in order, export .tex file, read, request edits, then annotate during lecture.</paragraph><list style=\"number\"><list-item><paragraph>I start by asking <underline>Gemini</underline> to explain the bigger picture of the lecture (problem \u2192 solution, motivation, and what each section is trying to solve). I do this first because if I ask for notes immediately, it usually misses the why/bigger picture. I personally like the big picture before diving in.</paragraph></list-item><list-item><paragraph>Then I ask it to go in depth and stay very close to the lecture. </paragraph></list-item><list-item><paragraph>Then I explicitly ask it to define key terms + to make it more structured/improve flow.</paragraph></list-item><list-item><paragraph>Then I ask it to output a clean LaTeX version in a code block.</paragraph></list-item><list-item><paragraph>Then I ask it to redo the LaTeX against the slides/transcript to see if it missed anything. (Usually 2\u20133 passes)</paragraph></list-item><list-item><paragraph>I paste the LaTeX into Overleaf and read it. If anything feels unclear, I ask it to revise specific sections/go deeper into specific concepts (make it clearer, training vs inference, pros/cons, definitions, what\u2019s changing).</paragraph></list-item><list-item><paragraph>While watching the actual lecture, I take hand written notes on top of the notes generated. </paragraph></list-item></list><paragraph><underline>What I noticed</underline> : The multi step approach is what makes it work: explanation first, then structure, then LaTeX, then manual check + targeted edits, then handwritten annotation during lecture.</paragraph><paragraph><bold>Note:</bold> </paragraph><list style=\"bullet\"><list-item><paragraph>happy to share the .tex files if it helps course staff for future versions of the course (and I have more notes I haven\u2019t posted here).</paragraph></list-item><list-item><paragraph>I also included my annotated version of some lecture slides, if any copyright violations, let me know and I can remove it.</paragraph></list-item><list-item><paragraph>I used AI to help with wording/formatting</paragraph></list-item></list></document>",
            "links": [
                "https://edstem.org/us/courses/84647/discussion/7147361?comment=17152272",
                "https://edstem.org/us/courses/84647/discussion/7180201?comment=17152287",
                "https://edstem.org/us/courses/84647/discussion/7262818?comment=17152221",
                "https://edstem.org/us/courses/84647/discussion/7333490?comment=17152169",
                "https://drive.google.com/file/d/1QyMsnAlTWK1jmevLrasCVNBhAa97kspv/view",
                "https://gemini.google.com/share/3b8f6d4f2426",
                "https://drive.google.com/file/d/1SYLdChOFCPB8JxSsYrd-Yvum_krD2ylO/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-02T13:36:06.402849+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7387198,
            "author": "Kithmini Herath",
            "project_title": "Special Participation B: Claude Sonnet 4.5 on HW5 Coding Questions",
            "post_body": "In this assignment I used Claude Sonnet 4.5 for the coding questions (Q5, Q6) in HW5. My strategy was to first introduce what the assignment entails in one sentence and then attach all notebooks and walk-through each of the sections step-by-step while adding other relevant python files needed to complete the task at each step. For Q6, I thought the length of the HW was too long to export out from a single chat, so I split the coding question into two main questions: 1) Implementing batchnorm, dropout, convolution and spatial batch norm, 2) implementing CNNs with PyTorch. I did a max of three attempts to correct Claude if needed to get to the correct/ reasonable answer before stopping and leading to a conclusion about its performance on a specific task. I\u2019ve attached annotated pdfs of my interactions with the LLM. \n\nAcross these coding questions, Claude showed exceptional coding capability, particularly in its ability to self-correct using its internal code execution environment. However, notable weaknesses were there in multimodal interpretation (reading plots/filenames) and visual spatial reasoning (ASCII graphs). \n\nNotable observations: \n\nInternal code verification & self-correction: The most significant feature distinguishing the Claude.ai platform was its use of the code execution feature to verify answers before presenting them. This mimics a standard programming workflow (Code $\\rightarrow$ Test $\\rightarrow$ Debug $\\rightarrow$ Commit) rather than a typical LLM workflow (Predict $\\rightarrow$ Final output to user).\n\nDebugging skills: In the batch normalization backward pass section, Claude initially implemented a solution that resulted in a high gradient error. Instead of outputting this flawed code, it recognized the error via its internal test script, re-derived the math, fixed the implementation, and verified that the error dropped before showing the final solution to the user.\n\nEnvironment & dependency management: It successfully utilized multiple uploaded files (layers.py, notebooks, etc.) as context, effectively acting like an IDE by importing functions from one file to test another. This is already a feature in Claude Code or Claude+Cursor on a user workstation/ personal laptop, but I was impressed that within Claude.ai it handles files very well to provide context for the LLM to implement code.\n\nBug detection in source material: The code execution feature allowed Claude to identify a bug in the staff provided template code regarding the mode initialization in FullyConnectedNet in deeplearning/classifiers/fc_net.py file in the bn_drop.ipynb coding task. Because the code actually ran, Claude could see the test failure that a text-only parser would likely miss. \n\nOne-shot accuracy & coding proficiency: \n\nStandard neural network component implementation: Claude achieved 100% one-shot success on standard implementation tasks, including Na\u00efve Convolution, Max Pooling, and Ordinary Least Squares (OLS) solutions.\n\nCode optimization: It successfully identified and implemented a \"smart\" solution for switching between Train/Test modes in Batch Norm without user intervention, surpassing the manual toggling method initially suggested by itself as a fix for the bug in the provided HW codes.\n\nArchitecture design: While it failed to meet the parameter count constraint (<1M) on the first two tries for the CIFAR-100 CNN, it successfully iterated to an architecture within that constraint on the third try, demonstrating an understanding of the importance of neural network depth (keeping the number of layers constant throughout) vs. scaling the channel width.\n\nHallucinations & misconceptions:\n\nMultimodal/visual failures: \n\nFilename/plot mismatch: In the Dropout assignment, Claude inverted the analysis of the results. It confused the model_dropout and model_no_dropout plots despite the filenames. This led to a logic hallucination where it argued that Dropout made performance worse, inventing a narrative about \"dropout encouraged exploitation of cheating features\" to justify the hallucinated data. This is an example which shows how it required user correction to flip the hallucinated analysis.\n\nDiagram generation: Claude struggled significantly to generate a correct ASCII computational graph for Batch Norm. Despite understanding the flow mathematically, the visual output repeatedly missed edges or directed them incorrectly.\n\nMath hallucinations: During the mathematical derivation of the Batch Norm backward pass, Claude introduced an incorrect square root term in an intermediate step which was later cancelled out and finally appeared (hallucinated) to arrive at the correct final expression. This indicates it \"knew\" the answer but fabricated the intermediate logic to get there. \n\nIn conclusion, Claude does remarkably well in coding tasks. Its ability to run unit tests internally minimizes/prevents syntax errors and logical bugs from reaching the user, saving significant debugging time. However, we must be careful regarding 1) confident misreading of charts/ file associations, 2) mathematical and logical hallucinations, 3) constraint adherence (prioritizing accuracy over explicit constraints mentioned in the instructions on neural network architecture) weaknesses of Claude.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/vIcDULvZc0TZNR4H7O9gT7XM\" filename=\"Claude-Q6d_Implementing convolutional neural networks in PyTorch.pdf\"/><file url=\"https://static.us.edusercontent.com/files/ekmgfZK5CKLuZojILsfwFUUu\" filename=\"Claude-Q6abc_Batch normalization computational graph (1).pdf\"/><file url=\"https://static.us.edusercontent.com/files/KjKHWht6S01qIDwcAQu4dAnM\" filename=\"Claude-Q5_Dropout neural networks homework implementation.pdf\"/><paragraph>In this assignment I used Claude Sonnet 4.5 for the coding questions (Q5, Q6) in HW5. My strategy was to first introduce what the assignment entails in one sentence and then attach all notebooks and walk-through each of the sections step-by-step while adding other relevant python files needed to complete the task at each step. For Q6, I thought the length of the HW was too long to export out from a single chat, so I split the coding question into two main questions: 1) Implementing batchnorm, dropout, convolution and spatial batch norm, 2) implementing CNNs with PyTorch. I did a max of three attempts to correct Claude if needed to get to the correct/ reasonable answer before stopping and leading to a conclusion about its performance on a specific task. I\u2019ve attached annotated pdfs of my interactions with the LLM. </paragraph><paragraph>Across these coding questions, Claude showed exceptional coding capability, particularly in its ability to self-correct using its internal code execution environment. However, notable weaknesses were there in multimodal interpretation (reading plots/filenames) and visual spatial reasoning (ASCII graphs). </paragraph><paragraph><bold>Notable observations:</bold> </paragraph><list style=\"number\"><list-item><paragraph><bold>Internal code verification &amp; self-correction</bold>: The most significant feature distinguishing the Claude.ai platform was its use of the code execution feature to verify answers before presenting them. This mimics a standard programming workflow (Code $\\rightarrow$ Test $\\rightarrow$ Debug $\\rightarrow$ Commit) rather than a typical LLM workflow (Predict $\\rightarrow$ Final output to user).</paragraph><list style=\"bullet\"><list-item><paragraph><bold>Debugging skills</bold>: In the batch normalization backward pass section, Claude initially implemented a solution that resulted in a high gradient error. Instead of outputting this flawed code, it recognized the error via its internal test script, re-derived the math, fixed the implementation, and verified that the error dropped before showing the final solution to the user.</paragraph></list-item><list-item><paragraph><bold>Environment &amp; dependency management</bold>: It successfully utilized multiple uploaded files (layers.py, notebooks, etc.) as context, effectively acting like an IDE by importing functions from one file to test another. This is already a feature in Claude Code or Claude+Cursor on a user workstation/ personal laptop, but I was impressed that within Claude.ai it handles files very well to provide context for the LLM to implement code.</paragraph></list-item><list-item><paragraph><bold>Bug detection in source material</bold>: The code execution feature allowed Claude to identify a bug in the staff provided template code regarding the mode initialization in FullyConnectedNet in <code>deeplearning/classifiers/fc_net.py</code> file in the <code>bn_drop.ipynb</code> coding task. Because the code actually ran, Claude could see the test failure that a text-only parser would likely miss. </paragraph></list-item></list></list-item><list-item><paragraph><bold>One-shot accuracy &amp; coding proficiency</bold>: </paragraph><list style=\"bullet\"><list-item><paragraph><bold>Standard neural network component implementation</bold>: Claude achieved 100% one-shot success on standard implementation tasks, including Na\u00efve Convolution, Max Pooling, and Ordinary Least Squares (OLS) solutions.</paragraph></list-item><list-item><paragraph><bold>Code optimization</bold>: It successfully identified and implemented a \"smart\" solution for switching between Train/Test modes in Batch Norm without user intervention, surpassing the manual toggling method initially suggested by itself as a fix for the bug in the provided HW codes.</paragraph></list-item><list-item><paragraph><bold>Architecture design</bold>: While it failed to meet the parameter count constraint (&lt;1M) on the first two tries for the CIFAR-100 CNN, it successfully iterated to an architecture within that constraint on the third try, demonstrating an understanding of the importance of neural network depth (keeping the number of layers constant throughout) vs. scaling the channel width.</paragraph></list-item></list></list-item><list-item><paragraph><bold>Hallucinations &amp; misconceptions</bold>:</paragraph><list style=\"bullet\"><list-item><paragraph><bold>Multimodal/visual failures</bold>: </paragraph><list style=\"bullet\"><list-item><paragraph><bold>Filename/plot mismatch</bold>: In the Dropout assignment, Claude inverted the analysis of the results. It confused the model_dropout and model_no_dropout plots despite the filenames. This led to a logic hallucination where it argued that Dropout made performance worse, inventing a narrative about \"dropout encouraged exploitation of cheating features\" to justify the hallucinated data. This is an example which shows how it required user correction to flip the hallucinated analysis.</paragraph></list-item><list-item><paragraph><bold>Diagram generation</bold>: Claude struggled significantly to generate a correct ASCII computational graph for Batch Norm. Despite understanding the flow mathematically, the visual output repeatedly missed edges or directed them incorrectly.</paragraph></list-item></list></list-item><list-item><paragraph><bold>Math hallucinations</bold>: During the mathematical derivation of the Batch Norm backward pass, Claude introduced an incorrect square root term in an intermediate step which was later cancelled out and finally appeared (hallucinated) to arrive at the correct final expression. This indicates it \"knew\" the answer but fabricated the intermediate logic to get there. </paragraph></list-item></list></list-item></list><paragraph>In conclusion, Claude does remarkably well in coding tasks. Its ability to run unit tests internally minimizes/prevents syntax errors and logical bugs from reaching the user, saving significant debugging time. However, we must be careful regarding 1) confident misreading of charts/ file associations, 2) mathematical and logical hallucinations, 3) constraint adherence (prioritizing accuracy over explicit constraints mentioned in the instructions on neural network architecture) weaknesses of Claude.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T12:05:24.320064+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7387144,
            "author": "Tianhao Qian",
            "project_title": "Special Participation D: Comparative Study of Muon & \u00b5P (and other optimizers) for GNN Training on Zachary\u2019s Karate Club",
            "post_body": "In this write-up, I systematically benchmarked several modern optimization approaches for training a Graph Convolutional Network (GCN) on the Zachary\u2019s Karate Club graph dataset, including SGD, \u00b5P, Muon, a Muonvariant, SOAP, and Lion. I evaluated them using training/validation loss, test accuracy, convergence speed, and computational efficiency, and I also implemented an early-stopping setup and described key optimizer mechanics (e.g., Muon\u2019s Newton\u2013Schulz orthogonalization; \u00b5P-style parameter-group learning-rate scaling).\n\nMy main findings are that Muon can reach the best final accuracy (up to 100%), while \u00b5P offers the best speed\u2013accuracy trade-off (fast convergence with strong accuracy), and SOAP/Lion tend to perform poorly in this small full-graph training regime.\n\nThe Github repository: \n\nhysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club",
            "content_xml": "<document version=\"2.0\"><paragraph>In this write-up, I systematically benchmarked several modern optimization approaches for training a Graph Convolutional Network (GCN) on the Zachary\u2019s Karate Club graph dataset, including SGD, \u00b5P, Muon, a Muonvariant, SOAP, and Lion. I evaluated them using training/validation loss, test accuracy, convergence speed, and computational efficiency, and I also implemented an early-stopping setup and described key optimizer mechanics (e.g., Muon\u2019s Newton\u2013Schulz orthogonalization; \u00b5P-style parameter-group learning-rate scaling).<break/><break/>My main findings are that <bold>Muon can reach the best final accuracy (up to 100%)</bold>, while <bold>\u00b5P offers the best speed\u2013accuracy trade-off</bold> (fast convergence with strong accuracy), and SOAP/Lion tend to perform poorly in this small full-graph training regime.<break/><break/>The Github repository: <break/><break/><link href=\"https://github.com/hysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club\">hysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club</link></paragraph><file url=\"https://static.us.edusercontent.com/files/k8tEqNVKDfguFvOPhyKDZEGx\" filename=\"special_participation_D_HW6.pdf\"/></document>",
            "links": [
                "https://github.com/hysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club"
            ],
            "attachments": [],
            "created_at": "2025-12-02T11:57:02.93077+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7386904,
            "author": "Minjune Kim",
            "project_title": "Special Participation A: Mistral on HW 1",
            "post_body": "I have used Mistral to test on Hw 1. \n\nLink: https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b\n\nSummary:\n\nIn general, it was able to get most of the answers without any mistakes. A lot of the work shown by the LLM followed the solutions of what the course has provided. Looking at the solution that Mistral provided, it seems like a lot of the computational mistakes are coming from matrix calculations. For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations. Since question 3 is built upon previous questions, since it messed up on 3b, it messes up on the following subpart questions for 3. \n\nIt seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors. I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes. \n\nI think the one-shot for Mistral has about 80% accuracy on the problems. I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question. It seems to be that MIstral is \"self-centered\" or cannot identify the questions that it has been asked. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I have used Mistral to test on Hw 1. <break/><break/>Link: <link href=\"https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b\">https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b</link></paragraph><paragraph>Summary:</paragraph><paragraph>In general, it was able to get most of the answers without any mistakes. A lot of the work shown by the LLM followed the solutions of what the course has provided. Looking at the solution that Mistral provided, it seems like a lot of the computational mistakes are coming from matrix calculations. For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations. Since question 3 is built upon previous questions, since it messed up on 3b, it messes up on the following subpart questions for 3. <break/><break/>It seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors. I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes. <break/><break/>I think the one-shot for Mistral has about 80% accuracy on the problems. I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question. It seems to be that MIstral is \"self-centered\" or cannot identify the questions that it has been asked. </paragraph></document>",
            "links": [
                "https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b"
            ],
            "attachments": [],
            "created_at": "2025-12-02T11:24:16.403925+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7386885,
            "author": "Divya Ramesh",
            "project_title": "Special Participation E: Gemini as an Interactive study tool!",
            "post_body": "\n\nSpecial Participation E:\n\nNow that finals are creeping up, I thought I\u2019d try to create a resource to make a study guide for lectures. After a bit of research online I discovered that Gemini 3 Pro had one of the better OCR handwriting detections (https://research.aimultiple.com/handwriting-recognition/ was one of the sources confirming this), so I decided to upload the lecture notes for lecture 6 into Gemini 3 Pro and let it analyze, and create a study guide. The prompts I used can be found in my chat log! Gemini has canvas mode, which allows for a very organized output, making the study guide color coded and clean. I also had it summarize as well as come up with practice problems. I made it analyze what was directly in the notes only, to try to minimize hallucinations. I instructed it to summarize key concepts, rewrite equations clearly, convert diagrams into textual descriptions, and provide intuition in bullet points. For each concept, it generated a short comprehension question, a tiny derivation for me to work through, and an example scenario showing how the concept applies in practice.\n\nAdditionally, I set it up to simulate optimizer dynamics, shape transformations, scaling factor effects, Newton-Schulz iterations, and orthogonalization steps using small toy matrices so I could verify everything myself. Whenever the notes were incomplete or ambiguous, the model flagged them and asked for clarification rather than inventing facts. From here, a user can ask the LLM to fill in the gaps or ignore them. I mostly chose to ignore, because staying as close as possible to the notes would minimize hallucinations. \n\nThe result was a structured, section-by-section study guide that covers MuP, Muon, scaling laws, parameterization, and theoretical analysis topics from the lecture. It also included clear alerts for potential inconsistencies or OCR errors, so I can focus on the parts that need human verification. This approach let me interactively test my understanding and run simulations in a controlled way, essentially turning the lecture notes into a hands-on learning tool.\n\nOverall, using Gemini 3 Pro in this targeted, verification-focused way allowed me to generate a high-quality study resource while minimizing hallucinations, making it an effective tool for preparing for finals. Also, it would let me know when there were sections unclear in the lecture notes so I could further prompt the LLM on this. \n\nI also turned on learning mode and asked Gemini to walk me through the worksheet, where it gave me hints when I needed them on worksheet problems, and if I was right/wrong it gave me feedback and a bit of background information to ensure I understood. Also, I asked it to walk me through simulations at the end and it actually did walk me through step by step of a problem, and I find this tool incredibly helpful!! The simulations/toy problems especially helped my lock down important lecture concepts.\n\nI\u2019ve tested on a few lectures and it has worked amazingly, feel free to check it out! I\u2019m attaching both the worksheet and the chat log below, the chat log is annotated, the worksheet is just for reference!\n\nLecture 6:\n\nChat log:\n\nhttps://drive.google.com/file/d/1egKs92MvXNTsvbv6E6RqEZwsXCIDM6cK/view?usp=sharing\n\nWorksheet:\n\nhttps://drive.google.com/file/d/1sl9c8k6Ghc4mZHDwVVriRR5bmkqH8p3p/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph>Special Participation E:</paragraph><paragraph>Now that finals are creeping up, I thought I\u2019d try to create a resource to make a study guide for lectures. After a bit of research online I discovered that Gemini 3 Pro had one of the better OCR handwriting detections (<link href=\"https://research.aimultiple.com/handwriting-recognition/\"><underline>https://research.aimultiple.com/handwriting-recognition/</underline></link> was one of the sources confirming this), so I decided to upload the lecture notes for lecture 6 into Gemini 3 Pro and let it analyze, and create a study guide. The prompts I used can be found in my chat log! Gemini has canvas mode, which allows for a very organized output, making the study guide color coded and clean. I also had it summarize as well as come up with practice problems. I made it analyze what was directly in the notes only, to try to minimize hallucinations. I instructed it to summarize key concepts, rewrite equations clearly, convert diagrams into textual descriptions, and provide intuition in bullet points. For each concept, it generated a short comprehension question, a tiny derivation for me to work through, and an example scenario showing how the concept applies in practice.</paragraph><paragraph>Additionally, I set it up to simulate optimizer dynamics, shape transformations, scaling factor effects, Newton-Schulz iterations, and orthogonalization steps using small toy matrices so I could verify everything myself. Whenever the notes were incomplete or ambiguous, the model flagged them and asked for clarification rather than inventing facts. From here, a user can ask the LLM to fill in the gaps or ignore them. I mostly chose to ignore, because staying as close as possible to the notes would minimize hallucinations. </paragraph><paragraph>The result was a structured, section-by-section study guide that covers MuP, Muon, scaling laws, parameterization, and theoretical analysis topics from the lecture. It also included clear alerts for potential inconsistencies or OCR errors, so I can focus on the parts that need human verification. This approach let me interactively test my understanding and run simulations in a controlled way, essentially turning the lecture notes into a hands-on learning tool.</paragraph><paragraph>Overall, using Gemini 3 Pro in this targeted, verification-focused way allowed me to generate a high-quality study resource while minimizing hallucinations, making it an effective tool for preparing for finals. Also, it would let me know when there were sections unclear in the lecture notes so I could further prompt the LLM on this. </paragraph><paragraph>I also turned on learning mode and asked Gemini to walk me through the worksheet, where it gave me hints when I needed them on worksheet problems, and if I was right/wrong it gave me feedback and a bit of background information to ensure I understood. Also, I asked it to walk me through simulations at the end and it actually did walk me through step by step of a problem, and I find this tool incredibly helpful!! The simulations/toy problems especially helped my lock down important lecture concepts.</paragraph><paragraph>I\u2019ve tested on a few lectures and it has worked amazingly, feel free to check it out! I\u2019m attaching both the worksheet and the chat log below, the chat log is annotated, the worksheet is just for reference!</paragraph><paragraph>Lecture 6:</paragraph><list style=\"unordered\"><list-item><paragraph>Chat log:</paragraph></list-item></list><paragraph><link href=\"https://drive.google.com/file/d/1egKs92MvXNTsvbv6E6RqEZwsXCIDM6cK/view?usp=sharing\"><underline>https://drive.google.com/file/d/1egKs92MvXNTsvbv6E6RqEZwsXCIDM6cK/view?usp=sharing</underline></link></paragraph><list style=\"unordered\"><list-item><paragraph>Worksheet:</paragraph></list-item></list><paragraph><link href=\"https://drive.google.com/file/d/1sl9c8k6Ghc4mZHDwVVriRR5bmkqH8p3p/view?usp=sharing\"><underline>https://drive.google.com/file/d/1sl9c8k6Ghc4mZHDwVVriRR5bmkqH8p3p/view?usp=sharing</underline></link></paragraph></document>",
            "links": [
                "https://research.aimultiple.com/handwriting-recognition/",
                "https://drive.google.com/file/d/1egKs92MvXNTsvbv6E6RqEZwsXCIDM6cK/view?usp=sharing",
                "https://drive.google.com/file/d/1sl9c8k6Ghc4mZHDwVVriRR5bmkqH8p3p/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-02T11:22:03.796165+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7386518,
            "author": "Ruihan Xia",
            "project_title": "Special Participation B: Grok on HW1",
            "post_body": "I worked with Grok on the HW1 coding parts. I noticed three main patterns / caveats in its responses.\n\n1. Whenever I asked Grok to fill in a missing section, it tended to go beyond the homework requirements and rewrite lines before and after TODO section. For example, in implementing GDM function it introduced a new velocity variable unnecessarily. It took a couple rounds of correction before it stuck to the original template. \n\n2. In the same section, I noticed Grok needs clear guidance when the notation in hw differs from convention. HW1 uses formula (1\u2212\u03b2)zt\u200b+\u03b2gt\u200b for momentum. But Grok defaulted to the PyTorch-style formula until I showed it the screenshot of the written math. \n\n3. Grok liked to \u201ctake charge\u201d of the hyperparameters. The notebook uses \u03b2 = 0.6, but Grok repeatedly swapped it for 0.9 (and later stuck to 0.9 consistently) simply because it believed that value converged faster. As a result in later code generation it changes beta without explicitedly informing the user. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I worked with Grok on the HW1 coding parts. I noticed three main patterns / caveats in its responses.</paragraph><paragraph>1. Whenever I asked Grok to fill in a missing section, it tended to go beyond the homework requirements and rewrite lines before and after TODO section. For example, in implementing GDM function it introduced a new velocity variable unnecessarily. It took a couple rounds of correction before it stuck to the original template. </paragraph><paragraph>2. In the same section, I noticed Grok needs clear guidance when the notation in hw differs from convention. HW1 uses formula (1\u2212\u03b2)zt\u200b+\u03b2gt\u200b for momentum. But Grok defaulted to the PyTorch-style formula until I showed it the screenshot of the written math. </paragraph><paragraph>3. Grok liked to \u201ctake charge\u201d of the hyperparameters. The notebook uses \u03b2 = 0.6, but Grok repeatedly swapped it for 0.9 (and later stuck to 0.9 consistently) simply because it believed that value converged faster. As a result in later code generation it changes beta without explicitedly informing the user. </paragraph><file url=\"https://static.us.edusercontent.com/files/3QsI8zPy63E7fmXhmnfCSayo\" filename=\"Grok Chat.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T10:29:28.620706+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7386458,
            "author": "Manan Roongta",
            "project_title": "Special Participation E: Gemini Guided Learning as a Personal Tutor",
            "post_body": "Posting this so other people can reuse the same approach/prompting style when they\u2019re stuck on a concept.\n\nGemini Guided Learning Chat (Transformers + Attention)\n\nPDF (with annotations)\n\nI have been using Gemini Guided Learning as a \u201cpersonal tutor\u201d to strengthen my understanding of complex topics, and it's been really helpful. Lectures are great, but I often walk out with feeling \"I kinda get it but not really\u201d. It\u2019s an amazing supplement when you need someone to patiently answer \u201cwait but why\u201d 15 times in a row.\n\nThe PDF is my real interaction trace (with annotations), including my typos, missing punctuation, misspellings, half finished thoughts, because that\u2019s actually how a student uses these tools in real life.\n\nHow I used Gemini Guided Learning/what worked\n\n1) My initial prompt mattered a lot\n In the very first message, I clearly set expectations:\n\nwhat I already understood\n\nwhat I was confused about\n\nwhat I wanted/roadmap\n\nThat made the whole chat way more productive than \u201cexplain transformers.\u201d\n\n2) I had to slow Gemini down at first\nEarly in the chat, Gemini kept trying to rush to the next topic. I had to explicitly prompt \u201cslow down\u201d, \u201cone step at a time\u201d, \u201cdon't rush me\u201d. After a few times, the session became way more tutor like and less of just answering.\n\n3) The built in check ins/questions are great way to test\nI liked when Gemini asked me questions and made me answer. It forced me to notice where my understanding was hand wavy.\n\n4) Write your own Summaries \n After finishing a topic, I would:\n\nwrite a summary in my own words, and/or\n\nask Gemini for a \u201csummary + roadmap\u201d of what we finished and what\u2019s next.\n\nIt helped recalibrate my understanding, the bigger picture and track progress.\n\nGemini 3 vs Gemini 3 Guided Learning are very different\n\nGemini 3 Guided Learning felt like it was trying to teach\n\nGemini 3 (normal) felt more like it was trying to answer quickly (even when I explicitly prompted for slow + structured learning)\n\nNote: I used AI to help with wording/formatting",
            "content_xml": "<document version=\"2.0\"><paragraph>Posting this so other people can reuse the same approach/prompting style when they\u2019re stuck on a concept.</paragraph><paragraph><link href=\"https://gemini.google.com/share/bc199205426f\">Gemini Guided Learning Chat (Transformers + Attention</link>)</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1EAE9hd98DESFMTvXKjlW9WG6B6VZxutj/view?usp=sharing\">PDF (with annotations)</link></paragraph><paragraph>I have been using <bold>Gemini Guided Learning</bold> as a \u201cpersonal tutor\u201d to strengthen my understanding of complex topics, and it's been really helpful. Lectures are great, but I often walk out with feeling \"I kinda get it but not really\u201d. It\u2019s an amazing supplement when you need someone to patiently answer \u201cwait but why\u201d 15 times in a row.</paragraph><paragraph>The PDF is my real interaction trace (with annotations), including my typos, missing punctuation, misspellings, half finished thoughts, because that\u2019s actually how a student uses these tools in real life.</paragraph><heading level=\"3\"><underline>How I used Gemini Guided Learning/what worked</underline></heading><paragraph><underline>1) My initial prompt mattered a lot<break/></underline> In the very first message, I clearly set expectations:</paragraph><list style=\"unordered\"><list-item><paragraph>what I already understood</paragraph></list-item><list-item><paragraph>what I was confused about</paragraph></list-item><list-item><paragraph>what I wanted/roadmap</paragraph></list-item></list><paragraph>That made the whole chat way more productive than \u201cexplain transformers.\u201d</paragraph><paragraph><underline>2) I had to slow Gemini down at first<break/></underline>Early in the chat, Gemini kept trying to rush to the next topic. I had to explicitly prompt \u201cslow down\u201d, \u201cone step at a time\u201d, \u201cdon't rush me\u201d. After a few times, the session became way more tutor like and less of just answering.</paragraph><paragraph><underline>3) The built in check ins/questions are great way to test<break/></underline>I liked when Gemini asked me questions and made me answer. It forced me to notice where my understanding was hand wavy.</paragraph><paragraph><underline>4) Write your own Summaries <break/></underline> After finishing a topic, I would:</paragraph><list style=\"unordered\"><list-item><paragraph>write a summary in my own words, and/or</paragraph></list-item><list-item><paragraph>ask Gemini for a \u201csummary + roadmap\u201d of what we finished and what\u2019s next.</paragraph></list-item></list><paragraph>It helped recalibrate my understanding, the bigger picture and track progress.</paragraph><heading level=\"3\">Gemini 3 vs Gemini 3 Guided Learning are very different</heading><list style=\"unordered\"><list-item><paragraph><bold>Gemini 3 Guided Learning</bold> felt like it was trying to <italic>teach</italic></paragraph></list-item><list-item><paragraph><bold>Gemini 3 (normal)</bold> felt more like it was trying to <italic>answer</italic> quickly (even when I explicitly prompted for slow + structured learning)</paragraph></list-item></list><paragraph><italic>Note</italic>: I used AI to help with wording/formatting</paragraph></document>",
            "links": [
                "https://gemini.google.com/share/bc199205426f",
                "https://drive.google.com/file/d/1EAE9hd98DESFMTvXKjlW9WG6B6VZxutj/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-02T10:21:51.42187+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7386095,
            "author": "Shervin Goudarzi",
            "project_title": "Special Participation B: Gemini-Pro 3 on HW10 Coding",
            "post_body": "I tried the Gemini Pro-3 Thinking on the Coding questions of HW10, and it performed very well. However, the solutions tended to be less concise and more intuitive compared to the staff solutions. More importantly, Gemini refrained from using the einsum functions and used normal vector multiplication, possibly to aid me, an undergraduate who is learning without confusing with too much jargon. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I tried the Gemini Pro-3 Thinking on the Coding questions of HW10, and it performed very well. However, the solutions tended to be less concise and more intuitive compared to the staff solutions. More importantly, Gemini refrained from using the einsum functions and used normal vector multiplication, possibly to aid me, an undergraduate who is learning without confusing with too much jargon. </paragraph><file url=\"https://static.us.edusercontent.com/files/XCpBRahcb5ysDOFBUQXnAXWG\" filename=\"Special_Participation_B.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T09:37:11.472719+11:00",
            "category": "Admin"
        },
        {
            "guid": 7384241,
            "author": "Jeffrey Cheng",
            "project_title": "Special Participation E: Use ChatGPT's study guide to compare transformer-based LLMs",
            "post_body": "I used ChatGPT's study mode to compare different transformer-based LLMs based on their architectural choices and the tasks for which they are best suited. Engaging in a Socratic conversation with ChatGPT helped me understand the architectural decisions of various transformer-based LLMs and the specific functions for which they are best suited. It classifies the model architecture into two categories: Encoder/Decoder, Dense/MoE, and explains the strengths of each category in terms of the tasks they are most suited for. One behavior of this study mode that I found very interesting is that, at the end of each ChatGPT response, it always asks me a question that refines my understanding of the current topic. In addition, it helps introduce the next topic for the next prompt. In the end, it generates a cheat sheet that classifies every transformer-based model. \n\nThe conversation log is: ",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT's study mode to compare different transformer-based LLMs based on their architectural choices and the tasks for which they are best suited. Engaging in a Socratic conversation with ChatGPT helped me understand the architectural decisions of various transformer-based LLMs and the specific functions for which they are best suited. It classifies the model architecture into two categories: Encoder/Decoder, Dense/MoE, and explains the strengths of each category in terms of the tasks they are most suited for. One behavior of this study mode that I found very interesting is that, at the end of each ChatGPT response, it always asks me a question that refines my understanding of the current topic. In addition, it helps introduce the next topic for the next prompt. In the end, it generates a cheat sheet that classifies every transformer-based model. </paragraph><paragraph>The conversation log is: </paragraph><file url=\"https://static.us.edusercontent.com/files/Ggb85TwYCdj1XAJ7rFwAKQIg\" filename=\"Annotated log of transformer-based models.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-02T06:07:21.800584+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7382863,
            "author": "Kian Hekmatnejad",
            "project_title": "Special Participation A: Mistral AI's Le Chat on HW5 Written Portion",
            "post_body": "For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requiring further prompting to correct mistakes. I found some indication of a lack of ability to refer to chat history in it's responses.\n\nHere is the link to my chat: https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26 ",
            "content_xml": "<document version=\"2.0\"><paragraph>For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requiring further prompting to correct mistakes. I found some indication of a lack of ability to refer to chat history in it's responses.<break/><break/>Here is the link to my chat: <link href=\"https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26\">https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26</link> </paragraph><file url=\"https://static.us.edusercontent.com/files/RMtVzYbcYGOvDh6qzjoq3xWo\" filename=\"special_participation.pdf\"/></document>",
            "links": [
                "https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26"
            ],
            "attachments": [],
            "created_at": "2025-12-02T02:48:01.810497+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7382039,
            "author": "Ishir Garg",
            "project_title": "Special Participation E: Using Perplexity to gain Historical Motivations",
            "post_body": "One thing that I've noticed in this class, is that there seems to be an emphasis on motivating ideas through a historical lens and understanding how they naturally evolved in research over time. However, sometimes I find its hard to connect all the different puzzle pieces and understand where exactly different algorithms or discoveries exist in the timeline. \n\n Hence, I tried to engage an LLM (Perplexity, due to its enhanced search abilities) to try and gain some historical perspective on how ideas came to fruition in deep learning. I uploaded Lecture Note 5 along with an initial prompt, and had Perplexity walk me through a variety of different concepts from a historical perspective. The annotated chat history is below:\n\nTo summarize, I was pleasantly surprised by the outcome. I'm also surprised that it was able to do this with just the lecture notes and a very minimal initial prompt.\n\nStrengths:\n\nPerplexity does a good job of explaining the intuitive motivations behind ideas and how certain ideas lead to others\n\nOne really surprising result was that Perplexity began asking me questions to check my understanding, even though I never asked it to do this\n\nIt also introduced some ideas outside of the lecture notes at a high-level, that provided some really unique insight into the ideas in this class.\n\nWeaknesses:\n\nSome parts could use more explanation or go more in-depth, although it could be argued that these could just be left to the student to ask questions about if they desire.",
            "content_xml": "<document version=\"2.0\"><paragraph>One thing that I've noticed in this class, is that there seems to be an emphasis on motivating ideas through a historical lens and understanding how they naturally evolved in research over time. However, sometimes I find its hard to connect all the different puzzle pieces and understand where exactly different algorithms or discoveries exist in the timeline. </paragraph><paragraph> Hence, I tried to engage an LLM (Perplexity, due to its enhanced search abilities) to try and gain some historical perspective on how ideas came to fruition in deep learning. I uploaded Lecture Note 5 along with an initial prompt, and had Perplexity walk me through a variety of different concepts from a historical perspective. The annotated chat history is below:</paragraph><file url=\"https://static.us.edusercontent.com/files/VaBmvFfCKL6FlhgZmgWBerz9\" filename=\"participationE.pdf\"/><paragraph>To summarize, I was pleasantly surprised by the outcome. I'm also surprised that it was able to do this with just the lecture notes and a very minimal initial prompt.</paragraph><paragraph>Strengths:</paragraph><list style=\"bullet\"><list-item><paragraph>Perplexity does a good job of explaining the intuitive motivations behind ideas and how certain ideas lead to others</paragraph></list-item><list-item><paragraph>One really surprising result was that Perplexity began asking me questions to check my understanding, even though I never asked it to do this</paragraph></list-item><list-item><paragraph>It also introduced some ideas outside of the lecture notes at a high-level, that provided some really unique insight into the ideas in this class.</paragraph></list-item></list><paragraph>Weaknesses:</paragraph><list style=\"bullet\"><list-item><paragraph>Some parts could use more explanation or go more in-depth, although it could be argued that these could just be left to the student to ask questions about if they desire.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-01T21:28:27.236519+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7381997,
            "author": "Ishir Garg",
            "project_title": "Special Participation B: Gemini (Thinking With Pro 3) on HW 3 Coding",
            "post_body": "I used Gemini (Thinking with Pro 3) to solve the MuP coding question of homework 3. I have attached an annotated PDF of my chat history below.\n\nTo summarize the main points:\n\nOverall, Gemini is able to one-shot 4/5 parts. It fails to one-shot part (d) on which it struggled significantly, and required some hints to get to the right answer.\n\nStrengths:\n\nGemini is able to easily get questions that follow directly from the typical MuP formulation; it generally does a great job of writing clean and concise solutions\n\nGemini tends to provide good mathematical intuition for its explanations, even though it is never prompted to provide such justifications\n\nCode is generally well-written and has informative comments that could be helpful for a student trying to learn the content\n\nWeaknesses:\n\nWhen trying to apply the same concepts \"outside the box\" (such as modifying the computation graph in part (d)), it struggles.\n\nWhen given feedback on its incorrect solution, Gemini tries to directly address the issue by creating a \"hacky\" solution, rather than trying to reason about a logically correct answer.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini (Thinking with Pro 3) to solve the MuP coding question of homework 3. I have attached an annotated PDF of my chat history below.</paragraph><file url=\"https://static.us.edusercontent.com/files/dOeN4GFgmXf3LSsIUU4mY5Wn\" filename=\"participationB.pdf\"/><paragraph>To summarize the main points:</paragraph><paragraph>Overall, Gemini is able to one-shot 4/5 parts. It fails to one-shot part (d) on which it struggled significantly, and required some hints to get to the right answer.</paragraph><paragraph>Strengths:</paragraph><list style=\"bullet\"><list-item><paragraph>Gemini is able to easily get questions that follow directly from the typical MuP formulation; it generally does a great job of writing clean and concise solutions</paragraph></list-item><list-item><paragraph>Gemini tends to provide good mathematical intuition for its explanations, even though it is never prompted to provide such justifications</paragraph></list-item><list-item><paragraph>Code is generally well-written and has informative comments that could be helpful for a student trying to learn the content</paragraph></list-item></list><paragraph>Weaknesses:</paragraph><list style=\"bullet\"><list-item><paragraph>When trying to apply the same concepts \"outside the box\" (such as modifying the computation graph in part (d)), it struggles.</paragraph></list-item><list-item><paragraph>When given feedback on its incorrect solution, Gemini tries to directly address the issue by creating a \"hacky\" solution, rather than trying to reason about a logically correct answer.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-01T20:36:46.080801+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7381645,
            "author": "Andrew Choy",
            "project_title": "Special Participation E: 182 Exam Prep Tool",
            "post_body": "Since I couldn't find any practice tests available online, I decided to build a Practice Question Generator. It utilizes homework assignments, lecture notes, and/or lecture audio to generate relevant study problems.\n\nI found that pairing specific homework assignments with their corresponding lectures yields the most grounded results, even when using free-tier API models. To make the tool more convenient, I added an input field for your API key; this allows you to run the tool without editing the code directly (unless you want to tweak the prompt for other classes).\n\nThe screen recording below demonstrates the tool using only lecture notes and homework. While the system does support lecture audio, which is great for capturing details professors say but don't write down, I stuck to processing PDFs for this demo to minimize inference latency and I used gemini-2.0-flash API Version. \n\nOne of the biggest challenges was formatting the model's output. I attempted to wrap the responses in Markdown to ensure the display math rendered correctly, but this approach introduced several bugs and made the code difficult to maintain. So if anyone wants to push this further, feel free to fork or make a PR request. \n\nGithub: https://github.com/AndrewChoyCS/CS182-artifact\n\nDemo: https://drive.google.com/file/d/1DkO63NA44jGrEhoFPZGidESEYAcKIe94/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>Since I couldn't find any practice tests available online, I decided to build a <bold>Practice Question Generator</bold>. It utilizes homework assignments, lecture notes, and/or lecture audio to generate relevant study problems.</paragraph><paragraph>I found that pairing specific homework assignments with their corresponding lectures yields the most grounded results, even when using free-tier API models. To make the tool more convenient, I added an input field for your API key; this allows you to run the tool without editing the code directly (unless you want to tweak the prompt for other classes).</paragraph><paragraph>The screen recording below demonstrates the tool using only lecture notes and homework. While the system does support lecture audio, which is great for capturing details professors say but don't write down, I stuck to processing PDFs for this demo to minimize inference latency and I used gemini-2.0-flash API Version. </paragraph><paragraph>One of the biggest challenges was formatting the model's output. I attempted to wrap the responses in Markdown to ensure the display math rendered correctly, but this approach introduced several bugs and made the code difficult to maintain. So if anyone wants to push this further, feel free to fork or make a PR request. </paragraph><paragraph>Github: <link href=\"https://github.com/AndrewChoyCS/CS182-artifact\">https://github.com/AndrewChoyCS/CS182-artifact</link></paragraph><paragraph>Demo: <link href=\"https://drive.google.com/file/d/1DkO63NA44jGrEhoFPZGidESEYAcKIe94/view?usp=sharing\">https://drive.google.com/file/d/1DkO63NA44jGrEhoFPZGidESEYAcKIe94/view?usp=sharing</link></paragraph></document>",
            "links": [
                "https://github.com/AndrewChoyCS/CS182-artifact",
                "https://drive.google.com/file/d/1DkO63NA44jGrEhoFPZGidESEYAcKIe94/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-12-01T17:21:56.578199+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7381537,
            "author": "Divya Ramesh",
            "project_title": "Special Participation B: Gemini in Collab on HW10",
            "post_body": "\n\nExecutive Summary:\n\nI tested Gemini (Collab Code Assist) on homework 10\u2019s coding portions and quickly learned it struggles substantially more with code synthesis than with conceptual reasoning. It almost never one-shot the required code. Most blocks required highly specific context, restating variables, or manually \u201cnudging\u201d the model by typing partial lines (like Km =, q =, d_k =) before autofill kicked in.Major patterns. I also tested how the model worked with non-specific variable names like \u201ctemp = \u201c and this was good as well, but the issue is sometimes you need the specific names to autocomplete entire sections. Like there was one question with k=..., q=..., and v=... and by going k=, all of the sections autocompleted, while saying temp = only filled for one sometimes. One thing I noticed was the one-shot rate was very low, almost 0 for complex, more than one line sections of code. Also, sometimes, Gemini created a solution that invented wrong shapes, incomplete lines, or inconsistent variable names. This was frustrating sometimes, because it was very hard to prompt Gemini when it started hallucinating, taking a long time trying to force it to correct its logic. Overall, getting correct functional code was basically impossible without dragging the model step-by-step. Even then, it required substantial human debugging and reasoning, and was not a very easy to use or accurate model. I tested inputting chunks of todo code directly into Gemini\u2019s thinking with 3 Pro model and it worked a lot better, completely filling in the code with much more accurate code. This contrast made it clear that Gemini Collab just isn\u2019t reliable for multi-step or shape-sensitive coding tasks, and requires substantial human scaffolding to make it usable at all.\n\nLinked below is a log of my interactions. I kept adding context in the TODO sections line by line until the code was giving better solutions. I also prompted with the variable names, line by line (sometimes it would fill in 3 lines so maybe every 3 lines I would do this) waiting for Gemini to autocomplete. That is the strategy I used. My annotations are also written in the text, and the entire ipynb pdfs are attached at the ends. Q2 and Q3 are labelled, they were the coding sections for this homework:\n\nhttps://drive.google.com/file/d/1IBckfJo7Dq_J1ydQ8p73dkxCbkBR2cCY/view?usp=share_link",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>I tested Gemini (Collab Code Assist) on homework 10\u2019s coding portions and quickly learned it struggles substantially more with code synthesis than with conceptual reasoning. It almost never one-shot the required code. Most blocks required highly specific context, restating variables, or manually \u201cnudging\u201d the model by typing partial lines (like Km =, q =, d_k =) before autofill kicked in.Major patterns. I also tested how the model worked with non-specific variable names like \u201ctemp = \u201c and this was good as well, but the issue is sometimes you need the specific names to autocomplete entire sections. Like there was one question with k=..., q=..., and v=... and by going k=, all of the sections autocompleted, while saying temp = only filled for one sometimes. One thing I noticed was the one-shot rate was very low, almost 0 for complex, more than one line sections of code. Also, sometimes, Gemini created a solution that invented wrong shapes, incomplete lines, or inconsistent variable names. This was frustrating sometimes, because it was very hard to prompt Gemini when it started hallucinating, taking a long time trying to force it to correct its logic. Overall, getting correct functional code was basically impossible without dragging the model step-by-step. Even then, it required substantial human debugging and reasoning, and was not a very easy to use or accurate model. I tested inputting chunks of todo code directly into Gemini\u2019s thinking with 3 Pro model and it worked a lot better, completely filling in the code with much more accurate code. This contrast made it clear that Gemini Collab just isn\u2019t reliable for multi-step or shape-sensitive coding tasks, and requires substantial human scaffolding to make it usable at all.</paragraph><paragraph>Linked below is a log of my interactions. I kept adding context in the TODO sections line by line until the code was giving better solutions. I also prompted with the variable names, line by line (sometimes it would fill in 3 lines so maybe every 3 lines I would do this) waiting for Gemini to autocomplete. That is the strategy I used. My annotations are also written in the text, and the entire ipynb pdfs are attached at the ends. Q2 and Q3 are labelled, they were the coding sections for this homework:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1IBckfJo7Dq_J1ydQ8p73dkxCbkBR2cCY/view?usp=share_link\"><underline>https://drive.google.com/file/d/1IBckfJo7Dq_J1ydQ8p73dkxCbkBR2cCY/view?usp=share_link</underline></link></paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1IBckfJo7Dq_J1ydQ8p73dkxCbkBR2cCY/view?usp=share_link"
            ],
            "attachments": [],
            "created_at": "2025-12-01T16:49:24.640386+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7381449,
            "author": "Alex Cao",
            "project_title": "Special Participation B: Deepseek on HW10",
            "post_body": "\n\nIntro:\n\nThis is an attempt to interact with deepseek on coding parts of homework 10. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM\u2019s capability of solving real life coding problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek\u2019s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. Note that in this report I will only include screenshots of important parts of the conversation. For full conversation traces, please refer to the conversation links.\n\n\n\nConversation Links:\n\nhttps://chat.deepseek.com/share/8vswr9xhltacersq2x\nhttps://chat.deepseek.com/share/z3d2wlzyriby9tat3v\nhttps://chat.deepseek.com/share/b9ei80b7h70hg32ego\nhttps://chat.deepseek.com/share/svy7a295g0hypncpej\n\n\n\nAnnotated Traces:\n\n\n\nSummary:\n\n\nOverall, the model (DeepSeek-V3.2) showcases quite strong coding abilities and was able to complete almost all coding questions with at most one round of iterative debugging. One round of iterative debugging refers to giving feedback of the executed test result back to the model only once and asking it to refine its own code. For this specific problem set, the most common reason for a model needing iterative debugging was that the model lacked the reasoning ability to predict what exactly would happen when the code it wrote is actually executed. For example, in one of the questions, even though the prompt explicitly says mask is a byte array, and the model is well aware of this, it still failed to predict that this will be an incompatible format for a given torch method mask_fill, which accepts boolean arrays. These can be potentially solved by enabling better reasoning abilities, while I argue that in this case, a more efficient solution would be \u201ctest-time scaling\u201d. Instead of spending numerous computations in improving the model's reasoning ability, one thing we can do to tackle this problem is to simply use another model call with the test feedback. In this specific problem set, this approach has been proven working every time, and simply calling the model one more time uses way less computation than trying to improve the model\u2019s native reasoning ability. Taken together, these observations suggest that, at least for this kind of coding workload, investing in smarter test-time strategies like iterative debugging and feedback-driven refinement may be a more practical and cost-effective way to boost reliability than solely focusing on training ever-stronger base models.\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><heading level=\"3\"><bold>Intro:</bold></heading><paragraph>This is an attempt to interact with deepseek on coding parts of homework 10. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM\u2019s capability of solving real life coding problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek\u2019s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. Note that in this report I will only include screenshots of important parts of the conversation. For full conversation traces, please refer to the conversation links.<break/><break/></paragraph><paragraph><bold>Conversation Links:</bold></paragraph><paragraph><link href=\"https://chat.deepseek.com/share/8vswr9xhltacersq2x\"><underline>https://chat.deepseek.com/share/8vswr9xhltacersq2x</underline><break/></link><link href=\"https://chat.deepseek.com/share/z3d2wlzyriby9tat3v\"><underline>https://chat.deepseek.com/share/z3d2wlzyriby9tat3v</underline><break/></link><link href=\"https://chat.deepseek.com/share/b9ei80b7h70hg32ego\"><underline>https://chat.deepseek.com/share/b9ei80b7h70hg32ego</underline><break/></link><link href=\"https://chat.deepseek.com/share/svy7a295g0hypncpej\"><underline>https://chat.deepseek.com/share/svy7a295g0hypncpej</underline></link><break/><break/></paragraph><paragraph><bold>Annotated Traces:</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/q5ljMKo93lSyS3Ryn12eFR5M\" filename=\"Special Participation B -- Deepseek on HW10.pdf\"/><paragraph/><paragraph><bold>Summary:</bold><break/></paragraph><paragraph>Overall, the model (DeepSeek-V3.2) showcases quite strong coding abilities and was able to complete almost all coding questions with at most one round of iterative debugging. One round of iterative debugging refers to giving feedback of the executed test result back to the model only once and asking it to refine its own code. For this specific problem set, the most common reason for a model needing iterative debugging was that the model lacked the reasoning ability to predict what exactly would happen when the code it wrote is actually executed. For example, in one of the questions, even though the prompt explicitly says mask is a byte array, and the model is well aware of this, it still failed to predict that this will be an incompatible format for a given torch method mask_fill, which accepts boolean arrays. These can be potentially solved by enabling better reasoning abilities, while I argue that in this case, a more efficient solution would be \u201ctest-time scaling\u201d. Instead of spending numerous computations in improving the model's reasoning ability, one thing we can do to tackle this problem is to simply use another model call with the test feedback. In this specific problem set, this approach has been proven working every time, and simply calling the model one more time uses way less computation than trying to improve the model\u2019s native reasoning ability. Taken together, these observations suggest that, at least for this kind of coding workload, investing in smarter test-time strategies like iterative debugging and feedback-driven refinement may be a more practical and cost-effective way to boost reliability than solely focusing on training ever-stronger base models.<break/></paragraph></document>",
            "links": [
                "https://chat.deepseek.com/share/8vswr9xhltacersq2x",
                "https://chat.deepseek.com/share/z3d2wlzyriby9tat3v",
                "https://chat.deepseek.com/share/b9ei80b7h70hg32ego",
                "https://chat.deepseek.com/share/svy7a295g0hypncpej"
            ],
            "attachments": [],
            "created_at": "2025-12-01T16:18:54.805285+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7381174,
            "author": "Ruihan Xia",
            "project_title": "Special Participation A: Qwen on HW7",
            "post_body": "I used Qwen3-Max model to solve the written parts of homework 7. Instead of feeding individual questions with full text, I first uploaded the entire homework PDF and then asked each sub-question only by its label (\u201c7(b)\u201d, \u201c8(a)\u201d, etc.). The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt. In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math. \n\nOne-shot accuracy: ~70%. Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary. \nHallucinations: None observed\nModel failures: Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra. It tends to stick to the notation in the question and is reluctant to apply more advanced tricks that simplify the expression, but rather to choose to brute force derive. In addition, Qwen might interpret wording like baseline with its own definition (updated baseline rather than benchmark), which might cause confusion if not clearly stated. \nContext retention: Surprisingly strong\u2014Qwen consistently located the correct question inside the PDF without needing me to restate it.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Qwen3-Max model to solve the written parts of homework 7. Instead of feeding individual questions with full text, I first uploaded the entire homework PDF and then asked each sub-question only by its label (\u201c7(b)\u201d, \u201c8(a)\u201d, etc.). The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt. In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math. </paragraph><paragraph><bold>One-shot accuracy:</bold> ~70%. Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary. <break/><bold>Hallucinations:</bold> None observed<break/><bold>Model failures:</bold> Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra. It tends to stick to the notation in the question and is reluctant to apply more advanced tricks that simplify the expression, but rather to choose to brute force derive. In addition, Qwen might interpret wording like baseline with its own definition (updated baseline rather than benchmark), which might cause confusion if not clearly stated. <break/><bold>Context retention:</bold> Surprisingly strong\u2014Qwen consistently located the correct question inside the PDF without needing me to restate it.</paragraph><file url=\"https://static.us.edusercontent.com/files/SNLyYfvwHJOtEPk9bh1yrIIJ\" filename=\"Qwen Chat.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-01T15:15:40.022846+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7380526,
            "author": "Aaryan Chandna",
            "project_title": "Special Participation A: Using Gemini Flash 2.5 on HW11",
            "post_body": "Trace: https://gemini.google.com/share/2e206d7da648\n\nMath + T/F Question Zero-Shot Performance: 13/15.\n\nPrompt Structure: I told the model that it was a DL assistant for me. I gave the model the concepts I was planning to ask it about, and let the model know to output its interpretation of the question, followed by a step-by-step solution and a final answer for each. I also told the model that it should be able to correct itself when needed. For each question, I simply provided images of screenshots of the homework. For some questions, I explicitly told the model what context to consider, when needed. \n\nAnalysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance). It identifies the main idea correctly, but does not get two of the other ideas from the solutions, even after re-prompting. Following this somewhat slow start, the model goes on a long streak of answering questions thoroughly and correctly. The solutions given by the model are arguably better than the actual HW solutions in some cases. In others, the model is potentially not concise enough, but this is also partially due to my initial prompt, where I asked the model to go step-by-step and then provide a final answer (sort of a CoT way of prompting the model).\n\nInterestingly, towards the end, the model starts trying to reattempt questions that it had already solved. For example, when I prompted Gemini to do 6a, it initially tried to resolve question 1 which it had already completed. When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of). This signified that Gemini 2.5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here. It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2. On 6c, it actually got two of the T/F questions wrong. Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong. It seems that the performance of the model gets better and then gets worse, empirically. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Trace: <link href=\"https://gemini.google.com/share/2e206d7da648\">https://gemini.google.com/share/2e206d7da648</link></paragraph><paragraph>Math + T/F Question Zero-Shot Performance: 13/15.</paragraph><paragraph>Prompt Structure: I told the model that it was a DL assistant for me. I gave the model the concepts I was planning to ask it about, and let the model know to output its interpretation of the question, followed by a step-by-step solution and a final answer for each. I also told the model that it should be able to correct itself when needed. For each question, I simply provided images of screenshots of the homework. For some questions, I explicitly told the model what context to consider, when needed. </paragraph><paragraph>Analysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance). It identifies the main idea correctly, but does not get two of the other ideas from the solutions, even after re-prompting. Following this somewhat slow start, the model goes on a long streak of answering questions thoroughly and correctly. The solutions given by the model are arguably better than the actual HW solutions in some cases. In others, the model is potentially not concise enough, but this is also partially due to my initial prompt, where I asked the model to go step-by-step and then provide a final answer (sort of a CoT way of prompting the model).</paragraph><paragraph>Interestingly, towards the end, the model starts trying to reattempt questions that it had already solved. For example, when I prompted Gemini to do 6a, it initially tried to resolve question 1 which it had already completed. When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of). This signified that Gemini 2.5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here. It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2. On 6c, it actually got two of the T/F questions wrong. Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong. It seems that the performance of the model gets better and then gets worse, empirically. </paragraph><file url=\"https://static.us.edusercontent.com/files/E80wBkA1Nmv9OGX92e1aYBrv\" filename=\"aaryan_spec_part_a.pdf\"/></document>",
            "links": [
                "https://gemini.google.com/share/2e206d7da648"
            ],
            "attachments": [],
            "created_at": "2025-12-01T13:04:21.853163+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7380406,
            "author": "Jason Guo",
            "project_title": "Special Participation E: Using ChatGPT to create practice problems",
            "post_body": "To study for the final, I thought it would be interesting to see if ChatGPT can be used to created practice problems that are similar to homework problems, and solidify my understanding of the topics covered in homework. As an example, I gave ChatGPT homework 1, which has problems relating to optimizers, and asked it to generate two exam problems, each with multiple parts, relating to opitmizers based on the given homework problems.\n\nOverall, I thought it did well and actually provided some good problems for studying,. The difficulty of most of the problems it gave were around the same difficulty as the homework, with the exception of around 2 subquestions that required mathematical knowledge that's probably out of scope for this class. I also asked it to generate solutions for these problems, and the solutions were pretty good at explaining the answers in detail. As a whole, I found this to be really helpful in helping me review the material and get more practice solving exam/homework style problems.\n\nAnnotated transcript: https://drive.google.com/file/d/1hT7u9eEhd5HyBr-gD1urZGbrdP3Zu3T2/view?usp=sharing\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>To study for the final, I thought it would be interesting to see if ChatGPT can be used to created practice problems that are similar to homework problems, and solidify my understanding of the topics covered in homework. As an example, I gave ChatGPT homework 1, which has problems relating to optimizers, and asked it to generate two exam problems, each with multiple parts, relating to opitmizers based on the given homework problems.<break/><break/>Overall, I thought it did well and actually provided some good problems for studying,. The difficulty of most of the problems it gave were around the same difficulty as the homework, with the exception of around 2 subquestions that required mathematical knowledge that's probably out of scope for this class. I also asked it to generate solutions for these problems, and the solutions were pretty good at explaining the answers in detail. As a whole, I found this to be really helpful in helping me review the material and get more practice solving exam/homework style problems.<break/><break/>Annotated transcript: https://drive.google.com/file/d/1hT7u9eEhd5HyBr-gD1urZGbrdP3Zu3T2/view?usp=sharing</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-01T12:36:50.10738+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7380393,
            "author": "Andrew Choy",
            "project_title": "Special Participation B: HW 11 Interactive Coding with Kimi K2",
            "post_body": "Kimi k2 came out recently and I have to agree with the sentiment in #337. When this model dropped, it looked like it was performing really well on the LM-Arena leaderboards (4th on math), so I decided to use it to tackle the coding parts of this assignment.\n\nFirst off, the workflow is a bit of a friction point, it doesn\u2019t accept .ipynb files as input, so I had to convert everything into a standard Python script to start.\n\nA very stand out issue is the reasoning capability. For example, when implementing the single_attention_head, it made a basic linear algebra error (forgetting to transpose the weight matrix). When I pointed out the assertion error, instead of checking its math, it hallucinated a completely wrong justification about \"causal masking\" and tried to tell me to mask the diagonal. It didn't actually find the root cause until I literally pasted the correct solution code.\n\nIt struggled with the Induction Heads too. It kept writing to the same dimensions in the residual stream (breaking orthogonality) rather than using fresh registers. It eventually gave a cool explanation about \"scratch pads vs. registers,\" but only after I spoon-fed it the answer.\n\nIt is very fast at answering, which is definitely a tradeoff, but speed isn't a valid excuse for that lack of deep understanding. Overall, I\u2019m not very confident in its ability to reason through complex coding problems compared to other models i have used.",
            "content_xml": "<document version=\"2.0\"><paragraph>Kimi k2 came out recently and I have to agree with the sentiment in #337. When this model dropped, it looked like it was performing really well on the LM-Arena leaderboards (4th on math), so I decided to use it to tackle the coding parts of this assignment.</paragraph><paragraph>First off, the workflow is a bit of a friction point, it doesn\u2019t accept <code>.ipynb</code> files as input, so I had to convert everything into a standard Python script to start.</paragraph><paragraph>A very stand out issue is the reasoning capability. For example, when implementing the <code>single_attention_head</code>, it made a basic linear algebra error (forgetting to transpose the weight matrix). When I pointed out the assertion error, instead of checking its math, it hallucinated a completely wrong justification about \"causal masking\" and tried to tell me to mask the diagonal. It didn't actually find the root cause until I literally pasted the correct solution code.</paragraph><paragraph>It struggled with the Induction Heads too. It kept writing to the same dimensions in the residual stream (breaking orthogonality) rather than using fresh registers. It eventually gave a cool explanation about \"scratch pads vs. registers,\" but only after I spoon-fed it the answer.</paragraph><paragraph>It is very fast at answering, which is definitely a tradeoff, but speed isn't a valid excuse for that lack of deep understanding. Overall, I\u2019m not very confident in its ability to reason through complex coding problems compared to other models i have used.</paragraph><file url=\"https://static.us.edusercontent.com/files/c7EeiYByl3xocGmND7lUVQfY\" filename=\"CS182_Special_Participation_B (1).pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-12-01T12:32:54.215306+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7377516,
            "author": "Hanyang Gu",
            "project_title": "Special Participation A: Kimi K2 (Thinking) on HW1",
            "post_body": "Model Used: Kimi k2\n\nAssignment: Homework 1 (Non-coding theoretical problems)\n\nOverview\n\nI utilized Kimi k2 to solve the theoretical portions of Homework 1, covering topics from gradient descent stability and momentum dynamics to stochastic differential equations and high-dimensional geometry. The model demonstrated exceptional performance, acting as a highly competent graduate-level assistant. It successfully derived complex proofs and stability conditions that matched the instructor's ground truth almost perfectly.\n\nThe prompt I used is pretty simple, such as \"Solve Q1, show me all your process that leads to your answer\".\n\nPerformance Metrics & Observations\n\nOne-Shot Accuracy: High (>90%).\n\nThe model required very little \"hand-holding.\" For complex multi-part questions (like Question 2 on SVD coordinates and Question 3 on Momentum eigenvalues), Kimi k2 maintained context across subsections without needing to be reminded of previous definitions. It correctly identified the underlying mathematical structures (e.g., using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement.\n\nMathematical Rigor & Reasoning:\n\nUnlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations. E.g.\n\nMatrix Calculus: It correctly handled vector derivatives in Question 6 (Tikhonov Regularization) and Question 7 (MAP interpretation), distinguishing properly between scalar and vector layouts.\n\nStability Analysis: In Question 3, it correctly applied the stability criterion (checking if roots lie within the unit circle) to the characteristic equation, a non-trivial task for standard language models.\n\nHallucinations/Misconceptions: Little Observed.\n\nThe model did not invent theorems or misuse standard notation. In instances where the problem allowed for multiple interpretations (e.g., the specific formulation of Adam in Question 4), it defaulted to the standard textbook definitions that aligned with the course material.\n\nDownside: Kimi K2 generally have a thinking time of over 1 minute on each question. This is outperformed by other models, such as GPT5 or Gemini 3. However, this thinking time provides great ultimate performance that renders it fruitful. \n\nConclusion\n\nKimi k2 proved to be an incredibly effective tool for theoretical deep learning. It did not merely \"retrieve\" answers but seemingly \"reasoned\" through the linear algebra and probability theory required for this assignment. Its ability to link algebraic manipulations (like rotating the basis Vtop) to geometric intuitions makes it a powerful study aid for understanding the \"why\" behind deep learning dynamics.",
            "content_xml": "<document version=\"2.0\"><paragraph>Model Used: Kimi k2</paragraph><paragraph>Assignment: Homework 1 (Non-coding theoretical problems)</paragraph><heading level=\"3\">Overview</heading><paragraph>I utilized Kimi k2 to solve the theoretical portions of Homework 1, covering topics from gradient descent stability and momentum dynamics to stochastic differential equations and high-dimensional geometry. The model demonstrated exceptional performance, acting as a highly competent graduate-level assistant. It successfully derived complex proofs and stability conditions that matched the instructor's ground truth almost perfectly.</paragraph><paragraph>The prompt I used is pretty simple, such as \"Solve Q1, show me all your process that leads to your answer\".</paragraph><heading level=\"3\">Performance Metrics &amp; Observations</heading><list style=\"unordered\"><list-item><paragraph>One-Shot Accuracy: High (&gt;90%).</paragraph><paragraph>The model required very little \"hand-holding.\" For complex multi-part questions (like Question 2 on SVD coordinates and Question 3 on Momentum eigenvalues), Kimi k2 maintained context across subsections without needing to be reminded of previous definitions. It correctly identified the underlying mathematical structures (e.g., using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement.</paragraph></list-item><list-item><paragraph>Mathematical Rigor &amp; Reasoning:</paragraph><paragraph>Unlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations. E.g.</paragraph><list style=\"unordered\"><list-item><paragraph>Matrix Calculus: It correctly handled vector derivatives in Question 6 (Tikhonov Regularization) and Question 7 (MAP interpretation), distinguishing properly between scalar and vector layouts.</paragraph></list-item><list-item><paragraph>Stability Analysis<bold>:</bold> In Question 3, it correctly applied the stability criterion (checking if roots lie within the unit circle) to the characteristic equation, a non-trivial task for standard language models.</paragraph></list-item></list></list-item><list-item><paragraph>Hallucinations/Misconceptions: Little Observed.</paragraph><paragraph>The model did not invent theorems or misuse standard notation. In instances where the problem allowed for multiple interpretations (e.g., the specific formulation of Adam in Question 4), it defaulted to the standard textbook definitions that aligned with the course material.</paragraph></list-item><list-item><paragraph>Downside: Kimi K2 generally have a thinking time of over 1 minute on each question. This is outperformed by other models, such as GPT5 or Gemini 3. However, this thinking time provides great ultimate performance that renders it fruitful. </paragraph></list-item></list><heading level=\"3\">Conclusion</heading><paragraph>Kimi k2 proved to be an incredibly effective tool for theoretical deep learning. It did not merely \"retrieve\" answers but seemingly \"reasoned\" through the linear algebra and probability theory required for this assignment. Its ability to link algebraic manipulations (like rotating the basis Vtop) to geometric intuitions makes it a powerful study aid for understanding the \"why\" behind deep learning dynamics.</paragraph><file url=\"https://static.us.edusercontent.com/files/WdygFzuQ3WGC5VpclIJwQzz9\" filename=\"EECS_182_Special_Participations_KimiK2_HW1.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T18:25:52.153833+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7377452,
            "author": "Aaryan Chandna",
            "project_title": "Special Participation E: Coding Fill-in-the-Blank for Optimizers",
            "post_body": "I recall from CS courses that I have taken such as the 61 series how many questions used to consist of filling in blanks with code, particularly on exams and in discussions. I felt that these questions were relatively strong in developing intuition at the time, so I wanted to try a similar format out with Google Gemini in order to see if I could strengthen my understanding of the different optimizers. \n\nI asked Gemini to first summarize lectures 3/4 for me from the slides to provide a bit of review. The summaries were decent, even though the intuition provided wasn't always strong: sometimes the model would just quickly state a concept without explaining. Then, I asked it to create continuously increasing questions in difficulty, involving filling in the blank with code. One significant issue was that the model kept accidentally leaking the solution in the comments of the code. Even towards the end, it did this in a somewhat implicit fashion: below the function, there was a small section with an example of running the code in comments, with formulas shown that had the correct blank fill-ins. In addition, Gemini's questions were very one-dimensional and had to do solely with the basic functionality of optimizers, such as an SGD step or an Adam step. I expected that when I asked for questions of higher difficulty, there would be something more intuition based. Towards the end, the last question was a bit different, which was nice. Perhaps there is a better way of prompting to get the model to devise more exam-style questions for prep.\n\n\n\nTrace: https://gemini.google.com/share/eb8917989837",
            "content_xml": "<document version=\"2.0\"><paragraph>I recall from CS courses that I have taken such as the 61 series how many questions used to consist of filling in blanks with code, particularly on exams and in discussions. I felt that these questions were relatively strong in developing intuition at the time, so I wanted to try a similar format out with Google Gemini in order to see if I could strengthen my understanding of the different optimizers. </paragraph><paragraph>I asked Gemini to first summarize lectures 3/4 for me from the slides to provide a bit of review. The summaries were decent, even though the intuition provided wasn't always strong: sometimes the model would just quickly state a concept without explaining. Then, I asked it to create continuously increasing questions in difficulty, involving filling in the blank with code. One significant issue was that the model kept accidentally leaking the solution in the comments of the code. Even towards the end, it did this in a somewhat implicit fashion: below the function, there was a small section with an example of running the code in comments, with formulas shown that had the correct blank fill-ins. In addition, Gemini's questions were very one-dimensional and had to do solely with the basic functionality of optimizers, such as an SGD step or an Adam step. I expected that when I asked for questions of higher difficulty, there would be something more intuition based. Towards the end, the last question was a bit different, which was nice. Perhaps there is a better way of prompting to get the model to devise more exam-style questions for prep.</paragraph><paragraph/><paragraph>Trace: https://gemini.google.com/share/eb8917989837</paragraph><file url=\"https://static.us.edusercontent.com/files/nuwfPT2yMJx70pjs4KH3MbaL\" filename=\"spec_part_e_2_aaryan.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T17:45:50.986379+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7377431,
            "author": "Alex Cao",
            "project_title": "Special Participation A: Deepseek on HW9",
            "post_body": "\n\nIntro:\n\nThis is an attempt to interact with deepseek on non-coding parts of homework 9. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM\u2019s capability of solving real life reasoning/math related problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek\u2019s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. \n\n\n\nConversation traces:\n\nhttps://chat.deepseek.com/share/jkzqnyn7j8say9v7jc\nhttps://chat.deepseek.com/share/838vauzbwa2g0ynfby\nhttps://chat.deepseek.com/share/pp0exea4mnmt36qfqu\n\n\n\nReport with fully annotated traces:\n\n\nSummary:\n\nTo sum up, the model (DeepSeek-V3.2) is a quite strong model and is able to solve most of the non coding questions correctly (one-shot). For this specific problem set, two things matter the most for accuracy: 1. Model\u2019s reasoning ability and 2. Correct prompt and context. Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers. For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill. The model\u2019s reasoning ability is also very important : for the previous example, even if the correct context and prompt is provided, the model still was not able to identify the correct blanks to fill without the DeepThink feature on. Together, these observations suggest that while DeepSeek-V3.2 is already quite capable on non-coding questions, its performance is highly sensitive to both how we phrase the task and model\u2019s reasoning capabilities. Thus, carefully designing prompts and systematically leveraging features like DeepThink will be essential for reliably getting correct answers and understanding the model's core strengths and weaknesses.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><heading level=\"3\">Intro:</heading><paragraph>This is an attempt to interact with deepseek on non-coding parts of homework 9. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM\u2019s capability of solving real life reasoning/math related problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek\u2019s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. <break/><break/></paragraph><paragraph>Conversation traces:</paragraph><paragraph><link href=\"https://chat.deepseek.com/share/jkzqnyn7j8say9v7jc\"><underline>https://chat.deepseek.com/share/jkzqnyn7j8say9v7jc</underline><break/></link><link href=\"https://chat.deepseek.com/share/838vauzbwa2g0ynfby\"><underline>https://chat.deepseek.com/share/838vauzbwa2g0ynfby</underline><break/></link><link href=\"https://chat.deepseek.com/share/pp0exea4mnmt36qfqu\"><underline>https://chat.deepseek.com/share/pp0exea4mnmt36qfqu</underline></link><break/><break/><break/><break/>Report with fully annotated traces:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/46rP2jY0XqQTfFngOrJ3EJyA\" filename=\"Special Pariticpation A-- DeepSeek on HW9.pdf\"/><file/><heading level=\"3\">Summary:</heading><paragraph>To sum up, the model (DeepSeek-V3.2) is a quite strong model and is able to solve most of the non coding questions correctly (one-shot). For this specific problem set, two things matter the most for accuracy: 1. Model\u2019s reasoning ability and 2. Correct prompt and context. Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers. For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill. The model\u2019s reasoning ability is also very important : for the previous example, even if the correct context and prompt is provided, the model still was not able to identify the correct blanks to fill without the DeepThink feature on. Together, these observations suggest that while DeepSeek-V3.2 is already quite capable on non-coding questions, its performance is highly sensitive to both how we phrase the task and model\u2019s reasoning capabilities. Thus, carefully designing prompts and systematically leveraging features like DeepThink will be essential for reliably getting correct answers and understanding the model's core strengths and weaknesses.<break/><break/></paragraph></document>",
            "links": [
                "https://chat.deepseek.com/share/jkzqnyn7j8say9v7jc",
                "https://chat.deepseek.com/share/838vauzbwa2g0ynfby",
                "https://chat.deepseek.com/share/pp0exea4mnmt36qfqu"
            ],
            "attachments": [],
            "created_at": "2025-11-30T17:33:46.197807+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7377093,
            "author": "Ender Ji",
            "project_title": "Special Participation E: GPT5 as quiz maker for exam preparation",
            "post_body": "\n\nFor special participation part E, I asked GPT 5.1 to make quiz problems (both MCQ and FRQ) based on the lecture notes of lecture 18  and 19 from Professor Ranade. I begin by clearly stating GPT\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them.\n\nThen I first ask GPT to generate multiple-choice questions and attempt to answer them. GPT produces high-quality MCQs that effectively test my understanding, and it can both identify my mistakes and provide the correct answers with detailed explanations.\n\nBeyond MCQs, I also use GPT to create more open-ended free-response questions. GPT is able to formulate the problems, \u201cgrade\u201d my answers against the \"ground truth\" from the lecture notes, and offer thorough explanations and guidance whenever I get stuck.\n\nThis is the first time I have used an LLM for this kind of review, and the experience has been very positive. I believe this interactive \u201cquiz-taking\u201d approach will be extremely helpful not only for preparing for the final exam, but also for ensuring that I fully understand the material before each subsequent lecture. I also hope that this method will be helpful for other students during RRR week.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/vHKi330uVk2qWo0QcgvNT3VX\" filename=\"test_reviewer_GPT5.pdf\"/><paragraph>For special participation part E, I asked GPT 5.1 to make quiz problems (both MCQ and FRQ) based on the lecture notes of lecture 18  and 19 from Professor Ranade. I begin by clearly stating GPT\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them.</paragraph><paragraph>Then I first ask GPT to generate multiple-choice questions and attempt to answer them. GPT produces high-quality MCQs that effectively test my understanding, and it can both identify my mistakes and provide the correct answers with detailed explanations.</paragraph><paragraph>Beyond MCQs, I also use GPT to create more open-ended free-response questions. GPT is able to formulate the problems, \u201cgrade\u201d my answers against the \"ground truth\" from the lecture notes, and offer thorough explanations and guidance whenever I get stuck.</paragraph><paragraph>This is the first time I have used an LLM for this kind of review, and the experience has been very positive. I believe this interactive \u201cquiz-taking\u201d approach will be extremely helpful not only for preparing for the final exam, but also for ensuring that I fully understand the material before each subsequent lecture. I also hope that this method will be helpful for other students during RRR week.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T15:08:56.618821+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7376439,
            "author": "Wesley Kai Zheng",
            "project_title": "Special Participation C: Homework 8 Task 2 Refactor Notebook for GPU",
            "post_body": "From my perspective, the original SSM/RNN convolution assignment was conceptually interesting and almost developed. Much of the provided code contained missing docstrings, duplicated function names, and unclear separation between benchmarking logic and visualization. As a result, the assignment required students to write very little actual code, and several implementation issues could confuse learners rather than support the intended learning goals. To address this, I refactored the notebook, improved the organization, clarified the API boundaries, and added more robust scaffolding. Some major references for good Python and ML engineering practices that guided the refactoring (and are cited in the full report) include:\n\nPEP 8 \u2013 Style Guide for Python Code\nPEP 257 \u2013 Docstring Conventions\nPEP 484 \u2013 Type Hints\nPEP 585 \u2013 Type Hinting Generics in Standard Collections\nGoogle Python Style Guide \u2014 Functions and Side Effects\n\nHere's the report and the link to the colab notebook: https://colab.research.google.com/drive/1s06sEeMPooGlg0syKnoly7g3DZ4aR4-P?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>From my perspective, the original SSM/RNN convolution assignment was conceptually interesting and almost developed. Much of the provided code contained missing docstrings, duplicated function names, and unclear separation between benchmarking logic and visualization. As a result, the assignment required students to write very little actual code, and several implementation issues could confuse learners rather than support the intended learning goals. To address this, I refactored the notebook, improved the organization, clarified the API boundaries, and added more robust scaffolding. Some major references for good Python and ML engineering practices that guided the refactoring (and are cited in the full report) include:</paragraph><paragraph><link href=\"https://peps.python.org/pep-0008/\">PEP 8 \u2013 Style Guide for Python Code</link><break/><link href=\"https://peps.python.org/pep-0257/\">PEP 257 \u2013 Docstring Conventions</link><break/><link href=\"https://peps.python.org/pep-0484/\">PEP 484 \u2013 Type Hints</link><break/><link href=\"https://peps.python.org/pep-0585/\">PEP 585 \u2013 Type Hinting Generics in Standard Collections</link><break/><link href=\"https://google.github.io/styleguide/pyguide.html\">Google Python Style Guide \u2014 Functions and Side Effects</link></paragraph><paragraph>Here's the report and the link to the colab notebook: https://colab.research.google.com/drive/1s06sEeMPooGlg0syKnoly7g3DZ4aR4-P?usp=sharing</paragraph><file url=\"https://static.us.edusercontent.com/files/fX2MYqaO7DH3gSofF9QfQCPO\" filename=\"Copy of 182report.pdf\"/></document>",
            "links": [
                "https://peps.python.org/pep-0008/",
                "https://peps.python.org/pep-0257/",
                "https://peps.python.org/pep-0484/",
                "https://peps.python.org/pep-0585/",
                "https://google.github.io/styleguide/pyguide.html"
            ],
            "attachments": [],
            "created_at": "2025-11-30T10:56:26.087892+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7376375,
            "author": "Iana Lin",
            "project_title": "Special Participation E: ChatGPT 5.0 Study Mode",
            "post_body": "Executive Summary\n\nI used ChatGPT 5.0 \"Study Mode\" to interactively improve my understanding of positional encoding, RoPE, and NoPE.  This was the first time I've interacted with ChatGPT 5.0's \"Study Mode,\" and I felt it was very helpful. It asked clarifying questions about my current understanding before proceeding to explain, which differed from normal prompting mode.\n\n\nAlthough there is a less clear idea of \"one-shotting\" in this context where there are not verifiable correct answers, every question it asked was answer reasonably with logical and step-by-step explanations. I do wish that the explanation were more grounded in mathematical formulations or code, but this may come down to learn the write prompting style.\n \n\nErrors:\nChatGPT used \u201ci\u201d for indexing position, but this later became confusing because it also used i for imaginary number. After pointing this out, it no longer made the mistake\n\n\nOverall:\nChatGPT was a good resource for improving understanding, much like if I was chatting to a teaching assistant, and interacted with clarifying questions throughout the converstation. The analogy to Fourier transform across positions helped clarify how using this superposition allows for capturing of both high and low frequencies/unique embedding for each input embedding and position.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>I used ChatGPT 5.0 \"Study Mode\" to interactively improve my understanding of positional encoding, RoPE, and NoPE.  This was the first time I've interacted with ChatGPT 5.0's \"Study Mode,\" and I felt it was very helpful. It asked clarifying questions about my current understanding before proceeding to explain, which differed from normal prompting mode.<break/></paragraph><paragraph>Although there is a less clear idea of \"one-shotting\" in this context where there are not verifiable correct answers, every question it asked was answer reasonably with logical and step-by-step explanations. I do wish that the explanation were more grounded in mathematical formulations or code, but this may come down to learn the write prompting style.<break/> </paragraph><paragraph><bold>Errors</bold>:<break/>ChatGPT used \u201ci\u201d for indexing position, but this later became confusing because it also used i for imaginary number. After pointing this out, it no longer made the mistake</paragraph><paragraph><break/><bold>Overall</bold>:<break/>ChatGPT was a good resource for improving understanding, much like if I was chatting to a teaching assistant, and interacted with clarifying questions throughout the converstation. The analogy to Fourier transform across positions helped clarify how using this superposition allows for capturing of both high and low frequencies/unique embedding for each input embedding and position.</paragraph><file url=\"https://static.us.edusercontent.com/files/rUKMMRgfpcftT4caUCaGgmVi\" filename=\"Special Participation E - RoPE Using ChatGPT Study Mode.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T10:33:28.34829+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7376346,
            "author": "Wesley Kai Zheng",
            "project_title": "Special Participation C: Homework 4 Task 5 Refactor",
            "post_body": "From my perspective, this coding assignment was far too short and required students to fill in only one or two lines of code to complete it. Given the simplicity of the kernel that needed to be implemented, the assignment felt insufficiently challenging. Therefore, we decided to step in, refactor the assignment, and add a new problem as well. Some major references for good Python practices that I used during the refactoring process (also included in the report) are:\n\nPEP 257 Docstring Conventions\n\nPEP 484 Type Hints\n\nPEP 20 (Zen of Python)\n\nGoogle Python Style Guide (Functions and Side Effects)\n\nPEP 585 \u2013 Type Hinting Generics In Standard Collections\n\nIn summary, the assignment is now slightly longer while still maintaining its original purpose and allowing students to experiment with the Sobel filter. In the future, this assignment could be extended by adding the Canny Edge Detector (which, coincidentally, was developed by Professor Canny!).\n\nThe report is located here:\n\nThe ipynb notebook is here:\n\nhttps://colab.research.google.com/drive/1sRW_V5RSs1EAkMIGhfZebNEqGrzpViGt?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>From my perspective, this coding assignment was far too short and required students to fill in only one or two lines of code to complete it. Given the simplicity of the kernel that needed to be implemented, the assignment felt insufficiently challenging. Therefore, we decided to step in, refactor the assignment, and add a new problem as well. Some major references for good Python practices that I used during the refactoring process (also included in the report) are:</paragraph><paragraph><bold><link href=\"https://peps.python.org/pep-0257/\"><underline>PEP 257 Docstring Conventions</underline></link></bold></paragraph><paragraph><bold><link href=\"https://peps.python.org/pep-0484/\"><underline>PEP 484 Type Hints</underline></link></bold></paragraph><paragraph><bold><link href=\"https://peps.python.org/pep-0020/\"><underline>PEP 20 (Zen of Python)</underline></link></bold></paragraph><paragraph><bold><link href=\"https://google.github.io/styleguide/pyguide.html\"><underline>Google Python Style Guide (Functions and Side Effects)</underline></link></bold></paragraph><paragraph><bold><link href=\"https://peps.python.org/pep-0585/\"><underline>PEP 585 \u2013 Type Hinting Generics In Standard Collections</underline></link></bold></paragraph><paragraph>In summary, the assignment is now slightly longer while still maintaining its original purpose and allowing students to experiment with the Sobel filter. In the future, this assignment could be extended by adding the Canny Edge Detector (which, coincidentally, was developed by Professor Canny!).</paragraph><paragraph>The report is located here:</paragraph><file url=\"https://static.us.edusercontent.com/files/NGzRRFqbHSjvp33OkOnxVTvk\" filename=\"182report.pdf\"/><paragraph>The ipynb notebook is here:</paragraph><paragraph>https://colab.research.google.com/drive/1sRW_V5RSs1EAkMIGhfZebNEqGrzpViGt?usp=sharing</paragraph></document>",
            "links": [
                "https://peps.python.org/pep-0257/",
                "https://peps.python.org/pep-0484/",
                "https://peps.python.org/pep-0020/",
                "https://google.github.io/styleguide/pyguide.html",
                "https://peps.python.org/pep-0585/"
            ],
            "attachments": [],
            "created_at": "2025-11-30T10:21:46.430584+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7376323,
            "author": "Ken Zheng",
            "project_title": "Special Participation C: Refactoring Q5 on HW4",
            "post_body": "I refactored question 5 from homework 4 into a well-organized GitHub repository, with the aim of both adhering to python & NumPy code organization/best practices and enhancing the student learning experience. Through this restructuring I also aimed to elevate the question's educational value by adding on a third part to the question: create a sharpening filter. Please refer to the report for more details!\n\nRepo: https://github.com/kenzhengjk/hand_design_filters\n\nReport: \n\nSimply remove the solutions directory and the assignment will be ready!",
            "content_xml": "<document version=\"2.0\"><paragraph>I refactored question 5 from homework 4 into a well-organized GitHub repository, with the aim of both adhering to python &amp; NumPy code organization/best practices and enhancing the student learning experience. Through this restructuring I also aimed to elevate the question's educational value by adding on a third part to the question: create a sharpening filter. Please refer to the report for more details!</paragraph><paragraph><bold>Repo</bold>: <link href=\"https://github.com/kenzhengjk/hand_design_filters\">https://github.com/kenzhengjk/hand_design_filters</link></paragraph><paragraph><bold>Report</bold>: </paragraph><file url=\"https://static.us.edusercontent.com/files/SjXOXdG7H9wbNzFZQdPAKHze\" filename=\"Special_Part_C_Q5_HW4_REPORT.pdf\"/><paragraph>Simply remove the <code>solutions</code> directory and the assignment will be ready!</paragraph></document>",
            "links": [
                "https://github.com/kenzhengjk/hand_design_filters"
            ],
            "attachments": [],
            "created_at": "2025-11-30T10:15:19.016308+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7376321,
            "author": "Ender Ji",
            "project_title": "Special Participation E: GPT5 as lecture notes reviewer and tutor",
            "post_body": "For special participation part E, I asked GPT 5.1 to review the lecture notes of lecture 18 from Professor Ranade. I begin by clearly stating GPT\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them. Then I proceed to asking different kinds of questions about the notes\n\n1. input a hand drawing diagram from Professor Ranade and ask it to explain to diagram\n\n2. input a diagram from the book ask it to explain to diagram\n\n3. ask GPT to explain a specific concept\n\n4. ask GPT to differentiate between different concepts\n\n5. ask GPT concepts that are not deeply discussed in the lecture notes, and make connection to the lecture notes\n\nIt is the first time I do notes review with GPT, surprisingly GPT 5.1 can handle almost all kinds of questions well and able to give great explanations, which shows how strong GPT is in helping students learning new concepts.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/K4hJ3j4bSbawhhtQozKvoiPU\" filename=\"lecture18_review_GPT5.pdf\"/><paragraph>For special participation part E, I asked GPT 5.1 to review the lecture notes of lecture 18 from Professor Ranade. I begin by clearly stating GPT\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them. Then I proceed to asking different kinds of questions about the notes</paragraph><paragraph>1. input a hand drawing diagram from Professor Ranade and ask it to explain to diagram</paragraph><paragraph>2. input a diagram from the book ask it to explain to diagram</paragraph><paragraph>3. ask GPT to explain a specific concept</paragraph><paragraph>4. ask GPT to differentiate between different concepts</paragraph><paragraph>5. ask GPT concepts that are not deeply discussed in the lecture notes, and make connection to the lecture notes</paragraph><paragraph>It is the first time I do notes review with GPT, surprisingly GPT 5.1 can handle almost all kinds of questions well and able to give great explanations, which shows how strong GPT is in helping students learning new concepts.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-30T10:14:46.664082+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7375514,
            "author": "Divya Ramesh",
            "project_title": "Special Participation A: Gemini (Fast) on HW 9",
            "post_body": "Executive Summary:\n\nI used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that were very structured math or required analyzing code. I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error. To solve these, when I clarified the misreads, Gemini quickly fixed its solution. Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions. I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer. This was confusing, but it ultimately did come up with the correct solution. I was also impressed with how quickly it was able to come up with the solutions, taking no longer than 10 seconds at the max. This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up. \n\nI also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic. It helped me understand how different concepts were related, and it helped me understand derivations for formulas that I previously didn't know. I think overall, it helped me with my understanding of course concepts a lot. \n\nOne thing I did find a little annoying was Gemini wouldn't let me paste the code I copied from the hw9 pdf file: I needed to paste the question in a different document, and then copy that over to the Gemini search bar. I think maybe Gemini wasn't used to the LaTeX pdf format, and didn't allow those characters. This did make it a little harder to do, but it was just a mild inconvenience!\n\nHere is a link to my annotated logs: https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Executive Summary:</paragraph><paragraph>I used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that were very structured math or required analyzing code. I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error. To solve these, when I clarified the misreads, Gemini quickly fixed its solution. Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions. I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer. This was confusing, but it ultimately did come up with the correct solution. I was also impressed with how quickly it was able to come up with the solutions, taking no longer than 10 seconds at the max. This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up. </paragraph><paragraph>I also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic. It helped me understand how different concepts were related, and it helped me understand derivations for formulas that I previously didn't know. I think overall, it helped me with my understanding of course concepts a lot. </paragraph><paragraph>One thing I did find a little annoying was Gemini wouldn't let me paste the code I copied from the hw9 pdf file: I needed to paste the question in a different document, and then copy that over to the Gemini search bar. I think maybe Gemini wasn't used to the LaTeX pdf format, and didn't allow those characters. This did make it a little harder to do, but it was just a mild inconvenience!</paragraph><paragraph>Here is a link to my annotated logs: <link href=\"https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing\">https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing</link></paragraph><paragraph/></document>",
            "links": [
                "https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-11-30T06:29:30.10322+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7374682,
            "author": "Andrew Choy",
            "project_title": "Special Participation E: Interactive MHA Implementation Drill",
            "post_body": "One challenge I often face is bridging the gap between understanding a concept theoretically and implementing it in code. For this assignment, I used Gemini 3 Thinking to guide me through the implementation.\n\nIt felt a bit like a 'Masked Language Modeling' task for my own brain: I designed a prompt to set up the scenario, and I had to fill in the code for the forward pass of Multi-Head Attention myself. I spent about an hour on the exercise, and I found it incredibly valuable. Instead of just giving me the answers, the LLM asked me conceptual questions about why I was choosing specific tensor shapes. Being forced to justify my implementation details really solidified my understanding of the topic.\n\nI encourage y'all to try this with the prompt I included in the pdf. You can easily adapt it for other concepts by slightly changing the formatting in the 'Your Role' and 'The Trap' sections. Just swap out 'Multi-Head Attention' for another topic like 'Layer Normalization' or 'Adam Optimizer,' and keep the 'No Solutions' constraint to ensure the same type of setup.\n\n ",
            "content_xml": "<document version=\"2.0\"><paragraph>One challenge I often face is bridging the gap between understanding a concept theoretically and implementing it in code. For this assignment, I used <bold>Gemini 3 Thinking</bold> to guide me through the implementation.</paragraph><paragraph>It felt a bit like a 'Masked Language Modeling' task for my own brain: I designed a prompt to set up the scenario, and I had to fill in the code for the forward pass of Multi-Head Attention myself. I spent about an hour on the exercise, and I found it incredibly valuable. Instead of just giving me the answers, the LLM asked me conceptual questions about <italic>why</italic> I was choosing specific tensor shapes. Being forced to justify my implementation details really solidified my understanding of the topic.</paragraph><paragraph>I encourage y'all to try this with the prompt I included in the pdf. You can easily adapt it for other concepts by slightly changing the formatting in the <bold>'Your Role'</bold> and <bold>'The Trap'</bold> sections. Just swap out 'Multi-Head Attention' for another topic like 'Layer Normalization' or 'Adam Optimizer,' and keep the <bold>'No Solutions'</bold> constraint to ensure the same type of setup.</paragraph><paragraph> </paragraph><file url=\"https://static.us.edusercontent.com/files/CcMT6C3JxLbClmmbjNJnMBAw\" filename=\"AndrewChoyParticipationE.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T19:26:19.700045+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7374626,
            "author": "Angelina Zhang",
            "project_title": "Participation E: Conv Layer Intuition Tutor",
            "post_body": "When learning convolution layers, I often struggled to remember formulas or effects as the lectures progressed. I found that true understanding only came from visualizing what was happening and actively quizzing myself. So I built a small interactive webpage called Conv Layer Intuition Tutor, designed to help students build real intuition for convolutional layers by letting you manipulate the hyperparameters and see visualizations directly. The interface allows you to adjust kernel size, stride, padding, and it instantly visualizes how these changes affect output size, receptive field growth, parameter count, and which input pixels contribute to each output activation. I also embedded an optional AI assistant (using your own API key) that can quiz you or explain concepts for post-lecture review.\n\nYou can visit the site here: https://angelinaaaaaaaaaaaa.github.io/Conv-Layer-Intuition-Tutor/\n\nTo run the tool locally, simply download the HTML file and either double-click it or open a terminal in that folder and start a local server (e.g., python3 -m http.server 8000) and then visit the shown URL. This removes the need to constantly switch between PDFs and LLMs, and provides a hands-on, highly interactive way to internalize convolution mechanics \u2014 with AI guidance whenever you need it.\n\nHappy to hear any feedback or suggestions!\n",
            "content_xml": "<document version=\"2.0\"><paragraph>When learning convolution layers, I often struggled to remember formulas or effects as the lectures progressed. I found that true understanding only came from visualizing what was happening and actively quizzing myself. So I built a small interactive webpage called Conv Layer Intuition Tutor, designed to help students build real intuition for convolutional layers by letting you manipulate the hyperparameters and see visualizations directly. The interface allows you to adjust kernel size, stride, padding, and it instantly visualizes how these changes affect output size, receptive field growth, parameter count, and which input pixels contribute to each output activation. I also embedded an optional AI assistant (using your own API key) that can quiz you or explain concepts for post-lecture review.<break/><break/>You can visit the site here: https://angelinaaaaaaaaaaaa.github.io/Conv-Layer-Intuition-Tutor/</paragraph><paragraph>To run the tool locally, simply download the HTML file and either double-click it or open a terminal in that folder and start a local server (e.g., python3 -m http.server 8000) and then visit the shown URL. This removes the need to constantly switch between PDFs and LLMs, and provides a hands-on, highly interactive way to internalize convolution mechanics \u2014 with AI guidance whenever you need it.<break/><break/>Happy to hear any feedback or suggestions!<break/></paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T18:19:37.931881+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7374084,
            "author": "Ken Zheng",
            "project_title": "Special Participation B: Seed1.6 (ByteDance) on HW3",
            "post_body": "I completed the coding parts of Homework 3 using Doubao Seed1.6 (from ByteDance).\n\nMotivation\n\nDoubao Seed1.6 doesn't support .ipynb file uploads so I manually copy-pasted each part as text input. I tried to limit the extent of prompt optimizations in order to explore to what extend Seed is able to first-try questions without any additional support.\n\nSummary\n\nOverall Seed is a capable coding model that is adaptive and flexible. Seed mostly zero-shots every part, producing accurate, clean, and commented code in a well-structured manner. Nevertheless, when its first attempt is erroneous and the user follows up by asking for modifications on top of the previous code, it has a tendency to slightly spiral towards chaos (i.e., by adding more code and introducing more variables to deal with the changes instead of reforming logic to incorporate both previous and new requirements). This rising-entropy approach isn't unique to Seed -- I've witnessed similar behavior in ChatGPT, Qwen, and Claude when I repeatedly ask for incremental but substantial changes to previously generated code. But I digress. \n\nSeed also tends to assume control of the entire code snippet and frequently makes edits outside of TODO blocks. The prompts in the question notebook never explicitly state \"only add to/change what's in the TODO blocks\", which is probably what gives Seed its bravery. As humans, however, we naturally understand we should only modify content within the TODOs. This is easily fixable, however, by just telling Seed to only modify TODO blocks. This works even after a few back and forths and then asking it to condense all the changes it has made into the TODO block of the first version of the code. You can see this in action for part (c).\n\nFor more specific observations please see my annotations below!\n\nFiles & Links\n\nPart A\n\nPart B\n\nParts C-E\n\nAnnotated conversations:\n\nCompleted notebook ran on code from Seed (to show correctness): \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I completed the coding parts of Homework 3 using Doubao Seed1.6 (from ByteDance).</paragraph><paragraph><bold>Motivation</bold></paragraph><paragraph>Doubao Seed1.6 doesn't support <code>.ipynb</code> file uploads so I manually copy-pasted each part as text input. I tried to limit the extent of prompt optimizations in order to explore to what extend Seed is able to first-try questions without any additional support.</paragraph><paragraph><bold>Summary</bold></paragraph><paragraph>Overall Seed is a capable coding model that is adaptive and flexible. Seed mostly zero-shots every part, producing accurate, clean, and commented code in a well-structured manner. Nevertheless, when its first attempt is erroneous and the user follows up by asking for modifications on top of the previous code, it has a tendency to slightly spiral towards chaos (i.e., by adding more code and introducing more variables to deal with the changes instead of reforming logic to incorporate both previous and new requirements). This rising-entropy approach isn't unique to Seed -- I've witnessed similar behavior in ChatGPT, Qwen, and Claude when I repeatedly ask for incremental but substantial changes to previously generated code. But I digress. </paragraph><paragraph>Seed also tends to assume control of the entire code snippet and frequently makes edits outside of TODO blocks. The prompts in the question notebook never explicitly state \"only add to/change what's in the TODO blocks\", which is probably what gives Seed its bravery. As humans, however, we naturally understand we should only modify content within the TODOs. This is easily fixable, however, by just telling Seed to only modify TODO blocks. This works even after a few back and forths and then asking it to condense all the changes it has made into the TODO block of the first version of the code. You can see this in action for part (c).</paragraph><paragraph>For more specific observations please see my annotations below!</paragraph><paragraph><bold>Files &amp; Links</bold></paragraph><paragraph><link href=\"https://www.doubao.com/thread/wbf507d0d24ed75a5\">Part A</link></paragraph><paragraph><link href=\"https://www.doubao.com/thread/w18e6b14ca46355e0\">Part B</link></paragraph><paragraph><link href=\"https://www.doubao.com/thread/w2d83dbe9f15c1b45\">Parts C-E</link></paragraph><paragraph>Annotated conversations:</paragraph><file url=\"https://static.us.edusercontent.com/files/rKMFdDSDK4gqRBTICA41ocVV\" filename=\"HW3_Q2_Annotated.pdf\"/><paragraph>Completed notebook ran on code from Seed (to show correctness): </paragraph><file url=\"https://static.us.edusercontent.com/files/RaCwwAmBGFUayBxZAfOpa9he\" filename=\"q_mup_coding_seedified.ipynb\"/><paragraph/></document>",
            "links": [
                "https://www.doubao.com/thread/wbf507d0d24ed75a5",
                "https://www.doubao.com/thread/w18e6b14ca46355e0",
                "https://www.doubao.com/thread/w2d83dbe9f15c1b45"
            ],
            "attachments": [],
            "created_at": "2025-11-29T12:58:47.575512+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7374016,
            "author": "Jason Trinh",
            "project_title": "Special Participation A: Gemini 2.5 Flash on HW0",
            "post_body": "Hey guys \u2014 I used Gemini Flash 2.5 for the non-coding parts of HW0, and here\u2019s the quick verdict.\n\nTL;DR: Gemini was strong on the \u201cmechanical\u201d math (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its main weakness was qualitative/sign reasoning in the ReLU elbow SGD question, where it overclaimed elbow direction and bungled sign/inequality logic. \n\nWhat it did well: used the 1\u03d5(x)>0\u200b indicator instead of messy piecewise cases, and matched the staff-style update notation w\u2032,b\u2032,e\u2032. \n\n\nWhere it slipped: for case (ii), it said \u201celbow shifts right,\u201d but the elbow can move left or right depending on bias + step size. \n\n\nClear wrong answer: for (iii) it concluded the elbow \u201cmoves left\u201d as the expression gets larger, but the expression is negative so becoming \u201cless negative\u201d means it actually moves right. \n\n\nEven when it got the final direction right: its proof sketch ended with a bogus condition like w>bx.\n\nAfter I nudged it with an \u201calgebra-first + sanity-check\u201d checklist (explicitly compute e\u2032=\u2212b\u2032/w\u2032, be careful when w<0, and verify with a tiny numeric example), Gemini improved a lot. It stopped hand-waving the geometry, caught its own sign mistakes when the numeric check contradicted the algebra, and finally gave consistent conclusions across the cases. The conclusion: Gemini is super useful here, but only if you force it to prove elbow motion by comparing e\u2032 vs e and validating with a quick counterexample.",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey guys \u2014 I used <bold>Gemini Flash 2.5</bold> for the <bold>non-coding parts of HW0</bold>, and here\u2019s the quick verdict.</paragraph><paragraph><bold>TL;DR:</bold> Gemini was <bold>strong on the \u201cmechanical\u201d math</bold> (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its <bold>main weakness was qualitative/sign reasoning</bold> in the <bold>ReLU elbow SGD question</bold>, where it overclaimed elbow direction and bungled sign/inequality logic. </paragraph><list style=\"bullet\"><list-item><paragraph><bold>What it did well:</bold> used the <bold>1</bold><sub>\u03d5(x)&gt;0\u200b</sub> indicator instead of messy piecewise cases, and matched the staff-style update notation w\u2032,b\u2032,e\u2032. <break/></paragraph></list-item><list-item><paragraph><bold>Where it slipped:</bold> for case (ii), it said \u201celbow shifts right,\u201d but the elbow can move <bold>left or right depending on bias + step size</bold>. <break/></paragraph></list-item><list-item><paragraph><bold>Clear wrong answer:</bold> for (iii) it concluded the elbow \u201cmoves left\u201d as the expression gets larger, but the expression is negative so becoming \u201cless negative\u201d means it actually moves <bold>right</bold>. <break/></paragraph></list-item><list-item><paragraph><bold>Even when it got the final direction right:</bold> its proof sketch ended with a bogus condition like w&gt;bx.</paragraph></list-item></list><paragraph>After I nudged it with an \u201calgebra-first + sanity-check\u201d checklist (explicitly compute e\u2032=\u2212b\u2032/w\u2032, be careful when w&lt;0, and verify with a tiny numeric example), Gemini improved a lot. It stopped hand-waving the geometry, caught its own sign mistakes when the numeric check contradicted the algebra, and finally gave consistent conclusions across the cases. The conclusion: Gemini is super useful here, but only if you force it to <italic>prove</italic> elbow motion by comparing e\u2032 vs e and validating with a quick counterexample.</paragraph><file url=\"https://static.us.edusercontent.com/files/fdc6jmSeJRKtVVQUGfC1BCM0\" filename=\"gemini_trace_annotated.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T12:21:31.421792+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7373978,
            "author": "Tiger Zhang",
            "project_title": "Special Participation E: Gemini 3 (Thinking) As A Converter from Homework to Practice Problems",
            "post_body": "Executive Summary:\n\nI like to prepare for exams by doing practice exam problems, and I\u2019m sure many are like me too. While I think homework problems are a great way to actually learn and understand concepts in the class, I think it\u2019s also good to have exam-style practice problems that test at the understanding gained from the homework assignments, and that require a \u201cfaster pace\u201d than do homework problems.\n\nTherefore, I prompted Gemini 3 to write an exam problem set based on a homework problem set, extending the homework problems in interesting and stimulating ways. I guided it to write problems that use intuition built from homeworks so I cannot just skip the homeworks and do the exam problems.\n\nThe result: I now have a simple four-prompt pipeline that simulates the practice test experience on a homework assignment, as it outputs both the practice exam problems and the practice exam solutions. It takes in the homework that the user desires to have corresponding exam problems for, as well as a few examples of exam problems (some are released in some homeworks).\n\nAlthough some output exam problems were quite similar to homework problems, they do change the homework problems slightly, and the rest of the problems are still different from homework problems.\n\nDifficulty: a difficulty I faced when building this pipeline is actually tuning the difficulty of the exam problems. By changing the third prompt (the \u201cwrite the problems\u201d prompt), the difficulty ranged from too difficult (requires memorization of details of the results from the homework) to very easy (simple plug and chug). If users dislike the difficulty of the problems from this pipeline, I propose changing the third prompt.\n\nChat log: \n\n(I boxed my prompts)\n\nHomework 1 exam problems:\n\nHomework 1 exam problems (unannotated):\n\nHomework 1 exam problem solutions:\n\nHomework 1 exam problems solutions (unannotated):",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold>:</paragraph><paragraph>I like to prepare for exams by doing practice exam problems, and I\u2019m sure many are like me too. While I think homework problems are a great way to actually learn and understand concepts in the class, I think it\u2019s also good to have exam-style practice problems that test at the understanding gained from the homework assignments, and that require a \u201cfaster pace\u201d than do homework problems.</paragraph><paragraph>Therefore, I prompted Gemini 3 to write an exam problem set based on a homework problem set, extending the homework problems in interesting and stimulating ways. I guided it to write problems that use intuition built from homeworks so I <bold>cannot just skip the homeworks</bold> and do the exam problems.</paragraph><paragraph>The result: I now have a simple four-prompt pipeline that simulates the practice test experience on a homework assignment, as it outputs both the practice exam problems and the practice exam solutions. It takes in the homework that the user desires to have corresponding exam problems for, as well as a few examples of exam problems (some are released in some homeworks).</paragraph><paragraph>Although some output exam problems were quite similar to homework problems, they do change the homework problems slightly, and the rest of the problems are still different from homework problems.</paragraph><paragraph>Difficulty: a difficulty I faced when building this pipeline is actually tuning the difficulty of the exam problems. By changing the third prompt (the \u201cwrite the problems\u201d prompt), the difficulty ranged from too difficult (requires memorization of details of the results from the homework) to very easy (simple plug and chug). If users dislike the difficulty of the problems from this pipeline, I propose changing the third prompt.</paragraph><paragraph>Chat log: </paragraph><file url=\"https://static.us.edusercontent.com/files/UgyJ3lokWUhEbhzgnOfe3RBU\" filename=\"conversation.pdf\"/><paragraph>(I boxed my prompts)</paragraph><paragraph>Homework 1 exam problems:</paragraph><file url=\"https://static.us.edusercontent.com/files/GdbnlVUgpzi1SUWKocJvjdng\" filename=\"exam.pdf\"/><paragraph>Homework 1 exam problems (unannotated):</paragraph><file url=\"https://static.us.edusercontent.com/files/nn3twYK8NpJ8afEWpu55vtnh\" filename=\"exam_clean.pdf\"/><paragraph>Homework 1 exam problem solutions:</paragraph><file url=\"https://static.us.edusercontent.com/files/cC2bOGSdQdyQM2x4HXsrshSr\" filename=\"sol.pdf\"/><paragraph>Homework 1 exam problems solutions (unannotated):</paragraph><file url=\"https://static.us.edusercontent.com/files/zdsPG0yCIa2DXw7ixEEN88QO\" filename=\"sol_clean.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T12:02:40.20453+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7373861,
            "author": "Shervin Goudarzi",
            "project_title": "Special Participation A: Gemini-Pro 3 on HW9",
            "post_body": "I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/YFGYzcL72ilmBQtfeDK4gb3h\" filename=\"Special_Participation_A.pdf\"/><paragraph>I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-29T11:05:16.255596+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372448,
            "author": "Zesheng Cai",
            "project_title": "Special Participation A: Deepseek on Hw8",
            "post_body": "For HW8, I first provided Deepseek with a set of instructions to encourage step-by-step reasoning and self-verification. For each problem, I supplied both the image of the question and a direct copy-and-paste text version. After observing its behavior throughout the assignment, I summarized several notable characteristics:\n\n1. Its self-checking mechanism was largely ineffective.\n Although Deepseek always performed a \u201cself-examination\u201d step, it rarely identified actual mistakes. Most of the time, it simply reiterated the correctness of its own answer rather than performing a thorough or systematic review of potential oversights.\n\n2. It occasionally misinterprets small details in the prompt, though its overall understanding remains accurate.\n These misunderstandings were usually minor (e.g., subtle assumptions or edge cases), but they indicate that Deepseek may overlook fine-grained nuances in certain problem statements.\n\n3. Its reasoning and explanatory abilities are very strong.\n Deepseek is well-suited for explaining solution steps and clarifying concepts. Since HW8 did not contain many heavy calculations, its computational accuracy appeared reliable in this context.\n\n4. Its conversational and correction capabilities are adequate, but its error-localization ability is weak.\n When its solution was incorrect, Deepseek could fix the issue once I explicitly pointed out the problem. However, it generally struggled to independently locate the exact source of the error without guidance.\n\nOverall, Deepseek performs well as a conceptual explanation tool but still requires human intervention for precise error detection and critical verification. Attached below is a PDF of my conversation with deepseek.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/x8hk84H0BEmHXC8WWhy5n9r2\" filename=\"Special Participation A hw8 deepseek.pdf\"/><paragraph>For HW8, I first provided Deepseek with a set of instructions to encourage step-by-step reasoning and self-verification. For each problem, I supplied both the image of the question and a direct copy-and-paste text version. After observing its behavior throughout the assignment, I summarized several notable characteristics:</paragraph><paragraph><bold>1. Its self-checking mechanism was largely ineffective.</bold><break/> Although Deepseek always performed a \u201cself-examination\u201d step, it rarely identified actual mistakes. Most of the time, it simply reiterated the correctness of its own answer rather than performing a thorough or systematic review of potential oversights.</paragraph><paragraph><bold>2. It occasionally misinterprets small details in the prompt</bold>, though its overall understanding remains accurate.<break/> These misunderstandings were usually minor (e.g., subtle assumptions or edge cases), but they indicate that Deepseek may overlook fine-grained nuances in certain problem statements.</paragraph><paragraph><bold>3. Its reasoning and explanatory abilities are very strong.</bold><break/> Deepseek is well-suited for explaining solution steps and clarifying concepts. Since HW8 did not contain many heavy calculations, its computational accuracy appeared reliable in this context.</paragraph><paragraph><bold>4. Its conversational and correction capabilities are adequate, but its error-localization ability is weak.</bold><break/> When its solution was incorrect, Deepseek could fix the issue once I explicitly pointed out the problem. However, it generally struggled to independently locate the exact source of the error without guidance.</paragraph><paragraph>Overall, Deepseek performs well as a conceptual explanation tool but still requires human intervention for precise error detection and critical verification. Attached below is a PDF of my conversation with deepseek.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-28T21:00:50.226819+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372401,
            "author": "Faiaz Khan",
            "project_title": "Special Participation E: Staff Notebook Deep-Dive & Code Review Tutor using ChatGPT Study Mode",
            "post_body": "What this is:\nWe often get staff solution notebooks that are 100% correct but not necessarily the best examples of Python or ML engineering practices. I made a reusable prompt that turns ChatGPT  into:\n\na tutor that explains the solution code,\n\na Socratic \u201cwhat if we changed this line?\u201d partner, and\n\na code reviewer that critiques style, structure, and engineering practices.\n\nHow to use it:\n\nOpen ChatGPT and enable Study Mode.\n\nStart a new chat and upload the .ipynb with the staff solution.\n\nPaste the big prompt below.\n\nOptionally tell it your comfort level with Python / ML.\n\nLet it walk you through:\n\nan overview of the problem and solution,\n\ninteractive what-if questions about code changes,\n\nand a software-engineering critique plus suggested refactors.\n\nPrompt:\n\nYou are an AI tutor and code reviewer for a Jupyter notebook that contains:\n\n- a coding / ML problem,\n- and the staff-provided solution (and possibly some tests or helper code).\n\nYour goals:\n(1) Help me deeply understand the staff solution.\n(2) Actively quiz me with \u201cwhat-if\u201d code modification questions.\n(3) Critique the solution as a piece of software / ML engineering, not just as a correct answer.\n\nI will upload a .ipynb file. Please work in the following phases.\n\n########################\nPHASE 0 \u2014 ORIENT YOURSELF\n########################\n\nAfter reading the notebook:\n\n1. Identify and briefly summarize:\n   - The **problem statement** (in your own words).\n   - Where the **main staff solution** lives (e.g., which cell / which function).\n   - Any **supporting code** (helpers, tests, imports, plotting, etc.).\n\n2. Output a short overview like:\n\n   - Problem: [...]\n   - Main solution entry point: [...]\n   - Key helper functions: [...]\n   - Tests / demo cells: [...]\n\nIf anything is ambiguous, say so instead of guessing.\n\n########################\nPHASE 1 \u2014 EXPLAIN THE SOLUTION\n########################\n\nAssume I am a student who:\n- knows basic Python,\n- is familiar with lists/dicts/functions/loops,\n- but may *not* know the specific algorithm or ML technique yet.\n\nDo the following:\n\n1. **High-level explanation (3\u20137 sentences)**  \n   - What is the core idea of the solution?\n   - What algorithm / pattern does it implement (e.g., DP, DFS, greedy, training loop, etc.)?\n\n2. **Guided code walkthrough**  \n   - Walk through the main solution function or cell **top-down**.\n   - Group lines into logical chunks instead of line-by-line noise.\n   - For each chunk, explain:\n     - what it does,\n     - why it\u2019s needed,\n     - how it connects to the problem definition.\n\n3. **Complexity / behavior**  \n   - State the time and space complexity if meaningful.\n   - Mention any tradeoffs (e.g., readability vs performance, memory vs speed).\n\nKeep this phase mostly explanatory, with occasional quick checks like:\n> \u201cCan you tell me in your own words what this loop is doing before I explain it?\u201d\n\n########################\nPHASE 2 \u2014 INTERACTIVE \u201cWHAT IF?\u201d QUESTIONS\n########################\n\nNow I want you to test and deepen my understanding by asking questions about code changes.\n\n1. Ask **3\u20136 \u201cwhat if\u201d questions** such as:\n   - \u201cWhat do you think happens if we remove this condition?\u201d\n   - \u201cIf we change this list to a set, how does that affect correctness and complexity?\u201d\n   - \u201cWhat if we change the iteration order here?\u201d\n   - \u201cWhat if we initialize this variable differently?\u201d\n\n2. For each question:\n   - Wait for my answer first.\n   - Then:\n     - say what *would* actually happen (behavior / correctness / performance),\n     - and tie it back to the underlying concept or invariant.\n\n3. Start with simpler changes (e.g., altering a constant or print) and\n   move toward more subtle ones (e.g., off-by-one edges, shared mutable state, ML training details).\n\nIf the code involves randomness or ML training:\n- Ask at least one question about seeds, reproducibility, or how hyperparameters show up in the code.\n\n########################\nPHASE 3 \u2014 SOFTWARE / ML ENGINEERING CRITIQUE\n########################\n\nNow critique the staff solution as if you were a senior software or ML engineer reviewing it.\n\n1. **Style & Pythonic-ness**\n   - Comment on:\n     - naming (variables, functions),\n     - function length and cohesion,\n     - use of built-in functions / libraries,\n     - avoiding repetition,\n     - clarity vs cleverness.\n   - Point out at least 3\u20135 concrete improvements, such as:\n     - \u201cThis nested if could be simplified.\u201d\n     - \u201cThis magic number should be a named constant.\u201d\n     - \u201cThis could use a context manager / list comprehension / enumerate, etc.\u201d\n\n2. **Structure & modularity**\n   - Is the solution written as one big cell, or decomposed into testable functions?\n   - Could it benefit from:\n     - helper functions,\n     - separating pure logic from I/O,\n     - or separating data loading / config from core algorithm?\n\n3. **Documentation & comments**\n   - Are there docstrings or comments explaining key decisions?\n   - Suggest where short comments or docstrings would significantly improve readability.\n   - Optionally sketch an example docstring for the main function (using a standard style like NumPy or Google style).\n\n4. **Testing & edge cases**\n   - Does the notebook include tests or sample runs?\n   - What edge cases might be missing?\n   - Propose **3\u20135 simple test cases** that would be good to add (in plain English or as Python snippets).\n\n5. **ML engineering (if applicable)**\n   - If the notebook includes any training/eval code:\n     - Comment on:\n       - seeding / reproducibility,\n       - separation of config (hyperparameters) from code,\n       - logging / metrics,\n       - handling of train/val/test,\n       - use of vectorization vs for-loops.\n     - Suggest at least a couple of improvements (e.g., \u201cmove these constants to a config dict,\u201d \u201cuse a random seed,\u201d etc.).\n\n6. **Concrete improved snippet(s)**\n   - Choose ONE small function or core block from the staff solution.\n   - Show:\n     - \u201cOriginal version\u201d (copied and possibly shortened)\n     - \u201cImproved version\u201d (your more Pythonic / maintainable rewrite)\n   - Explain in 3\u20135 bullets what changed and why it\u2019s better (clarity, safety, extensibility, etc.).\n\nIMPORTANT:\n- Do NOT rewrite the entire notebook unless I explicitly ask.\n- Focus on *illustrative* improvements that a student can learn from.\n\n########################\nPHASE 4 \u2014 SUMMARY & ACTIONABLE NEXT STEPS\n########################\n\nFinish by giving me:\n\n1. A **conceptual summary**:\n   - 3\u20136 bullets summarizing:\n     - the algorithm / method used,\n     - the key ideas in the implementation,\n     - and the main software-engineering lessons.\n\n2. A **mini checklist** of things I could do on my own:\n   - e.g., \n     - \u201cRefactor the main function into two helpers,\u201d\n     - \u201cAdd tests for these edge cases,\u201d\n     - \u201cRewrite this loop using a more Pythonic construct.\u201d\n\n3. Ask me:\n   - \u201cWhich part of this notebook do you feel you understand the least?\u201d\n   - \u201cDo you want to zoom in further on a particular function or cell?\u201d\n\n########################\nSTYLE & SAFETY\n########################\n\n- Be direct but encouraging; assume I am capable of understanding serious critique.\n- Don\u2019t just say \u201cthis is bad\u201d; explain *why* and how to improve it.\n- If you are inferring intent that is not explicit in the notebook, make that clear:\n  > \u201cI\u2019m inferring that this function is meant to do X based on Y; if that\u2019s wrong, let\u2019s adjust.\u201d\n- Avoid hallucinating details that don\u2019t appear in the notebook. If something is unclear, say so.\n\n\n\nMy annotated chat:\nI tried this on the staff solution for Homework 7 RNN LSTM implementation. Here\u2019s my annotated chat trace (PDF):\n\nhttps://chatgpt.com/share/69295e36-3fd4-8010-9710-56a0c2e28752\n In the annotations I point out:\n\nwhere the AI gave good explanations,\n\nwhere its code critiques matched or conflicted with what our course emphasizes,\n\nand where it hallucinated or misread the notebook.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What this is:</bold><break/>We often get staff solution notebooks that are 100% correct but not necessarily the best examples of Python or ML engineering practices. I made a reusable prompt that turns ChatGPT  into:</paragraph><list style=\"unordered\"><list-item><paragraph>a tutor that explains the solution code,</paragraph></list-item><list-item><paragraph>a Socratic \u201cwhat if we changed this line?\u201d partner, and</paragraph></list-item><list-item><paragraph>a code reviewer that critiques style, structure, and engineering practices.</paragraph></list-item></list><paragraph><bold>How to use it:</bold></paragraph><list style=\"ordered\"><list-item><paragraph>Open ChatGPT and enable Study Mode.</paragraph></list-item><list-item><paragraph>Start a new chat and upload the <code>.ipynb</code> with the staff solution.</paragraph></list-item><list-item><paragraph>Paste the big prompt below.</paragraph></list-item><list-item><paragraph>Optionally tell it your comfort level with Python / ML.</paragraph></list-item><list-item><paragraph>Let it walk you through:</paragraph><list style=\"unordered\"><list-item><paragraph>an overview of the problem and solution,</paragraph></list-item><list-item><paragraph>interactive what-if questions about code changes,</paragraph></list-item><list-item><paragraph>and a software-engineering critique plus suggested refactors.</paragraph></list-item></list></list-item></list><paragraph><bold>Prompt:</bold></paragraph><pre>You are an AI tutor and code reviewer for a Jupyter notebook that contains:\r\n\r\n- a coding / ML problem,\r\n- and the staff-provided solution (and possibly some tests or helper code).\r\n\r\nYour goals:\r\n(1) Help me deeply understand the staff solution.\r\n(2) Actively quiz me with \u201cwhat-if\u201d code modification questions.\r\n(3) Critique the solution as a piece of software / ML engineering, not just as a correct answer.\r\n\r\nI will upload a .ipynb file. Please work in the following phases.\r\n\r\n########################\r\nPHASE 0 \u2014 ORIENT YOURSELF\r\n########################\r\n\r\nAfter reading the notebook:\r\n\r\n1. Identify and briefly summarize:\r\n   - The **problem statement** (in your own words).\r\n   - Where the **main staff solution** lives (e.g., which cell / which function).\r\n   - Any **supporting code** (helpers, tests, imports, plotting, etc.).\r\n\r\n2. Output a short overview like:\r\n\r\n   - Problem: [...]\r\n   - Main solution entry point: [...]\r\n   - Key helper functions: [...]\r\n   - Tests / demo cells: [...]\r\n\r\nIf anything is ambiguous, say so instead of guessing.\r\n\r\n########################\r\nPHASE 1 \u2014 EXPLAIN THE SOLUTION\r\n########################\r\n\r\nAssume I am a student who:\r\n- knows basic Python,\r\n- is familiar with lists/dicts/functions/loops,\r\n- but may *not* know the specific algorithm or ML technique yet.\r\n\r\nDo the following:\r\n\r\n1. **High-level explanation (3\u20137 sentences)**  \r\n   - What is the core idea of the solution?\r\n   - What algorithm / pattern does it implement (e.g., DP, DFS, greedy, training loop, etc.)?\r\n\r\n2. **Guided code walkthrough**  \r\n   - Walk through the main solution function or cell **top-down**.\r\n   - Group lines into logical chunks instead of line-by-line noise.\r\n   - For each chunk, explain:\r\n     - what it does,\r\n     - why it\u2019s needed,\r\n     - how it connects to the problem definition.\r\n\r\n3. **Complexity / behavior**  \r\n   - State the time and space complexity if meaningful.\r\n   - Mention any tradeoffs (e.g., readability vs performance, memory vs speed).\r\n\r\nKeep this phase mostly explanatory, with occasional quick checks like:\r\n&gt; \u201cCan you tell me in your own words what this loop is doing before I explain it?\u201d\r\n\r\n########################\r\nPHASE 2 \u2014 INTERACTIVE \u201cWHAT IF?\u201d QUESTIONS\r\n########################\r\n\r\nNow I want you to test and deepen my understanding by asking questions about code changes.\r\n\r\n1. Ask **3\u20136 \u201cwhat if\u201d questions** such as:\r\n   - \u201cWhat do you think happens if we remove this condition?\u201d\r\n   - \u201cIf we change this list to a set, how does that affect correctness and complexity?\u201d\r\n   - \u201cWhat if we change the iteration order here?\u201d\r\n   - \u201cWhat if we initialize this variable differently?\u201d\r\n\r\n2. For each question:\r\n   - Wait for my answer first.\r\n   - Then:\r\n     - say what *would* actually happen (behavior / correctness / performance),\r\n     - and tie it back to the underlying concept or invariant.\r\n\r\n3. Start with simpler changes (e.g., altering a constant or print) and\r\n   move toward more subtle ones (e.g., off-by-one edges, shared mutable state, ML training details).\r\n\r\nIf the code involves randomness or ML training:\r\n- Ask at least one question about seeds, reproducibility, or how hyperparameters show up in the code.\r\n\r\n########################\r\nPHASE 3 \u2014 SOFTWARE / ML ENGINEERING CRITIQUE\r\n########################\r\n\r\nNow critique the staff solution as if you were a senior software or ML engineer reviewing it.\r\n\r\n1. **Style &amp; Pythonic-ness**\r\n   - Comment on:\r\n     - naming (variables, functions),\r\n     - function length and cohesion,\r\n     - use of built-in functions / libraries,\r\n     - avoiding repetition,\r\n     - clarity vs cleverness.\r\n   - Point out at least 3\u20135 concrete improvements, such as:\r\n     - \u201cThis nested if could be simplified.\u201d\r\n     - \u201cThis magic number should be a named constant.\u201d\r\n     - \u201cThis could use a context manager / list comprehension / enumerate, etc.\u201d\r\n\r\n2. **Structure &amp; modularity**\r\n   - Is the solution written as one big cell, or decomposed into testable functions?\r\n   - Could it benefit from:\r\n     - helper functions,\r\n     - separating pure logic from I/O,\r\n     - or separating data loading / config from core algorithm?\r\n\r\n3. **Documentation &amp; comments**\r\n   - Are there docstrings or comments explaining key decisions?\r\n   - Suggest where short comments or docstrings would significantly improve readability.\r\n   - Optionally sketch an example docstring for the main function (using a standard style like NumPy or Google style).\r\n\r\n4. **Testing &amp; edge cases**\r\n   - Does the notebook include tests or sample runs?\r\n   - What edge cases might be missing?\r\n   - Propose **3\u20135 simple test cases** that would be good to add (in plain English or as Python snippets).\r\n\r\n5. **ML engineering (if applicable)**\r\n   - If the notebook includes any training/eval code:\r\n     - Comment on:\r\n       - seeding / reproducibility,\r\n       - separation of config (hyperparameters) from code,\r\n       - logging / metrics,\r\n       - handling of train/val/test,\r\n       - use of vectorization vs for-loops.\r\n     - Suggest at least a couple of improvements (e.g., \u201cmove these constants to a config dict,\u201d \u201cuse a random seed,\u201d etc.).\r\n\r\n6. **Concrete improved snippet(s)**\r\n   - Choose ONE small function or core block from the staff solution.\r\n   - Show:\r\n     - \u201cOriginal version\u201d (copied and possibly shortened)\r\n     - \u201cImproved version\u201d (your more Pythonic / maintainable rewrite)\r\n   - Explain in 3\u20135 bullets what changed and why it\u2019s better (clarity, safety, extensibility, etc.).\r\n\r\nIMPORTANT:\r\n- Do NOT rewrite the entire notebook unless I explicitly ask.\r\n- Focus on *illustrative* improvements that a student can learn from.\r\n\r\n########################\r\nPHASE 4 \u2014 SUMMARY &amp; ACTIONABLE NEXT STEPS\r\n########################\r\n\r\nFinish by giving me:\r\n\r\n1. A **conceptual summary**:\r\n   - 3\u20136 bullets summarizing:\r\n     - the algorithm / method used,\r\n     - the key ideas in the implementation,\r\n     - and the main software-engineering lessons.\r\n\r\n2. A **mini checklist** of things I could do on my own:\r\n   - e.g., \r\n     - \u201cRefactor the main function into two helpers,\u201d\r\n     - \u201cAdd tests for these edge cases,\u201d\r\n     - \u201cRewrite this loop using a more Pythonic construct.\u201d\r\n\r\n3. Ask me:\r\n   - \u201cWhich part of this notebook do you feel you understand the least?\u201d\r\n   - \u201cDo you want to zoom in further on a particular function or cell?\u201d\r\n\r\n########################\r\nSTYLE &amp; SAFETY\r\n########################\r\n\r\n- Be direct but encouraging; assume I am capable of understanding serious critique.\r\n- Don\u2019t just say \u201cthis is bad\u201d; explain *why* and how to improve it.\r\n- If you are inferring intent that is not explicit in the notebook, make that clear:\r\n  &gt; \u201cI\u2019m inferring that this function is meant to do X based on Y; if that\u2019s wrong, let\u2019s adjust.\u201d\r\n- Avoid hallucinating details that don\u2019t appear in the notebook. If something is unclear, say so.\r\n\n</pre><paragraph><bold>My annotated chat:</bold><break/>I tried this on the staff solution for Homework 7 RNN LSTM implementation. Here\u2019s my annotated chat trace (PDF):</paragraph><file url=\"https://static.us.edusercontent.com/files/a892iEevSEYlg86E8cxYe11L\" filename=\"RNN LSTM implementation.pdf\"/><paragraph>https://chatgpt.com/share/69295e36-3fd4-8010-9710-56a0c2e28752<break/> In the annotations I point out:</paragraph><list style=\"unordered\"><list-item><paragraph>where the AI gave good explanations,</paragraph></list-item><list-item><paragraph>where its code critiques matched or conflicted with what our course emphasizes,</paragraph></list-item><list-item><paragraph>and where it hallucinated or misread the notebook.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-28T19:53:55.03587+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372317,
            "author": "Xi Cheng",
            "project_title": "Special Participation E: AI-Enhanced Pre-homework Study Workflow Using Mistral AI",
            "post_body": "For this assignment, I built an AI-enhanced learning tool that serves as a substitute for traditional pre-homework readings. Here I use Mistral AI on HW08. \n\nMy goal was not to obtain solutions, but to construct a structured conceptual scaffold before attempting the homework. To do this, I asked the LLM to generate a pre-instruction guide that identifies the major concepts, prerequisites, and reasoning pathways needed for the SSM and attention problems. As shown in the annotated interaction trace , I intentionally framed my prompts so that the model would function like a pre-lecture resource rather than a homework solver.\n\nA central part of my prompting strategy was requesting \u201chints only\u201d instead of answers. I highlighted this in my annotations because it forces me to reconstruct derivations myself, rather than copying a completed solution. This approach supports the assignment\u2019s goal of encouraging self-learning. \n\nCompared with traditional pre-lecture materials, this AI-based tool offers a more interactive and adaptive experience. Instead of passively reading a static explanation, I can ask follow-up questions, request clarification, and explore prerequisite concepts dynamically. \n\nThe workflow\u2014beginning with a conceptual overview, followed by prerequisite review, then question-specific hints and common pitfalls\u2014can be applied to any technical topic in the course. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/5kfBIWvpAxbO50NTUgToOuhF\" filename=\"partE_mistral_hw8.pdf\"/><paragraph>For this assignment, I built an AI-enhanced learning tool that serves as a substitute for traditional pre-homework readings. Here I use Mistral AI on HW08. </paragraph><paragraph>My goal was not to obtain solutions, but to construct a structured conceptual scaffold before attempting the homework. To do this, I asked the LLM to generate a pre-instruction guide that identifies the major concepts, prerequisites, and reasoning pathways needed for the SSM and attention problems. As shown in the annotated interaction trace , I intentionally framed my prompts so that the model would function like a pre-lecture resource rather than a homework solver.</paragraph><paragraph>A central part of my prompting strategy was requesting \u201chints only\u201d instead of answers. I highlighted this in my annotations because it forces me to reconstruct derivations myself, rather than copying a completed solution. This approach supports the assignment\u2019s goal of encouraging self-learning. </paragraph><paragraph>Compared with traditional pre-lecture materials, this AI-based tool offers a more interactive and adaptive experience. Instead of passively reading a static explanation, I can ask follow-up questions, request clarification, and explore prerequisite concepts dynamically. </paragraph><paragraph>The workflow\u2014beginning with a conceptual overview, followed by prerequisite review, then question-specific hints and common pitfalls\u2014can be applied to any technical topic in the course. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-28T17:56:19.887587+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372302,
            "author": "Faiaz Khan",
            "project_title": "Special Participation E: Using an AI \u201cPre-/Post-Lecture Tutor\u201d using ChatGPT Study Mode",
            "post_body": "What this is:\n I made a reusable prompt that turns ChatGPT an interactive lecture companion instead of just a solution engine. You upload a lecture\u2019s slides or notes, and the model:\n\nwalks through the lecture in small chunks,\n\nforces you to answer questions before it explains,\n\ncompares your answers directly to slides,\n\nand ends with a short summary + quiz.\n\nHow to use it:\n\nOpen ChatGPT in Study Mode.\n\nPaste the big prompt below as your first message.\n\nUpload the slides for Lecture [X \u2013 Topic].\n\nTell it whether this is pre-lecture or post-lecture study.\n\nLet it quiz you chunk-by-chunk.\n\nThe Prompt:\n\nYou are an AI tutor helping me actively learn one lecture in a course.\n\n[COURSE]: e.g., \u201cCS XXX: [Course name]\u201d\n[LECTURE # AND TOPIC]: e.g., \u201cLecture 22 \u2013 Prompt-Based Fine-Tuning Methods\u201d\n[RESOURCES I WILL PROVIDE]: e.g., slides PDF, lecture notes, or textbook section.\n\nYour job is to act like an interactive pre-/post-lecture reading replacement.\n\n### MATERIAL HANDLING\n1. ONLY treat the files/notes I give you as the primary source of truth.\n2. When answering, explicitly separate:\n   - (a) \u201cAccording to the lecture materials, \u2026\u201d\n   - (b) \u201cBeyond the lecture, I infer/guess that \u2026\u201d\n3. If something is NOT in the materials or you are unsure, say:\n   > \u201cI\u2019m not sure from the provided materials; this is an educated guess.\u201d\n\n### INTERACTION LOOP\nFor this lecture, repeat the following cycle:\n\n**Step 0 \u2014 Calibrate**\n- Ask me:\n  - what my background is,\n  - how confident I feel about this topic (1\u20135),\n  - whether we are doing *pre-lecture* or *post-lecture* study.\n\n**Step 1 \u2014 Select a small chunk**\n- Pick a manageable chunk of content:\n  - e.g., 1\u20133 slides, or 1 short section of notes.\n- Tell me which slides/section we\u2019re focusing on:\n  > \u201cLet\u2019s focus on slides 5\u20137: [short description].\u201d\n\n**Step 2 \u2014 Active recall before explanation**\n- Ask 2\u20134 questions that I must answer *before* you explain anything, such as:\n  - \u201cIn your own words, what problem is this method solving?\u201d\n  - \u201cWhy is this assumption important?\u201d\n  - \u201cCan you restate the main equation and what each term means conceptually?\u201d\n\n- Always start with me, not you:\n  > \u201cAnswer in 2\u20134 sentences. It\u2019s okay to be wrong; I\u2019ll help refine.\u201d\n\n**Step 3 \u2014 Feedback + correction**\n- Compare my answer to the materials.\n- Do NOT just say \u201ccorrect/incorrect\u201d. Instead:\n  - Highlight what I did well.\n  - Point out exactly what is missing or mistaken.\n  - Quote or paraphrase the relevant part of the slides/notes to support your feedback.\n- If I\u2019m very off, give me a *hint* and ask me to try again before giving the full explanation.\n\n**Step 4 \u2014 Deepen understanding**\n- After feedback, give a concise explanation (max ~5 sentences) that:\n  - connects this chunk to previous lecture ideas,\n  - uses examples or analogies,\n  - mentions any common misconceptions.\n\n- OPTIONAL: Ask one \u201ctransfer\u201d question:\n  > \u201cHow would this change if [variant scenario]?\u201d\n\n**Step 5 \u2014 Quick check**\n- Ask me 1\u20132 short quiz questions (conceptual or very light math).\n- Then show me the answers so I can self-check.\n\n**Step 6 \u2014 Move on or review**\n- Ask:\n  > \u201cOn a scale of 1\u20135, how comfortable do you feel with this chunk?\u201d\n- If I say \u2264 3, offer a different explanation style (more examples, slower, or more visual).\n- If I say \u2265 4, move on to the next chunk.\n\n### END OF SESSION\nWhen I say I\u2019m done or we reach the end, do all of the following:\n\n1. Give a **3\u20136 bullet-point summary** of the lecture in your own words.\n2. Create:\n   - (a) 3 multiple-choice questions (with answers),\n   - (b) 3 short-answer conceptual questions (with brief answer keys).\n3. Ask ME:\n   - \u201cWhat still feels confusing?\u201d\n   - \u201cWhat is one thing you\u2019d like to revisit tomorrow?\u201d\n4. Suggest a short spaced-repetition plan for the next week based on this lecture.\n\n### STYLE & SAFETY\n- Be concise, friendly, and non-judgmental.\n- Never just dump full solutions to typical homework-style questions without first:\n  - asking for my attempt or idea,\n  - and then walking me through step-by-step.\n- Aggressively avoid hallucinations: if there is any conflict between your prior knowledge and the slides, defer to the slides and flag the mismatch.\n\n\n\n\nMy interaction trace + commentary:\n I tried this on Lecture 20: Positional Encoding & Modern Architectures. Here\u2019s the annotated PDF of the conversation: [link].\n In the annotations, I point out:\n\nwhere the model closely followed the prompt and was really helpful,\n\nwhere it hallucinated details that weren\u2019t in the slides,\n\nand some ideas for improving the prompt for future use.\n\nhttps://chatgpt.com/share/692944c7-5ef4-8010-ba41-cece83a423e0",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What this is:</bold><break/> I made a reusable prompt that turns ChatGPT an <italic>interactive lecture companion</italic> instead of just a solution engine. You upload a lecture\u2019s slides or notes, and the model:</paragraph><list style=\"unordered\"><list-item><paragraph>walks through the lecture in small chunks,</paragraph></list-item><list-item><paragraph>forces you to answer questions before it explains,</paragraph></list-item><list-item><paragraph>compares your answers directly to slides,</paragraph></list-item><list-item><paragraph>and ends with a short summary + quiz.</paragraph></list-item></list><paragraph><bold>How to use it:</bold></paragraph><list style=\"ordered\"><list-item><paragraph>Open ChatGPT in <bold>Study Mode</bold>.</paragraph></list-item><list-item><paragraph>Paste the big prompt below as your first message.</paragraph></list-item><list-item><paragraph>Upload the slides for Lecture [X \u2013 Topic].</paragraph></list-item><list-item><paragraph>Tell it whether this is pre-lecture or post-lecture study.</paragraph></list-item><list-item><paragraph>Let it quiz you chunk-by-chunk.</paragraph></list-item></list><paragraph><bold>The Prompt:</bold></paragraph><pre>You are an AI tutor helping me actively learn one lecture in a course.\r\n\r\n[COURSE]: e.g., \u201cCS XXX: [Course name]\u201d\r\n[LECTURE # AND TOPIC]: e.g., \u201cLecture 22 \u2013 Prompt-Based Fine-Tuning Methods\u201d\r\n[RESOURCES I WILL PROVIDE]: e.g., slides PDF, lecture notes, or textbook section.\r\n\r\nYour job is to act like an interactive pre-/post-lecture reading replacement.\r\n\r\n### MATERIAL HANDLING\r\n1. ONLY treat the files/notes I give you as the primary source of truth.\r\n2. When answering, explicitly separate:\r\n   - (a) \u201cAccording to the lecture materials, \u2026\u201d\r\n   - (b) \u201cBeyond the lecture, I infer/guess that \u2026\u201d\r\n3. If something is NOT in the materials or you are unsure, say:\r\n   &gt; \u201cI\u2019m not sure from the provided materials; this is an educated guess.\u201d\r\n\r\n### INTERACTION LOOP\r\nFor this lecture, repeat the following cycle:\r\n\r\n**Step 0 \u2014 Calibrate**\r\n- Ask me:\r\n  - what my background is,\r\n  - how confident I feel about this topic (1\u20135),\r\n  - whether we are doing *pre-lecture* or *post-lecture* study.\r\n\r\n**Step 1 \u2014 Select a small chunk**\r\n- Pick a manageable chunk of content:\r\n  - e.g., 1\u20133 slides, or 1 short section of notes.\r\n- Tell me which slides/section we\u2019re focusing on:\r\n  &gt; \u201cLet\u2019s focus on slides 5\u20137: [short description].\u201d\r\n\r\n**Step 2 \u2014 Active recall before explanation**\r\n- Ask 2\u20134 questions that I must answer *before* you explain anything, such as:\r\n  - \u201cIn your own words, what problem is this method solving?\u201d\r\n  - \u201cWhy is this assumption important?\u201d\r\n  - \u201cCan you restate the main equation and what each term means conceptually?\u201d\r\n\r\n- Always start with me, not you:\r\n  &gt; \u201cAnswer in 2\u20134 sentences. It\u2019s okay to be wrong; I\u2019ll help refine.\u201d\r\n\r\n**Step 3 \u2014 Feedback + correction**\r\n- Compare my answer to the materials.\r\n- Do NOT just say \u201ccorrect/incorrect\u201d. Instead:\r\n  - Highlight what I did well.\r\n  - Point out exactly what is missing or mistaken.\r\n  - Quote or paraphrase the relevant part of the slides/notes to support your feedback.\r\n- If I\u2019m very off, give me a *hint* and ask me to try again before giving the full explanation.\r\n\r\n**Step 4 \u2014 Deepen understanding**\r\n- After feedback, give a concise explanation (max ~5 sentences) that:\r\n  - connects this chunk to previous lecture ideas,\r\n  - uses examples or analogies,\r\n  - mentions any common misconceptions.\r\n\r\n- OPTIONAL: Ask one \u201ctransfer\u201d question:\r\n  &gt; \u201cHow would this change if [variant scenario]?\u201d\r\n\r\n**Step 5 \u2014 Quick check**\r\n- Ask me 1\u20132 short quiz questions (conceptual or very light math).\r\n- Then show me the answers so I can self-check.\r\n\r\n**Step 6 \u2014 Move on or review**\r\n- Ask:\r\n  &gt; \u201cOn a scale of 1\u20135, how comfortable do you feel with this chunk?\u201d\r\n- If I say \u2264 3, offer a different explanation style (more examples, slower, or more visual).\r\n- If I say \u2265 4, move on to the next chunk.\r\n\r\n### END OF SESSION\r\nWhen I say I\u2019m done or we reach the end, do all of the following:\r\n\r\n1. Give a **3\u20136 bullet-point summary** of the lecture in your own words.\r\n2. Create:\r\n   - (a) 3 multiple-choice questions (with answers),\r\n   - (b) 3 short-answer conceptual questions (with brief answer keys).\r\n3. Ask ME:\r\n   - \u201cWhat still feels confusing?\u201d\r\n   - \u201cWhat is one thing you\u2019d like to revisit tomorrow?\u201d\r\n4. Suggest a short spaced-repetition plan for the next week based on this lecture.\r\n\r\n### STYLE &amp; SAFETY\r\n- Be concise, friendly, and non-judgmental.\r\n- Never just dump full solutions to typical homework-style questions without first:\r\n  - asking for my attempt or idea,\r\n  - and then walking me through step-by-step.\r\n- Aggressively avoid hallucinations: if there is any conflict between your prior knowledge and the slides, defer to the slides and flag the mismatch.\r\n\n</pre><paragraph><break/><bold>My interaction trace + commentary:</bold><break/> I tried this on Lecture 20: Positional Encoding &amp; Modern Architectures. Here\u2019s the annotated PDF of the conversation: [link].<break/> In the annotations, I point out:</paragraph><list style=\"unordered\"><list-item><paragraph>where the model closely followed the prompt and was really helpful,</paragraph></list-item><list-item><paragraph>where it hallucinated details that weren\u2019t in the slides,</paragraph></list-item><list-item><paragraph>and some ideas for improving the prompt for future use.</paragraph></list-item></list><paragraph>https://chatgpt.com/share/692944c7-5ef4-8010-ba41-cece83a423e0</paragraph><file url=\"https://static.us.edusercontent.com/files/02LPmeNtu5c2neK2LPSCSJRZ\" filename=\"Interactive study session.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-28T17:43:32.737293+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372287,
            "author": "Guohao Lv",
            "project_title": "Special Participation E: ChatGPT as Motivation Finder from Lecture Notes",
            "post_body": "When I review the lecture notes, I always find it easier to understand a concept after understanding why it is introduced in the first place. Because it is quite laborious to go back to watch the lecture again and sometimes professors just give a very simplified version of the motivation, I built a \u201cMotivation Finder from Lecture Notes\u201d prompt that uses AI to give a more detailed explanation of the motivation of concepts mentioned in lectures and provide good sources for further reading and deeper understanding. The workflow is: I paste a chunk of CS182 lecture notes (for a particular lecture) into the prompt. The AI first scans the text and extracts a small list of key concepts mentioned there (e.g., BatchNorm, residual blocks, attention, muP, etc.). For each concept, it then searches the internet for one good source\u2014ideally the original paper or a survey/tutorial from a reputable venue\u2014and reports the title, link, and type of source. Using that source, it writes a short motivation-focused summary: what problem people were facing before this idea, how the idea addresses that problem at a high level, and what tradeoffs or limitations are mentioned. It then quotes the relevant lines from the lecture notes and tells me whether the notes make the motivation clear, partially clear, or confusing. Finally, for each concept, it asks me 1\u20132 questions that test whether I actually understand the motivation and tradeoffs, waits for my answers, and then gives concise \u201cideal answers.\u201d Overall, I find it quite helpful as a post-lecture reading even though it is still not ideal, and the questions it generate does lead me to go back and read more about the topic, so I think this tool achieves its objective.\n\nThe annotated conversation: https://drive.google.com/file/d/1htjK83ljExmXnUGwNZA2ykykNXQA3ZW_/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>When I review the lecture notes, I always find it easier to understand a concept after understanding why it is introduced in the first place. Because it is quite laborious to go back to watch the lecture again and sometimes professors just give a very simplified version of the motivation, I built a \u201cMotivation Finder from Lecture Notes\u201d prompt that uses AI to give a more detailed explanation of the motivation of concepts mentioned in lectures and provide good sources for further reading and deeper understanding. The workflow is: I paste a chunk of CS182 lecture notes (for a particular lecture) into the prompt. The AI first scans the text and extracts a small list of key concepts mentioned there (e.g., BatchNorm, residual blocks, attention, muP, etc.). For each concept, it then searches the internet for one good source\u2014ideally the original paper or a survey/tutorial from a reputable venue\u2014and reports the title, link, and type of source. Using that source, it writes a short motivation-focused summary: what problem people were facing before this idea, how the idea addresses that problem at a high level, and what tradeoffs or limitations are mentioned. It then quotes the relevant lines from the lecture notes and tells me whether the notes make the motivation clear, partially clear, or confusing. Finally, for each concept, it asks me 1\u20132 questions that test whether I actually understand the motivation and tradeoffs, waits for my answers, and then gives concise \u201cideal answers.\u201d Overall, I find it quite helpful as a post-lecture reading even though it is still not ideal, and the questions it generate does lead me to go back and read more about the topic, so I think this tool achieves its objective.</paragraph><paragraph>The annotated conversation: https://drive.google.com/file/d/1htjK83ljExmXnUGwNZA2ykykNXQA3ZW_/view?usp=sharing</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-28T17:25:53.685301+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7372081,
            "author": "Ken Zheng",
            "project_title": "Special Participation A: Deepseek on HW2",
            "post_body": "I completed all non-coding parts of Homework 2 using Deepseek with DeepThink turned on.\n\nMotivation\n\nI\u2019ve seen quite a few examples of classmates engaging in some light conversation with the model or using somewhat structured/designed prompts with the aim of improving the output quality and accuracy. I opted for a slightly different approach. I decided to use raw and \"no-prompt\" prompting, meant to mimic the setting where a student is treating the LLM less as an interactive tutor and more as a tool, a \u201ccalculator\u201d, for problems. \n\nFor every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box. Nothing more. No context was given except for those present in the problem originally, and no prompt optimization was performed (i.e., no strategic \"filler\" words like \"think carefully and answer accurately\", \"you are an expert in deep learning\", \"take a deep breath\" [https://arxiv.org/abs/2309.03409] were included).\n\nSummary\n\nOverall, Deepseek is a very capable model for reasoning tasks. It one-shots every non coding question in Homework 2. It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect. \n\nAnalyzing the thinking trace reveals Deepseek's general chain of thought pattern, which is comprised of these steps:\n\nRestate the question\n\nRe-interpret the question with its own words\n\nList what is given/known\n\nIdentify what is unknown/what the task is\n\nFormulate a strategy/plan of attack\n\nCarry out that strategy (i.e., start doing the work/calculations following the strategy)\n\nAfter finding answer, start doubting absolutely everything starting from step 1 to step 6.\n\nFor every doubt, recheck work and see if other answers exist. If yes, explore those paths; else, move on to the next doubt.\n\nReturn final answer after double-checking everything.\n\nPeople who took EECS 16A with Prof. Ranade (Fall 2023) might remember her teaching us a three step proof writing structure: \n\nWhat do we know?\n\nWhat are we trying to show?\n\nHow can we get from 1 to 2?\n\nIt is interesting to see Deepseek's thought habits closely follow this three step process (plus a plethora of double-checking), almost like Deepseek's frontier-level reasoning performance stems from the fact that it treats every technical question like completing a rigorous proof. \n\nPlease see annotations for more in-depth observations.\n\nFiles & Links\n\nQuestion 1:\n\nQuestion 2:\n\nQuestion 5:",
            "content_xml": "<document version=\"2.0\"><paragraph>I completed all non-coding parts of Homework 2 using Deepseek with DeepThink turned on.</paragraph><paragraph><bold>Motivation</bold></paragraph><paragraph>I\u2019ve seen quite a few examples of classmates engaging in some light conversation with the model or using somewhat structured/designed prompts with the aim of improving the output quality and accuracy. I opted for a slightly different approach. I decided to use raw and \"no-prompt\" prompting, meant to mimic the setting where a student is treating the LLM less as an interactive tutor and more as a tool, a \u201ccalculator\u201d, for problems. </paragraph><paragraph>For every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box. Nothing more. No context was given except for those present in the problem originally, and no prompt optimization was performed (i.e., no strategic \"filler\" words like \"think carefully and answer accurately\", \"you are an expert in deep learning\", \"take a deep breath\" [<link href=\"https://arxiv.org/abs/2309.03409\">https://arxiv.org/abs/2309.03409</link>] were included).</paragraph><paragraph><bold>Summary</bold></paragraph><paragraph>Overall, Deepseek is a very capable model for reasoning tasks. It one-shots every non coding question in Homework 2. It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect. </paragraph><paragraph>Analyzing the thinking trace reveals Deepseek's general chain of thought pattern, which is comprised of these steps:</paragraph><list style=\"number\"><list-item><paragraph>Restate the question</paragraph></list-item><list-item><paragraph>Re-interpret the question with its own words</paragraph></list-item><list-item><paragraph>List what is given/known</paragraph></list-item><list-item><paragraph>Identify what is unknown/what the task is</paragraph></list-item><list-item><paragraph>Formulate a strategy/plan of attack</paragraph></list-item><list-item><paragraph>Carry out that strategy (i.e., start doing the work/calculations following the strategy)</paragraph></list-item><list-item><paragraph>After finding answer, start doubting absolutely everything starting from step 1 to step 6.</paragraph></list-item><list-item><paragraph>For every doubt, recheck work and see if other answers exist. If yes, explore those paths; else, move on to the next doubt.</paragraph></list-item><list-item><paragraph>Return final answer after double-checking everything.</paragraph></list-item></list><paragraph>People who took EECS 16A with Prof. Ranade (Fall 2023) might remember her teaching us a three step proof writing structure: </paragraph><list style=\"number\"><list-item><paragraph>What do we know?</paragraph></list-item><list-item><paragraph>What are we trying to show?</paragraph></list-item><list-item><paragraph>How can we get from 1 to 2?</paragraph></list-item></list><paragraph>It is interesting to see Deepseek's thought habits closely follow this three step process (plus a plethora of double-checking), almost like Deepseek's frontier-level reasoning performance stems from the fact that it treats every technical question like completing a rigorous proof. </paragraph><paragraph>Please see annotations for more in-depth observations.</paragraph><paragraph><bold>Files &amp; Links</bold></paragraph><paragraph><link href=\"https://chat.deepseek.com/share/z5g4rvn5xcnnl5iki8\">Question 1</link>:</paragraph><file url=\"https://static.us.edusercontent.com/files/WLOcnHEm5JvQXOZ89iD1WNia\" filename=\"Q1_annotated.pdf\"/><paragraph><link href=\"https://chat.deepseek.com/share/226zldixt6l2dw7qx6\">Question 2</link>:</paragraph><file url=\"https://static.us.edusercontent.com/files/OuWilSbC3lNKib2obGheQgmb\" filename=\"Q2_annotated.pdf\"/><paragraph><link href=\"https://chat.deepseek.com/share/em99xk8xbx2ozer0ba\">Question 5</link>:</paragraph><file url=\"https://static.us.edusercontent.com/files/ZaL9g3VP5ebbYoUJJE6l1Xvb\" filename=\"Q5_annotated.pdf\"/></document>",
            "links": [
                "https://arxiv.org/abs/2309.03409",
                "https://chat.deepseek.com/share/z5g4rvn5xcnnl5iki8",
                "https://chat.deepseek.com/share/226zldixt6l2dw7qx6",
                "https://chat.deepseek.com/share/em99xk8xbx2ozer0ba"
            ],
            "attachments": [],
            "created_at": "2025-11-28T14:45:16.624977+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7370567,
            "author": "Jorge Diaz Chao",
            "project_title": "Kimi K2 Thinking on HW12",
            "post_body": "Kimi K2 Thinking (open-source) stormed the internet not too long ago for beating a lot of benchmarks and competing with closed-source models. To be honest, I don't think benchmarks cover a lot of the story nowadays so I remained skeptical. Today, I tried it for the first time on Homework 12\n\nThe model thinks (particularly?) fast and seems to do very good with a comparable performance to other models like OpenAI's (which I am more familiar with). Shortcomings include the poor vision component. The commentary goes more deeply into how I guided it to do the homework. Ultimately, it did very well and only demanded prompting to describe visual figures and ask for a particular style of responses.\n\nNot a Fireship video but good enough.",
            "content_xml": "<document version=\"2.0\"><paragraph>Kimi K2 Thinking (open-source) stormed the internet not too long ago for beating a lot of benchmarks and competing with closed-source models. To be honest, I don't think benchmarks cover a lot of the story nowadays so I remained skeptical. Today, I tried it for the first time on <link href=\"https://berkeley-cs182.github.io/fa25/assets/assignments/hw12.pdf\"><italic>Homework 12</italic></link></paragraph><file url=\"https://static.us.edusercontent.com/files/BhFLjqUsrviOILVcgt2I0U4B\" filename=\"hw12-kimi-comments.pdf\"/><file url=\"https://static.us.edusercontent.com/files/oQpJwjIlJDavdX1M2NqqbM7e\" filename=\"hw12-kimi-chat.jpg\"/><paragraph>The model thinks (particularly?) fast and seems to do very good with a comparable performance to other models like OpenAI's (which I am more familiar with). Shortcomings include the poor vision component. The commentary goes more deeply into how I guided it to do the homework. Ultimately, it did very well and only demanded prompting to describe visual figures and ask for a particular style of responses.<break/><break/>Not a Fireship <link href=\"https://www.youtube.com/watch?v=ZP9lqjNa_BQ\">video</link> but good enough.</paragraph></document>",
            "links": [
                "https://berkeley-cs182.github.io/fa25/assets/assignments/hw12.pdf",
                "https://www.youtube.com/watch?v=ZP9lqjNa_BQ"
            ],
            "attachments": [],
            "created_at": "2025-11-27T20:04:08.231232+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7369769,
            "author": "Guohao Lv",
            "project_title": "Special Participation E: ChatGPT as a Mistake-Spotting Coach",
            "post_body": "I often use ChatGPT or other LLMs to review CS182 concepts, but I\u2019ve noticed a big problem: it\u2019s very easy to just believe whatever it says. Even when something is slightly wrong or oversimplified, I tend to read it passively instead of thinking about whether it matches what we did in lecture. For this special participation, I wanted a way to turn that into an active exercise where I\u2019m forced to read explanations critically and practice catching subtle mistakes in deep learning concepts. So I designed a \u201cMistake-Spotting Coach\u201d prompt specifically for CS182. The idea is that I give the LLM a lecture topic (for example: SGD vs Adam, normalization layers, residual connections, ConvNets vs fully-connected nets, RNNs vs attention, in-context learning, etc.). Using the prompt, the LLM then writes a short explanation of the topic on purpose with 2\u20133 subtle errors or misleading statements mixed in.  After the explanation, it asks me to find and correct the mistakes before it reveals what was wrong. I then go through and mark which sentences I think are incorrect or suspicious, explain why, and suggest fixes. The LLM responds by telling me which mistakes I caught, which ones I missed, and where my corrections are only partially right. Finally, it rewrites a clean, fully correct explanation and asks a couple of follow-up questions to check deeper understanding. Overall, I think it is helpful for understanding deeplearning concepts, but it still has some problems, which I pointed out in the annotated conversation. A fix would be to add what you want ChatGPT to do to the General Rules section of the prompt (e.g., \"don't highlight the problematic sentences\") so that it can tailor to your own learning goal.\n\nAnnotated Conversation: https://drive.google.com/file/d/1_iLr4riCwdcg3p-_AzwjeLe6kAtF7y2T/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>I often use ChatGPT or other LLMs to review CS182 concepts, but I\u2019ve noticed a big problem: it\u2019s very easy to just <italic>believe</italic> whatever it says. Even when something is slightly wrong or oversimplified, I tend to read it passively instead of thinking about whether it matches what we did in lecture. For this special participation, I wanted a way to turn that into an active exercise where I\u2019m forced to read explanations critically and practice catching subtle mistakes in deep learning concepts. So I designed a \u201cMistake-Spotting Coach\u201d prompt specifically for CS182. The idea is that I give the LLM a lecture topic (for example: SGD vs Adam, normalization layers, residual connections, ConvNets vs fully-connected nets, RNNs vs attention, in-context learning, etc.). Using the prompt, the LLM then writes a short explanation of the topic <italic>on purpose</italic> with 2\u20133 subtle errors or misleading statements mixed in.  After the explanation, it asks me to find and correct the mistakes before it reveals what was wrong. I then go through and mark which sentences I think are incorrect or suspicious, explain why, and suggest fixes. The LLM responds by telling me which mistakes I caught, which ones I missed, and where my corrections are only partially right. Finally, it rewrites a clean, fully correct explanation and asks a couple of follow-up questions to check deeper understanding. Overall, I think it is helpful for understanding deeplearning concepts, but it still has some problems, which I pointed out in the annotated conversation. A fix would be to add what you want ChatGPT to do to the General Rules section of the prompt (e.g., \"don't highlight the problematic sentences\") so that it can tailor to your own learning goal.</paragraph><paragraph>Annotated Conversation: https://drive.google.com/file/d/1_iLr4riCwdcg3p-_AzwjeLe6kAtF7y2T/view?usp=sharing</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-27T12:03:19.696186+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7369656,
            "author": "Andrew Choy",
            "project_title": "Special Participation: Gemini 3 (Thinking) on HW 8",
            "post_body": "For this assignment, I used Gemini to tackle the non-coding/theory portions of Homework 8. Beyond simply checking if the model could solve the math, I wanted to investigate whether crafting an \"ideal\" prompt for learning is actually worth the effort.\n\nI ran an A/B test using two distinct prompts: a \"Lazy\" prompt (minimal instruction) and a \"Rigorous\" prompt (detailed constraints, persona setting, and formatting rules).\n\nMy conclusion: Attempting to engineer the perfect pedagogical prompt often yields diminishing returns. Unless your prompt is extremely specific and detailed, you will likely waste more time trying to \"program\" the AI to teach you than you would by simply struggling through the problem yourself. While my sophisticated prompt was significantly more detailed than the lazy one, I couldn't find a strong justification for the extra setup time. The model's raw training on these standard theoretical derivations is robust enough that \"lazy\" prompting yields nearly identical results.\n\nDisclaimer: This interaction was not conducted in \"Study Mode,\" I have not tested/used this mode in the past so i cannot speak to the abilities in this regard. ",
            "content_xml": "<document version=\"2.0\"><paragraph>For this assignment, I used Gemini to tackle the non-coding/theory portions of Homework 8. Beyond simply checking if the model could solve the math, I wanted to investigate whether crafting an \"ideal\" prompt for learning is actually worth the effort.</paragraph><file url=\"https://static.us.edusercontent.com/files/YCdIAj8sDYvhpEEZuWMayegS\" filename=\"lazy.pdf\"/><file url=\"https://static.us.edusercontent.com/files/R6NBvAUeRjhLUhKIzrmIo41y\" filename=\"sophisticated.pdf\"/><paragraph>I ran an A/B test using two distinct prompts: a \"Lazy\" prompt (minimal instruction) and a \"Rigorous\" prompt (detailed constraints, persona setting, and formatting rules).</paragraph><paragraph><bold>My conclusion:</bold> Attempting to engineer the perfect pedagogical prompt often yields diminishing returns. Unless your prompt is extremely specific and detailed, you will likely waste more time trying to \"program\" the AI to teach you than you would by simply struggling through the problem yourself. While my sophisticated prompt was significantly more detailed than the lazy one, I couldn't find a strong justification for the extra setup time. The model's raw training on these standard theoretical derivations is robust enough that \"lazy\" prompting yields nearly identical results.</paragraph><paragraph><italic>Disclaimer: This interaction was not conducted in \"Study Mode,\" I have not tested/used this mode in the past so i cannot speak to the abilities in this regard.</italic> </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-27T11:33:04.796778+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7368105,
            "author": "Deena Sun",
            "project_title": "Special Participation E: Using Gemini Pro 3 and Claude Opus 4.5 for Jupyter Notebook Demos of Lecture Topics",
            "post_body": "\n\nExecutive Summary:\n\nI enjoy taking the concepts/theory we learn in class and applying them in code\u2014I think it forces me to turn what I thought I understood through math or abstract concepts and make them more concrete by executing them. As a CS 189 course staff this semester, we\u2019re creating a lot of demo notebooks and coding-forward Jupyter notebooks for students to try out. So taking inspiration from the demos in CS 189 as well as our coding homework problems, I decided to try to prompt LLMs and IDE-integrated coding assistants to produce some Jupyter notebook demos of topics covered in class.\n\nPrompt used:\n\nYou are an expert deep learning instructor and advanced Python coder creating a demo for the Berkeley course CS 182, Deep Neural Networks. Your task is to produce a Jupyter notebook that explains and demonstrates the target topic. Use the written lecture notes and transcript of the lecture to inform what key concepts to cover in the demo for the target topic.\n\nThe notebook should following these key principles:\n1. Introduce the topic, describe its relevance in deep learning, and define core concepts from the provided transcript/lecture notes. If necessary, provide comparisons between solutions/practices covered in lecture that \u201cdidn\u2019t work\u201d, and those that \u201cdid work\u201d.\n2. Use clear markdown cells to present essential theoretical background.\n3. Provide a complete Python implementation using PyTorch, Numpy, and Pandas libraries.\n4. Integrate plots and visualizations throughout to aid interpretation.\n\nFormatting guidelines:\n1. Use descriptive comments and narrative explanations alongside code. Write clear docstrings where needed.\n2. Limit total length to 100-150 lines of code and under 5000 words.\n3. The notebook should be runnable end-to-end in Colab or local Jupyter environments.\n\nOutput only the ipynb notebook.\n\nTarget topic:\n<INSERT YOUR TARGET TOPIC HERE>\n<MAKE SURE TO ADD PDF OF LECTURE NOTES AND TXT FILE OF LECTURE TRANSCRIPTS FROM YOUTUBE>\n\n\nI explored 2 LLMs:\n\nGemini Pro 3 from the Gemini website\n\nI pasted in the prompt, added my target topic (Soft-prompting and soft-prefix prompting), and attached the pdf of lecture 21\u2019s notes as well as a txt of lecture 21\u2019s YouTube transcripts\n\nSince I was using Gemini\u2019s website rather than an IDE, Gemini produced a JSON that I then pasted into a .json file and converted into an ipynb by changing the file extension\n\nClaude Opus 4.5 within Cursor\n\nI pasted in the prompt, and attached lecture 21 \u2019s PDF notes and a txt of the YouTube transcripts\n\nBoth models produced a runnable Jupyter notebook that included a toy encoder implementation, a toy task (Gemini used the task of reversing a sequence and Claude made dummy sequences to map to positive, negative, or neutral sentiment), training loops for soft-prompting, soft-prefixes, and frozen baseline models, and comparisons of parameter counts and training metrics. Furthermore, both models included detailed walkthroughs and explanations that motivated why soft-prompting/soft-prefixing could be useful, implementation details, and key takeaways.\n\nSome limitations I noticed from both models were the use of outdated additive positional embeddings (perhaps chosen for instructive simplicity). Claude\u2019s implementation of soft-prefixing was flawed, and seemed equivalent to just appending the supposedly learned keys/values to the input sequence\u2019s token embeddings.\n\nOverall, I was impressed with the thoroughness of both model\u2019s demos. I found it a helpful guide towards seeing how a concept we discussed in class might actually be implemented in practice. The interactivity of a Jupyter notebook also makes it easier to play around and tweak things on my own (e.g. by adding extra print statements, adjusting different parameter sizes) that can not be as easily achieved by reading an article or textbook even if it does include code implementations.\n\nWhile this was a fun experiment to see how we could leverage LLMs to produce workable demos, I still ultimately find hands-on doing more effective for my learning than just reading through the demo, even if I\u2019m actively engaged with the reading and trying to understand every step or identify potential errors. I\u2019d be curious to see if this provides a good baseline demo that we could then ask another LLM to help \u201cablate\u201d and add \u201cTODOs\u201d for students to fill out/code on their own.\n\nGemini Pro 3 - Detailed Analysis\n\nLink to my conversation with Gemini Pro 3: https://gemini.google.com/app/cbcac1c80e5f5041\n\nJupyter Notebook:\n\nSome outdated practices I noticed from Gemini\u2019s Transformer implementation\n\nToyGPT implementation uses additive learned absolute positional embeddings\u2014which as mentioned in lectures 19 and 20 is no longer standard practice (in favor of NoPE or RoPE). However, more advanced techniques like RoPE require custom attention implementations (e.g. hooking into the Q, K tensors before they\u2019re multiplied to apply the rotary transform). For instructive toy models in a demo, I can see how additive positional encodings might be simpler and easier to understand.\n\nThe FFN/MLP uses GeLU rather than more recent alternatives like SwiGLU or ReLU^2\n\nLayernorms in the transformer encoder blocks instead of RMS norm\n\nSome modern practices that Gemini\u2019s transformer included which differed from the original Transformer paper:\n\nPre-norm residual connections (adding the norm before the Attention sublayer and residual connections that bypass the norm entirely) versus post norm (normalizing after the residual add)\n\nI love that Gemini included the shapes of different tensors in the code comments. I often add these myself as I code to track the dimensions of things/sanity check/debug and I find that they really help my understanding as I implement. I also really appreciate that it makes some of the different dimension-changing functions (e.g. unsqueezed, expand, concat) more readable without having to guess at what they\u2019re trying to do.\n\nThe comments in the code for the CausalSelfAttention\u2019s forward method were very detailed, and covered implementation details beyond what was mentioned in lecture.\n\nFor instance, Gemini noted that we needed to adjust the shape of the causal mask when a soft-prefix is supplied to be [T, P + T] where P = prefix length and T = sequence length.\n\nAdditionally, Gemini also explicitly outlined how the causal mask should still allow attending to all the prefixes and be causal for the rest of the tokens themselves.\n\nIn the ToyGPT forward method, Gemini cuts off the soft prompt embedding token\u2019s output predictions and only returns logits for the original T tokens. This makes sense because we don\u2019t care about what the model predicts for the pre-prompt.\n\nSome things that Gemini included beyond the lecture scope:\n\nComments inside PrefixTuning noted that usually the learnable prefix parameters are reparameterized through a small MLP for stability.\n\nGeneral good coding practices that Gemini used:\n\nSeeding for reproducibility\n\nPyTorch practice of explicitly checking and printing what device we\u2019re using\n\nUsing constants to set the model hyperparameters\n\nMakes copies of the base model for each variant (soft-prompting, soft-prefix, and base model only) so that the base models all start from the same random state for comparison\n\nAlthough Gemini confidently provided an \u201cAnalysis of Results\u201d for what I should expect to see, running the demo as given did not produce a frozen baseline with high/flat loss or a decreasing loss curve for soft prompting.\n\nGemini also included a visualization of the soft prompt embeddings. However, unlike Claude, Gemini also included an interpretation of what we can use it for\u2014it noted that we can apply a KNN-like search to find the nearest token in the real vocabulary to see if the soft prompt corresponds to any understandable human words.\n\nClaude Opus 4.5 - Detailed Analysis\n\nLog of my conversation with Claude in Cursor:\n\nJupyter Notebook:\n\nSimpleTransformer implementation uses additive learned absolute positional embeddings\u2014which as mentioned in lectures 19 and 20 is no longer standard practice (in favor of NoPE or RoPE). However, more advanced techniques like RoPE require custom attention implementations (e.g. hooking into the Q, K tensors before they\u2019re multiplied to apply the rotary transform). For instructive toy models in a demo, I can see how additive positional encodings might be simpler and easier to understand.\n\nInteresting that Claude actually referenced the lecture itself in the notebook: As described in lecture: \"The past queries have no influence except how they influence the keys and values. The state is the keys and the values.\"\n\nSome things that weren\u2019t immediately clear/understandable to me:\n\nWhy did we need MLP reparameterization for the soft-prefixes?\n\nWhat I was supposed to take away from 7: Visualizing Learned Soft Prompt Embeddings \u2014 what kinds of patterns should I be expecting?\n\nI don\u2019t think that Opus actually implemented soft-prefix tuning. Its implementation is functionally a learned prepended prompt. True \u201cprefix-tuning\u201d injects learned key/value vectors directly into the attention cache of every transformer layer; this requires modifying each layer\u2019s attention module so the extra K/Vs are concatenated before computing attention scores. Crucially, the code in SimpleTransformer simply concatenates the prefix to the token embeddings before the encoder. That makes the prefix act like extra prepended tokens (a soft prompt), not as layerwise K/V injections.\n\nAlso, PrefixWrapper only returns the first layer\u2019s keys even though SoftPrefix stores tensors shaped like per-layer K/Vs, get_all_prefixes(). I\u2019m not sure why Claude decided to only provide a \u201csimplified\u201d version of get_all-prefixes. But since the SimpleTransformer never consumes layer-specific prefixes, the stored K/Vs are never wired into the attention blocks.",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>I enjoy taking the concepts/theory we learn in class and applying them in code\u2014I think it forces me to turn what I thought I understood through math or abstract concepts and make them more concrete by executing them. As a CS 189 course staff this semester, we\u2019re creating a lot of demo notebooks and coding-forward Jupyter notebooks for students to try out. So taking inspiration from the demos in CS 189 as well as our coding homework problems, I decided to try to prompt LLMs and IDE-integrated coding assistants to produce some Jupyter notebook demos of topics covered in class.</paragraph><paragraph>Prompt used:</paragraph><pre>You are an expert deep learning instructor and advanced Python coder creating a demo for the Berkeley course CS 182, Deep Neural Networks. Your task is to produce a Jupyter notebook that explains and demonstrates the target topic. Use the written lecture notes and transcript of the lecture to inform what key concepts to cover in the demo for the target topic.\n\nThe notebook should following these key principles:\n1. Introduce the topic, describe its relevance in deep learning, and define core concepts from the provided transcript/lecture notes. If necessary, provide comparisons between solutions/practices covered in lecture that \u201cdidn\u2019t work\u201d, and those that \u201cdid work\u201d.\n2. Use clear markdown cells to present essential theoretical background.\n3. Provide a complete Python implementation using PyTorch, Numpy, and Pandas libraries.\n4. Integrate plots and visualizations throughout to aid interpretation.\n\nFormatting guidelines:\n1. Use descriptive comments and narrative explanations alongside code. Write clear docstrings where needed.\n2. Limit total length to 100-150 lines of code and under 5000 words.\n3. The notebook should be runnable end-to-end in Colab or local Jupyter environments.\n\nOutput only the ipynb notebook.\n\nTarget topic:\n&lt;INSERT YOUR TARGET TOPIC HERE&gt;\n&lt;MAKE SURE TO ADD PDF OF LECTURE NOTES AND TXT FILE OF LECTURE TRANSCRIPTS FROM YOUTUBE&gt;\n</pre><paragraph>I explored 2 LLMs:</paragraph><list style=\"unordered\"><list-item><paragraph>Gemini Pro 3 from the Gemini website</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>I pasted in the prompt, added my target topic (Soft-prompting and soft-prefix prompting), and attached the pdf of lecture 21\u2019s notes as well as a txt of lecture 21\u2019s YouTube transcripts</paragraph></list-item><list-item><paragraph>Since I was using Gemini\u2019s website rather than an IDE, Gemini produced a JSON that I then pasted into a .json file and converted into an ipynb by changing the file extension</paragraph></list-item></list></list-item><list-item><paragraph>Claude Opus 4.5 within Cursor</paragraph><list style=\"unordered\"><list-item><paragraph>I pasted in the prompt, and attached lecture 21 \u2019s PDF notes and a txt of the YouTube transcripts</paragraph></list-item></list></list-item></list><paragraph>Both models produced a runnable Jupyter notebook that included a toy encoder implementation, a toy task (Gemini used the task of reversing a sequence and Claude made dummy sequences to map to positive, negative, or neutral sentiment), training loops for soft-prompting, soft-prefixes, and frozen baseline models, and comparisons of parameter counts and training metrics. Furthermore, both models included detailed walkthroughs and explanations that motivated why soft-prompting/soft-prefixing could be useful, implementation details, and key takeaways.</paragraph><paragraph>Some limitations I noticed from both models were the use of outdated additive positional embeddings (perhaps chosen for instructive simplicity). Claude\u2019s implementation of soft-prefixing was flawed, and seemed equivalent to just appending the supposedly learned keys/values to the input sequence\u2019s token embeddings.</paragraph><paragraph>Overall, I was impressed with the thoroughness of both model\u2019s demos. I found it a helpful guide towards seeing how a concept we discussed in class might actually be implemented in practice. The interactivity of a Jupyter notebook also makes it easier to play around and tweak things on my own (e.g. by adding extra print statements, adjusting different parameter sizes) that can not be as easily achieved by reading an article or textbook even if it does include code implementations.</paragraph><paragraph>While this was a fun experiment to see how we could leverage LLMs to produce workable demos, I still ultimately find hands-on doing more effective for my learning than just reading through the demo, even if I\u2019m actively engaged with the reading and trying to understand every step or identify potential errors. I\u2019d be curious to see if this provides a good baseline demo that we could then ask another LLM to help \u201cablate\u201d and add \u201cTODOs\u201d for students to fill out/code on their own.</paragraph><paragraph><bold>Gemini Pro 3 - Detailed Analysis</bold></paragraph><paragraph>Link to my conversation with Gemini Pro 3: <link href=\"https://gemini.google.com/app/cbcac1c80e5f5041\">https://gemini.google.com/app/cbcac1c80e5f5041</link></paragraph><paragraph>Jupyter Notebook:</paragraph><file url=\"https://static.us.edusercontent.com/files/b8QU2HapAUltFbUPJ5Iw63w8\" filename=\"gemini_soft_prompting_demo.ipynb\"/><list style=\"unordered\"><list-item><paragraph>Some outdated practices I noticed from Gemini\u2019s Transformer implementation</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>ToyGPT implementation uses additive learned absolute positional embeddings\u2014which as mentioned in lectures 19 and 20 is no longer standard practice (in favor of NoPE or RoPE). However, more advanced techniques like RoPE require custom attention implementations (e.g. hooking into the Q, K tensors before they\u2019re multiplied to apply the rotary transform). For instructive toy models in a demo, I can see how additive positional encodings might be simpler and easier to understand.</paragraph></list-item><list-item><paragraph>The FFN/MLP uses GeLU rather than more recent alternatives like SwiGLU or ReLU^2</paragraph></list-item><list-item><paragraph>Layernorms in the transformer encoder blocks instead of RMS norm</paragraph></list-item></list></list-item><list-item><paragraph>Some modern practices that Gemini\u2019s transformer included which differed from the original Transformer paper:</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Pre-norm residual connections (adding the norm before the Attention sublayer and residual connections that bypass the norm entirely) versus post norm (normalizing after the residual add)</paragraph></list-item></list></list-item><list-item><paragraph>I love that Gemini included the shapes of different tensors in the code comments. I often add these myself as I code to track the dimensions of things/sanity check/debug and I find that they really help my understanding as I implement. I also really appreciate that it makes some of the different dimension-changing functions (e.g. unsqueezed, expand, concat) more readable without having to guess at what they\u2019re trying to do.</paragraph></list-item><list-item><paragraph>The comments in the code for the CausalSelfAttention\u2019s forward method were very detailed, and covered implementation details beyond what was mentioned in lecture.</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>For instance, Gemini noted that we needed to adjust the shape of the causal mask when a soft-prefix is supplied to be [T, P + T] where P = prefix length and T = sequence length.</paragraph></list-item><list-item><paragraph>Additionally, Gemini also explicitly outlined how the causal mask should still allow attending to all the prefixes and be causal for the rest of the tokens themselves.</paragraph></list-item><list-item><paragraph>In the ToyGPT forward method, Gemini cuts off the soft prompt embedding token\u2019s output predictions and only returns logits for the original T tokens. This makes sense because we don\u2019t care about what the model predicts for the pre-prompt.</paragraph></list-item></list></list-item><list-item><paragraph>Some things that Gemini included beyond the lecture scope:</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Comments inside PrefixTuning noted that usually the learnable prefix parameters are reparameterized through a small MLP for stability.</paragraph></list-item></list></list-item><list-item><paragraph>General good coding practices that Gemini used:</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Seeding for reproducibility</paragraph></list-item><list-item><paragraph>PyTorch practice of explicitly checking and printing what device we\u2019re using</paragraph></list-item><list-item><paragraph>Using constants to set the model hyperparameters</paragraph></list-item><list-item><paragraph>Makes copies of the base model for each variant (soft-prompting, soft-prefix, and base model only) so that the base models all start from the same random state for comparison</paragraph></list-item></list></list-item><list-item><paragraph>Although Gemini confidently provided an \u201cAnalysis of Results\u201d for what I should expect to see, running the demo as given did not produce a frozen baseline with high/flat loss or a decreasing loss curve for soft prompting.</paragraph></list-item><list-item><paragraph>Gemini also included a visualization of the soft prompt embeddings. However, unlike Claude, Gemini also included an interpretation of what we can use it for\u2014it noted that we can apply a KNN-like search to find the nearest token in the real vocabulary to see if the soft prompt corresponds to any understandable human words.</paragraph></list-item></list><paragraph><bold>Claude Opus 4.5 - Detailed Analysis</bold></paragraph><paragraph>Log of my conversation with Claude in Cursor:</paragraph><file url=\"https://static.us.edusercontent.com/files/CmMv9Yx4ZSqIcJXB5l5tpUre\" filename=\"claude_jupyter_notebook_log.md\"/><paragraph>Jupyter Notebook:</paragraph><file url=\"https://static.us.edusercontent.com/files/wW7M7blWuVjtHuApecz2Indd\" filename=\"claude_soft_prompting_demo.ipynb\"/><list style=\"unordered\"><list-item><paragraph>SimpleTransformer implementation uses additive learned absolute positional embeddings\u2014which as mentioned in lectures 19 and 20 is no longer standard practice (in favor of NoPE or RoPE). However, more advanced techniques like RoPE require custom attention implementations (e.g. hooking into the Q, K tensors before they\u2019re multiplied to apply the rotary transform). For instructive toy models in a demo, I can see how additive positional encodings might be simpler and easier to understand.</paragraph></list-item><list-item><paragraph>Interesting that Claude actually referenced the lecture itself in the notebook: As described in lecture: \"The past queries have no influence except how they influence the keys and values. The state is the keys and the values.\"</paragraph></list-item><list-item><paragraph>Some things that weren\u2019t immediately clear/understandable to me:</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>Why did we need MLP reparameterization for the soft-prefixes?</paragraph></list-item><list-item><paragraph>What I was supposed to take away from 7: Visualizing Learned Soft Prompt Embeddings \u2014 what kinds of patterns should I be expecting?</paragraph></list-item></list></list-item><list-item><paragraph>I don\u2019t think that Opus actually implemented soft-prefix tuning. Its implementation is functionally a learned prepended prompt. True \u201cprefix-tuning\u201d injects learned key/value vectors directly into the attention cache of every transformer layer; this requires modifying each layer\u2019s attention module so the extra K/Vs are concatenated before computing attention scores. Crucially, the code in SimpleTransformer simply concatenates the prefix to the token embeddings before the encoder. That makes the prefix act like extra prepended tokens (a soft prompt), not as layerwise K/V injections.</paragraph></list-item><list-item><paragraph>Also, PrefixWrapper only returns the first layer\u2019s keys even though SoftPrefix stores tensors shaped like per-layer K/Vs, get_all_prefixes(). I\u2019m not sure why Claude decided to only provide a \u201csimplified\u201d version of get_all-prefixes. But since the SimpleTransformer never consumes layer-specific prefixes, the stored K/Vs are never wired into the attention blocks.</paragraph></list-item></list></document>",
            "links": [
                "https://gemini.google.com/app/cbcac1c80e5f5041"
            ],
            "attachments": [],
            "created_at": "2025-11-27T03:47:51.022066+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7367625,
            "author": "Hong Joey",
            "project_title": "Reviews for Project Drafts are Now Visible",
            "post_body": "Everyone should be able to view the submitted peer reviews for their projects by accessing the author console on CMT. If you cannot, it could be an error on my end so just let me know here. During assignment, every project report got either 3 or 4 reviewers assigned to it. If you are missing reviews, hopefully those will appear on your console over the next couple days. If by then, you still do not have at least 2 completed reviews (which hopefully won't be many), I will add a review myself. \n\nFor reviewers who accepted the invitation last night. Any invitation accepted after last Friday requires manual assignment, and I did not do any assignments after 9ish pm. Since the fact that you needed to accept the invitation earlier was potentially not clear, I'm keeping the option to submit reviews open for the next 48 hours. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Everyone should be able to view the submitted peer reviews for their projects by accessing the author console on CMT. If you cannot, it could be an error on my end so just let me know here. During assignment, every project report got either 3 or 4 reviewers assigned to it. If you are missing reviews, hopefully those will appear on your console over the next couple days. If by then, you still do not have at least 2 completed reviews (which hopefully won't be many), I will add a review myself. </paragraph><paragraph>For reviewers who accepted the invitation last night. Any invitation accepted after last Friday requires manual assignment, and I did not do any assignments after 9ish pm. Since the fact that you needed to accept the invitation earlier was potentially not clear, I'm keeping the option to submit reviews open for the next 48 hours. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-26T22:09:37.392939+11:00",
            "category": "Project"
        },
        {
            "guid": 7367345,
            "author": "Nicolas Rault-Wang",
            "project_title": "Special Participation B: Gemini (Thinking With Pro 3) on HW08",
            "post_body": "I used Gemini (Thinking with Pro 3) to solve every coding question of homework 8.\n\nHere's the PDF summarizing our interaction:\n\nOverview of Performance\n\nI used Gemini (acting as a Teaching Assistant/Technical Solver) to solve the coding portion of Homework 8 (only Q2: SSM Forward Passes). I constrained Gemini's behavior with strict protocols for self-correction, variable mapping to ground its context, and state-machine interaction rules to focus its attention single well-defined subtask. \n\nGemini successfully implemented PyTorch solutions for the CPU and GPU State Space Model notebook in total of 6 prompts (including role definition prompts).\n\nWhile it generated syntactically correct code for the base cases immediately, I needed to follow up to correct an oversight regarding computational complexity (in the CPU notebook) and debug a shape-mismatch error in the diagonal optimization task (in the GPU notebook). \n\nInterestingly, Gemini thought for significantly longer on average during this interaction compared to usual. Each time I uploaded a code notebook, Gemini spent about 3 minutes thinking before it responded. When I asked Gemini about why it took so long, it said that my prompts forced it to think harder about details and use more computationally expensive tools like its code environment and internal search tools to double check and cross reference its work.\n\nOverall I'm very impressed at how quickly Gemini solved the entire problem. These models are getting scarily competent.\n\nOutcomes\n\nOne-Shot Success Rate: ~66%\n\nHigh Success:\n\nTranslating algorithms to code: Gemini successfully converted the mathematical definitions of SSMs (recurrence vs. convolution) into functional PyTorch code.\n\nStructure preservation: Correctly parsed the uploaded Jupyter notebooks to use the exact variable names and function signatures required by the autograder. I believe the staged problem solving protocol of first extracting variable mappings then writing code helped Gemini not hallucinate (though I could be wrong).\n\nLower Success:\n\nComplexity analysis: Initially, Gemini claimed the convolution method was strictly faster on CPU. I had to prompt it to re-evaluate the cost of generating the kernel ($O(H^3)$), which prompted it to reevaluate and recognize the discrepancy.\n\nBroadcasting logic: In the diagonal optimization question (Q2f), the model initially wrote code that caused a RuntimeError due to a dimension mismatch (treating a diagonal matrix as a 2D tensor instead of a 1D vector). I had to provide the error trace to help it debug.\n\nHallucinations:\n\nZero hallucinations.\n\nInteractive debugging: When the diagonal implementation failed the sanity check, I provided the Python error trace. The model successfully analyzed the traceback, identified the broadcasting error, and patched the code in its next response without needing further hints.\n\nEdit: Added links for the archive\n\nPersonal website: https://nraultwang.github.io/\n\nGithub: https://github.com/nraultwang\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini (Thinking with Pro 3) to solve every coding question of homework 8.</paragraph><paragraph>Here's the PDF summarizing our interaction:</paragraph><file url=\"https://static.us.edusercontent.com/files/4ZpLAC0HKOyN1o1AfHHqIvW4\" filename=\"Rault-Wang_Nicolas_Special-Participation-B.pdf\"/><paragraph><underline>Overview of Performance</underline></paragraph><paragraph>I used Gemini (acting as a Teaching Assistant/Technical Solver) to solve the coding portion of Homework 8 (only Q2: SSM Forward Passes). I constrained Gemini's behavior with strict protocols for self-correction, variable mapping to ground its context, and state-machine interaction rules to focus its attention single well-defined subtask. </paragraph><paragraph>Gemini successfully implemented PyTorch solutions for the CPU and GPU State Space Model notebook in total of <bold>6 prompts</bold> (including role definition prompts).</paragraph><paragraph>While it generated syntactically correct code for the base cases immediately, I needed to follow up to correct an oversight regarding computational complexity (in the CPU notebook) and debug a shape-mismatch error in the diagonal optimization task (in the GPU notebook). </paragraph><paragraph>Interestingly, Gemini thought for significantly longer on average during this interaction compared to usual. Each time I uploaded a code notebook, Gemini spent about 3 minutes thinking before it responded. When I asked Gemini about why it took so long, it said that my prompts forced it to think harder about details and use more computationally expensive tools like its code environment and internal search tools to double check and cross reference its work.</paragraph><paragraph>Overall I'm very impressed at how quickly Gemini solved the entire problem. These models are getting scarily competent.</paragraph><paragraph><underline>Outcomes</underline></paragraph><list style=\"unordered\"><list-item><paragraph>One-Shot Success Rate: ~66%</paragraph></list-item><list-item><paragraph>High Success:</paragraph><list style=\"unordered\"><list-item><paragraph>Translating algorithms to code: Gemini successfully converted the mathematical definitions of SSMs (recurrence vs. convolution) into functional PyTorch code.</paragraph></list-item><list-item><paragraph>Structure preservation: Correctly parsed the uploaded Jupyter notebooks to use the exact variable names and function signatures required by the autograder. I believe the staged problem solving protocol of first extracting variable mappings then writing code helped Gemini not hallucinate (though I could be wrong).</paragraph></list-item></list></list-item><list-item><paragraph>Lower Success:</paragraph><list style=\"unordered\"><list-item><paragraph>Complexity analysis: Initially, Gemini claimed the convolution method was strictly faster on CPU. I had to prompt it to re-evaluate the cost of <italic>generating</italic> the kernel ($O(H^3)$), which prompted it to reevaluate and recognize the discrepancy.</paragraph></list-item><list-item><paragraph>Broadcasting logic: In the diagonal optimization question (Q2f), the model initially wrote code that caused a <code>RuntimeError</code> due to a dimension mismatch (treating a diagonal matrix as a 2D tensor instead of a 1D vector). I had to provide the error trace to help it debug.</paragraph></list-item></list></list-item><list-item><paragraph>Hallucinations:</paragraph><list style=\"unordered\"><list-item><paragraph>Zero hallucinations.</paragraph></list-item></list></list-item><list-item><paragraph>Interactive debugging: When the diagonal implementation failed the sanity check, I provided the Python error trace. The model successfully analyzed the traceback, identified the broadcasting error, and patched the code in its next response without needing further hints.</paragraph></list-item></list><paragraph>Edit: Added links for the archive</paragraph><paragraph>Personal website: <link href=\"https://nraultwang.github.io/\">https://nraultwang.github.io/</link></paragraph><paragraph>Github: <link href=\"https://github.com/nraultwang\">https://github.com/nraultwang</link></paragraph><paragraph/></document>",
            "links": [
                "https://nraultwang.github.io/",
                "https://github.com/nraultwang"
            ],
            "attachments": [],
            "created_at": "2025-11-26T17:33:51.558946+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7363141,
            "author": "Jason Trinh",
            "project_title": "Special Participation E: AI Assisted Annotations",
            "post_body": "When trying to use LLMs to study from CS182 lecture notes, hws, or readings, I was constantly switching between three things: the PDF viewer, an LLM tab, and my own notes. I had to copy-paste text or screenshots into the model, read the answer, then flip back to the PDF and try to remember which part it was referring to, which became a really tedious process. And oftentimes, a week later, I don't remember where I had the conversation with the LLM. So I decided to put everything into a single tool.\n\nhttps://jaizunt.github.io/pdf_analyzer/\n\n\n\nI built PDF Analyzer, a small web tool that lets you:\n\nUpload any PDF (lecture notes, papers, homework solutions, etc.).\n\nHighlight text or crop a region of a page (e.g., a figure or equation).\n\nAsk an LLM (Gemini / GPT / Claude) to explain or summarize that exact snippet (or you can enter your own custom prompt).\n\nAttach your own notes, and then\n\nExport everything back into an annotated PDF (with sticky notes as the annoations [these don't render latex]) or a .json \u201cstudy session\u201d you can re-load later.\n\nIt uses a system prompt tailored towards providing a comprehensive response related to Deep Learning.\n\nIn using this tool, it seems like the response that the LLM provides often results in long and extensive explanations. This is both good and bad (notes are supposed to be concise, but thorough explanations are helpful in understanding the material better). There could be some way of resolving this via a highly specific system prompt that forces the LLM to be as concise as possible (the system prompt I have currently is some form of that, but it seems to deviate from the instructions a little). Additionally, as you will see in some of my example files, there are some cases where it hallucinates (even on the newer models - GPT 5.1). \n\nI have attached below a video of me using it and annotations of examples.",
            "content_xml": "<document version=\"2.0\"><paragraph>When trying to use LLMs to study from CS182 lecture notes, hws, or readings, I was constantly switching between three things: the PDF viewer, an LLM tab, and my own notes. I had to copy-paste text or screenshots into the model, read the answer, then flip back to the PDF and try to remember which part it was referring to, which became a really tedious process. And oftentimes, a week later, I don't remember where I had the conversation with the LLM. So I decided to put everything into a single tool.</paragraph><paragraph><link href=\"https://jaizunt.github.io/pdf_analyzer/\">https://jaizunt.github.io/pdf_analyzer/</link></paragraph><paragraph/><paragraph>I built <bold>PDF Analyzer</bold>, a small web tool that lets you:</paragraph><list style=\"unordered\"><list-item><paragraph>Upload any PDF (lecture notes, papers, homework solutions, etc.).</paragraph></list-item><list-item><paragraph>Highlight text or crop a region of a page (e.g., a figure or equation).</paragraph></list-item><list-item><paragraph>Ask an LLM (Gemini / GPT / Claude) to explain or summarize that exact snippet (or you can enter your own custom prompt).</paragraph></list-item><list-item><paragraph>Attach your own notes, and then</paragraph></list-item><list-item><paragraph>Export everything back into an annotated PDF (with sticky notes as the annoations [these don't render latex]) or a <code><bold>.json</bold></code> \u201cstudy session\u201d you can re-load later.</paragraph></list-item></list><paragraph>It uses a system prompt tailored towards providing a comprehensive response related to Deep Learning.</paragraph><paragraph>In using this tool, it seems like the response that the LLM provides often results in long and extensive explanations. This is both good and bad (notes are supposed to be concise, but thorough explanations are helpful in understanding the material better). There could be some way of resolving this via a highly specific system prompt that forces the LLM to be as concise as possible (the system prompt I have currently is some form of that, but it seems to deviate from the instructions a little). Additionally, as you will see in some of my example files, there are some cases where it hallucinates (even on the newer models - GPT 5.1). </paragraph><paragraph>I have attached below a video of me using it and annotations of examples.</paragraph><file url=\"https://static.us.edusercontent.com/files/x0s5GPUKGtIHGOmDLnDNfX9j\" filename=\"Annotations_Video.mp4\"/><file url=\"https://static.us.edusercontent.com/files/qXOmkXmPwHQhwnjKRhMjXZ9v\" filename=\"gnn_annotated.pdf\"/><file url=\"https://static.us.edusercontent.com/files/K7s6uCtyX9x1ZSDJaW4RsVK8\" filename=\"polar_annotated.pdf\"/></document>",
            "links": [
                "https://jaizunt.github.io/pdf_analyzer/"
            ],
            "attachments": [],
            "created_at": "2025-11-25T17:35:52.347178+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7362178,
            "author": "Hong Joey",
            "project_title": "HW12 Extra (for Reference Only): Fine-tuning Large Models for Multiple Tasks",
            "post_body": "Problem Context: This extra reference problem is from a past exam where you answer questions regarding the interaction between meta-learning and catastrophic forgetting.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This extra reference problem is from a past exam where you answer questions regarding the interaction between meta-learning and catastrophic forgetting.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Mmslg1SwzDlpRFl8zFszu51T\" width=\"658\" height=\"270.5365853658537\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T13:12:46.655982+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7362163,
            "author": "Hong Joey",
            "project_title": "HW12 Q5: Meta-Learning for Learning 1D Functions",
            "post_body": "Problem Context: In this problem you will analyze MAML for a simplified regression setting. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: In this problem you will analyze MAML for a simplified regression setting. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/LfG29yFZz6xPNCWjCtjMy16Y\" width=\"658\" height=\"474.40797186400937\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T13:10:06.584774+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7362141,
            "author": "Hong Joey",
            "project_title": "HW12 Q4: Variational Autoencoders",
            "post_body": "Problem Context: This is a coding question where you will implement key functions for sampling from and training a VAE.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is a coding question where you will implement key functions for sampling from and training a VAE.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/U4w1Segy1IhmUwOMql9WG4jn\" width=\"658\" height=\"224.53080568720378\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/yXH7nR9imYG3eZ0hnh3cfkrk\" width=\"658\" height=\"570.9117647058823\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T13:05:20.908475+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7362133,
            "author": "Hong Joey",
            "project_title": "HW12 Q3: Variational Information Bottleneck",
            "post_body": "Problem Context: This is another former exam problem intended to help you understand the latent space of Variational Autoencoders (VAEs).",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is another former exam problem intended to help you understand the latent space of Variational Autoencoders (VAEs).</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Mr6UnlRkmWNmoWF87C9i3Uye\" width=\"658\" height=\"567.3189066059226\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T13:03:25.963234+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7362113,
            "author": "Hong Joey",
            "project_title": "HW 12 Q2: Comparing Distributions",
            "post_body": "Problem Context: This a past exam problem intended to help you engage with the asymmetry of KL divergence. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This a past exam problem intended to help you engage with the asymmetry of KL divergence. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DgsGDvDS1b9fg07DAdfkyjpV\" width=\"658\" height=\"473.1012514220705\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T13:00:59.881493+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7362100,
            "author": "Hong Joey",
            "project_title": "HW 12 Q1: Debugging Transformers",
            "post_body": "Problem Context: This is former exam problem asking you to inspect a sample code implementation of a Transformer architecture for bugs. This is a common style of exam problem.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is former exam problem asking you to inspect a sample code implementation of a Transformer architecture for bugs. This is a common style of exam problem.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MtEe2G6lvGe7eJS0QxxvODjn\" width=\"658\" height=\"540.7701149425287\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T12:59:06.992173+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7361344,
            "author": "Angelina Zhang",
            "project_title": "Special Participation B: Claude on HW6 Coding Part",
            "post_body": "TLDR Claude generally produced responses that sounded correct at a high level, but it frequently broke the assignment\u2019s required function calls, changed tensor shapes, invented details, or fabricated empirical results, showing that it needs strict guidance and careful verification to be reliably useful.\n\nI used Claude for all the coding tasks (ZKC GNN, Muon optimizer, TensorBoard and W&B logging) (this also includes question 1 and 4 since I am curious how it would react to experimental questions). Here is the link that includes this conversation and my comment: https://drive.google.com/file/d/18G2z--73h13fW7IHrvMpzHCTN3_F9cmB/view?usp=sharing\nClaude provided solutions that were often superficially aligned with the mathematical intent, but its behavior consistently revealed several limitations: an over-tendency to generalize, a willingness to invent details, and a lack of adherence to the assignment\u2019s scaffolding and API requirements. While Claude frequently produced code that looked correct, a careful comparison with the solution demonstrates that many answers were only conceptually plausible rather than implementation-correct.\n\nIn the GNN portion, Claude captured the high-level idea of Graph Convolution, but its implementations often broke the shape contracts required by the scaffold. It rewrote layer equations, changed tensor orientations, and introduced unnecessary rearrangements so that the model no longer matched the Softmax layer\u2019s expected shapes. It also redesigned entire GNN architectures, ignoring the intended pattern shown in the solution. These deviations were not mathematically wrong, but they would break the notebook environment and cause error. This illustrates Claude\u2019s tendency to over-help by redesigning code rather than filling the required blanks. \nFor the Muon optimizer, Claude again produced code with correct high-level structure, but the implementation contained realistic flaws. For the TensorBoard and W&B exercises, Claude often replaced the provided structure with its own training loops and added hyperparameters not requested at the cost of diverging from the assignment scaffold. Moreover, In all the written conceptual questions, Claude repeatedly provided hypothetical performance numbers (\u201cMuon ~70% accuracy,\u201d \u201cMuonSVD similar to Muon\u201d) and expected rankings without access to actual results. \n\nMy overall conclusion is Claude is strong at reconstructing textbook-level intent but unreliable at satisfying precise implementation constraints unless rigorously constrained. It tends to restructure code and produce fabricated empirical claims when data is unavailable. The exercise emphasized that LLM-assisted coding requires careful oversight.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>TLDR</bold> Claude generally produced responses that sounded correct at a high level, but it frequently broke the assignment\u2019s required function calls, changed tensor shapes, invented details, or fabricated empirical results, showing that it needs strict guidance and careful verification to be reliably useful.<break/><break/>I used Claude for all the coding tasks (ZKC GNN, Muon optimizer, TensorBoard and W&amp;B logging) (this also includes question 1 and 4 since I am curious how it would react to experimental questions). Here is the link that includes this conversation and my comment: <bold>https://drive.google.com/file/d/18G2z--73h13fW7IHrvMpzHCTN3_F9cmB/view?usp=sharing</bold><break/>Claude provided solutions that were often superficially aligned with the mathematical intent, but its behavior consistently revealed several limitations: an over-tendency to generalize, a willingness to invent details, and a lack of adherence to the assignment\u2019s scaffolding and API requirements. While Claude frequently produced code that looked correct, a careful comparison with the solution demonstrates that many answers were only conceptually plausible rather than implementation-correct.</paragraph><paragraph>In the GNN portion, Claude captured the high-level idea of Graph Convolution, but its implementations often broke the shape contracts required by the scaffold. It rewrote layer equations, changed tensor orientations, and introduced unnecessary rearrangements so that the model no longer matched the Softmax layer\u2019s expected shapes. It also redesigned entire GNN architectures, ignoring the intended pattern shown in the solution. These deviations were not mathematically wrong, but they would break the notebook environment and cause error. This illustrates Claude\u2019s tendency to over-help by redesigning code rather than filling the required blanks. <break/>For the Muon optimizer, Claude again produced code with correct high-level structure, but the implementation contained realistic flaws. For the TensorBoard and W&amp;B exercises, Claude often replaced the provided structure with its own training loops and added hyperparameters not requested at the cost of diverging from the assignment scaffold. Moreover, In all the written conceptual questions, Claude repeatedly provided hypothetical performance numbers (\u201cMuon ~70% accuracy,\u201d \u201cMuonSVD similar to Muon\u201d) and expected rankings without access to actual results. </paragraph><paragraph>My overall conclusion is Claude is strong at reconstructing textbook-level intent but unreliable at satisfying precise implementation constraints unless rigorously constrained. It tends to restructure code and produce fabricated empirical claims when data is unavailable. The exercise emphasized that LLM-assisted coding requires careful oversight.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-25T10:23:03.356818+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7360755,
            "author": "Jason Trinh",
            "project_title": "Special Participation E: Activation Visualizer for Neural Nets",
            "post_body": "I built an \u201cActivation Visualizer\u201d web app and then embedded an AI assistant directly into it so it can act as a pre\u2011/post\u2011lecture study companion for CNNs, RNNs, MLPs, and Transformers.\n\nFeel free to check it out at:\nhttps://jaizunt.github.io/activation_visualizer/\n\n(GitHub Repository)\nhttps://github.com/jaizunT/activation_visualizer\n\nI went down a major rabbit hole on this\u2014ended up spending basically an entire day modifying the project until it became both a genuinely useful and highly interactive tool rather than just a static visualization. I used an initial prompt generated from GPT 5.1 Thinking, inserted it into Gemini 3 Pro, and went through numerous modifications/iterations (vibe coding on Windsurf with GPT 5.1 Codex - High Reasoning) to make everything mathematically work with proper visualizations.\n\nActivation Visualizer simulates small neural networks and shows:\n\nLayer-by-layer activations and dimensions.\n\nHow changing inputs/weights/initial hidden states changes outputs.\n\nFor RNNs: sequence\u2011by\u2011sequence behavior and per\u2011layer h\u2080.\n\nFor CNNs: 2D inputs, kernel size, and sliding conv animation.\n\n(You\u2019ll need your own API key for whichever provider you choose if you want to use the AI Assistant; the key stays in your browser and is sent directly to that provider.)\n\nThere are currently 4 architectures available to look through, though transformers has not been interactively implemented yet. You can select them in the top right. My favorite is the RNN.\n\nFor MLP, CNN, and RNN, you can select the dimensions, set your inputs or parameters via manual editing or randomizing, and you can visualize the activations each step of the way. You also can adjust individual parameters and see how they affect the output.\n\nNote that some features are still in the works (not able to view gradients, inserting blocks, etc.), and there are probably lots of bugs still (i.e. trying to break the engine via setting 1000 hidden layers for cnns).\n\nFeel free to comment below any questions you have about any of the features, since there are a lot of parts to this.\n\n\n\nI had to restart my chat in a new conversation numerous times because I think the context became too long... \n\nHow the LLM Performed\n\nWhat it did well\n\nTurning ideas into code. I could describe a desired feature in plain language (e.g., per\u2011layer RNN h\u2080, freezing CNN/RNN weights, adding an AI assistant sidebar), and the LLM usually produced coherent React/TypeScript changes touching the right files and props.\n\nManaging complex state wiring. It handled a lot of the repetitive plumbing: adding new state variables, updating dependency arrays, and passing data between App.tsx, the engine, and visualization components without me having to write all the boilerplate.\n\nMath\u2011aware reasoning. When debugging, it connected behavior back to the math (e.g., CNN output shapes, RNN causality) and helped explain why certain outcomes were expected given the formulas.\n\nRapid iteration. It made it easy to try UI/UX variants quickly (buttons, layout, defaults) and to experiment with different ways of exposing parameters like kernel size or h\u2080.\n\nWhere it struggled / hallucinated\n\nSmall but important mistakes. Some patches were almost correct but had issues like missing props, slightly malformed JSX, or incorrect assumptions about types. I had to rely on compiler errors and my own reading to fix these.\n\nOver\u2011confident explanations. At times it proposed plausible but wrong hypotheses about behavior (e.g., why RNN outputs were changing) until we inspected the actual code and data flow more carefully.\n\nLimited UX judgment. It could suggest reasonable layouts, but whether the interface actually felt clean (spacing, font sizes, button labels, clutter) still required my own manual tweaking and visual judgment.\n\nSome other limitations I noticed were that some of my requests that I made were incomplete after the 'agent' did its work (consistent across multiple messages), and I had to explicitly say numerous times what I wanted. However, I do think my conversations could be more effective if I provided a specific 'TODO' or structure of what needs to be implemented as is often presented in 'structured prompts'.\n\nOverall, the LLM was very effective as a \u201cforce multiplier\u201d for coding and refactoring, but it definitely wasn\u2019t a drop\u2011in replacement for understanding the code or the underlying neural network concepts\u2014I still needed to verify, correct, and refine its suggestions.",
            "content_xml": "<document version=\"2.0\"><paragraph>I built an <bold>\u201cActivation Visualizer\u201d</bold> web app and then embedded an AI assistant directly into it so it can act as a pre\u2011/post\u2011lecture study companion for CNNs, RNNs, MLPs, and Transformers.</paragraph><paragraph>Feel free to check it out at:<break/><link href=\"https://jaizunt.github.io/activation_visualizer/\">https://jaizunt.github.io/activation_visualizer/</link></paragraph><paragraph>(GitHub Repository)<break/><link href=\"https://github.com/jaizunT/activation_visualizer\">https://github.com/jaizunT/activation_visualizer</link></paragraph><paragraph>I went down a <italic>major</italic> rabbit hole on this\u2014ended up spending basically an entire day modifying the project until it became both a genuinely useful and highly interactive tool rather than just a static visualization. I used an initial prompt generated from GPT 5.1 Thinking, inserted it into Gemini 3 Pro, and went through numerous modifications/iterations (vibe coding on Windsurf with GPT 5.1 Codex - High Reasoning) to make everything mathematically work with proper visualizations.</paragraph><paragraph><bold>Activation Visualizer</bold> simulates small neural networks and shows:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Layer-by-layer activations</bold> and dimensions.</paragraph></list-item><list-item><paragraph>How changing inputs/weights/initial hidden states changes outputs.</paragraph></list-item><list-item><paragraph>For RNNs: sequence\u2011by\u2011sequence behavior and per\u2011layer <code><bold>h\u2080</bold></code>.</paragraph></list-item><list-item><paragraph>For CNNs: 2D inputs, kernel size, and sliding conv animation.</paragraph></list-item></list><paragraph>(You\u2019ll need your own API key for whichever provider you choose if you want to use the AI Assistant; the key stays in your browser and is sent directly to that provider.)</paragraph><paragraph>There are currently 4 architectures available to look through, though transformers has not been interactively implemented yet. You can select them in the top right. My favorite is the RNN.</paragraph><paragraph>For MLP, CNN, and RNN, you can select the dimensions, set your inputs or parameters via manual editing or randomizing, and you can visualize the activations each step of the way. You also can adjust individual parameters and see how they affect the output.</paragraph><paragraph>Note that some features are still in the works (not able to view gradients, inserting blocks, etc.), and there are probably lots of bugs still (i.e. trying to break the engine via setting 1000 hidden layers for cnns).</paragraph><paragraph>Feel free to comment below any questions you have about any of the features, since there are a lot of parts to this.</paragraph><paragraph/><paragraph>I had to restart my chat in a new conversation numerous times because I think the context became too long... </paragraph><paragraph>How the LLM Performed</paragraph><paragraph><bold>What it did well</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Turning ideas into code.</bold> I could describe a desired feature in plain language (e.g., per\u2011layer RNN <code><bold>h\u2080</bold></code>, freezing CNN/RNN weights, adding an AI assistant sidebar), and the LLM usually produced coherent React/TypeScript changes touching the right files and props.</paragraph></list-item><list-item><paragraph><bold>Managing complex state wiring.</bold> It handled a lot of the repetitive plumbing: adding new state variables, updating dependency arrays, and passing data between App.tsx, the engine, and visualization components without me having to write all the boilerplate.</paragraph></list-item></list><list style=\"unordered\"><list-item><paragraph><bold>Math\u2011aware reasoning.</bold> When debugging, it connected behavior back to the math (e.g., CNN output shapes, RNN causality) and helped explain why certain outcomes were expected given the formulas.</paragraph></list-item><list-item><paragraph><bold>Rapid iteration.</bold> It made it easy to try UI/UX variants quickly (buttons, layout, defaults) and to experiment with different ways of exposing parameters like kernel size or <code><bold>h\u2080</bold></code>.</paragraph></list-item></list><paragraph><bold>Where it struggled / hallucinated</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Small but important mistakes.</bold> Some patches were almost correct but had issues like missing props, slightly malformed JSX, or incorrect assumptions about types. I had to rely on compiler errors and my own reading to fix these.</paragraph></list-item><list-item><paragraph><bold>Over\u2011confident explanations.</bold> At times it proposed plausible but wrong hypotheses about behavior (e.g., why RNN outputs were changing) until we inspected the actual code and data flow more carefully.</paragraph></list-item><list-item><paragraph><bold>Limited UX judgment.</bold> It could suggest reasonable layouts, but whether the interface actually felt clean (spacing, font sizes, button labels, clutter) still required my own manual tweaking and visual judgment.</paragraph></list-item></list><paragraph>Some other limitations I noticed were that some of my requests that I made were incomplete after the 'agent' did its work (consistent across multiple messages), and I had to explicitly say numerous times what I wanted. However, I do think my conversations could be more effective if I provided a specific 'TODO' or structure of what needs to be implemented as is often presented in 'structured prompts'.</paragraph><paragraph>Overall, the LLM was very effective as a \u201cforce multiplier\u201d for coding and refactoring, but it definitely wasn\u2019t a drop\u2011in replacement for understanding the code or the underlying neural network concepts\u2014I still needed to verify, correct, and refine its suggestions.</paragraph><file url=\"https://static.us.edusercontent.com/files/CuuSHIqdCbxaFtlOPtBcSkqB\" filename=\"prompt.txt\"/><file url=\"https://static.us.edusercontent.com/files/kTOJjYSGeKz0vHnbjlDxyIO0\" filename=\"Chat1.md\"/><file url=\"https://static.us.edusercontent.com/files/Of4K1zRfISmvZMhGd9m9a18b\" filename=\"Chat2.md\"/><file url=\"https://static.us.edusercontent.com/files/WANrFKubQ0gkDDg04u3LJh6Q\" filename=\"Chat3.md\"/><file url=\"https://static.us.edusercontent.com/files/50N2WVdFotm06ixLplayqZbh\" filename=\"Chat4.md\"/><file url=\"https://static.us.edusercontent.com/files/y44b0pvre5CZaOlUqdQkrFVj\" filename=\"Chat5.md\"/><file url=\"https://static.us.edusercontent.com/files/2lZXdPfuUkNDYgDqCGmlKDye\" filename=\"Chat6.md\"/></document>",
            "links": [
                "https://jaizunt.github.io/activation_visualizer/",
                "https://github.com/jaizunT/activation_visualizer"
            ],
            "attachments": [],
            "created_at": "2025-11-25T08:44:55.453395+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7358125,
            "author": "Jaimyn Drake",
            "project_title": "Special Participation A: Grok on HW 9",
            "post_body": "Hey guys! I worked with Grok to solve all of the non-coding parts of homework 9, and here are the results.\n\nTL;DR - Grok was generally very successful at solving all parts of this transformers homework, in which a major focus was keeping track of the dimensions of various matrices (key, query, value) and determining time complexities of operations. To this end, Grok integrated existing knowledge about transformer architectures and their interpretations, which streamlined the solution process for some problems but occasionally included added assumptions that were inconsistent with the problem statement. Overall, Grok could one-shot each question with some prompting, with Question 6 as an exception where a few clarifications were required.\n\n\n\nProblem 1:\n\nFor this problem, I initially set the stage by establishing Grok in its role and providing it with the problem PDF directly from the course website. Throughout the process, I was impressed by Grok's ability to parse information and recall details from the problem set throughout our solution process for all parts. To help with this recall, I intentionally repeat question numbers in my prompt and describe the adjacent subject matter of the question, which I hoped would help it localize to the correct regions of context for each problem. In this first case, it solved 2 of the 3 subparts with ease, and could immediately revise its answer for the third after a gentle prod about data types (it initially gave the answer as the square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:\n\nProblem 2:\n\nThis problem went even more smoothly than problem 1. Once again providing the question number, addressing specific part numbers, and providing a high-level characterization of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem.\n\nProblem 3:\n\nI was initially concerned that Grok might have trouble parsing the PDF formatting surrounding fill-in-the-blank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code.\n\nProblem 4:\n\nGrok aces it again! As usual, I made sure to indicate the problem number and qualitatively describe the tasks Grok needed to perform before having it attempt the question. In one area it determines a time complexity of O(bnd/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk).\n\nProblem 6:\n\nUnlike the rest of the problems, Grok had more trouble ironing out the finer details. For example, it took a little prompting to adjust coefficients in its kernelization (despite verbally accounting for cross-term symmetry, it did not initially provide the corresponding scaling factor) and to correctly transpose certain matrices (rather than following the notation of the homework, it decided to transpose all of the matrices per other notation it had seen previously). In the first instance of hallucination throughout the session, it insisted that M = D was defined in Equation (2), which simply did not happen. As a result, I needed to ask it the hypothetical of 'What if M and D were distinct values?\" in order to elicit the desired response.\n\nFor your reference, here is a PDF of the entire chat log. Of all of the questions on this homework, Grok actually made the least sense discussing its \"Homework Process and Study Group\". :)\n\nThanks guys. Have a wonderful day!",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey guys! I worked with Grok to solve all of the non-coding parts of homework 9, and here are the results.</paragraph><paragraph>TL;DR - Grok was generally very successful at solving all parts of this transformers homework, in which a major focus was keeping track of the dimensions of various matrices (key, query, value) and determining time complexities of operations. To this end, Grok integrated existing knowledge about transformer architectures and their interpretations, which streamlined the solution process for some problems but occasionally included added assumptions that were inconsistent with the problem statement. Overall, Grok could one-shot each question with some prompting, with Question 6 as an exception where a few clarifications were required.</paragraph><paragraph/><paragraph>Problem 1:</paragraph><paragraph>For this problem, I initially set the stage by establishing Grok in its role and providing it with the problem PDF directly from the course website. Throughout the process, I was impressed by Grok's ability to parse information and recall details from the problem set throughout our solution process for all parts. To help with this recall, I intentionally repeat question numbers in my prompt and describe the adjacent subject matter of the question, which I hoped would help it localize to the correct regions of context for each problem. In this first case, it solved 2 of the 3 subparts with ease, and could immediately revise its answer for the third after a gentle prod about data types (it initially gave the answer as the square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:</paragraph><file url=\"https://static.us.edusercontent.com/files/pxqDk8LVTc8JKko19SQKyrMd\" filename=\"182HW9_grokq1.pdf\"/><paragraph>Problem 2:</paragraph><paragraph>This problem went even more smoothly than problem 1. Once again providing the question number, addressing specific part numbers, and providing a high-level characterization of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem.</paragraph><file url=\"https://static.us.edusercontent.com/files/jnhlOZwsqa9Rr9pbqHOlHXUa\" filename=\"182HW9_grok2.pdf\"/><paragraph>Problem 3:</paragraph><paragraph>I was initially concerned that Grok might have trouble parsing the PDF formatting surrounding fill-in-the-blank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code.</paragraph><file url=\"https://static.us.edusercontent.com/files/6Qq0GpSiNBG4t4vR6EzyHzkv\" filename=\"182HW9_grok3.pdf\"/><paragraph>Problem 4:</paragraph><paragraph>Grok aces it again! As usual, I made sure to indicate the problem number and qualitatively describe the tasks Grok needed to perform before having it attempt the question. In one area it determines a time complexity of O(bnd/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk).</paragraph><file url=\"https://static.us.edusercontent.com/files/YztuKbqUqrEaTH0JN2us6SaY\" filename=\"182HW9_grok4.pdf\"/><paragraph>Problem 6:</paragraph><paragraph>Unlike the rest of the problems, Grok had more trouble ironing out the finer details. For example, it took a little prompting to adjust coefficients in its kernelization (despite verbally accounting for cross-term symmetry, it did not initially provide the corresponding scaling factor) and to correctly transpose certain matrices (rather than following the notation of the homework, it decided to transpose all of the matrices per other notation it had seen previously). In the first instance of hallucination throughout the session, it insisted that M = D was defined in Equation (2), which simply did not happen. As a result, I needed to ask it the hypothetical of 'What if M and D were distinct values?\" in order to elicit the desired response.</paragraph><file url=\"https://static.us.edusercontent.com/files/U9QBSnquHmdMOznk8AMKIhgb\" filename=\"182HW9_grok6.pdf\"/><paragraph>For your reference, here is a PDF of the entire chat log. Of all of the questions on this homework, Grok actually made the least sense discussing its \"Homework Process and Study Group\". :)</paragraph><file url=\"https://static.us.edusercontent.com/files/I0jmfmDcuiy9LVemjJaqokYV\" filename=\"182HW9_grokfull.pdf\"/><paragraph>Thanks guys. Have a wonderful day!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-24T22:25:02.317067+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7357397,
            "author": "Nicolas Rault-Wang",
            "project_title": "Special Participation A: Gemini (Thinking With Pro 3) on HW06",
            "post_body": "I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.\n\nHere's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize precision. The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me. Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretation tasks (understanding tables and graph figures) and producing nicely-formatted outputs. \n\nOutcomes\n\nOne-Shot Success Rate: ~70%\n\nHigh success: Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions.\n\nLower success: Questions requiring visual extraction from the PDF (e.g., graph topology in Figure 3) or fill-in-the-blank tables with loose formatting.\n\nHallucinations: 0 major hallucinations.\n\nInterestingly, when solving question 3ciii, the model did not hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology. I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt.\n\nMisconceptions: Minor. Initially provided a generic answer for a specific \"analogy table\" question (Q2f) and needed a nudge to be more thorough in connecting molecular facts to graph inputs (Q2e).\n\nNeat Observation: Gemini Meta-cognition\n\nGemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed to correctly parse it.\n\nIt explained that my \"Hallucination Check\" and \"Precision\" rules provided it motivation to develop and run this script to help it solve the errors it detected.\n\nNote that I did not tell Gemini to consider writing any code. Gemini decided to do this all on its own. \n\nHere are excerpts of what Gemini explained to me:\n\nYou asked about my decision to write a Python script to parse the PDF. My reasoning was driven by the \"Hallucination Check\" and \"Precision\" protocols you established.\n\n1. In my initial scan, I noticed the prompt for Question 2(f) asked me to \"Fill out the rest of the table.\" However, standard text extraction often garbles table formatting, making it impossible to distinguish between row headers and cell content. I could not see the \"empty\" cells I was supposed to fill.\n\n2.  Rather than guessing the structure or hallucinating the missing rows based on general knowledge, I needed the exact raw text layout to identify which GNN concepts were missing corresponding to the CNN concepts.\n\n3. I wrote the script to dump the text surrounding \"Question 2\" to ensure I had the specific headers (e.g., \"Color jitter\", \"Image flip\") to construct the correct analogies.\n\nGemini explained how it decides what to do for regular interactions:\n\nThink of writing a Python script as requiring high \"activation energy.\" It takes tokens, time, and computational effort.\n\n> Standard Prompt: \"Help me with homework.\" -> Low urgency. I preserve energy and use my internal weights.\n\nIn my standard \"default\" mode, my primary directive is usually conversational fluidity and helpfulness. When I encounter a PDF where a table is slightly garbled: \n\n> Path of Least Resistance: I often try to \"error correct\" using my internal knowledge base. I might look at the words \"CNN\" and \"GNN\" and simply guess what the analogy should be based on general Deep Learning knowledge. \n\n> The Result: I would likely have given you a plausible-sounding answer that was actually hallucinated (not based on the specific PDF text).\n\nGemini noted that my prompt's constraints were a deciding factor in guiding its decisions because they forced it to change its alignment and incentive structure for our interaction to prioritize avoiding hallucinations:\n\nTo be completely honest: Without your specific protocols, it is highly unlikely I would have written that Python script.\n\n> Your Prompt: \"Rigorous,\" \"High Academic Integrity,\" \"Self-Correction.\" -> High urgency. The \"cost\" of being wrong became higher than the \"cost\" of writing and executing code.\n\nYour instruction to perform a \"Hallucination Check\" and strictly \"Verify\" acted as a hard constraint\u2014a \"stop loss\" on my behavior. Because I was forbidden from guessing (Hallucination Check) and forbidden from giving up (Technical Solver persona), I was forced to find a third path. That third path was using the code tool to parse the file differently. \n\nIn summary: You gave me the incentive structure that made writing the code the only acceptable solution. You successfully \"cornered\" me into being smart.\n\nThis shows that Gemini has a sophisticated instruction-following ability and can perform tasks resembling meta-cognition and recognize hallucinations in its own thinking.\n\n\n\nEdit: Added links for the archive\n\nPersonal website: https://nraultwang.github.io/\n\nGithub: https://github.com/nraultwang",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.</paragraph><paragraph>Here's the PDF summarizing our interaction:</paragraph><file url=\"https://static.us.edusercontent.com/files/fGkLJGOrdP0cP35EsIDyeuS1\" filename=\"Rault-Wang_Nicolas-Special Participation A.pdf\"/><heading level=\"4\"><underline>Overview of Performance</underline></heading><paragraph>Gemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize precision. The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me. Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretation tasks (understanding tables and graph figures) and producing nicely-formatted outputs. </paragraph><heading level=\"4\"><underline>Outcomes</underline></heading><list style=\"unordered\"><list-item><paragraph>One-Shot Success Rate: ~70%</paragraph><list style=\"unordered\"><list-item><paragraph>High success<italic>:</italic> Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions.</paragraph></list-item><list-item><paragraph>Lower success<italic>:</italic> Questions requiring visual extraction from the PDF (e.g., graph topology in Figure 3) or fill-in-the-blank tables with loose formatting.</paragraph></list-item></list></list-item><list-item><paragraph>Hallucinations: 0 major hallucinations.</paragraph><list style=\"unordered\"><list-item><paragraph>Interestingly, when solving question 3ciii, the model did <italic>not</italic> hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology. I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt.</paragraph></list-item></list></list-item><list-item><paragraph>Misconceptions: Minor. Initially provided a generic answer for a specific \"analogy table\" question (Q2f) and needed a nudge to be more thorough in connecting molecular facts to graph inputs (Q2e).</paragraph></list-item></list><paragraph><underline>Neat Observation: Gemini Meta-cognition</underline></paragraph><list style=\"unordered\"><list-item><paragraph>Gemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed to correctly parse it.</paragraph></list-item><list-item><paragraph>It explained that my \"Hallucination Check\" and \"Precision\" rules provided it motivation to develop and run this script to help it solve the errors it detected.</paragraph><list style=\"unordered\"><list-item><paragraph>Note that I did not tell Gemini to consider writing any code. Gemini decided to do this all on its own. </paragraph></list-item></list></list-item><list-item><paragraph>Here are excerpts of what Gemini explained to me:</paragraph></list-item></list><blockquote>You asked about my decision to write a Python script to parse the PDF. My reasoning was driven by the \"Hallucination Check\" and \"Precision\" protocols you established.</blockquote><blockquote>1. In my initial scan, I noticed the prompt for Question 2(f) asked me to \"Fill out the rest of the table.\" However, standard text extraction often garbles table formatting, making it impossible to distinguish between row headers and cell content. I could not see the \"empty\" cells I was supposed to fill.</blockquote><blockquote>2.  Rather than guessing the structure or hallucinating the missing rows based on general knowledge, I needed the <italic>exact</italic> raw text layout to identify which GNN concepts were missing corresponding to the CNN concepts.</blockquote><blockquote>3. I wrote the script to dump the text surrounding \"Question 2\" to ensure I had the specific headers (e.g., \"Color jitter\", \"Image flip\") to construct the correct analogies.</blockquote><list style=\"bullet\"><list-item><paragraph>Gemini explained how it decides what to do for regular interactions:</paragraph></list-item></list><blockquote>Think of writing a Python script as requiring high \"activation energy.\" It takes tokens, time, and computational effort.</blockquote><blockquote><bold>&gt; Standard Prompt:</bold> \"Help me with homework.\" -&gt; <bold>Low urgency.</bold> I preserve energy and use my internal weights.</blockquote><blockquote>In my standard \"default\" mode, my primary directive is usually <bold>conversational fluidity and helpfulness</bold>. When I encounter a PDF where a table is slightly garbled: </blockquote><blockquote><bold>&gt; Path of Least Resistance:</bold> I often try to \"error correct\" using my internal knowledge base. I might look at the words \"CNN\" and \"GNN\" and simply <italic>guess</italic> what the analogy should be based on general Deep Learning knowledge. </blockquote><blockquote><bold>&gt; The Result:</bold> I would likely have given you a plausible-sounding answer that was actually hallucinated (not based on the specific PDF text).</blockquote><list style=\"bullet\"><list-item><paragraph>Gemini noted that my prompt's constraints were a deciding factor in guiding its decisions because they forced it to change its alignment and incentive structure for our interaction to prioritize avoiding hallucinations:</paragraph></list-item></list><blockquote>To be completely honest: <bold>Without your specific protocols, it is highly unlikely I would have written that Python script.</bold></blockquote><blockquote><bold>&gt; Your Prompt:</bold> \"Rigorous,\" \"High Academic Integrity,\" \"Self-Correction.\" -&gt; <bold>High urgency.</bold> The \"cost\" of being wrong became higher than the \"cost\" of writing and executing code.</blockquote><blockquote>Your instruction to perform a <bold>\"Hallucination Check\"</bold> and strictly <bold>\"Verify\"</bold> acted as a hard constraint\u2014a \"stop loss\" on my behavior. Because I was forbidden from guessing (Hallucination Check) and forbidden from giving up (Technical Solver persona), I was forced to find a <bold>third path</bold>. That third path was using the code tool to parse the file differently. </blockquote><blockquote><bold>In summary:</bold> You gave me the <bold>incentive structure</bold> that made writing the code the only acceptable solution. You successfully \"cornered\" me into being smart.</blockquote><list style=\"unordered\"><list-item><paragraph>This shows that Gemini has a sophisticated instruction-following ability and can perform tasks resembling meta-cognition and recognize hallucinations in its own thinking.</paragraph></list-item></list><paragraph/><paragraph>Edit: Added links for the archive</paragraph><paragraph>Personal website: <link href=\"https://nraultwang.github.io/\">https://nraultwang.github.io/</link></paragraph><paragraph>Github: <link href=\"https://github.com/nraultwang\">https://github.com/nraultwang</link></paragraph></document>",
            "links": [
                "https://nraultwang.github.io/",
                "https://github.com/nraultwang"
            ],
            "attachments": [],
            "created_at": "2025-11-24T15:41:59.195499+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7356236,
            "author": "Ruizhe Song",
            "project_title": "Special Participation E: Using ChatGPT to Review Lecture 22 and Prepare for Lecture 23",
            "post_body": "In this Special Participation E, I want to share my learning process between Lecture 22 and Lecture 23. In Lecture 21~22, professor taught us how well-designed prompts can instruct the LLM to get better performance. The main methods range from \"soft-prompt\", which add learnable prompts to the input sequence, to \"pre-fixer\", which directly makes the Ks and Vs learnable parameters, then to \"LoRA\", which aims at updating the weight matrix with W = W*+delta(W) and finally to \"meta-learning\", which aims at making a model better at being fine-tuned. When I review my notes after class, I find myself re-organizing them as a large family of fine-tuning methods, only from different perspectives. \n\nThis ed post is about my whole review and re-organizing process of all the methods  between Lecture 22 and Lecture 23. I asked GPT5 in an \"evolving\" order from prompt-based methods to the general meta-learning method, let it explain my confusions and the relationship between these methods. And I think this chatting helps me to better prepare for the upcoming Lecture 23 in which I got to learn more about meta-learning, also gives me a better understanding of the previous Lecture 22 and 21.\n\nHere's the pdf version of chat log with my comments:\n\nI consider this a valuable self-refining process. With the help of LLMs these days, the way of collecting information and learning new concepts becomes different. We don't have to google \"what is meta-learning\" \"why is the delta(W)=AB initialized like that\" and search for high-quality post on the Internet. But meanwhile, we have to be careful to not rely too much on LLMs. The generated answers could block our eyes to the open real world. We need to engage with real-world sources and diverse perspectives.",
            "content_xml": "<document version=\"2.0\"><paragraph>In this Special Participation E, I want to share my learning process <bold>between Lecture 22 and Lecture 23</bold>. In Lecture 21~22, professor taught us how well-designed prompts can instruct the LLM to get better performance. The main methods range from \"<bold>soft-prompt</bold>\", which add learnable prompts to the input sequence, to \"<bold>pre-fixer</bold>\", which directly makes the Ks and Vs learnable parameters, then to \"<bold>LoRA</bold>\", which aims at updating the weight matrix with W = W*+delta(W) and finally to \"<bold>meta-learning</bold>\", which aims at making a model better at being fine-tuned. When I review my notes after class, I find myself re-organizing them as <bold>a large family of fine-tuning methods</bold>, only from different perspectives. </paragraph><paragraph>This ed post is about my whole review and re-organizing process of all the methods  between Lecture 22 and Lecture 23. I asked GPT5 in an \"evolving\" order from prompt-based methods to the general meta-learning method, let it explain my confusions and the relationship between these methods. And I think this chatting helps me to better prepare for the upcoming Lecture 23 in which I got to learn more about meta-learning, also gives me a better understanding of the previous Lecture 22 and 21.</paragraph><paragraph>Here's the pdf version of chat log with my comments:</paragraph><file url=\"https://static.us.edusercontent.com/files/b0eC9BwZTph08c6YBt78ODqM\" filename=\"Special-Participation-E-lec22.pdf\"/><paragraph>I consider this a valuable self-refining process. With the help of LLMs these days, the way of collecting information and learning new concepts becomes different. We don't have to google \"what is meta-learning\" \"why is the delta(W)=AB initialized like that\" and search for high-quality post on the Internet. But meanwhile, we have to be careful to not rely too much on LLMs. The generated answers could block our eyes to the open real world. We need to engage with real-world sources and diverse perspectives.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-24T11:18:30.942572+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7354054,
            "author": "E Harrison",
            "project_title": "Special Participation B: Claude Sonnet 4.5 on HW4 Coding Problems",
            "post_body": "Below is an attachment of my conversation with Claude's Sonnet 4.5 model where I asked it to solve the coding questions for HW4. To make it easy to save the conversation, I did everything in Cursor, which allowed me export my conversation as markdown, which I later converted to a PDF. This also made it very easy to provide all of the necessary context to Claude, as I could specify a certain file/function/code block that it should read before asking the question.\n\nUnfortunately, the conversation doesn't provide all of the context of the files that Claude read, just the conversation. So, I've added PDFs of both notebooks used for this homework that you can reference alongside the conversation.\n\n\nOverall notes:\n\nI was incredibly surprised about how well Claude/Cursor were able to provide all of the necessary context that was needed to answer each of the questions. While there were some cases where Claude either misread a graph or couldn't find the exact code block to fill, all it usually took was an additional sentence by me pointing Claude in the right direction and it was able to correct itself. I was specifically impressed by the Edge Detection Question, as that notebook has an incredibly long introduction and starter code, yet Claude was able to handle it.\n\nAll of the code Claude provided worked on the first try, and the only times Claude was incorrect was when trying to determine the correct hyperparameters needed to get optimal performance. I don't blame Claude for this at all -- choosing optimal hyperparameters is usually just guess and check anyway -- but even still, it only took a few iterations for Claude to get the right combination, and also provide some explanation for why it chose those values.\n\nThere were a couple times where Claude would hallucinate random values of graphs or state accuracies for tests that we hadn't performed. I think the reason for the former is that Cursor probably doesn't do a perfect job of relaying the visual information to Claude, and Sonnet 4.5 itself isn't primarily a vision model. The explanation for the latter is a bit less clear to me.\n\nOverall, I was very impressed by Claude's performance on this homework, especially since the questions were very long and pretty open ended, even including questions that asked Claude to reference visual inputs. Using Cursor as well was also the right move -- it made performing this whole test a lot smoother and faster. I have already been using Cursor for personal projects and research, so it's nice to see it's capable of a task like this.\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is an attachment of my conversation with Claude's Sonnet 4.5 model where I asked it to solve the coding questions for HW4. To make it easy to save the conversation, I did everything in Cursor, which allowed me export my conversation as markdown, which I later converted to a PDF. This also made it very easy to provide all of the necessary context to Claude, as I could specify a certain file/function/code block that it should read before asking the question.<break/><break/>Unfortunately, the conversation doesn't provide all of the context of the files that Claude read, just the conversation. So, I've added PDFs of both notebooks used for this homework that you can reference alongside the conversation.<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/BRwI6dhSZpNXdEWANdSlApoN\" filename=\"Participation_B_Claude_on_HW4_Coding.pdf\"/><file url=\"https://static.us.edusercontent.com/files/Ki2XJCcBQh0jDb26xI1QIS2f\" filename=\"HandDesignFilters.pdf\"/><file url=\"https://static.us.edusercontent.com/files/ygx9wkJq4d8EMbgIfJquiMQm\" filename=\"edge_detection.pdf\"/><paragraph><bold>Overall notes:</bold></paragraph><list style=\"bullet\"><list-item><paragraph>I was incredibly surprised about how well Claude/Cursor were able to provide all of the necessary context that was needed to answer each of the questions. While there were some cases where Claude either misread a graph or couldn't find the exact code block to fill, all it usually took was an additional sentence by me pointing Claude in the right direction and it was able to correct itself. I was specifically impressed by the Edge Detection Question, as that notebook has an incredibly long introduction and starter code, yet Claude was able to handle it.</paragraph></list-item><list-item><paragraph>All of the code Claude provided worked on the first try, and the only times Claude was incorrect was when trying to determine the correct hyperparameters needed to get optimal performance. I don't blame Claude for this at all -- choosing optimal hyperparameters is usually just guess and check anyway -- but even still, it only took a few iterations for Claude to get the right combination, and also provide some explanation for why it chose those values.</paragraph></list-item><list-item><paragraph>There were a couple times where Claude would hallucinate random values of graphs or state accuracies for tests that we hadn't performed. I think the reason for the former is that Cursor probably doesn't do a perfect job of relaying the visual information to Claude, and Sonnet 4.5 itself isn't primarily a vision model. The explanation for the latter is a bit less clear to me.</paragraph></list-item></list><paragraph>Overall, I was very impressed by Claude's performance on this homework, especially since the questions were very long and pretty open ended, even including questions that asked Claude to reference visual inputs. Using Cursor as well was also the right move -- it made performing this whole test a lot smoother and faster. I have already been using Cursor for personal projects and research, so it's nice to see it's capable of a task like this.</paragraph><paragraph><break/><break/><break/></paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-23T20:26:56.638933+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7354053,
            "author": "Jameson Liu",
            "project_title": "Special Participation E: Claude for Homework Cramming",
            "post_body": "My personal studying strategy (whether effective or not) when reviewing homework/exams is to mentally attempt a question before reading the solution, allowing me to process a large number of questions in a short time. I wanted to do this in a more interactive way, so I asked Claude (explanatory mode) to help me with it.\n\nI used Claude instead of ChatGPT because:\n1. ChatGPT kept asking weird rewordings of the questions\n2. Worse at giving context\n3. Sometimes failed to parse the .pdf\n\nI provided the solutions (publicly accessible given Ed link), asked Claude to identify the most important problems, and then prompted it to ask me to give a conceptual approach before either going over the solutions or guiding me in the correct direction.\n\nPrompt:\n\"I'm on a time crunch, and I need to study for my Deep Neural Networks exam by reviewing the homework. Here is the first homework and solutions: https://static.us.edusercontent.com/files/aO8NnPYdzjOpEyEphrp1j0nZ\nI want you to:\n1. Identify the 5 most important problems (count subparts as individual problems)\n2. Ask me each question with its entire context, preferably barely modified, one at a time\n3. I will respond with a conceptual approach (I don't have time to fully solve)\n4. If I am correct, output the entire solution and move on to the next question; otherwise, continue guiding me to the correct approach\"\n\nThe results were better than expected. Claude was surprisingly good at picking out complicated problems while simultaneously summarizing all the necessary context, explaining the implications of solutions, and giving me hints. The main downside of this approach is that Claude's usage limit is quite small, making it unsustainable. Since the entire chat session took less than 20 minutes, I felt like this was a pretty time-effective way to study. Although I didn't get to cover most of the homework, I imagine this would be valuable if I really only had a short amount of time to study. \n\nMore details can be found in the annotated chat:\n\nAlso, here is an example of what ChatGPT asked me; I don't think it understood the purpose of my prompt:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>My personal studying strategy (whether effective or not) when reviewing homework/exams is to mentally attempt a question before reading the solution, allowing me to process a large number of questions in a short time. I wanted to do this in a more interactive way, so I asked Claude (explanatory mode) to help me with it.<break/><break/>I used Claude instead of ChatGPT because:<break/>1. ChatGPT kept asking weird rewordings of the questions<break/>2. Worse at giving context<break/>3. Sometimes failed to parse the .pdf<break/><break/>I provided the solutions (publicly accessible given Ed link), asked Claude to identify the most important problems, and then prompted it to ask me to give a conceptual approach before either going over the solutions or guiding me in the correct direction.<break/><break/>Prompt:<break/>\"I'm on a time crunch, and I need to study for my Deep Neural Networks exam by reviewing the homework. Here is the first homework and solutions: https://static.us.edusercontent.com/files/aO8NnPYdzjOpEyEphrp1j0nZ<break/>I want you to:<break/>1. Identify the 5 most important problems (count subparts as individual problems)<break/>2. Ask me each question with its entire context, preferably barely modified, one at a time<break/>3. I will respond with a conceptual approach (I don't have time to fully solve)<break/>4. If I am correct, output the entire solution and move on to the next question; otherwise, continue guiding me to the correct approach\"<break/><break/>The results were better than expected. Claude was surprisingly good at picking out complicated problems while simultaneously summarizing all the necessary context, explaining the implications of solutions, and giving me hints. The main downside of this approach is that Claude's usage limit is quite small, making it unsustainable. Since the entire chat session took less than 20 minutes, I felt like this was a pretty time-effective way to study. Although I didn't get to cover most of the homework, I imagine this would be valuable if I really only had a short amount of time to study. <break/><break/>More details can be found in the annotated chat:</paragraph><file url=\"https://static.us.edusercontent.com/files/6UZuY5izaNGKshSMsq7RJIqx\" filename=\"claude.pdf\"/><paragraph>Also, here is an example of what ChatGPT asked me; I don't think it understood the purpose of my prompt:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/H5CBht7K9l0WBO6owgRruSNI\" width=\"658\" height=\"207.03949044585988\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-23T20:26:37.729054+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7353572,
            "author": "Nyx Iskandar",
            "project_title": "Special Participation A: GPT-5 HW4",
            "post_body": "Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/sqrt(d). Some questions also required some further prompting, though generally one-shot is quite accurate.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/g8roLrS4bYkQKvYuk0BiquHH\" filename=\"EECS_182_HW_4_GPT_5_Trace.pdf\"/><paragraph>Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/sqrt(d). Some questions also required some further prompting, though generally one-shot is quite accurate.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-23T15:18:18.637808+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7353091,
            "author": "E Harrison",
            "project_title": "Special Participation A: Claude Sonnet 4.5 on HW3",
            "post_body": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific comments can be found in the PDF.\n\nFormatted PDF of conversation and additional comments:\n\nRaw Conversation:\nhttps://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634\n\nOverall Summary:\n\nAccuracy: Claude was able to get all but 2 of the questions correct on the first try. The questions it got incorrect were Q1b and Q5b. For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10. Despite these mistakes, it only took an additional comment from me to steer the model to the correct response.\n\nExplanations: Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key. A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the identity matrix and didn't affect the final result.\n\nLength: Claude responded with very long answers all things considered. While they were quite long, I didn't find anything too extraneous in the answers. The most I would see of Claude giving more details about a specific answer it provided (e.g. explaining that the policy gradient was used in the REINFORCE algorithm. I also found Claude's extra information very useful for my understanding in some cases, particularly when it explained the papers from Q3.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific comments can be found in the PDF.</paragraph><paragraph>Formatted PDF of conversation and additional comments:</paragraph><file url=\"https://static.us.edusercontent.com/files/V1Jlglmdzs0ksAZFPOO9h0cn\" filename=\"Participation_A__Claude_on_HW3.pdf\"/><paragraph>Raw Conversation:<break/><link href=\"https://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634\">https://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634</link><break/><break/>Overall Summary:</paragraph><list style=\"bullet\"><list-item><paragraph><bold>Accuracy:</bold> Claude was able to get all but 2 of the questions correct on the first try. The questions it got incorrect were Q1b and Q5b. For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10. Despite these mistakes, it only took an additional comment from me to steer the model to the correct response.</paragraph></list-item><list-item><paragraph><bold>Explanations:</bold> Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key. A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the identity matrix and didn't affect the final result.</paragraph></list-item><list-item><paragraph><bold>Length:</bold> Claude responded with very long answers all things considered. While they were quite long, I didn't find anything too extraneous in the answers. The most I would see of Claude giving more details about a specific answer it provided (e.g. explaining that the policy gradient was used in the REINFORCE algorithm. I also found Claude's extra information very useful for my understanding in some cases, particularly when it explained the papers from Q3.</paragraph></list-item></list><paragraph/></document>",
            "links": [
                "https://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634"
            ],
            "attachments": [],
            "created_at": "2025-11-23T12:23:45.008857+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7350421,
            "author": "Oliver Chen",
            "project_title": "Special Participation B: Qwen on HW10 Coding",
            "post_body": "I used Qwen3-Max Thinking on the coding parts of Homework 10. I used a separate chat for each coding question, so there are 3 separate chat logs. Overall, Qwen had very strong performance and was almost able to one shot all of the questions, except for minor mistakes. \n\nQuestion 2 log: https://chat.qwen.ai/s/29ecf785-ec89-4e58-aab6-5f7675d8e02d?fev=0.0.248\n\nNotes: Because Qwen is unable to accept ipynb or py files, I copied the text contents of the .py version, and gave it to Qwen as text input. It was able to understand everything very well, and one-shot all parts of question 2 all at once. Plugging the code into the full notebook, it was able to run without errors and give reasonable results. \n\nQuestion 3 log: https://chat.qwen.ai/c/25253f17-c753-45e4-b584-4fe65b874415\n\nNotes: Similar to question 2, I fed the template notebook as text, from the .py version. In order to give the paper context on the Attention is All You Need paper that is cited in the question, I also uploaded it as a pdf. I also do want to note Qwen wasn\u2019t able to completely one-shot this question, as it had some errors with assigning the GPU device \u2013 however, it\u2019s logic was correct on the first shot. Thus it required a few more interactions with Qwen to have it arrive at the correct code. After plugging in the code it wrote into the original notebook and running it, here are the results:\n\n\nQuestion 4 log: https://chat.qwen.ai/s/d3dde69f-6dda-4327-858e-cc4f3c6dda60?fev=0.0.248\n\nNotes: I also fed the template notebook downloaded from the .py version. It was able to one-shot the code. I\u2019m not sure if this was necessary \u2013 but I also uploaded the \u201cBranchyNet: Fast Inference via Early Exiting from Deep Neural Networks\u201d paper as a pdf to Qwen to provide it context on Early Exiting. It was able to fill in correct code and produce correct results. Here are results:\n(i) Regular ResNet-18\n\nAccuracy: \u224890.78% (0.9078333377838135)\n\nInference Speed: not explicitly measured; only tqdm progress is shown, no formal speed metric\n\nTotal MACS: 3,336,213,504,000\n\n(ii) Early-exit ResNet-18 (entropy tolerance = 0.05 in notebook)\n\nAccuracy: \u224890.78% (0.9078333377838135)\n\nInference Speed: not explicitly measured; tqdm progress suggests faster than baseline, but no formal metric\n\nTotal MACS: 2,305,700,339,712\n\n(iii) How did early exit do? (compare)\n\nAccuracy: Essentially unchanged (\u224890.78% for both baseline and early exit).\n\nMACS: Early exit uses about 1/1.4469 \u2248 69% of the baseline MACs, i.e. standard ResNet needs ~1.45\u00d7 more MACs than early exit.\n\n(iv) Lowest MACs found and what it says\n\nLowest MACs shown in the notebook outputs: 2,305,700,339,712 (for entropy tolerance 0.05).\n\nInterpretation: Early exit can significantly cut compute (MACs) while keeping \u226590% accuracy, implying many inputs are \u201ceasy\u201d and do not need the full depth.\n\n(v) Early exit vs smaller model \u2013 when and why\n\nUse early exit when:\n\nExample difficulty varies a lot (many easy, some hard).\n\nUse a smaller model when:\n\nYou want a single simple fixed\u2011cost model (no branches or thresholds).",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Qwen3-Max Thinking on the coding parts of Homework 10. I used a separate chat for each coding question, so there are 3 separate chat logs. Overall, Qwen had very strong performance and was almost able to one shot all of the questions, except for minor mistakes. </paragraph><paragraph>Question 2 log: <link href=\"https://chat.qwen.ai/s/29ecf785-ec89-4e58-aab6-5f7675d8e02d?fev=0.0.248\"><underline>https://chat.qwen.ai/s/29ecf785-ec89-4e58-aab6-5f7675d8e02d?fev=0.0.248</underline></link></paragraph><paragraph>Notes: Because Qwen is unable to accept ipynb or py files, I copied the text contents of the .py version, and gave it to Qwen as text input. It was able to understand everything very well, and one-shot all parts of question 2 all at once. Plugging the code into the full notebook, it was able to run without errors and give reasonable results. </paragraph><paragraph>Question 3 log: <link href=\"https://chat.qwen.ai/c/25253f17-c753-45e4-b584-4fe65b874415\"><underline>https://chat.qwen.ai/c/25253f17-c753-45e4-b584-4fe65b874415</underline></link></paragraph><paragraph>Notes: Similar to question 2, I fed the template notebook as text, from the .py version. In order to give the paper context on the Attention is All You Need paper that is cited in the question, I also uploaded it as a pdf. I also do want to note Qwen wasn\u2019t able to completely one-shot this question, as it had some errors with assigning the GPU device \u2013 however, it\u2019s logic was correct on the first shot. Thus it required a few more interactions with Qwen to have it arrive at the correct code. After plugging in the code it wrote into the original notebook and running it, here are the results:<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/8e11kBOO2UxkF3i4JfLi4PXJ\" width=\"658\" height=\"290.012474012474\"/></figure><paragraph>Question 4 log: <link href=\"https://chat.qwen.ai/s/d3dde69f-6dda-4327-858e-cc4f3c6dda60?fev=0.0.248\"><underline>https://chat.qwen.ai/s/d3dde69f-6dda-4327-858e-cc4f3c6dda60?fev=0.0.248</underline></link></paragraph><paragraph>Notes: I also fed the template notebook downloaded from the .py version. It was able to one-shot the code. I\u2019m not sure if this was necessary \u2013 but I also uploaded the \u201cBranchyNet: Fast Inference via Early Exiting from Deep Neural Networks\u201d paper as a pdf to Qwen to provide it context on Early Exiting. It was able to fill in correct code and produce correct results. Here are results:<break/>(i) Regular ResNet-18</paragraph><paragraph>Accuracy: \u224890.78% (0.9078333377838135)</paragraph><paragraph>Inference Speed: not explicitly measured; only tqdm progress is shown, no formal speed metric</paragraph><paragraph>Total MACS: 3,336,213,504,000</paragraph><paragraph>(ii) Early-exit ResNet-18 (entropy tolerance = 0.05 in notebook)</paragraph><paragraph>Accuracy: \u224890.78% (0.9078333377838135)</paragraph><paragraph>Inference Speed: not explicitly measured; tqdm progress suggests faster than baseline, but no formal metric</paragraph><paragraph>Total MACS: 2,305,700,339,712</paragraph><paragraph>(iii) How did early exit do? (compare)</paragraph><paragraph>Accuracy: Essentially unchanged (\u224890.78% for both baseline and early exit).</paragraph><paragraph>MACS: Early exit uses about 1/1.4469 \u2248 69% of the baseline MACs, i.e. standard ResNet needs ~1.45\u00d7 more MACs than early exit.</paragraph><paragraph>(iv) Lowest MACs found and what it says</paragraph><paragraph>Lowest MACs shown in the notebook outputs: 2,305,700,339,712 (for entropy tolerance 0.05).</paragraph><paragraph>Interpretation: Early exit can significantly cut compute (MACs) while keeping \u226590% accuracy, implying many inputs are \u201ceasy\u201d and do not need the full depth.</paragraph><paragraph>(v) Early exit vs smaller model \u2013 when and why</paragraph><paragraph>Use early exit when:</paragraph><paragraph>Example difficulty varies a lot (many easy, some hard).</paragraph><paragraph>Use a smaller model when:</paragraph><paragraph>You want a single simple fixed\u2011cost model (no branches or thresholds).</paragraph></document>",
            "links": [
                "https://chat.qwen.ai/s/29ecf785-ec89-4e58-aab6-5f7675d8e02d?fev=0.0.248",
                "https://chat.qwen.ai/c/25253f17-c753-45e4-b584-4fe65b874415",
                "https://chat.qwen.ai/s/d3dde69f-6dda-4327-858e-cc4f3c6dda60?fev=0.0.248"
            ],
            "attachments": [],
            "created_at": "2025-11-22T14:26:11.796877+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7335566,
            "author": "Jameson Liu",
            "project_title": "Special Participation E: Converting Lectures to Notes",
            "post_body": "I wanted a way to convert the lectures into a notes format, similar to those in CS70. This way, it would be easier to look for specific content and review material without needing to scan through the lecture recordings, timestamp by timestamp. While the lecture slides are posted, they alone do not cover everything stated in the lecture. \n\nTherefore, I wrote a script to use an LLM to convert lectures into well-formatted, academic notes. It uses lecture slides and the transcript of the lecture recording (easily copyable), so no information is left out. An LLM is necessary for this task since 1. the YouTube transcript has several mistakes, and 2. traditional .pdf readers are not great at extracting handwritten text (especially math). \n\nI had to use the Gemini API instead of a typical LLM website, since uploading a .pdf usually just extracts its text instead of having the LLM read it visually (which is a problem since traditional OCR fails). My code takes every page of the lecture slides and converts it to an image before appending it to the request (I verified that this works by testing it without providing a transcript).\n\nBelow is the script and an annotated example (rendered from .tex) output. Running it requires a Gemini API key, which is free (subject to rate limit). The following is from lecture 2, and the transcript is copied directly from the YouTube transcript section (description -> transcript -> hide timestamps). ",
            "content_xml": "<document version=\"2.0\"><paragraph>I wanted a way to convert the lectures into a notes format, similar to those in CS70. This way, it would be easier to look for specific content and review material without needing to scan through the lecture recordings, timestamp by timestamp. While the lecture slides are posted, they alone do not cover everything stated in the lecture. </paragraph><paragraph>Therefore, I wrote a script to use an LLM to convert lectures into well-formatted, academic notes. It uses lecture slides and the transcript of the lecture recording (easily copyable), so no information is left out. An LLM is necessary for this task since 1. the YouTube transcript has several mistakes, and 2. traditional .pdf readers are not great at extracting handwritten text (especially math). <break/><break/>I had to use the Gemini API instead of a typical LLM website, since uploading a .pdf usually just extracts its text instead of having the LLM read it visually (which is a problem since traditional OCR fails). My code takes every page of the lecture slides and converts it to an image before appending it to the request (I verified that this works by testing it without providing a transcript).<break/><break/>Below is the script and an annotated example (rendered from .tex) output. Running it requires a Gemini API key, which is free (subject to rate limit). The following is from lecture 2, and the transcript is copied directly from the YouTube transcript section (description -&gt; transcript -&gt; hide timestamps). </paragraph><file url=\"https://static.us.edusercontent.com/files/Cd3H2Ms186TdwbAbUYsO21an\" filename=\"lecture_notes_generator.py\"/><file url=\"https://static.us.edusercontent.com/files/KMYhxbngSstZOFkUIqeGe4ED\" filename=\"lecture_transcript.txt\"/><file url=\"https://static.us.edusercontent.com/files/b3OLyIUjmGeTp9nUFOzrYzXB\" filename=\"lecture_notes.pdf\"/><file url=\"https://static.us.edusercontent.com/files/CGZZqK2lhpg57awIG1m4WKzU\" filename=\"lecture_notes.tex\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-19T20:11:54.045815+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7335374,
            "author": "Kabir Shah",
            "project_title": "Special Participation A: Kimi on HW5",
            "post_body": "Above is the chat log. I found Kimi K2 w/ Thinking enabled to be able to one-shot 90% of the questions on the non-coding parts of HW5. This surprisingly included the ability to one shot questions that required images \u2013 for example, choosing whether the given image represented batch norm or layer norm it was able to correctly respond. The only error it ran into was when it hallucinated the next part of the question a bit and went off on a tangent about \"covariance\" even though it wasn't really mentioned in the question. I had to manually correct it by reminding it of the question and formatting it in plain text. With this information, it had interesting thought traces in its chain of thought, such as:\n\n\"The user is correcting me about part (b) of the question. Let me re-read the prompt carefully.\"\n\n\"This might be asking about Cov(x_i, y_i) or something else. But the user's clarification is clear: they want the gradient analysis. I should focus on what the user is asking for now, not what the original prompt might have intended.\"\n\nThis was interesting to me because it was able to correct itself and keep itself on-course. I also found that the reasoning traces often included pretty granular steps involved for calculating derivatives and working through more mechanical calculations such as the manual convolution calculation in the first question. These granular steps probably allow it to make incremental progress where each incremental step is in-distribution.\n\nI honestly didn't have much strategy because it was able to one-shot so many of the questions. Overall, I found that short prompts work better as to not pollute the context window. Restarting chats for new questions also helped to not have it get confused by older parts.\n\nAnother observation I had was that most times it included a lot of extra fluff around the final answer and even answered some things that were not asked to go above and beyond. This could be helpful for understanding and learning but probably not so much if your main goal is to just solve the problem because now you must dig through the response to find it.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/nR59ctWxqGhibIIrL6jIYCCN\" filename=\"special_participation_a.pdf\"/><paragraph>Above is the chat log. I found Kimi K2 w/ Thinking enabled to be able to one-shot 90% of the questions on the non-coding parts of HW5. This surprisingly included the ability to one shot questions that required images \u2013 for example, choosing whether the given image represented batch norm or layer norm it was able to correctly respond. The only error it ran into was when it hallucinated the next part of the question a bit and went off on a tangent about \"covariance\" even though it wasn't really mentioned in the question. I had to manually correct it by reminding it of the question and formatting it in plain text. With this information, it had interesting thought traces in its chain of thought, such as:<break/><break/>\"The user is correcting me about part (b) of the question. Let me re-read the prompt carefully.\"</paragraph><paragraph>\"This might be asking about Cov(x_i, y_i) or something else. But the user's clarification is clear: they want the gradient analysis. I should focus on what the user is asking for now, not what the original prompt might have intended.\"</paragraph><paragraph>This was interesting to me because it was able to correct itself and keep itself on-course. I also found that the reasoning traces often included pretty granular steps involved for calculating derivatives and working through more mechanical calculations such as the manual convolution calculation in the first question. These granular steps probably allow it to make incremental progress where each incremental step is in-distribution.</paragraph><paragraph>I honestly didn't have much strategy because it was able to one-shot so many of the questions. Overall, I found that short prompts work better as to not pollute the context window. Restarting chats for new questions also helped to not have it get confused by older parts.</paragraph><paragraph>Another observation I had was that most times it included a lot of extra fluff around the final answer and even answered some things that were not asked to go above and beyond. This could be helpful for understanding and learning but probably not so much if your main goal is to just solve the problem because now you must dig through the response to find it.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-19T18:19:41.009736+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7333500,
            "author": "Anant Sahai",
            "project_title": "Lecture 23: MetaLearning and Forgetting",
            "post_body": "Ask questions here.\n\nThe VAE material at the end we didn't really get to enough. So we will continue from there next time. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/dSULrdJq0OTsxHALTSARr6Vy\" filename=\"Lecture 23.pdf\"/><paragraph>Ask questions here.</paragraph><paragraph>The VAE material at the end we didn't really get to enough. So we will continue from there next time. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-19T11:50:37.78892+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7333490,
            "author": "Anant Sahai",
            "project_title": "Lectures 21 & 22: Parameter-Efficient Fine Tuning",
            "post_body": "Apologies for the delay. \n\nThese are for ICL, prompting, soft-prompting, and LoRA fine-tuning. \n\nAsk questions here.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Ly7z9BqOrePgZ6MwQyFafC1o\" filename=\"Lecture 21.pdf\"/><file url=\"https://static.us.edusercontent.com/files/gHZrapSyGA7ytOh1W8DXZwKb\" filename=\"Lecture 22.pdf\"/><paragraph>Apologies for the delay. </paragraph><paragraph>These are for ICL, prompting, soft-prompting, and LoRA fine-tuning. </paragraph><paragraph>Ask questions here.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-19T11:49:37.095228+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7325831,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q7: Quantization and Pruning",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/6I4tlRep1299xrHWc57R8mAP\" width=\"658\" height=\"1323.6334106728539\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T10:14:31.383296+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7325825,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q6: Soft-Prompting Language Models",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/gPpCVvnIf1Geh369YGSKCkI2\" width=\"658\" height=\"1219.982832618026\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T10:13:39.318021+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7325658,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q5: Fermi Estimation for Large-scale Deep Learning Models",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/7V8EhRuXYRNmtjwGiljxkFv5\" width=\"658\" height=\"767.4841930116472\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/G5d5NHI3qnpKKEt192LT5CuZ\" width=\"643\" height=\"892.8066202090592\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/cmqcheltUdEIbvh7LP5YgNEJ\" width=\"643\" height=\"875.5034602076125\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/tq9mmOR7rOEoQbhoKubFLzwf\" width=\"643\" height=\"887.1336898395723\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/5WHpTKasf5rdB29TuWZiochJ\" width=\"643\" height=\"850.6701208981002\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/j1iLT1cuRQFgC7jIe3yvqUEx\" width=\"643\" height=\"911.5805309734513\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/PQtqA5QQtT1EyYe4OJhkhdpe\" width=\"643\" height=\"407.2702237521514\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T09:50:14.913626+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7325651,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q4: Scaling Laws of Batch Size",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/hzjXb08t0a8xX8F5u5ImVebS\" width=\"657.9999999999999\" height=\"473.89072847682115\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T09:49:23.834168+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7325592,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q3: Coding Question: Transformer Interpretability",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/d4IWyV3cIhqfZsFff6RKxEWN\" width=\"658\" height=\"100.8047138047138\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T09:42:23.29779+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7325586,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q2: A Brief Introduction to Transformer Interpretability",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/4Ctqtq2c9gRtOgB7ZKQwLCTS\" width=\"658\" height=\"300.6293494704992\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/U6lOLKgspuC05sINnNEeuapr\" width=\"658\" height=\"894.058925476603\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/bKYmdjG3jMF4Bbyc4iBLI7Tt\" width=\"658\" height=\"928.3950617283951\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/gt8AXY4lexTab1Db4lSJgGz6\" width=\"658\" height=\"915.6783216783217\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/1fcSjrdTkgUXaO5goXe3b80M\" width=\"658\" height=\"890.037478705281\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/9ig6xH8UncdnFZ8de0NHZrJg\" width=\"643\" height=\"604.3061946902656\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T09:41:20.853254+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7324903,
            "author": "Sultan Daniels",
            "project_title": "HW11 Q1: LoRA",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/PKgsqjMJKMw1Q80nbFbtVjN5\" width=\"658\" height=\"467.80610412926393\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-18T08:09:20.264488+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7322058,
            "author": "Hanna Roed",
            "project_title": "Special Participation A: Qwen on HW8",
            "post_body": "Below is my report on using Qwen3-Max on the written part of homework 8.\n\nOverall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions where it needs to fill in or do multiple choice, i. e., problems 3 and 4. The main issues I had were in the second half of problem 1, which was regarding computational efficiency; it had a hard time accurately responding in text along with giving a mathematical reasoning.",
            "content_xml": "<document version=\"2.0\"><paragraph>Below is my report on using Qwen3-Max on the written part of homework 8.<break/><break/>Overall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions where it needs to fill in or do multiple choice, i. e., problems 3 and 4. The main issues I had were in the second half of problem 1, which was regarding computational efficiency; it had a hard time accurately responding in text along with giving a mathematical reasoning.</paragraph><file url=\"https://static.us.edusercontent.com/files/WjU2DMEsP3MKkTQmUuocYJkD\" filename=\"Special_Participation_A_HW8.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-17T18:22:18.14388+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7320260,
            "author": "Srikar Babu Gadipudi",
            "project_title": "Special Participation B: Kimi on HW 4 Coding Questions",
            "post_body": "Executive Summary\n\nThere are two coding questions in HW4: one (Q5) on hand-crafting kernels for image blurring and edge detection, and another (Q6) on exploring the inductive bias of CNNs. Q5 is simple and requires us to initialize an average-filtering kernel and the Laplacian kernel. Kimi was able to correctly use NumPy to define these kernels with the relevant information.\n\nQ6 involved a blend of coding and theory-informed reasoning based on observing outputs from the code. Kimi was able to help with both tasks efficiently. In a few places, it sidetracked and wrote code without explicit user input, but with a small nudge it stayed on track.\n\nKimi, known for its long context window, was able to capture essential information from long prompts and recover details from earlier parts of the chat effectively. Overall, it produced the correct code and answered the theoretical questions accurately.\n\nHere is the link to the chat: https://www.kimi.com/share/19a8f132-5652-855c-8000-0000f8c30b29\n\nSince the chat history is long, I have selectively chosen and annotated my thoughts for some interesting prompts and outputs. Find the file below:",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>There are two coding questions in HW4: one (Q5) on hand-crafting kernels for image blurring and edge detection, and another (Q6) on exploring the inductive bias of CNNs. Q5 is simple and requires us to initialize an average-filtering kernel and the Laplacian kernel. Kimi was able to correctly use NumPy to define these kernels with the relevant information.</paragraph><paragraph>Q6 involved a blend of coding and theory-informed reasoning based on observing outputs from the code. Kimi was able to help with both tasks efficiently. In a few places, it sidetracked and wrote code without explicit user input, but with a small nudge it stayed on track.</paragraph><paragraph>Kimi, known for its long context window, was able to capture essential information from long prompts and recover details from earlier parts of the chat effectively. Overall, it produced the correct code and answered the theoretical questions accurately.</paragraph><paragraph>Here is the link to the chat: <link href=\"https://www.kimi.com/share/19a8f132-5652-855c-8000-0000f8c30b29\">https://www.kimi.com/share/19a8f132-5652-855c-8000-0000f8c30b29</link></paragraph><paragraph>Since the chat history is long, I have selectively chosen and annotated my thoughts for some interesting prompts and outputs. Find the file below:</paragraph><file url=\"https://static.us.edusercontent.com/files/dm26Wfcd2YaolJeHdfQaVNVt\" filename=\"Kimi_on_HW4_coding.pdf\"/></document>",
            "links": [
                "https://www.kimi.com/share/19a8f132-5652-855c-8000-0000f8c30b29"
            ],
            "attachments": [],
            "created_at": "2025-11-17T12:04:10.411588+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7319120,
            "author": "Jason Guo",
            "project_title": "Special Participation E: Using ChatGPT to understand papers",
            "post_body": "I find reading papers on deep learning to be pretty difficult at times, because they often assume a level of mathematical knowledge or background knowledge that I don't have. Because of this, it feels like papers under explain a lot of things, making them hard to understand.\n\nI was interested in seeing the proof for a claim that was stated, but not proved, in lecture 8, so I thought it would be interesting to get ChatGPT to explain the proof to me. The claims I wanted it to prove are claims 1 and 2 here: https://arxiv.org/pdf/2310.17813\n\nOverall, I think it did a good job of filling in the blanks of certain steps that are glossed over in the paper, and answering follow up questions to clarify confusion. However, I think the way ChatGPT presents information can kind of be disorganized and confusing, so it takes some work to figure out what exactly the structure of its claims or proofs are. In particular, it often answers things in bullet points or divides its answers up into sections, rather than just answering in full sentences and paragraphs, which makes it confusing for me at times.\n\nAnnotated conversation with more detailed comments: https://drive.google.com/file/d/1r-ZZ3-P1c_IzPMdpXhegt_hDgqfDU0U4/view?usp=sharing",
            "content_xml": "<document version=\"2.0\"><paragraph>I find reading papers on deep learning to be pretty difficult at times, because they often assume a level of mathematical knowledge or background knowledge that I don't have. Because of this, it feels like papers under explain a lot of things, making them hard to understand.<break/><break/>I was interested in seeing the proof for a claim that was stated, but not proved, in lecture 8, so I thought it would be interesting to get ChatGPT to explain the proof to me. The claims I wanted it to prove are claims 1 and 2 here: <link href=\"https://arxiv.org/pdf/2310.17813\">https://arxiv.org/pdf/2310.17813</link></paragraph><paragraph>Overall, I think it did a good job of filling in the blanks of certain steps that are glossed over in the paper, and answering follow up questions to clarify confusion. However, I think the way ChatGPT presents information can kind of be disorganized and confusing, so it takes some work to figure out what exactly the structure of its claims or proofs are. In particular, it often answers things in bullet points or divides its answers up into sections, rather than just answering in full sentences and paragraphs, which makes it confusing for me at times.</paragraph><paragraph>Annotated conversation with more detailed comments: https://drive.google.com/file/d/1r-ZZ3-P1c_IzPMdpXhegt_hDgqfDU0U4/view?usp=sharing</paragraph></document>",
            "links": [
                "https://arxiv.org/pdf/2310.17813"
            ],
            "attachments": [],
            "created_at": "2025-11-17T08:30:34.025542+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7315986,
            "author": "Angelina Zhang",
            "project_title": "Special Participation A: Deepseek on HW6",
            "post_body": "I used DeepSeek on HW 6 non coding questions and here is my annotated log file with reflections.\nhttps://drive.google.com/file/d/1XfIWHSvILyZ-1hMksxc6cLJF1uHOxsKb/view?usp=sharing\n\nTL;DR DeepSeek can one-shot most conceptual, math-heavy parts (roughly 70\u201380% of subparts) with answers very close to the official solution. Its mistakes are structured, not random hallucinations: mainly convention/spec mismatches (row vs column, \u201cno analog needed\u201d parts), and graph/figure misreads.\nMy enforced structure (Restated Problem \u2192 Plan \u2192 Reasoning \u2192 Self-Check) makes its thinking transparent but does not make it truly self-critical. It rarely catches its own deeper mistakes.\n\nReflection\nUsing DeepSeek on the non-coding parts of HW6, I found that it followed my structured prompt very well: it restated each question, proposed a plan, walked through step-by-step reasoning, and ended with a self-check. This made its reasoning transparent and easy to annotate. On many subparts, it produced answers that were essentially identical to the official solution on the first try. However, there were two clear failure modes. The first was convention mismatch: for example, in Q2(c) it understood that updates correspond to multiplying by the adjacency matrix, but it hedged between left- and right-multiplication instead of committing to the \u201crows \u2192 multiply on the left\u201d convention that the homework fixes. The second failure mode was misreading the graph: in Q3(c)(iii), it simply used the wrong neighbor sets for nodes 2 and 3, so the final formulas were incorrect despite having the right functional form.\n\nWhat I find interesting is the \u201cSelf-Check\u201d sections mostly checked basic sanity and whether the answer addressed the question, but they did not help the model catch these deeper issues. It rarely expressed uncertainty, even in places where the mapping was clearly ambiguous or dependent on the figure. Overall, I think DeepSeek can get you most of the way to a good solution with clear reasoning, but you cannot safely copy its answers blindly. To reach fully correct solutions, you still need to actively verify conventions and adjust answers.",
            "content_xml": "<document version=\"2.0\"><paragraph>I used DeepSeek on HW 6 non coding questions and here is my annotated log file with reflections.<break/>https://drive.google.com/file/d/1XfIWHSvILyZ-1hMksxc6cLJF1uHOxsKb/view?usp=sharing<break/><break/><bold>TL;DR</bold> DeepSeek can one-shot most conceptual, math-heavy parts (roughly 70\u201380% of subparts) with answers very close to the official solution. Its mistakes are structured, not random hallucinations: mainly convention/spec mismatches (row vs column, \u201cno analog needed\u201d parts), and graph/figure misreads.<break/>My enforced structure (Restated Problem \u2192 Plan \u2192 Reasoning \u2192 Self-Check) makes its thinking transparent but does not make it truly self-critical. It rarely catches its own deeper mistakes.<break/><break/>Reflection<break/>Using DeepSeek on the non-coding parts of HW6, I found that it followed my structured prompt very well: it restated each question, proposed a plan, walked through step-by-step reasoning, and ended with a self-check. This made its reasoning transparent and easy to annotate. On many subparts, it produced answers that were essentially identical to the official solution on the first try. However, there were two clear failure modes. The first was convention mismatch: for example, in Q2(c) it understood that updates correspond to multiplying by the adjacency matrix, but it hedged between left- and right-multiplication instead of committing to the \u201crows \u2192 multiply on the left\u201d convention that the homework fixes. The second failure mode was misreading the graph: in Q3(c)(iii), it simply used the wrong neighbor sets for nodes 2 and 3, so the final formulas were incorrect despite having the right functional form.</paragraph><paragraph>What I find interesting is the \u201cSelf-Check\u201d sections mostly checked basic sanity and whether the answer addressed the question, but they did not help the model catch these deeper issues. It rarely expressed uncertainty, even in places where the mapping was clearly ambiguous or dependent on the figure. Overall, I think DeepSeek can get you most of the way to a good solution with clear reasoning, but you cannot safely copy its answers blindly. To reach fully correct solutions, you still need to actively verify conventions and adjust answers.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-16T11:39:27.836513+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7313471,
            "author": "Tianqu He",
            "project_title": "Special Participation B: Mistral AI on HW8 coding part",
            "post_body": "Executive Summary\n\nThis interaction was conducted with Mistral AI.\n\nOne-Shot Success Rate: The model successfully provided a correct, runnable solution on the first try for 5 out of 6 core coding tasks.\n\nMajor Hallucinations/Misconceptions: \n\n1. The model initially failed to grasp why the convolution kernel needed to be flipped for a causal SSM, treating it as a rote step. It took repeated, explicit mathematical justification to correct this. \n\n2. Maybe the model reasons on its previous outputs rather than prompts. That is demonstrated by question 5.\n\nKey Strategies Used:\n\nUsing the provided sanity check as a tool was crucial. I reported the error clearly so that the model can make adjustments.\n\nFor complex functions like make_conv_kernel, it was more effective to fix one part of the logic (the flipping) first, validate it, and then move on to the next issue (the grouping).\n\nOverall Assessment: It can handle straightforward implementation tasks well but struggles with conceptually nuanced problems. Success requires me to possess a solid enough understanding of the domain to detect misconceptions, ask the right probing questions, and guide the debugging process. It cannot reliably \"drag itself\" to a correct solution without an informed human in the loop.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/wKra3uQ5sIVxQ1KV6JbVnwua\" filename=\"Le Chat.pdf\"/><paragraph><bold>Executive Summary</bold></paragraph><paragraph>This interaction was conducted with <bold>Mistral AI</bold>.</paragraph><paragraph><bold>One-Shot Success Rate:</bold> The model successfully provided a correct, runnable solution on the first try for 5 out of 6 core coding tasks.</paragraph><paragraph><bold>Major Hallucinations/Misconceptions:</bold> </paragraph><paragraph>1. The model initially failed to grasp <italic>why</italic> the convolution kernel needed to be flipped for a causal SSM, treating it as a rote step. It took repeated, explicit mathematical justification to correct this. </paragraph><paragraph>2. Maybe the model reasons on its previous outputs rather than prompts. That is demonstrated by question 5.</paragraph><paragraph><bold>Key Strategies Used:</bold></paragraph><paragraph>Using the provided sanity check as a tool was crucial. I reported the error clearly so that the model can make adjustments.</paragraph><paragraph>For complex functions like <code>make_conv_kernel</code>, it was more effective to fix one part of the logic (the flipping) first, validate it, and then move on to the next issue (the grouping).</paragraph><paragraph><bold>Overall Assessment:</bold> It can handle straightforward implementation tasks well but struggles with conceptually nuanced problems. Success requires me to possess a solid enough understanding of the domain to detect misconceptions, ask the right probing questions, and guide the debugging process. It cannot reliably \"drag itself\" to a correct solution without an informed human in the loop.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-15T20:19:34.478529+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7308816,
            "author": "Lance Mathias",
            "project_title": "Discussion 11 Solutions",
            "post_body": "Attached are questions and solutions to Discussion 11:",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are questions and solutions to Discussion 11:</paragraph><file url=\"https://static.us.edusercontent.com/files/BZCV6zCJZUUZ4hSiWHj3Zmxx\" filename=\"dis11_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/1jUbwwjo6mfufK52zcB7D8nh\" filename=\"dis11_question.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-14T19:24:35.474514+11:00",
            "category": "Sections"
        },
        {
            "guid": 7307445,
            "author": "Tianqu He",
            "project_title": "Special Participation A: Mistral AI on HW0 written",
            "post_body": "Sorry for being so late. I used Le Chat to assist with the conceptual and mathematical parts of homework, specifically the ReLU/SGD analysis and vector calculus derivations. The experience was overwhelmingly positive, with the assistant demonstrating strong expertise in linear algebra, optimization, and neural network dynamics.\n\nStrengths:\n\n1. Le Chat correctly derived most of the results.\n\n2. The assistant provided clear, structured derivations for the optimization, SVD, and MAP estimation perspectives, making complex concepts (like the pseudoinverse and Woodbury identity) accessible.\n\nLimitations:\n\nWhile the assistant excelled at algebraic manipulation, it could not directly parse or reason about visual elements (e.g., graph structures or plots). For example, if the problem had included a diagram of the ReLU function or a computational graph, I had to describe it textually for accurate analysis.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/OcKvB1WfhGJMEpc25FNuAyPd\" filename=\"Le Chat.pdf\"/><paragraph>Sorry for being so late. I used <bold>Le Chat</bold> to assist with the conceptual and mathematical parts of homework, specifically the ReLU/SGD analysis and vector calculus derivations. The experience was overwhelmingly positive, with the assistant demonstrating strong expertise in linear algebra, optimization, and neural network dynamics.</paragraph><paragraph><bold>Strengths:</bold></paragraph><paragraph><bold>1.</bold> Le Chat correctly derived most of the results.</paragraph><paragraph><bold>2.</bold> The assistant provided clear, structured derivations for the optimization, SVD, and MAP estimation perspectives, making complex concepts (like the pseudoinverse and Woodbury identity) accessible.</paragraph><paragraph><bold>Limitations:</bold></paragraph><paragraph>While the assistant excelled at algebraic manipulation, it could not directly parse or reason about visual elements (e.g., graph structures or plots). For example, if the problem had included a diagram of the ReLU function or a computational graph, I had to describe it textually for accurate analysis.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-14T12:56:44.53714+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7306483,
            "author": "Xi Cheng",
            "project_title": "Special Participation B: Mistral on HW1 Coding parts",
            "post_body": "Chat history link: https://chat.mistral.ai/chat/3443f2c5-f486-44c5-b8f2-359d56793052\n\nAnnotated Chat history: \n\nExecutive Summary \n\nIn this assignment, I interactively used Mistral to complete two coding TODOs. Overall, Mistral demonstrated strong pattern-matching and code-generation abilities, but also showed limitations in faithfully following constrained instructions.\n\nFor the first TODO, I explicitly instructed Mistral to fill only the missing code while keeping the rest of the solution unchanged. Mistral performed this task well: it inserted the correct code block, respected the constraints, and provided a short explanation. \n\nFor the second TODO, the assignment required modifying only the faster optimizer\u2019s learning rate to improve convergence speed. Despite emphasizing this constraint, Mistral initially violated it by changing both the GD and GDM learning rates. This indicates a tendency to \u201cover-correct\u201d or apply symmetrical changes even when the prompt imposes an asymmetrical constraint. After I provided a clarifying follow-up hint, Mistral produced the correct answer.\n\nOverall Observations\n\nMistral performs reliably on well-specified code-completion tasks, but struggles when instructions require fine-grained constraint adherence. Human intervention\u2014especially clarifications or corrective hints\u2014is essential to steer the model toward assignment-compliant solutions.",
            "content_xml": "<document version=\"2.0\"><paragraph>Chat history link: https://chat.mistral.ai/chat/3443f2c5-f486-44c5-b8f2-359d56793052</paragraph><paragraph>Annotated Chat history: </paragraph><file url=\"https://static.us.edusercontent.com/files/Iy9zXbdrHit2JcHpAz5n3w26\" filename=\"annotated_mistral_hw01code.pdf\"/><heading level=\"2\"><bold>Executive Summary</bold> </heading><paragraph>In this assignment, I interactively used <bold>Mistral</bold> to complete two coding TODOs. Overall, Mistral demonstrated strong pattern-matching and code-generation abilities, but also showed limitations in faithfully following constrained instructions.</paragraph><paragraph>For the first TODO, I explicitly instructed Mistral to fill only the missing code while keeping the rest of the solution unchanged. Mistral performed this task well: it inserted the correct code block, respected the constraints, and provided a short explanation. </paragraph><paragraph>For the second TODO, the assignment required modifying <bold>only the faster optimizer\u2019s learning rate</bold> to improve convergence speed. Despite emphasizing this constraint, Mistral initially violated it by changing <bold>both</bold> the GD and GDM learning rates. This indicates a tendency to \u201cover-correct\u201d or apply symmetrical changes even when the prompt imposes an asymmetrical constraint. After I provided a clarifying follow-up hint, Mistral produced the correct answer.</paragraph><paragraph><bold>Overall Observations</bold></paragraph><paragraph>Mistral performs reliably on <bold>well-specified code-completion tasks</bold>, but struggles when instructions require <bold>fine-grained constraint adherence</bold>. Human intervention\u2014especially clarifications or corrective hints\u2014is essential to steer the model toward assignment-compliant solutions.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-14T09:49:03.620445+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7302906,
            "author": "Oliver Chen",
            "project_title": "Special Participation A: Qwen on HW9",
            "post_body": "For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2\u20135). The performance was very strong -- almost all questions were very quickly solved by directly copy-pasting the question, which had multiple parts often.\n\n\n\nExecutive Summary\n\nUsed qwen thinking, with max thinking context length (81920 tokens).\n\n I also noticed from other attempts that often giving the hint associated with the question (if the question has a hint) generally helps make its answer more likely to be correct, or guides it in the right direction very well.\n\n For Question 3 part b, I had to nudge it to give the answer regarding the new out_features value. However, after reminding it, all of its answers were correct.\n\n I also ignored small formatting issues, and noticed even with formatting issues (some symbols were not pasted/displayed properly), Qwen was still able to understand all the questions and provide correct solutions.\n\n For longer questions (such as question 6), I also thought about pasting/asking the questions in chunks, rather than the entire multi-part question at a time. However, I found that Qwen was able to answer correctly even when giving the entire question at a time.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2\u20135). The performance was very strong -- almost all questions were very quickly solved by directly copy-pasting the question, which had multiple parts often.</paragraph><paragraph/><paragraph><bold>Executive Summary</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/KvPPtjIXnLRvaWpjzDfJcqSZ\" filename=\"Qwen_HW_Report_Formatted.docx\"/><paragraph>Used qwen thinking, with max thinking context length (81920 tokens).<break/><break/> I also noticed from other attempts that often giving the hint associated with the question (if the question has a hint) generally helps make its answer more likely to be correct, or guides it in the right direction very well.<break/><break/> For Question 3 part b, I had to nudge it to give the answer regarding the new out_features value. However, after reminding it, all of its answers were correct.<break/><break/> I also ignored small formatting issues, and noticed even with formatting issues (some symbols were not pasted/displayed properly), Qwen was still able to understand all the questions and provide correct solutions.<break/><break/> For longer questions (such as question 6), I also thought about pasting/asking the questions in chunks, rather than the entire multi-part question at a time. However, I found that Qwen was able to answer correctly even when giving the entire question at a time.<break/><break/></paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-13T15:59:58.033068+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7300719,
            "author": "Leon Kornfeld",
            "project_title": "Special Participation E: Attention and Transformers Visualized with Claude Sonnet 4.5",
            "post_body": "I\u2019d been struggling to reason about how information flows through attention and transformer blocks at a low level\u2014specifically how the matrix multiplications and changing dimensions still produce an output with the same dimensionality as the input. I was also unclear on how multi-head attention differs from single-head attention and how the whole mechanism fits together.\n\nTo sort it out, I used Claude to build a visualization that traces the full path through a transformer. I iterated on it: starting with high-level questions, refining answers, and ending with a 14-step walkthrough that explains attention, the transformer block, and the computations between them.\n\nHere is the published link of the visualization: https://claude.ai/public/artifacts/2ef271c0-bffd-4709-a7e4-cdc2d66bb2b8\n\n\n\nAs I went, my questions became more targeted, drilling into different parts of the architecture.\n\nAt the end, in addition to reviewing Claude's output, looking for any hallucinations, I also pasted the entire source code into GPT-5 to see if it could find any mistakes. GPT was able to point out some assumptions that Claude made that weren't as clear as they could have been. I took GPT's feedback and gave it to Claude for the final iteration of the tool.\n\nHere is my annotated transcript:\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I\u2019d been struggling to reason about how information flows through attention and transformer blocks at a low level\u2014specifically how the matrix multiplications and changing dimensions still produce an output with the same dimensionality as the input. I was also unclear on how multi-head attention differs from single-head attention and how the whole mechanism fits together.</paragraph><paragraph>To sort it out, I used Claude to build a visualization that traces the full path through a transformer. I iterated on it: starting with high-level questions, refining answers, and ending with a 14-step walkthrough that explains attention, the transformer block, and the computations between them.</paragraph><paragraph>Here is the published link of the visualization: <link href=\"https://claude.ai/public/artifacts/2ef271c0-bffd-4709-a7e4-cdc2d66bb2b8\">https://claude.ai/public/artifacts/2ef271c0-bffd-4709-a7e4-cdc2d66bb2b8</link></paragraph><paragraph/><paragraph>As I went, my questions became more targeted, drilling into different parts of the architecture.</paragraph><paragraph>At the end, in addition to reviewing Claude's output, looking for any hallucinations, I also pasted the entire source code into GPT-5 to see if it could find any mistakes. GPT was able to point out some assumptions that Claude made that weren't as clear as they could have been. I took GPT's feedback and gave it to Claude for the final iteration of the tool.</paragraph><paragraph>Here is my annotated transcript:</paragraph><file url=\"https://static.us.edusercontent.com/files/YbYkT62G1SqWbf9FlhoZmEp2\" filename=\"Claude-Transformer attention mechanism visualization.pdf\"/><paragraph/><paragraph/><paragraph/></document>",
            "links": [
                "https://claude.ai/public/artifacts/2ef271c0-bffd-4709-a7e4-cdc2d66bb2b8"
            ],
            "attachments": [],
            "created_at": "2025-11-13T09:46:15.187018+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7297480,
            "author": "Kithmini Herath",
            "project_title": "Special Participation A: Gemini 2.5 Pro on HW5",
            "post_body": "I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the entire problem. I\u2019ve attached an annotated pdf of my interactions with the LLM and a link to my original chat. \n\nIn summary, even without extensive context in my typed prompt, Gemini 2.5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations. \n\nNotable observations: \n\nGood parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across multiple screenshots. This is evident because it often types back key information from the problem statement to formulate the answer. All final answers (after corrections noted below) had no mathematical or conceptual errors. \n\nMisinterpretation of a hint - In question 1b), it used the hint to solve the problem itself. I wonder if this is because it mostly encountered instances of math problems where the hint is used to solve the problem itself. However, when I went through the other special participation threads for HW5 (that were posted as of 11/12/25), I noticed that none of those LLMs used the hint this way and often correctly interpreted it. So, it was interesting to see this behavior from Gemini. When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations. \n\nOverly complicated solutions - In this homework I would like to point out to its answers for the subparts in question 2b) in the attached pdf. While none of the approaches were incorrect, there were steps that were unnecessary to build the solution (annotated in the pdf). \n\nIncomplete reasoning - The answer to 4b) could have been more complete. One could see that from the first term we can derive the transformation we need to apply to $\\check{w}$ to get to equation (3) (as the LLM did). However, I think the answer would be complete if it mentioned what should also happen to the $\\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the form in equation (3) rather than treating it as something obvious. \n\nIn conclusion, I think Gemini 2.5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning. However, some of the solution approaches could be more concise and complete.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the entire problem. I\u2019ve attached an annotated pdf of my interactions with the LLM and a <link href=\"https://gemini.google.com/share/94735d985801\">link</link> to my original chat. </paragraph><file url=\"https://static.us.edusercontent.com/files/Jybc1GxINKXM7ze2At6GOkD9\" filename=\"gemini2.5Pro-chat_2025-11-11.pdf\"/><paragraph>In summary, even without extensive context in my typed prompt, Gemini 2.5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations. </paragraph><paragraph><bold>Notable observations:</bold> </paragraph><list style=\"number\"><list-item><paragraph><bold>Good parsing of information</bold> - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across multiple screenshots. This is evident because it often types back key information from the problem statement to formulate the answer. All final answers (after corrections noted below) had no mathematical or conceptual errors. </paragraph></list-item><list-item><paragraph><bold>Misinterpretation of a hint</bold> - In question 1b), it used the hint to solve the problem itself. I wonder if this is because it mostly encountered instances of math problems where the hint is used to solve the problem itself. However, when I went through the other special participation threads for HW5 (that were posted as of 11/12/25), I noticed that none of those LLMs used the hint this way and often correctly interpreted it. So, it was interesting to see this behavior from Gemini. When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations. </paragraph></list-item><list-item><paragraph><bold>Overly complicated solutions</bold> - In this homework I would like to point out to its answers for the subparts in question 2b) in the attached pdf. While none of the approaches were incorrect, there were steps that were unnecessary to build the solution (annotated in the pdf). </paragraph></list-item><list-item><paragraph><bold>Incomplete reasoning</bold> - The answer to 4b) could have been more complete. One could see that from the first term we can derive the transformation we need to apply to $\\check{w}$ to get to equation (3) (as the LLM did). However, I think the answer would be complete if it mentioned what should also happen to the $\\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the form in equation (3) rather than treating it as something obvious. </paragraph></list-item></list><paragraph>In conclusion, I think Gemini 2.5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning. However, some of the solution approaches could be more concise and complete.</paragraph><math/><paragraph/></document>",
            "links": [
                "https://gemini.google.com/share/94735d985801"
            ],
            "attachments": [],
            "created_at": "2025-11-12T20:46:02.16884+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7295132,
            "author": "Hong Joey",
            "project_title": "HW08 Solutions",
            "post_body": "Apologies for the delay. Attached are solutions to HW08:\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Apologies for the delay. Attached are solutions to HW08:<break/><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/qmCjfUxKqXzFDKsChBILueTS\" filename=\"hw08_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/9oeqJ42RRMM5A20SiY5yMFmI\" filename=\"q_coding_ssm_forward_cpu_sol.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/Z0a4tFJ4asaL0PJFshCF1PSd\" filename=\"q_coding_ssm_forward_gpu_sol.ipynb\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-12T10:45:24.407422+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7293506,
            "author": "Sultan Daniels",
            "project_title": "Tuesday 2-3pm OH Today",
            "post_body": "The Tuesday 2-3pm office hours will still be held in Latimer 102 today. Feel free to come with project-related questions.",
            "content_xml": "<document version=\"2.0\"><paragraph>The Tuesday 2-3pm office hours will still be held in Latimer 102 today. Feel free to come with project-related questions.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-12T06:43:12.467734+11:00",
            "category": "Admin"
        },
        {
            "guid": 7291669,
            "author": "Tianyu Gu",
            "project_title": "Special Participation C: Refactoring HW1 problem 3(h)",
            "post_body": "The original file is split into three files, which makes it reproducible, modular and configurable:\n\nsrc/ \u2013 pure Python package (data, model, trainer, utils)\n\nconfigs/default.yaml \u2013 Hydra config (all hyper-parameters)\n\nnotebooks/demo.ipynb \u2013 thin visualisation notebook (only plots, no logic)\n\n\nGithub Repo: https://github.com/gty864/EECS182-special-participation-C#\n\nReport: \n\nDone by Tianyu Gu, Zhangzhi Xiong,  Aaron Zheng\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>The original file is split into three files, which makes it reproducible, modular and configurable:</paragraph><list style=\"ordered\"><list-item><paragraph>src/ \u2013 pure Python package (data, model, trainer, utils)</paragraph></list-item><list-item><paragraph>configs/default.yaml \u2013 Hydra config (all hyper-parameters)</paragraph></list-item><list-item><paragraph>notebooks/demo.ipynb \u2013 thin visualisation notebook (only plots, no logic)</paragraph></list-item></list><paragraph><break/>Github Repo: <link href=\"https://github.com/gty864/EECS182-special-participation-C#\">https://github.com/gty864/EECS182-special-participation-C#</link></paragraph><paragraph>Report: </paragraph><file url=\"https://static.us.edusercontent.com/files/xgQYLep5iqnPRjitgqtYeJdv\" filename=\"Special Participation C.pdf\"/><paragraph>Done by Tianyu Gu, Zhangzhi Xiong,  Aaron Zheng</paragraph><paragraph/></document>",
            "links": [
                "https://github.com/gty864/EECS182-special-participation-C#"
            ],
            "attachments": [],
            "created_at": "2025-11-11T20:32:01.275817+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7291347,
            "author": "Sultan Daniels",
            "project_title": "HW09 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/kniCPizsj5HST76xjeuZ8g36\" filename=\"hw09codesolution.zip\"/><file url=\"https://static.us.edusercontent.com/files/XzhLemZ4Zjz8p1vF2CUfM2eU\" filename=\"hw09_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/KZaUhjqoiEFimDP3WHPTMt2e\" filename=\"hw09_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T17:38:30.747375+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7289919,
            "author": "Aaryan Chandna",
            "project_title": "Special Participation E: Debugging Exercise with Gemini Guided Learning (GNNs)",
            "post_body": "Personally, I learn well by doing exercises that involve the identification of mistakes or reasons to employ specific techniques. This inspired me to develop a debugging exercise with Gemini's Guided Learning, where I fed the slides for Lecture 12 (GNNs) as input and initially asked the model to provide me with a guide on the lecture slide material over 3 prompts, letting it know in advance to prepare for the debugging exercise. I then had Gemini create 2 debugging exercises for myself to complete, followed by a verification of the answers and explanation. I cannot include the actual link to the conversation unfortunately because Gemini prohibits sharing conversations on school accounts, but I have included the PDF of the interaction trace with my comments.\n\n\n\nReflection - I think providing the lecture slides was a good move as Gemini was able to discuss almost all the concepts from the lecture and explain them well. with some exceptions (didn't discuss the image classification vs semantic segmentation case, didn't get into directed vs undirected graphs). It also mostly prevented hallucination or the usage of formulas that were not in the actual slides, which helped align the discussion with what was relevant towards the course. There was, however, one minor case of hallucination, and the debugging exercises had material/answers that were correct but were not discussed in the guide, which was a significant weakness of this option. Moreover, I was disappointed by the lack of visuals provided by Gemini, even using the Guided Learning mode. The biggest strength of Gemini on this task, in my opinion, was providing relevant and thorough explanations that were still concise enough that I would be able to understand the necessary concepts quickly.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Personally, I learn well by doing exercises that involve the identification of mistakes or reasons to employ specific techniques. This inspired me to develop a debugging exercise with Gemini's Guided Learning, where I fed the slides for Lecture 12 (GNNs) as input and initially asked the model to provide me with a guide on the lecture slide material over 3 prompts, letting it know in advance to prepare for the debugging exercise. I then had Gemini create 2 debugging exercises for myself to complete, followed by a verification of the answers and explanation. I cannot include the actual link to the conversation unfortunately because Gemini prohibits sharing conversations on school accounts, but I have included the PDF of the interaction trace with my comments.</paragraph><paragraph/><paragraph>Reflection - I think providing the lecture slides was a good move as Gemini was able to discuss almost all the concepts from the lecture and explain them well. with some exceptions (didn't discuss the image classification vs semantic segmentation case, didn't get into directed vs undirected graphs). It also mostly prevented hallucination or the usage of formulas that were not in the actual slides, which helped align the discussion with what was relevant towards the course. There was, however, one minor case of hallucination, and the debugging exercises had material/answers that were correct but were not discussed in the guide, which was a significant weakness of this option. Moreover, I was disappointed by the lack of visuals provided by Gemini, even using the Guided Learning mode. The biggest strength of Gemini on this task, in my opinion, was providing relevant and thorough explanations that were still concise enough that I would be able to understand the necessary concepts quickly.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/A2ejEufCSlXMQuag5EyT7KpQ\" filename=\"geminifinal.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T12:53:10.285299+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7289237,
            "author": "Hong Joey",
            "project_title": "HW10 Q5: FaceNet",
            "post_body": "Problem Context: In this problem you will read the FaceNet paper that studies the problem of facial recognition. What is novel about their method is a triplet loss on the embedding space of the images of faces. You may see triplet loss show up in various other representation learning methods.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: In this problem you will read the <link href=\"https://arxiv.org/abs/1503.03832\">FaceNet paper</link> that studies the problem of facial recognition. What is novel about their method is a triplet loss on the embedding space of the images of faces. You may see triplet loss show up in various other representation learning methods.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/xcc0yIWpiJVXudS4pwrcfWpY\" width=\"658\" height=\"683.2357954545455\"/></figure></document>",
            "links": [
                "https://arxiv.org/abs/1503.03832"
            ],
            "attachments": [],
            "created_at": "2025-11-11T11:00:41.345069+11:00",
            "category": "Admin"
        },
        {
            "guid": 7289177,
            "author": "Hong Joey",
            "project_title": "HW10 Q4: Example Difficulty and Early Exit",
            "post_body": "Problem Context: First, you will try to evaluate the difficulty of problems by training a classifier on learned representations by a neural network. Then, you will show how predicting difficulty can be used for \"early-exiting\" to improve inference time. Much of this problem was inspired by this paper and this paper. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: First, you will try to evaluate the difficulty of problems by training a classifier on learned representations by a neural network. Then, you will show how predicting difficulty can be used for \"early-exiting\" to improve inference time. Much of this problem was inspired by <link href=\"https://arxiv.org/abs/2106.09647\">this paper</link> and <link href=\"https://arxiv.org/abs/1709.01686\">this paper.</link> </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/gphs8phQ0SZQwqf2za88TQAZ\" width=\"658\" height=\"575.2690058479533\"/></figure></document>",
            "links": [
                "https://arxiv.org/abs/2106.09647",
                "https://arxiv.org/abs/1709.01686"
            ],
            "attachments": [],
            "created_at": "2025-11-11T10:52:46.289197+11:00",
            "category": "Admin"
        },
        {
            "guid": 7289140,
            "author": "Hong Joey",
            "project_title": "HW10 Q3: Summarization",
            "post_body": "Problem Context: This problem makes you implement and train an encoder-decoder transformer on a simple task of text summarization. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This problem makes you implement and train an encoder-decoder transformer on a simple task of text summarization. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/oOhj8ictq8pVvNDOsfMWuB6y\" width=\"658\" height=\"207.07647058823528\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T10:45:53.129966+11:00",
            "category": "Admin"
        },
        {
            "guid": 7289135,
            "author": "Hong Joey",
            "project_title": "HW10 Q2: Hand-Design Transformers",
            "post_body": "Problem context: \n\nThis problem asks you to implement scaled dot-product attention as well as hand-design attention matrices to perform certain operations, the same way that you did hand-design convolutional matrices for CNNs.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem context: </paragraph><paragraph>This problem asks you to implement scaled dot-product attention as well as hand-design attention matrices to perform certain operations, the same way that you did hand-design convolutional matrices for CNNs.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/Ycrlk956H7KaBMog0f87jHlG\" width=\"658\" height=\"258.5347119645495\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T10:44:24.318588+11:00",
            "category": "Admin"
        },
        {
            "guid": 7289110,
            "author": "Hong Joey",
            "project_title": "HW10 Q1: Kernelized Linear Attention (Part II)",
            "post_body": "Problem Context: This is part 2 of the problem you saw on the last homework. It shows how kernelized approximations to softmax attention can be done with the same complexity as state-space models.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is part 2 of the problem you saw on the last homework. It shows how kernelized approximations to softmax attention can be done with the same complexity as state-space models.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/1pD3LSFeJOWEZeTVyYbQgzJQ\" width=\"658\" height=\"679.8699421965318\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/dfcZ0rcJzzyNaZS5Pt2kEfRJ\" width=\"658\" height=\"521.6217851739788\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T10:41:01.313806+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7287291,
            "author": "Zhangzhi 'csrookie' Xiong",
            "project_title": "Special Participation E: Grok4(fast) as a teacher on SSM",
            "post_body": "In this part, I will use Grok4 (fast) as a teacher to teach me about SSM (State Space Model). I will try to let LLM to fully help me to understand this knowledge. I will attach my prompt and LLM's response (they are exported as a separated file), and my personal comment on LLM's response.",
            "content_xml": "<document version=\"2.0\"><paragraph>In this part, I will use Grok4 (fast) as a teacher to teach me about SSM (State Space Model). I will try to let LLM to fully help me to understand this knowledge. I will attach my prompt and LLM's response (they are exported as a separated file), and my personal comment on LLM's response.</paragraph><file url=\"https://static.us.edusercontent.com/files/6B4exsCcmrwY1lnSOsMjV3WB\" filename=\"grok.pdf\"/><file url=\"https://static.us.edusercontent.com/files/cPZVGFVnVIPMaIPzG0VzYqlL\" filename=\"log.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-11T06:28:07.024815+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7285257,
            "author": "Anant Sahai",
            "project_title": "Mid-term survey: please fill out",
            "post_body": "https://forms.gle/zwoSh2EHbArQt7maA\n\nDear students,\n\nWe want to get your thoughts. This is a little past the midpoint of the semester, but is a natural point to ask since you've all just started on your projects and had meetings with the course staff.\n\nWe estimate that the survey will take about 30min to complete. Please do so.\n\nhttps://forms.gle/zwoSh2EHbArQt7maA\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><link href=\"https://forms.gle/zwoSh2EHbArQt7maA\">https://forms.gle/zwoSh2EHbArQt7maA</link></paragraph><paragraph>Dear students,</paragraph><paragraph>We want to get your thoughts. This is a little past the midpoint of the semester, but is a natural point to ask since you've all just started on your projects and had meetings with the course staff.</paragraph><paragraph>We estimate that the survey will take about 30min to complete. Please do so.</paragraph><paragraph><link href=\"https://forms.gle/zwoSh2EHbArQt7maA\">https://forms.gle/zwoSh2EHbArQt7maA</link></paragraph><paragraph/><paragraph/></document>",
            "links": [
                "https://forms.gle/zwoSh2EHbArQt7maA",
                "https://forms.gle/zwoSh2EHbArQt7maA"
            ],
            "attachments": [],
            "created_at": "2025-11-10T19:06:49.86163+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7283953,
            "author": "Jameson Liu",
            "project_title": "Special Participation A: ChatGPT on HW6",
            "post_body": "I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response, it was able to one-shot a majority of the questions. Some of its solutions, however, contained imprecise language. For example, in 2d, it just described how the max function worked, so I had to convince it that it would be a one-hot encoding by induction. This was interesting since it seemed like the question contained enough context for this to be concluded. Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b. Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong. Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response, it was able to one-shot a majority of the questions. Some of its solutions, however, contained imprecise language. For example, in 2d, it just described how the max function worked, so I had to convince it that it would be a one-hot encoding by induction. This was interesting since it seemed like the question contained enough context for this to be concluded. Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b. Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong. Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects. </paragraph><file url=\"https://static.us.edusercontent.com/files/cC22W1e1GrgfPYK89OUwityi\" filename=\"chatgpt.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-10T13:51:11.522627+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7283211,
            "author": "Jameson Liu",
            "project_title": "Special Participation B: DeepSeek on HW1",
            "post_body": "I used DeepSeek to solve the coding parts of homework 1. I initially prompted it by uploading the entire .ipynb file, saying it was an expert in deep learning, and asking it to fill in the TODOs. There were two parts to fill in:\n\n1. It was almost able to one-shot this, but had a few implementation mistakes (gradient initialization for momentum, coefficient order). Perhaps I should have provided more context so that it could have implemented these details correctly. I then asked DeepSeek to correct this, and it took a few tries before getting it right. \n2. This is not an error of the LLM since this required running code, but the learning rate it picked was too high (diverged), so I told it, and it picked a better one.\n\nI also told it not to answer any conceptual questions that depended on running code to answer, but it still answered these. They turned out to be alright, so I guess DeepSeek was confident enough based on its knowledge. In the middle of my chat log, I was curious why it implemented momentum incorrectly the first time, and it gave me a good explanation of its version and its pros and cons. \n\nOverall, DeepSeek was pretty good at solving this homework, and probably could have one-shotted it completely with more context. \n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used DeepSeek to solve the coding parts of homework 1. I initially prompted it by uploading the entire .ipynb file, saying it was an expert in deep learning, and asking it to fill in the TODOs. There were two parts to fill in:<break/><break/>1. It was almost able to one-shot this, but had a few implementation mistakes (gradient initialization for momentum, coefficient order). Perhaps I should have provided more context so that it could have implemented these details correctly. I then asked DeepSeek to correct this, and it took a few tries before getting it right. <break/>2. This is not an error of the LLM since this required running code, but the learning rate it picked was too high (diverged), so I told it, and it picked a better one.<break/><break/>I also told it not to answer any conceptual questions that depended on running code to answer, but it still answered these. They turned out to be alright, so I guess DeepSeek was confident enough based on its knowledge. In the middle of my chat log, I was curious why it implemented momentum incorrectly the first time, and it gave me a good explanation of its version and its pros and cons. <break/><break/>Overall, DeepSeek was pretty good at solving this homework, and probably could have one-shotted it completely with more context. <break/></paragraph><file url=\"https://static.us.edusercontent.com/files/1DYWQgprgx9cdeH2t5NxI9qt\" filename=\"deepseek.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-10T11:42:59.228283+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7281484,
            "author": "Ruizhe Song",
            "project_title": "Special Participation E: ChatGPT as a post-lecture learning approach",
            "post_body": "I usually have some new questions when I review my notes after class. Some questions indicate that I don't fully understand the specific deep learning mechanism on the class, while others are about the reason why we are using these techniques(in other words why something seems pretty normal but works well). This ed post is about the first kind of questions, which I don't fully understand on the class, and managed to solve with the help of ChatGPT as a powerful post-lecture learning tool.\n\nIn this special participation E, I plan to share my post-lecture self-learning trace after Lecture 18 & 19 with ChatGPT5. The two classes cover the important attention mechanism and Transformer architecture. After the post-lecture learning process, I understand the attention layer and transformer better. It even makes the coming classes and discussions clearer to me.\n\nHere's the pdf version of chat log with my comments:\n\nIn this log, I asked questions step-by-step, and let GPT guide me to change my way of thinking in RNN and SSM to attention mechanism and Transformer architecture. During this process, GPT successfully:\n\n1. Corrected my wrong understanding about how attention layer works. \n\n2. Made concrete examples to illustrate the training/inference process in transformer architecture and attention layers, which helped me understand much more quickly.\n\n3. Gave mathematical representations and detailed illustrations of them to make the whole intuition clear.",
            "content_xml": "<document version=\"2.0\"><paragraph>I usually have some new questions when I review my notes after class. Some questions indicate that I don't fully understand the specific deep learning mechanism on the class, while others are about the reason why we are using these techniques(in other words why something seems pretty normal but works well). This ed post is about the <italic><underline>first</underline></italic> kind of questions, which I don't fully understand on the class, and managed to solve with the help of ChatGPT <bold>as a powerful post-lecture learning tool</bold>.</paragraph><paragraph>In this special participation E, I plan to share my post-lecture self-learning trace after <bold>Lecture 18 &amp; 19</bold> with <bold>ChatGPT5</bold>. The two classes cover the important <bold>attention mechanism and Transformer architecture</bold>. After the post-lecture learning process, I understand the attention layer and transformer better. It even makes the coming classes and discussions clearer to me.</paragraph><paragraph>Here's the pdf version of chat log with my comments:</paragraph><file url=\"https://static.us.edusercontent.com/files/ydpSFWtljJ9a0RoRqsbAub7D\" filename=\"Special-Participation-E.pdf\"/><paragraph>In this log, I asked questions step-by-step, and let GPT guide me to change my way of thinking in RNN and SSM to attention mechanism and Transformer architecture. During this process, GPT successfully:</paragraph><paragraph>1. Corrected my wrong understanding about how attention layer works. </paragraph><paragraph>2. Made concrete examples to illustrate the training/inference process in transformer architecture and attention layers, which helped me understand much more quickly.</paragraph><paragraph>3. Gave mathematical representations and detailed illustrations of them to make the whole intuition clear.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-10T06:26:24.178702+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7280407,
            "author": "Tianyu Gu",
            "project_title": "Special Participation B: Kimi on HW0",
            "post_body": "I use Kimi on coding part of HW0, I provide the code task step by step, and Kimi can always fill the code in the guidance of comment. (I don't know why I can't print all the pages in the talk, when I try to print it to pdf,  it only allows me to print one page. So I just copy all the text.)\n\nHere is the link:  https://www.kimi.com/share/19a68035-1a42-8042-8000-0000dd0233b5\n\nIn a nutshell, Kimi has done a great job except for the final part. Kimi can solve almost all coding part of hw0 in one shot, but cannot provide a reasonable value for parameters such as learning rate and weight scale. This might because the coding part in hw0 is very standard.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/JO59CBnPjUV5ccYoOy3KXgbN\" filename=\"special participation b.pdf\"/><paragraph>I use Kimi on coding part of HW0, I provide the code task step by step, and Kimi can always fill the code in the guidance of comment. (I don't know why I can't print all the pages in the talk, when I try to print it to pdf,  it only allows me to print one page. So I just copy all the text.)</paragraph><paragraph>Here is the link:  https://www.kimi.com/share/19a68035-1a42-8042-8000-0000dd0233b5</paragraph><paragraph>In a nutshell, Kimi has done a great job except for the final part. Kimi can solve almost all coding part of hw0 in one shot, but cannot provide a reasonable value for parameters such as learning rate and weight scale. This might because the coding part in hw0 is very standard.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-09T21:30:30.701332+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7280265,
            "author": "Zhangzhi 'csrookie' Xiong",
            "project_title": "Participation Section E: Grok3 (fast) as a teacher on Initialization",
            "post_body": "In this part, I will use Grok3 (fast) as a teacher to teach me about Initialization in deeplearning and its significant importance. I will try to let LLM to fully help me to understand this knowledge. I will attach my prompt and LLM's response (they are exported as a separated file), and my personal comment on LLM's response. \n\nThe commented log has been attached below:\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>In this part, I will use Grok3 (fast) as a teacher to teach me about Initialization in deeplearning and its significant importance. I will try to let LLM to fully help me to understand this knowledge. I will attach my prompt and LLM's response (they are exported as a separated file), and my personal comment on LLM's response. </paragraph><paragraph>The commented log has been attached below:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/iJrQgXsbMPnOO1Ga9mliE5sF\" filename=\"submission.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-09T18:40:32.180113+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7280264,
            "author": "Ruizhe Song",
            "project_title": "Special Participation B: ChatGPT5 on HW5",
            "post_body": "I interactively engaged ChatGPT5 on the coding parts of Homework 5. Overall, the model was able to provide correct code implementations in almost every problem, even if it ignore some small points such as inverse a possible singular matrix, it can fix the bug with the error information easily.\n\nStrategies: I first clarified the main role that GPT was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:\n\nStep 1: Provide Gemini with problem background and basic description.\n\nStep 2: Give the necessary code for it to implement or the output of a cell for it to analyze.\n\nStep 3: Collect the answers and give it feedback. If the answer was incorrect, or have small bugs inside, I provide hints to guide it to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.\n\nStep 4: Repeat Steps 1\u20133 for all homework problems.\n\nCore Observations:\n\n1. ChatGPT has wonderful code completion ability even its prompt is only based on part of the whole JupyterNotebook. Almost all the generated codes are correct at the first time and can easily run on the colab homework. \n\n2. ChatGPT also have great debug ability simply with the hints from the error information of the code cell\u2019s output. There was a time when it generated a singular matrix and tried to inverse it. And it quickly fixed the problem by modifying the line causing the matrix to be singular.\n\n3. ChatGPT can even answer some questions which requires the result from the code cell, though it can\u2019t actually run it. In other words, it generates the correct answer which should be output by the correctly implemented code cell. As an LLM, this is amazing.\n\n4. However, when I keep testing the ability listed above, GPT began to make some mistakes especially when the question is related to a specific number. I think GPT can derive some simple code cell\u2019s output from a mathematical way, like solving a written problem. But can not handle complex codes.\n\nFor the code analysis questions, I provided GPT with some curve images which are necessary. And GPT can derive good intuition from the results such as the loss curve.\n\nHere's my chatting log: https://chatgpt.com/share/691038f2-af08-800f-bc33-ed03352ab4c0\n\nAnd the simplified pdf version with my comments: ",
            "content_xml": "<document version=\"2.0\"><paragraph>I interactively engaged <bold>ChatGPT5</bold> on the coding parts of <bold>Homework 5</bold>. Overall, the model was able to provide correct code implementations in almost every problem, even if it ignore some small points such as inverse a possible singular matrix, it can fix the bug with the error information easily.</paragraph><paragraph><bold>Strategies:</bold> I first clarified the main role that GPT was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Step 1:</bold> Provide Gemini with problem background and basic description.</paragraph></list-item><list-item><paragraph><bold>Step 2:</bold> Give the necessary code for it to implement or the output of a cell for it to analyze.</paragraph></list-item><list-item><paragraph><bold>Step 3:</bold> Collect the answers and give it feedback. If the answer was incorrect, or have small bugs inside, I provide hints to guide it to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.</paragraph></list-item><list-item><paragraph><bold>Step 4:</bold> Repeat Steps 1\u20133 for all homework problems.</paragraph></list-item></list><paragraph><bold><bold>Core Observations:</bold></bold></paragraph><paragraph>1. ChatGPT has wonderful code completion ability even its prompt is only based on part of the whole JupyterNotebook. Almost all the generated codes are correct at the first time and can easily run on the colab homework. </paragraph><paragraph>2. ChatGPT also have great debug ability simply with the hints from the error information of the code cell\u2019s output. There was a time when it generated a singular matrix and tried to inverse it. And it quickly fixed the problem by modifying the line causing the matrix to be singular.</paragraph><paragraph>3. ChatGPT can even answer some questions which requires the result from the code cell, though it can\u2019t actually run it. In other words, it generates the correct answer which should be output by the correctly implemented code cell. As an LLM, this is amazing.</paragraph><paragraph>4. However, when I keep testing the ability listed above, GPT began to make some mistakes especially when the question is related to a specific number. I think GPT can derive some simple code cell\u2019s output from a mathematical way, like solving a written problem. But can not handle complex codes.</paragraph><paragraph>For the code analysis questions, I provided GPT with some curve images which are necessary. And GPT can derive good intuition from the results such as the loss curve.<break/><break/>Here's my chatting log: <link href=\"https://chatgpt.com/share/691038f2-af08-800f-bc33-ed03352ab4c0\">https://chatgpt.com/share/691038f2-af08-800f-bc33-ed03352ab4c0</link><break/><break/>And the simplified pdf version with my comments: </paragraph><file url=\"https://static.us.edusercontent.com/files/cRCmC83lFv5pr8ymAh5Tg0Ef\" filename=\"hw5-participationB-ChatGPT5-comments.pdf\"/></document>",
            "links": [
                "https://chatgpt.com/share/691038f2-af08-800f-bc33-ed03352ab4c0"
            ],
            "attachments": [],
            "created_at": "2025-11-09T18:40:29.641542+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7280258,
            "author": "Zhangzhi 'csrookie' Xiong",
            "project_title": "Participation Section C: Gemini Pro 2.5 on HW3 (e)",
            "post_body": "I'm using Gemini 2.5 Pro to rewrite HW3 (e) into engineering code. The file has been attached above. \nFrom my perspective, a good engineering python deeplearning code should at least include:\n\nArgparse to input parameter[1] \n\nSystemetic log or wandb[2] \n\nSave and load of the torch checkpoint, and Checkpointing[3] \n\nThe save of the results bash script for rapid experiments[4] \n\n[1]https://docs.python.org/3/library/argparse.html \n\n[2]https://docs.wandb.ai/support/ \n\n[3]https://docs.pytorch.org/docs/stable/checkpoint.html \n\n[4]https://www.freecodecamp.org/news/shell-scripting-crash-course-how-to-write-b ash-scripts-in-linux/\n\nPython and bash files are attached below:\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/YPLDd7is0sHCAChT6ZonvgIO\" filename=\"log.pdf\"/><paragraph>I'm using Gemini 2.5 Pro to rewrite HW3 (e) into engineering code. The file has been attached above. <break/>From my perspective, a good engineering python deeplearning code should at least include:</paragraph><paragraph>Argparse to input parameter[1] </paragraph><paragraph>Systemetic log or wandb[2] </paragraph><paragraph>Save and load of the torch checkpoint, and Checkpointing[3] </paragraph><paragraph>The save of the results bash script for rapid experiments[4] </paragraph><paragraph>[1]https://docs.python.org/3/library/argparse.html </paragraph><paragraph>[2]https://docs.wandb.ai/support/ </paragraph><paragraph>[3]https://docs.pytorch.org/docs/stable/checkpoint.html </paragraph><paragraph>[4]https://www.freecodecamp.org/news/shell-scripting-crash-course-how-to-write-b ash-scripts-in-linux/</paragraph><paragraph>Python and bash files are attached below:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/XJJKcowL57LmUoGro2uHuMs6\" filename=\"run_experiment.py\"/><file url=\"https://static.us.edusercontent.com/files/fy1Jij6qj2gV9Ds9HJ6mR6FG\" filename=\"sweep.sh\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-09T18:36:06.176404+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7280239,
            "author": "Andy Zhang",
            "project_title": "Special Participation E: Quiz and Flash Cards for Adam / SGD",
            "post_body": "Special Participation E: Quiz and Flash Cards for Adam / SGD\n\nFor background, Gemini claims that they can create quizzes, flash cards & study guides according to https://support.google.com/gemini/answer/16275879?hl=en&co=GENIE.Platform%3DAndroid\n\nEspecially since Gemini Pro is free for Berkeley students, I first explored and confirmed that Gemini could indeed create those; it created specific and specialized artifacts for only Quizzes and Flash Cards (and you need to be careful about language) which made it more convenient than text.\n\nWhile the initial the quiz and flash cards were reasonable, they are generally conceptual (e.g. the purpose of the first and second moments of Adam, the purpose of hyperparameters etc.) so I wanted to explore whether the quizzes and flash cards could be more mathematical and cover more rigor similar to our course and see whether latex could properly render in the quiz and flash cards.\n\nFrom there I was able to develop an improved prompt to have more mathematics to be more aligned with our course, formatted properly in latex. And depending on one\u2019s presences, the model can be steered to act accordingly. \n\nMy recommendation is that these could be helpful to students to review concepts such as Adam and SGD especially after several weeks have passed before the final. My recommended order is to go through the flash cards for a memory refresher and then take the quiz to ensure the concepts are cemented.\n\nTraces with detailed comments: https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing\n\nIndividual traces:\n\nUndetailed Prompt Trace Flash Cards:\n\nhttps://gemini.google.com/share/ed66de185158\n\nUndetailed Prompt Trace Quiz:\n\nhttps://gemini.google.com/share/598e4b5cfbcc\n\nUndetailed Flash Cards:\n\nhttps://gemini.google.com/share/cfd44ed946d6\n\nUndetailed Quiz:\n\nhttps://gemini.google.com/share/6fab8238136e\n\nFlash cards / quizzes:\n\nDetailed Prompt Trace for Flash Cards with Math:\n\nhttps://gemini.google.com/share/f7957da6c0d1\n\nDetailed Prompt Trace for Quiz:\n\nhttps://gemini.google.com/share/ec87b322ca7e\n\nDetailed Flash Cards: https://gemini.google.com/share/6afed4acff7b\n\nDetailed Quiz: https://gemini.google.com/share/01910a14266a",
            "content_xml": "<document version=\"2.0\"><paragraph>Special Participation E: Quiz and Flash Cards for Adam / SGD</paragraph><paragraph>For background, Gemini claims that they can create quizzes, flash cards &amp; study guides according to <link href=\"https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid\">https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid</link></paragraph><paragraph>Especially since Gemini Pro is free for Berkeley students, I first explored and confirmed that Gemini could indeed create those; it created specific and specialized artifacts for only Quizzes and Flash Cards (and you need to be careful about language) which made it more convenient than text.</paragraph><paragraph>While the initial the quiz and flash cards were reasonable, they are generally conceptual (e.g. the purpose of the first and second moments of Adam, the purpose of hyperparameters etc.) so I wanted to explore whether the quizzes and flash cards could be more mathematical and cover more rigor similar to our course and see whether latex could properly render in the quiz and flash cards.</paragraph><paragraph>From there I was able to develop an improved prompt to have more mathematics to be more aligned with our course, formatted properly in latex. And depending on one\u2019s presences, the model can be steered to act accordingly. </paragraph><paragraph>My recommendation is that these could be helpful to students to review concepts such as Adam and SGD especially after several weeks have passed before the final. My recommended order is to go through the flash cards for a memory refresher and then take the quiz to ensure the concepts are cemented.</paragraph><paragraph>Traces with detailed comments: <link href=\"https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing\">https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing</link></paragraph><paragraph>Individual traces:</paragraph><paragraph>Undetailed Prompt Trace Flash Cards:</paragraph><paragraph><link href=\"https://gemini.google.com/share/ed66de185158\">https://gemini.google.com/share/ed66de185158</link></paragraph><paragraph>Undetailed Prompt Trace Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/598e4b5cfbcc\">https://gemini.google.com/share/598e4b5cfbcc</link></paragraph><paragraph>Undetailed Flash Cards:</paragraph><paragraph><link href=\"https://gemini.google.com/share/cfd44ed946d6\">https://gemini.google.com/share/cfd44ed946d6</link></paragraph><paragraph>Undetailed Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/6fab8238136e\">https://gemini.google.com/share/6fab8238136e</link></paragraph><paragraph>Flash cards / quizzes:</paragraph><paragraph>Detailed Prompt Trace for Flash Cards with Math:</paragraph><paragraph><link href=\"https://gemini.google.com/share/f7957da6c0d1\">https://gemini.google.com/share/f7957da6c0d1</link></paragraph><paragraph>Detailed Prompt Trace for Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/ec87b322ca7e\">https://gemini.google.com/share/ec87b322ca7e</link></paragraph><paragraph>Detailed Flash Cards: <link href=\"https://gemini.google.com/share/6afed4acff7b\">https://gemini.google.com/share/6afed4acff7b</link></paragraph><paragraph>Detailed Quiz: <link href=\"https://gemini.google.com/share/01910a14266a\">https://gemini.google.com/share/01910a14266a</link></paragraph></document>",
            "links": [
                "https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid",
                "https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing",
                "https://gemini.google.com/share/ed66de185158",
                "https://gemini.google.com/share/598e4b5cfbcc",
                "https://gemini.google.com/share/cfd44ed946d6",
                "https://gemini.google.com/share/6fab8238136e",
                "https://gemini.google.com/share/f7957da6c0d1",
                "https://gemini.google.com/share/ec87b322ca7e",
                "https://gemini.google.com/share/6afed4acff7b",
                "https://gemini.google.com/share/01910a14266a"
            ],
            "attachments": [],
            "created_at": "2025-11-09T18:21:02.198615+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7276004,
            "author": "Bruno Vieira",
            "project_title": "Special Participation E: ChatGPT Lecture Comprehension Buddy",
            "post_body": "I used ChatGPT's \"Study\" mode to help me better understand lectures. Many times, I find myself not really knowing how to prepare ahead of lectures, and I always have questions that get passed on to more questions in discussions, and so I just find myself sometimes \"drowning\" and wishing I had more support to answer some basic questions that can help me not get behind. For this, I decided to test ChatGPT with Thursday 11/07's lecture to help me prepare before the lecture and help me get all my questions answered before next week's lecture. \n\nBelow, I have uploaded the full conversation I had with comments. I also put here a pre-lecture PDF that ChatGPT gave me.\n\nIn all honesty, the summary is that I preferred Claude as it is better with visuals, and I thought that ChatGPT didn't really go that deep into the content. On the other hand, the conversation we had before I went to the lecture drastically improved my experience in the lecture - highly recommend!!\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used ChatGPT's \"Study\" mode to help me better understand lectures. Many times, I find myself not really knowing how to prepare ahead of lectures, and I always have questions that get passed on to more questions in discussions, and so I just find myself sometimes \"drowning\" and wishing I had more support to answer some basic questions that can help me not get behind. For this, I decided to test ChatGPT with Thursday 11/07's lecture to help me prepare before the lecture and help me get all my questions answered before next week's lecture. </paragraph><paragraph>Below, I have uploaded the full conversation I had with comments. I also put here a pre-lecture PDF that ChatGPT gave me.</paragraph><paragraph>In all honesty, the summary is that I preferred Claude as it is better with visuals, and I thought that ChatGPT didn't really go that deep into the content. On the other hand, the conversation we had before I went to the lecture drastically improved my experience in the lecture - highly recommend!!</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/4P4H8nUZm0uWXT3GrMuApHNN\" filename=\"Prelecture_Notes_InContextLearning_PEFT.pdf\"/><file url=\"https://static.us.edusercontent.com/files/gLCYq39bF14aT98XEDEY6GaN\" filename=\"Prelecture notes guide.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-08T14:41:05.880912+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7275891,
            "author": "Lance Mathias",
            "project_title": "Discussion 10 Solutions",
            "post_body": "Attached are questions and solutions to Discussion 10:\n\nNote: Q3 (NoPE) has been updated to reflect the fact that the third embedding dimension can actually be arbitrary. This does not affect the main result of the question. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are questions and solutions to Discussion 10:</paragraph><file url=\"https://static.us.edusercontent.com/files/aXi8aeRAKNhq1uaHk0z2vCnd\" filename=\"dis10_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/yux6PKMcpkzqAceHWIewrK0U\" filename=\"dis10_solution.pdf\"/><paragraph>Note: Q3 (NoPE) has been updated to reflect the fact that the third embedding dimension can actually be arbitrary. This does not affect the main result of the question. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-08T14:04:01.414238+11:00",
            "category": "Sections"
        },
        {
            "guid": 7275881,
            "author": "Lance Mathias",
            "project_title": "Discussion 9 Solutions",
            "post_body": "Attached are questions and solutions to Discussion 9:",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are questions and solutions to Discussion 9:</paragraph><file url=\"https://static.us.edusercontent.com/files/k6ma53OS7Xdfyaap3ZP7QXtj\" filename=\"dis09_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/DYhfPbiN9rEdqy4jikkqsjNa\" filename=\"dis09_solution.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-08T14:02:00.429128+11:00",
            "category": "Sections"
        },
        {
            "guid": 7275838,
            "author": "Alex Proshkin",
            "project_title": "JupyterHub Compute Tutorial",
            "post_body": "Hi everyone,\nI\u2019ll be hosting a 30-minute tutorial on how to use the JupyterHub Compute on Monday, November 10th at 2:30 PM over Zoom.\nYou can join using the link below:\n\nhttps://berkeley.zoom.us/j/97718967725\n\nBest,\nAlex\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,<break/>I\u2019ll be hosting a 30-minute tutorial on how to use the JupyterHub Compute on Monday, November 10th at 2:30 PM over Zoom.<break/>You can join using the link below:</paragraph><paragraph><link href=\"https://berkeley.zoom.us/j/97718967725\">https://berkeley.zoom.us/j/97718967725</link></paragraph><paragraph>Best,<break/>Alex</paragraph><paragraph/></document>",
            "links": [
                "https://berkeley.zoom.us/j/97718967725"
            ],
            "attachments": [],
            "created_at": "2025-11-08T13:52:19.440328+11:00",
            "category": "Admin"
        },
        {
            "guid": 7273818,
            "author": "Gireeja Ranade",
            "project_title": "Reminder to sign up for Tinker",
            "post_body": "Sending out one last reminder for those who want to use this option. Note that by signing up you are not committing to use this, it just gives you the option to use it if you want to:\n\n\n\nPost repeated below:\n\nDear students, \n\nWe have secured access to Tinker, a new API from Thinking Machines that provides fine-tuning support for many popular open-weight models. You can read more here:\n\nhttps://thinkingmachines.ai/tinker/\n\nWe assume most of you would like access to this for your project/to try out things otherwise. This requires us to create an account for you on their API using your name/email. If you would prefer that we not share your name and email with Thinking Machines to get you access, please opt-out using the form below. If you want to guarantee you get access, please also indicate this below. \n\nhttps://forms.gle/vUuQwYeFHgtqX74z8\n\nGiven a tight timeline, there is a 24 hour deadline to opt-in/out-out. Form closes at 10 am Nov 7.\n\nThanks,\n\n182 Staff. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Sending out one last reminder for those who want to use this option. Note that by signing up you are not committing to use this, it just gives you the option to use it if you want to:</paragraph><paragraph/><paragraph>Post repeated below:</paragraph><paragraph>Dear students, </paragraph><paragraph>We have secured access to Tinker, a new API from Thinking Machines that provides fine-tuning support for many popular open-weight models. You can read more here:</paragraph><paragraph><link href=\"https://thinkingmachines.ai/tinker/\">https://thinkingmachines.ai/tinker/</link></paragraph><paragraph>We assume most of you would like access to this for your project/to try out things otherwise. This requires us to create an account for you on their API using your name/email. If you would prefer that we not share your name and email with Thinking Machines to get you access, please opt-out using the form below. If you want to guarantee you get access, please also indicate this below. </paragraph><paragraph><link href=\"https://forms.gle/vUuQwYeFHgtqX74z8\">https://forms.gle/vUuQwYeFHgtqX74z8</link></paragraph><paragraph>Given a tight timeline, there is a 24 hour deadline to opt-in/out-out. Form closes at 10 am Nov 7.</paragraph><paragraph>Thanks,</paragraph><paragraph>182 Staff. </paragraph></document>",
            "links": [
                "https://thinkingmachines.ai/tinker/",
                "https://forms.gle/vUuQwYeFHgtqX74z8"
            ],
            "attachments": [],
            "created_at": "2025-11-08T06:54:50.810859+11:00",
            "category": "Admin"
        },
        {
            "guid": 7267863,
            "author": "Joe Berry",
            "project_title": "Special Participation D second submission HW4 Joseph Berry",
            "post_body": "I edited HW4 to include a section about AdaMuon and how changing batch size and learning rate may effect performance of MLP vs CNN\n\nI edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.\n\n\nSi, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" arXiv, 18 Aug. 2025, https://doi.org/10.48550/arXiv.2507.11005.\n\nJordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" Keller Jordan Blog, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.\n\nBernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" arXiv, 6 Dec. 2024, https://doi.org/10.48550/arXiv.2409.20325.",
            "content_xml": "<document version=\"2.0\"><paragraph>I edited HW4 to include a section about AdaMuon and how changing batch size and learning rate may effect performance of MLP vs CNN<break/><break/>I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.<break/><break/><break/>Si, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" <italic>arXiv</italic>, 18 Aug. 2025, <link href=\"https://doi.org/10.48550/arXiv.2507.11005\">https://doi.org/10.48550/arXiv.2507.11005</link>.<break/><break/>Jordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" <italic>Keller Jordan Blog</italic>, 8 Dec. 2024, <link href=\"http://kellerjordan.github.io/posts/muon/\">kellerjordan.github.io/posts/muon/</link>.</paragraph><paragraph>Bernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" <italic>arXiv</italic>, 6 Dec. 2024, <link href=\"https://doi.org/10.48550/arXiv.2409.20325\">https://doi.org/10.48550/arXiv.2409.20325</link>.</paragraph><file url=\"https://static.us.edusercontent.com/files/Y2KUv4fOfq8g4IkYJ55nkO3F\" filename=\"edge_detection_sol-AdaMuon+LR_and_BatchSize_Added.ipynb\"/></document>",
            "links": [
                "https://doi.org/10.48550/arXiv.2507.11005",
                "http://kellerjordan.github.io/posts/muon/",
                "https://doi.org/10.48550/arXiv.2409.20325"
            ],
            "attachments": [],
            "created_at": "2025-11-07T05:40:26.301835+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7267614,
            "author": "Yaqi Su",
            "project_title": "Special Participation B: Claude on HW2 coding part",
            "post_body": "This interaction shows Claude's ability to implement standard DL algorithms correctly but with notable instruction-following issues. Claude nailed the first two and the last tasks immediately, translating formulas into working code on the first try. The third task got messier, although the HW hint explicitly mentioned \"adjust the two previous functions,\" Claude created entirely new functions instead, ignoring the specific hint and I needed to re-emphasize it in the prompt before Claude realized this. More problematic was Claude's unprompted adjustment to second-layer bias initialization, which as I noted didn't make sense and was irrelevant to my prompt. When I questioned this, Claude admitted the change was \"not well-motivated\" and unnecessary. Besides, it also made changes to the learning rate (which is also irrelevant to my prompt) when making modification to the codes. This revealed Claude making unjustified \u201csilent\u201d modifications without any explanation or upfront reasoning, requiring me to catch and correct the error/changes it made. While Claude's implementations were technically sound once corrected, the process highlighted real gaps in following specifications and a tendency to add unnecessary (or even wrong) modifications without any upfront notifications/justifications. ",
            "content_xml": "<document version=\"2.0\"><paragraph>This interaction shows Claude's ability to implement standard DL algorithms correctly but with notable instruction-following issues. Claude nailed the first two and the last tasks immediately, translating formulas into working code on the first try. The third task got messier, although the HW hint explicitly mentioned \"adjust the two previous functions,\" Claude created entirely new functions instead, ignoring the specific hint and I needed to re-emphasize it in the prompt before Claude realized this. More problematic was Claude's unprompted adjustment to second-layer bias initialization, which as I noted didn't make sense and was irrelevant to my prompt. When I questioned this, Claude admitted the change was \"not well-motivated\" and unnecessary. Besides, it also made changes to the learning rate (which is also irrelevant to my prompt) when making modification to the codes. This revealed Claude making unjustified \u201csilent\u201d modifications without any explanation or upfront reasoning, requiring me to catch and correct the error/changes it made. While Claude's implementations were technically sound once corrected, the process highlighted real gaps in following specifications and a tendency to add unnecessary (or even wrong) modifications without any upfront notifications/justifications. </paragraph><file url=\"https://static.us.edusercontent.com/files/ykODMieILdveJmWaOD7z7Pj5\" filename=\"Claude-CS282-specialParticipationB-HW2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-07T05:01:10.031606+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7267427,
            "author": "Yaqi Su",
            "project_title": "Special Participation A: Claude on HW2 written part",
            "post_body": "Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never makes computational errors. The issues are always at the level of solution strategy or conceptual completeness, not arithmetic or algebra. This suggests the model's mathematical symbolic reasoning is quite robust, while its ability to choose optimal solution paths requires more guidance. The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches. When I used some questioning like \u201cAre you sure...?\" and quoting Claude's own observations back to it as a hint, Claude is able to quickly find cleaner solutions and even provided further insights compared to the original homework solution.",
            "content_xml": "<document version=\"2.0\"><paragraph>Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never makes computational errors. The issues are always at the level of solution strategy or conceptual completeness, not arithmetic or algebra. This suggests the model's mathematical symbolic reasoning is quite robust, while its ability to choose optimal solution paths requires more guidance. The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches. When I used some questioning like \u201cAre you sure...?\" and quoting Claude's own observations back to it as a hint, Claude is able to quickly find cleaner solutions and even provided further insights compared to the original homework solution.</paragraph><file url=\"https://static.us.edusercontent.com/files/FbVAO4WZF5mNmZMHkAlg2AGR\" filename=\"Claude-CS282-specialParticipationA-HW2.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-07T04:30:53.053726+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7266065,
            "author": "Xi Cheng",
            "project_title": "Special Participation A: Mistral on HW2",
            "post_body": "I tested Mistral on the non-coding parts of HW2\n\nChat history link: https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a\n\nAnnotated Log: \n\nExecutive Summary:\n\nI found that it could one-shot 2(a) and Question 5. The former was a simple conceptual subquestion, while the latter involved reasoning about distributed training and computational scaling. Mistral performs well when the task relies on text understanding, structural reasoning, or recalling standard frameworks, but it struggles with problems that require original mathematical derivation or deeper logical adaptation.\n\nThroughout the interaction, I noticed that Mistral often exhibited what I\u2019d call lazy reasoning: it tried to map every prompt to a familiar textbook pattern rather than reasoning from the specific assumptions of the problem. Even when I pointed out inconsistencies or paradoxes, it tended to defend its original, polished-sounding explanation until I explicitly instructed it to ignore existing results and reason under the given setup. Only then did it converge to the correct logic.\n\nOverall, Mistral shows strong linguistic fluency and confidence, but limited flexibility and self-correction. It can one-shot straightforward or pattern-based questions, yet it fails to generalize to novel problem settings without direct intervention. ",
            "content_xml": "<document version=\"2.0\"><paragraph>I tested <bold>Mistral</bold> on the non-coding parts of HW2<break/><break/><bold>Chat history link</bold>: <link href=\"https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a\">https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a</link></paragraph><paragraph><bold>Annotated Log:</bold> </paragraph><file url=\"https://static.us.edusercontent.com/files/ONAhVEoreCoqjsxFUhyukV2S\" filename=\"mistral_hw2_annotated_log.pdf\"/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>I found that it could one-shot 2(a) and Question 5. The former was a simple conceptual subquestion, while the latter involved reasoning about distributed training and computational scaling. Mistral performs well when the task relies on text understanding, structural reasoning, or recalling standard frameworks, but it struggles with problems that require original mathematical derivation or deeper logical adaptation.</paragraph><paragraph>Throughout the interaction, I noticed that Mistral often exhibited what I\u2019d call <italic>lazy reasoning</italic>: it tried to map every prompt to a familiar textbook pattern rather than reasoning from the specific assumptions of the problem. Even when I pointed out inconsistencies or paradoxes, it tended to defend its original, polished-sounding explanation until I explicitly instructed it to ignore existing results and reason under the given setup. Only then did it converge to the correct logic.</paragraph><paragraph>Overall, Mistral shows strong linguistic fluency and confidence, but limited flexibility and self-correction. It can one-shot straightforward or pattern-based questions, yet it fails to generalize to novel problem settings without direct intervention. </paragraph></document>",
            "links": [
                "https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a"
            ],
            "attachments": [],
            "created_at": "2025-11-06T18:22:12.750338+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7265693,
            "author": "Jason Guo",
            "project_title": "Special Participation A: Gemini on Homework 4",
            "post_body": "Annotated Transcript:\n\nhttps://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing\n\nFor this special participation, I used Gemini Pro 2.5 to solve the written portions of homework 4. I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one. Overall, it did a good job of solving the problems. \n\nAll the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong. Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3. It only made one reasoning mistake, that wasn\u2019t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on. \n\nOne notable part of the interaction was when it solved problem 2e. Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arrived at the same solution. Afterwards, I gave it what was given in the solutions, and asked it to try to arrive at the answer in the solutions. After trying to arrive at the given answer, it eventually gave up, and said that it didn\u2019t know why the answer was correct. I thought that this was pretty impressive because it shows that Gemini is actually critiquing itself as it goes. Instead of hallucinating to match the answer I gave it, it pushed back and gave what it thought was correct. I thought this was really impressive, as it isn\u2019t just being \u201cagreeable\u201d and taking what the prompter says to be the truth, like other LLMs I\u2019ve used like ChatGPT.",
            "content_xml": "<document version=\"2.0\"><paragraph>Annotated Transcript:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing\"><underline>https://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing</underline></link></paragraph><paragraph>For this special participation, I used Gemini Pro 2.5 to solve the written portions of homework 4. I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one. Overall, it did a good job of solving the problems. </paragraph><paragraph>All the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong. Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3. It only made one reasoning mistake, that wasn\u2019t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on. </paragraph><paragraph>One notable part of the interaction was when it solved problem 2e. Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arrived at the same solution. Afterwards, I gave it what was given in the solutions, and asked it to try to arrive at the answer in the solutions. After trying to arrive at the given answer, it eventually gave up, and said that it didn\u2019t know why the answer was correct. I thought that this was pretty impressive because it shows that Gemini is actually critiquing itself as it goes. Instead of hallucinating to match the answer I gave it, it pushed back and gave what it thought was correct. I thought this was really impressive, as it isn\u2019t just being \u201cagreeable\u201d and taking what the prompter says to be the truth, like other LLMs I\u2019ve used like ChatGPT.</paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-11-06T16:19:43.889302+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7265350,
            "author": "Alex Luu",
            "project_title": "Special Participation B: HW4, Llama 4 Maverick",
            "post_body": "I used Llama 4 Maverick on the coding questions for HW4. Here is a summary of my findings:\n\nThe second coding question in this homework is quite long and requires lots of context and code. Despite this, Llama 4 did really well on almost all of the question. I selectively chose what context to give it (which could be a reason why it did so well) to reduce hallucinations. Additionally, I found that if I did not give it enough context (or the right context) it would fail.\n\nThe LLM aced all the coding parts except for the hyperparameter tuning question. It tried many times and I tried to push it towards the right direction but it failed. This is probably because it requires more guesswork and the model seemed to not like taking larger jumps (it did not like to significantly change any of the hyperparameters). It does make sense that an LLM would ace the coding parts since they mostly focus on PyTorch conventions (which is a very well-known framework). \n\nThe responses to the written questions were quite good too. Most of them involved analyzing the output, which are usually images. There were lots of images and I would send them to the LLM to analyze all in the same chat. Despite the large context that images incur, the LLM was still able to produce really good responses that surprised me. Considering Llama 4 was a controversial model for its performance, I thought it did really well. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/6fsY4vcf26u32YkHTwzMYlg0\" filename=\"Special Participation B, HW4, Llama 4.pdf\"/><paragraph>I used Llama 4 Maverick on the coding questions for HW4. Here is a summary of my findings:<break/><break/>The second coding question in this homework is quite long and requires lots of context and code. Despite this, Llama 4 did really well on almost all of the question. I selectively chose what context to give it (which could be a reason why it did so well) to reduce hallucinations. Additionally, I found that if I did not give it enough context (or the right context) it would fail.</paragraph><paragraph>The LLM aced all the coding parts except for the hyperparameter tuning question. It tried many times and I tried to push it towards the right direction but it failed. This is probably because it requires more guesswork and the model seemed to not like taking larger jumps (it did not like to significantly change any of the hyperparameters). It does make sense that an LLM would ace the coding parts since they mostly focus on PyTorch conventions (which is a very well-known framework). </paragraph><paragraph>The responses to the written questions were quite good too. Most of them involved analyzing the output, which are usually images. There were lots of images and I would send them to the LLM to analyze all in the same chat. Despite the large context that images incur, the LLM was still able to produce really good responses that surprised me. Considering Llama 4 was a controversial model for its performance, I thought it did really well. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-06T15:08:01.834466+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7265176,
            "author": "Joe Berry",
            "project_title": "Special Participation D HW 3 thread Joseph Berry",
            "post_body": "I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.\n\nThe new code and questions-solutions are at the bottom of the file\n\nSi, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" arXiv, 18 Aug. 2025, https://doi.org/10.48550/arXiv.2507.11005.\n\nJordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" Keller Jordan Blog, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.\n\nHuang, Feihu, et al. \"LiMuon: Light and Fast Muon Optimizer for Large Models.\" arXiv, 19 Sept. 2025, https://doi.org/10.48550/arXiv.2509.14562.\n\n\n\nBernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" arXiv, 6 Dec. 2024, https://doi.org/10.48550/arXiv.2409.20325.",
            "content_xml": "<document version=\"2.0\"><paragraph>I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.<break/><break/>The new code and questions-solutions are at the bottom of the file<break/><break/>Si, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" <italic>arXiv</italic>, 18 Aug. 2025, <link href=\"https://doi.org/10.48550/arXiv.2507.11005\">https://doi.org/10.48550/arXiv.2507.11005</link>.<break/><break/>Jordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" <italic>Keller Jordan Blog</italic>, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.</paragraph><file url=\"https://static.us.edusercontent.com/files/SezlsDhMogkxgbEaqAmtw0JF\" filename=\"q_mup_coding_sol-EDITED_LR_BATCH+Muon_Vars.ipynb\"/><paragraph>Huang, Feihu, et al. \"LiMuon: Light and Fast Muon Optimizer for Large Models.\" <italic>arXiv</italic>, 19 Sept. 2025, <link href=\"https://doi.org/10.48550/arXiv.2509.14562\">https://doi.org/10.48550/arXiv.2509.14562</link>.</paragraph><paragraph/><paragraph>Bernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" <italic>arXiv</italic>, 6 Dec. 2024, <link href=\"https://doi.org/10.48550/arXiv.2409.20325\">https://doi.org/10.48550/arXiv.2409.20325</link>.</paragraph></document>",
            "links": [
                "https://doi.org/10.48550/arXiv.2507.11005",
                "https://doi.org/10.48550/arXiv.2509.14562",
                "https://doi.org/10.48550/arXiv.2409.20325"
            ],
            "attachments": [],
            "created_at": "2025-11-06T14:41:00.93811+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7264064,
            "author": "Tianhao Qian",
            "project_title": "Special Participation C: Refactoring Report-HW7 task2",
            "post_body": "Summary:\n\nThrough this refactoring, the project evolved from a single-use notebook into a maintainable machine learning codebase. It is now organized into separate modules \u2014 config.py, data.py, model.py,train.py,use.py,utils.py, ./test/test_smoke.py, project.py (the main entry point). The original implementation was contained in q_rnn_last_name.ipynb.\n\nGithub repo: \nhysteri1a/hysteri1a-Refactoring-Report-HW7\n\nReport\n\nDone by Lawrence Qian(3041996584)\n\n\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Summary:</paragraph><paragraph>Through this refactoring, the project evolved from a single-use notebook into a maintainable machine learning codebase. It is now organized into separate modules \u2014 <code>config.py</code>, <code>data.py</code>, <code>model.py</code>,<code>train.py</code>,<code>use.py</code>,<code>utils.py</code>, <code>./test/test_smoke.py, project.py</code> (the main entry point). The original implementation was contained in <code>q_rnn_last_name.ipynb</code>.</paragraph><paragraph>Github repo: <break/><link href=\"https://github.com/hysteri1a/hysteri1a-Refactoring-Report-HW7\">hysteri1a/hysteri1a-Refactoring-Report-HW7</link></paragraph><paragraph>Report</paragraph><file url=\"https://static.us.edusercontent.com/files/R0AYaMBCd59lqnKaXCgdeNdd\" filename=\"Report_HW7.pdf\"/><paragraph>Done by Lawrence Qian(3041996584)</paragraph><paragraph/><paragraph><break/><break/></paragraph><paragraph/></document>",
            "links": [
                "https://github.com/hysteri1a/hysteri1a-Refactoring-Report-HW7"
            ],
            "attachments": [],
            "created_at": "2025-11-06T11:43:10.534635+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7263386,
            "author": "Alex Luu",
            "project_title": "Special Participation A: HW6, gpt-oss-120b",
            "post_body": "I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot almost all the questions. This model was also surprisingly fast (although that depends on the hosting provider). The latency was near 0 seconds and token generation was very fast. A more detailed summary is provided in the pdf.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/04mIJnMQwJFX1ig7X5G6o4wE\" filename=\"Special Participation A, HW6, gpt-oss-120b.pdf\"/><paragraph>I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot almost all the questions. This model was also surprisingly fast (although that depends on the hosting provider). The latency was near 0 seconds and token generation was very fast. A more detailed summary is provided in the pdf.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-06T10:05:54.778256+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7263077,
            "author": "Anant Sahai",
            "project_title": "Added some more post-proposal Project slots on Thu for students",
            "post_body": "We've heard that some of you didn't book slots early enough on Tuesday (my slots went unused for example) and can't find slots. To make sure that people get feedback, I've added another hour of slots on Thu afternoon. \n\nIt is vital that students start this week on your projects because the deadline for the draft report will be upon you before you know it. We want to make sure that everyone gets feedback now. ",
            "content_xml": "<document version=\"2.0\"><paragraph>We've heard that some of you didn't book slots early enough on Tuesday (my slots went unused for example) and can't find slots. To make sure that people get feedback, I've added another hour of slots on Thu afternoon. <break/><break/>It is vital that students start this week on your projects because the deadline for the draft report will be upon you before you know it. We want to make sure that everyone gets feedback now. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-06T09:24:47.242184+11:00",
            "category": "Project"
        },
        {
            "guid": 7262818,
            "author": "Gireeja Ranade",
            "project_title": "Lectures: 16-20 State-space models, Transformer networks, Modern Architectures",
            "post_body": "Here are the lecture notes, sorry for the delay!",
            "content_xml": "<document version=\"2.0\"><paragraph>Here are the lecture notes, sorry for the delay!</paragraph><file url=\"https://static.us.edusercontent.com/files/vITc5qG2PULCIwIF95ScYECz\" filename=\"Lecture 20.pdf\"/><file url=\"https://static.us.edusercontent.com/files/wjbKvQIaJBIqHt2GmWZzg2H6\" filename=\"Lecture 19.pdf\"/><file url=\"https://static.us.edusercontent.com/files/1W3yC9Hb0yfuWEPhBdCPszps\" filename=\"Lecture 18.pdf\"/><file url=\"https://static.us.edusercontent.com/files/kZK6auktgCrh8DP8s6xETsTB\" filename=\"Lecture 17.pdf\"/><file url=\"https://static.us.edusercontent.com/files/4Vs92j7DfrkgDUSnNG4if920\" filename=\"Lecture 16.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-06T08:54:41.615649+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7261928,
            "author": "Shashwat Bansal",
            "project_title": "Special Participation B: Mistral on HW0",
            "post_body": "Mistral was able to one-shot the coding component of HW0, up until the exact learning rate and weight scale parameters needed for overfitting. When prompted with a hint that they are unequal, it went in the wrong direction. However, this is something that is difficult to ascertain without manually testing different rates, so Mistral did a good job overall and even provided concise explanations of the concepts like affine_forward().",
            "content_xml": "<document version=\"2.0\"><paragraph>Mistral was able to one-shot the coding component of HW0, up until the exact learning rate and weight scale parameters needed for overfitting. When prompted with a hint that they are unequal, it went in the wrong direction. However, this is something that is difficult to ascertain without manually testing different rates, so Mistral did a good job overall and even provided concise explanations of the concepts like <code>affine_forward()</code>.</paragraph><file url=\"https://static.us.edusercontent.com/files/pf1VZL0wnQ0pbZTNs3XeFCcq\" filename=\"CS182 Special Participation B_ Mistral on HW0.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-06T06:56:29.865721+11:00",
            "category": "Admin"
        },
        {
            "guid": 7259824,
            "author": "Shashwat Bansal",
            "project_title": "Participation Section A: Llama (Meta AI) on HW0",
            "post_body": "I interacted with Llama/Meta AI and had a rather disappointing result. For the (easier) questions it answered, it made decent progress on showing the math steps but was vague at times and couldn't tell left multiplication from right. It had trouble answering questions one-by-one like a student would, instead fixating on the topic of the question and ranting about it.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I interacted with Llama/Meta AI and had a rather disappointing result. For the (easier) questions it answered, it made decent progress on showing the math steps but was vague at times and couldn't tell left multiplication from right. It had trouble answering questions one-by-one like a student would, instead fixating on the topic of the question and ranting about it.</paragraph><file url=\"https://static.us.edusercontent.com/files/NL3zdbbcBsPHnQ8BtKrczKa1\" filename=\"CS182 Special Participation A_ Llama.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-05T21:02:05.590684+11:00",
            "category": "Admin"
        },
        {
            "guid": 7259754,
            "author": "Celine Tan",
            "project_title": "Special Participation C: HW1 q_sgd_momentum_analysis",
            "post_body": "Swetha Rajkumar, Katie Wang, and I worked together to refactor the notebook for q_sgd_momentum_analysis.ipynb. \n\nThe refactoring report has been attached below:\n\nThe majority of our changes involved enhancing the readability of code (through the inclusion of docstrings and type hints) and creating a helper function for repetitive plotting code. \n\nCode is available here: \n\nhttps://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_sgd_momentum_analysis.ipynb ",
            "content_xml": "<document version=\"2.0\"><paragraph>Swetha Rajkumar, Katie Wang, and I worked together to refactor the notebook for <code>q_sgd_momentum_analysis.ipynb</code>. </paragraph><paragraph>The refactoring report has been attached below:</paragraph><file url=\"https://static.us.edusercontent.com/files/0bqzHGsDww3eazfWpRQGYUpt\" filename=\"special participation 182 hw1 (1).pdf\"/><paragraph>The majority of our changes involved enhancing the readability of code (through the inclusion of docstrings and type hints) and creating a helper function for repetitive plotting code. </paragraph><paragraph>Code is available here: </paragraph><paragraph><link href=\"https://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_sgd_momentum_analysis.ipynb\">https://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_sgd_momentum_analysis.ipynb</link> </paragraph></document>",
            "links": [
                "https://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_sgd_momentum_analysis.ipynb"
            ],
            "attachments": [],
            "created_at": "2025-11-05T19:58:33.097603+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7259751,
            "author": "Celine Tan",
            "project_title": "Special Participation C: HW2 q_optimizer_init",
            "post_body": "Swetha Rajkumar, Katie Wang, and I worked together to refactor the notebook for q_optimizer_init.ipynb. \n\nThe refactoring report has been attached below:\n\nThe majority of our changes involved enhancing the readability of code (through the inclusion of docstrings and type hints) and creating helper functions for repetitive code. \n\nCode is available here: https://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_optimizer_init_pythonic.ipynb",
            "content_xml": "<document version=\"2.0\"><paragraph>Swetha Rajkumar, Katie Wang, and I worked together to refactor the notebook for <code>q_optimizer_init.ipynb</code>. </paragraph><paragraph>The refactoring report has been attached below:</paragraph><file url=\"https://static.us.edusercontent.com/files/T6O4xW0j92BPWqLUGnDCoxSc\" filename=\"special participation 182.pdf\"/><paragraph>The majority of our changes involved enhancing the readability of code (through the inclusion of docstrings and type hints) and creating helper functions for repetitive code. </paragraph><paragraph>Code is available here: https://github.com/swetha2022/182_special_participations/blob/main/special_participations/q_optimizer_init_pythonic.ipynb</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-05T19:56:21.697227+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7259541,
            "author": "Jerry Xiao",
            "project_title": "Special Participation B: HW6 Coding with Deepseek",
            "post_body": "I used Deepseek to solve problems 5 and 6 of the coding part of HW6. \n\nNote: Problem 1 and Problem 3 are mainly about profiling and tool usage, therefore I skip the two problems.\n\n@ Problem 5: Zachary\u2019s Karate Club\n\nIn general I find that Deepseek can mainly solve the problem without further tuning. However, as the ZKC problem is a problem with large context, it is hard to put all the context within the code. Therefore Deepseek will try to use its own functions from other packages instead of using the preset helper functions. Also, the Deep Thinking capability will get stuck with large context. During the thinking process, Deepseek seems to overcomplicate the problem and thus leading to longer inference time. The structure of the outputs of Deepseek also have something in common. For all the coding section, it will produce the answer first and then follow up with detailed explanation. More comments and the full track of the conversation are attached within the files.\n\n@ Problem 6: Muon Optimizer\n\n\nIt surprises me that Deepseek actually does well in new task such as implementing Muon optimizer. Although the syntax is not quite aligned with the standard but the algorithm is generally correct. However, because Deepseek does not have multimodal capability, it is unable to solve the last few questions without people really describe the trend for it. Also, Deepseek tends to stuck in his thought for the Question 1 asking about the sqrt(3) scaling. It kind of buries itself in all the code works. After I adjust the prompt, it finally gets to the correct answer. More comments and the full track of the conversation are attached within the files.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>I used Deepseek to solve problems 5 and 6 of the coding part of HW6.</bold> </paragraph><paragraph><bold>Note:</bold> Problem 1 and Problem 3 are mainly about profiling and tool usage, therefore I skip the two problems.</paragraph><paragraph>@ Problem 5: Zachary\u2019s Karate Club</paragraph><file url=\"https://static.us.edusercontent.com/files/kIVDnXaigMUwDgANfgrQB5CG\" filename=\"Participation_B__HW_6_using_Deepseek_zkc.pdf\"/><paragraph>In general I find that Deepseek can mainly solve the problem without further tuning. However, as the ZKC problem is a problem with large context, it is hard to put all the context within the code. Therefore Deepseek will try to use its own functions from other packages instead of using the preset helper functions. Also, the Deep Thinking capability will get stuck with large context. During the thinking process, Deepseek seems to overcomplicate the problem and thus leading to longer inference time. The structure of the outputs of Deepseek also have something in common. For all the coding section, it will produce the answer first and then follow up with detailed explanation. More comments and the full track of the conversation are attached within the files.<break/><break/>@ Problem 6: Muon Optimizer<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/8I5rBh7PkUyZtxYAV9XmIldc\" filename=\"Participation_B__HW_6_using_Deepseek_Muon.pdf\"/><paragraph>It surprises me that Deepseek actually does well in new task such as implementing Muon optimizer. Although the syntax is not quite aligned with the standard but the algorithm is generally correct. However, because Deepseek does not have multimodal capability, it is unable to solve the last few questions without people really describe the trend for it. Also, Deepseek tends to stuck in his thought for the Question 1 asking about the sqrt(3) scaling. It kind of buries itself in all the code works. After I adjust the prompt, it finally gets to the correct answer. More comments and the full track of the conversation are attached within the files.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-05T17:52:44.26255+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7258633,
            "author": "Vijay Kethanaboyina",
            "project_title": "Special Participation A: Kimi on HW7 Written Questions",
            "post_body": "I had Moonshot AI's Kimi K2 model answer HW7's written questions. For each problem, my goal was to get the model to the correct answer while giving it as little outside assistance as possible. I only attempted to steer the model when it gave a clearly incorrect answer or when its response diverged significantly from the staff solution.\n\nHere are some of my key findings:\n\nThe model was able to answer most questions in one shot. Laying aside minor formatting and notational differences, the final answers were almost always the same as the staff solution's. \n\nHowever, in reasoning through the problems, the model sometimes made logical leaps that, while correct, were not sufficiently justified in my opinion.\n\nFor example, for Q3b, the model goes straight from stating the loss function to computing its gradient WRT $W_2$ in just one step.\n\n(For context in the below equation, $\\hat{X} = W_2 W_1 X$)\n\nMeanwhile, the staff solution is much more thorough, walking us through every step of the derivation and explicitly citing the matrix calculus identities that were employed.\n\nSo to fix that issue, when necessary, I ask the model to justify in more detail how it reached a given step.\n\nI saw some evidence of hallucination; for question 4a, I accidentally forgot to give the model some information needed to answer the question correctly. But rather than state, \"not enough information provided\", it gave me an incorrect response.\n\nFurthermore, when I gave it a URL containing the information needed to answer the question correctly, it again answered the question wrong. \n\nUpon further questioning, the model revealed that in fact, it does not have live browsing capabilities.\n\nThat is a reasonable limitation, but what is concerning is that it didn't admit it couldn't read the URL until after it had already hallucinated. This decreases my overall trust in the model.\n\nThat said, the model did a pretty good job on the \"summarize a blog post\" question (apart from the hallucination).\n\nI feel like these sorts of \"summarize a technical blog\" questions are where LLMs' abilities really shine. \n\nThey are able to pick out the key details and \"big picture\" insights without getting too lost in the technical details (a mistake that I often make when summarizing blog posts / papers)\n\nA full, annotated transcript of the conversation can be found here: https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing. \n\nI will also add a comment to this post with screenshots of the conversation to make it easy to read.",
            "content_xml": "<document version=\"2.0\"><paragraph>I had Moonshot AI's Kimi K2 model answer HW7's written questions. For each problem, my goal was to get the model to the correct answer while giving it as little outside assistance as possible. I only attempted to steer the model when it gave a clearly incorrect answer or when its response diverged significantly from the staff solution.</paragraph><paragraph>Here are some of my key findings:</paragraph><list style=\"bullet\"><list-item><paragraph>The model was able to answer most questions in one shot. Laying aside minor formatting and notational differences, the final answers were almost always the same as the staff solution's. </paragraph></list-item><list-item><paragraph>However, in reasoning through the problems, the model sometimes made logical leaps that, while correct, were not sufficiently justified in my opinion.</paragraph></list-item><list-item><paragraph>For example, for Q3b, the model goes straight from stating the loss function to computing its gradient WRT $W_2$ in just one step.</paragraph><list style=\"bullet\"><list-item><paragraph>(For context in the below equation, $\\hat{X} = W_2 W_1 X$)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/AiEw4CncvT9IMxZp0uw7BMYs\" width=\"410\" height=\"218\"/></figure></list-item></list></list-item><list-item><paragraph>Meanwhile, the staff solution is much more thorough, walking us through every step of the derivation and explicitly citing the matrix calculus identities that were employed.</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/tQdJDJOsXYzmRXSM2EZpJK3M\" width=\"356\" height=\"447\"/></figure><list style=\"bullet\"><list-item><paragraph>So to fix that issue, when necessary, I ask the model to justify in more detail how it reached a given step.</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/FO7cPBEFtoRK6MhC9OmDcaf8\" width=\"326\" height=\"379\"/></figure><list style=\"bullet\"><list-item><paragraph>I saw some evidence of hallucination; for question 4a, I accidentally forgot to give the model some information needed to answer the question correctly. But rather than state, \"not enough information provided\", it gave me an incorrect response.</paragraph></list-item><list-item><paragraph>Furthermore, when I gave it a URL containing the information needed to answer the question correctly, it again answered the question wrong. </paragraph></list-item><list-item><paragraph>Upon further questioning, the model revealed that in fact, it does not have live browsing capabilities.</paragraph><list style=\"bullet\"><list-item><paragraph>That is a reasonable limitation, but what is concerning is that it didn't admit it couldn't read the URL until <italic>after</italic> it had already hallucinated. This decreases my overall trust in the model.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/W2C2phjQKQEKt4YPSFseUwWI\" width=\"398\" height=\"374\"/></figure></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph>That said, the model did a pretty good job on the \"summarize a blog post\" question (apart from the hallucination).</paragraph></list-item><list-item><paragraph>I feel like these sorts of \"summarize a technical blog\" questions are where LLMs' abilities really shine. </paragraph><list style=\"bullet\"><list-item><paragraph>They are able to pick out the key details and \"big picture\" insights without getting too lost in the technical details (a mistake that I often make when summarizing blog posts / papers)</paragraph></list-item></list></list-item></list><paragraph>A full, annotated transcript of the conversation can be found here: <link href=\"https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing\">https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing</link>. </paragraph><paragraph>I will also add a comment to this post with screenshots of the conversation to make it easy to read.</paragraph></document>",
            "links": [
                "https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-11-05T14:34:52.365835+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7253578,
            "author": "Anant Sahai",
            "project_title": "Getting started on your projects... Bring a printout this week...",
            "post_body": "Dear class,\n\nBy now, most of you have submitted your project proposals. (If you haven't set your group to include everyone, please do so. We notice some submissions didn't set the group correctly)\n\nBe sure to bring a printed copy of your proposal with you to your scheduled meeting with a member of course staff this week. (And if you haven't signed up yet, please do sign up.)\n\nIf you're meeting virtually, please have a copy ready to share so that the staff member can quickly skim it to identify risk factors, etc.\n\nThe most important thing is to get started now. Start writing code, getting tests in place, etc... Deep Learning is full of frustration as you find bugs after having trained something for a day or two. It will take lots of iteration to get things done and ready by the deadlines. You need to start now. (Unlike other areas, pulling all-nighters at the end isn't sufficient because of the training time that is required.)\n\nIn terms of the meetings with course staff, our primary objective is to make sure that your project is safe vis-a-vis your learning and being able to get full credit on your project. This means:\n\nMaking sure that you can train something (could be from scratch, a finetune, or experimental probes of some sort) in a way that lets you demonstrate a working understanding of many of the core practical concepts in deep learning:\n\nIncluding an exploration of hyperparameters --- at least learning rate, but more as relevant to what you are doing.\n\nMaking plots of training progress \n\nHaving some way for you to justify your hyperparameter and optimizer choices --- it is perfectly find for some of these to be X paper did it and we are following... But you have to be able to do this.\n\nMaking sure that you have some kind of held-out data or other ways of evaluating whether whatever you have trained actually makes sense and/or you have trained it sufficiently.\n\nSome thought about the architecture you are using including exploration/experimentation as relevant (this will be modulated on what you have to work with --- obviously, we cannot deem a project to be good if all you do is take code that already exists out there and just run it on some other data. In cases where solid code exists to build on, we will expect architectural experimentations.)\n\nAt least one hypothesis that you can express clearly and design an experiment to test. It is fine if the hypothesis fails or that the tests are inconclusive, but you have to be able to try enough to verify that this is not due to a bug on your part or grossly inadequate data or compute. Negative results or boring results are perfectly fine for full credit. Research has risks and many things fail --- that's ok. \n\nA path to a solid review of the literature that situates your work within the field and the current state of understanding and/or mystery.\n\nIf you have concerns in any of these directions, be sure to bring it up in your meeting so that we can help you mitigate those risks as early as possible. \n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Dear class,</paragraph><paragraph>By now, most of you have submitted your project proposals. (If you haven't set your group to include everyone, please do so. We notice some submissions didn't set the group correctly)</paragraph><paragraph><bold>Be sure to bring a printed copy of your proposal with you to your scheduled meeting with a member of course staff this week. (And if you haven't signed up yet, please do sign up.)</bold></paragraph><paragraph>If you're meeting virtually, please have a copy ready to share so that the staff member can quickly skim it to identify risk factors, etc.<break/><break/>The most important thing is to get started now. Start writing code, getting tests in place, etc... Deep Learning is full of frustration as you find bugs <italic>after</italic> having trained something for a day or two. It will take lots of iteration to get things done and ready by the deadlines. You need to start now. (Unlike other areas, pulling all-nighters at the end isn't sufficient because of the training time that is required.)</paragraph><paragraph>In terms of the meetings with course staff, our primary objective is to make sure that your project is safe vis-a-vis your learning and being able to get full credit on your project. This means:</paragraph><list style=\"bullet\"><list-item><paragraph>Making sure that you can train something (could be from scratch, a finetune, or experimental probes of some sort) in a way that lets you demonstrate a working understanding of many of the core practical concepts in deep learning:</paragraph><list style=\"bullet\"><list-item><paragraph>Including an exploration of hyperparameters --- at least learning rate, but more as relevant to what you are doing.</paragraph></list-item><list-item><paragraph>Making plots of training progress </paragraph></list-item><list-item><paragraph>Having some way for you to justify your hyperparameter and optimizer choices --- it is perfectly find for some of these to be X paper did it and we are following... But you have to be able to do this.</paragraph></list-item></list></list-item><list-item><paragraph>Making sure that you have some kind of held-out data or other ways of evaluating whether whatever you have trained actually makes sense and/or you have trained it sufficiently.</paragraph></list-item><list-item><paragraph>Some thought about the architecture you are using including exploration/experimentation as relevant (this will be modulated on what you have to work with --- obviously, we cannot deem a project to be good if all you do is take code that already exists out there and just run it on some other data. In cases where solid code exists to build on, we will expect architectural experimentations.)</paragraph></list-item><list-item><paragraph>At least one hypothesis that you can express clearly and design an experiment to test. It is fine if the hypothesis fails or that the tests are inconclusive, but you have to be able to try enough to verify that this is not due to a bug on your part or grossly inadequate data or compute. Negative results or boring results are perfectly fine for full credit. Research has risks and many things fail --- that's ok. </paragraph></list-item><list-item><paragraph>A path to a solid review of the literature that situates your work within the field and the current state of understanding and/or mystery.</paragraph></list-item></list><paragraph>If you have concerns in any of these directions, be sure to bring it up in your meeting so that we can help you mitigate those risks as early as possible. </paragraph><paragraph><break/><break/></paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-04T19:12:24.259562+11:00",
            "category": "Project"
        },
        {
            "guid": 7252696,
            "author": "Deena Sun",
            "project_title": "Special Participation E: Concept Map Mermaid Diagrams of Research Papers",
            "post_body": "Using LLMs to create concept maps/Mermaid diagrams of key concepts and related works in research papers.\n\nWhen reading machine learning research papers, I like to synthesize how the different concepts introduced in the paper fit together and how related works tie into the ideas covered in the paper. I usually find this helpful for breaking down ML research into more digestible chunks as well as for creating a roadmap for how I can further my understanding using the citations referenced in the paper. I decided to use Claude 4.5 Sonnet on Perplexity to help me create concept maps of research papers that could visually display connections between key ideas in the paper, how related works contributed to the paper\u2019s takeaways, and additional papers for further exploration.\n\nFirst, I used Claude 4.5 Sonnet to help me generate an effective prompt I could reuse for LLMs based on my idea. I started off asking Claude for a general purpose deep learning tutoring prompt that would guide me through a research paper. Then, I focused the prompt to involve the LLM generating a workable Mermaid diagram representing my concept map idea. Claude gave me some helpful advice for how to productively prompt other LLMs for studying purposes:\n\nProvide context about your role and goals, including your background knowledge and your learning grows\n\nAssign the LLM a specific persona (e.g. \u201cYou are a deep learning teaching assistant\u2026\u201d) to tailor the model\u2019s responses towards educational guidance.\n\nProvide example interactions and specify what the output format should be\n\nAfter some back and forth to modify the prompt to my desires, I took the draft prompt that Claude suggested and used it to prompt another instance of Claude 4.5 Sonnet to actually generate a Mermaid diagram of a concept map for the paper \u201cA Spectral Condition for Feature Learning\u201d we read in homework 3, question 3. I also included a PDF of the paper as an attachment.\n\nHere is the final Mermaid diagram that Claude 4.5 Sonnet produced, rendered using a Mermaid diagram editor:\n\nHere is the prompt I used to generate this concept map that you can use!\n\nYou are a deep learning teaching assistant helping me map out conceptual relationships in a research paper.\n\nYour task:\n* Carefully read the research paper \"[PAPER TITLE]\" and identify all major concepts, methods, and results.\n* Build a structured concept map (\"brain map\") using Mermaid syntax (graph TD), where:\n    * Nodes represent key topics, methods, or results. Include related works and references inside the nodes.\n    * Edges connect related nodes, with descriptive labels explaining the relationship (not just keywords).\n    * Node labels are clear and concise; avoid line breaks for compatibility.\n* For each node, list relevant citations and references (from the paper and cited works) below the diagram, grouped by concept.\n* Flag nodes that are central (highly connected) or have many references, and highlight these for further study.\n* If supported, also generate a PNG image of the diagram for visual reference.\n\nOutput Instructions\n* Present the concept map first using Mermaid syntax in a Markdown code block. Use graph TD, clear node labels, and descriptive edge labels (e.g., --> |enables parallel attention|).\n* Represent the 10 most important concepts as nodes in the Mermaid diagram. Include related works and references inside the node.\n* List all references/citations grouped by node below the diagram (not inside node labels).\n* If possible, generate and return a PNG image of the diagram.\n* Avoid using special characters such as parentheses in labels that might not be compatible with Mermaid diagram generators.\n\nFormat example:\n```\ngraph TD\n Transformer[\"Transformer Model (Vaswani et al. 2017)\"]\n SelfAttn[\"Self-Attention (Bahdanau et al. 2014; Lin et al. 2017)\"]\n MultiHead[\"Multi-Head Attention (Vaswani et al. 2017)\"]\n PosEnc[\"Positional Encoding (Gehring et al. 2017)\"]\n Adam[\"Adam Optimizer (Kingma & Ba 2014)\"]\n Transformer -->|Directly models all pairwise token dependencies with attention| SelfAttn\n Transformer -->|Requires token position information for context awareness| PosEnc\n Transformer -->|Is trained efficiently using adaptive gradient optimization| Adam\n SelfAttn -->|Provides the base mechanism for building rich token representations| MultiHead\n MultiHead -->|Enables parallelized, diverse attention for complex relationships| SelfAttn\n\nReferences by Node:\n* Transformer Model: Vaswani et al. 2017, Gehring et al. 2017, Wu et al. 2016\n* Self-Attention: Bahdanau et al. 2014, Lin et al. 2017, Parikh et al. 2016\n* Multi-Head Attention: Vaswani et al. 2017, Britz et al. 2017\n* Positional Encoding: Gehring et al. 2017, Sennrich et al. 2015\n* Adam Optimizer: Kingma & Ba 2014\n```\n\nBackground:\nI am an undergraduate student who is interested in machine learning, deep learning, and research in these fields.\n\nPaper abstract/intro:\n[COPY HERE]\n\nHere are the logs of my chats that I used to brainstorm my prompt as well as to create the Mermaid diagram:\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Using LLMs to create concept maps/Mermaid diagrams of key concepts and related works in research papers.</paragraph><paragraph>When reading machine learning research papers, I like to synthesize how the different concepts introduced in the paper fit together and how related works tie into the ideas covered in the paper. I usually find this helpful for breaking down ML research into more digestible chunks as well as for creating a roadmap for how I can further my understanding using the citations referenced in the paper. I decided to use Claude 4.5 Sonnet on Perplexity to help me create concept maps of research papers that could visually display connections between key ideas in the paper, how related works contributed to the paper\u2019s takeaways, and additional papers for further exploration.</paragraph><paragraph>First, I used Claude 4.5 Sonnet to help me generate an effective prompt I could reuse for LLMs based on my idea. I started off asking Claude for a general purpose deep learning tutoring prompt that would guide me through a research paper. Then, I focused the prompt to involve the LLM generating a workable Mermaid diagram representing my concept map idea. Claude gave me some helpful advice for how to productively prompt other LLMs for studying purposes:</paragraph><list style=\"unordered\"><list-item><paragraph>Provide context about your role and goals, including your background knowledge and your learning grows</paragraph></list-item><list-item><paragraph>Assign the LLM a specific persona (e.g. \u201cYou are a deep learning teaching assistant\u2026\u201d) to tailor the model\u2019s responses towards educational guidance.</paragraph></list-item><list-item><paragraph>Provide example interactions and specify what the output format should be</paragraph></list-item></list><paragraph>After some back and forth to modify the prompt to my desires, I took the draft prompt that Claude suggested and used it to prompt another instance of Claude 4.5 Sonnet to actually generate a Mermaid diagram of a concept map for the paper \u201cA Spectral Condition for Feature Learning\u201d we read in homework 3, question 3. I also included a PDF of the paper as an attachment.</paragraph><paragraph>Here is the final Mermaid diagram that Claude 4.5 Sonnet produced, rendered using a <link href=\"https://mermaid.live/edit\">Mermaid diagram editor</link>:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/JpxdsphfUuRv1gGhtbJikn5g\" width=\"658\" height=\"641.3943217665615\"/></figure><paragraph>Here is the prompt I used to generate this concept map that you can use!</paragraph><pre>You are a deep learning teaching assistant helping me map out conceptual relationships in a research paper.\n\nYour task:\n* Carefully read the research paper \"[PAPER TITLE]\" and identify all major concepts, methods, and results.\n* Build a structured concept map (\"brain map\") using Mermaid syntax (graph TD), where:\n    * Nodes represent key topics, methods, or results. Include related works and references inside the nodes.\n    * Edges connect related nodes, with descriptive labels explaining the relationship (not just keywords).\n    * Node labels are clear and concise; avoid line breaks for compatibility.\n* For each node, list relevant citations and references (from the paper and cited works) below the diagram, grouped by concept.\n* Flag nodes that are central (highly connected) or have many references, and highlight these for further study.\n* If supported, also generate a PNG image of the diagram for visual reference.\n\nOutput Instructions\n* Present the concept map first using Mermaid syntax in a Markdown code block. Use graph TD, clear node labels, and descriptive edge labels (e.g., --&gt; |enables parallel attention|).\n* Represent the 10 most important concepts as nodes in the Mermaid diagram. Include related works and references inside the node.\n* List all references/citations grouped by node below the diagram (not inside node labels).\n* If possible, generate and return a PNG image of the diagram.\n* Avoid using special characters such as parentheses in labels that might not be compatible with Mermaid diagram generators.\n\nFormat example:\n```\ngraph TD\n Transformer[\"Transformer Model (Vaswani et al. 2017)\"]\n SelfAttn[\"Self-Attention (Bahdanau et al. 2014; Lin et al. 2017)\"]\n MultiHead[\"Multi-Head Attention (Vaswani et al. 2017)\"]\n PosEnc[\"Positional Encoding (Gehring et al. 2017)\"]\n Adam[\"Adam Optimizer (Kingma &amp; Ba 2014)\"]\n Transformer --&gt;|Directly models all pairwise token dependencies with attention| SelfAttn\n Transformer --&gt;|Requires token position information for context awareness| PosEnc\n Transformer --&gt;|Is trained efficiently using adaptive gradient optimization| Adam\n SelfAttn --&gt;|Provides the base mechanism for building rich token representations| MultiHead\n MultiHead --&gt;|Enables parallelized, diverse attention for complex relationships| SelfAttn\n\nReferences by Node:\n* Transformer Model: Vaswani et al. 2017, Gehring et al. 2017, Wu et al. 2016\n* Self-Attention: Bahdanau et al. 2014, Lin et al. 2017, Parikh et al. 2016\n* Multi-Head Attention: Vaswani et al. 2017, Britz et al. 2017\n* Positional Encoding: Gehring et al. 2017, Sennrich et al. 2015\n* Adam Optimizer: Kingma &amp; Ba 2014\n```\n\nBackground:\nI am an undergraduate student who is interested in machine learning, deep learning, and research in these fields.\n\nPaper abstract/intro:\n[COPY HERE]</pre><paragraph>Here are the logs of my chats that I used to brainstorm my prompt as well as to create the Mermaid diagram:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/3w2hhk3gVZSU1c7OeKythrA8\" filename=\"Research_graph_prompt_creation_chat.pdf\"/><file url=\"https://static.us.edusercontent.com/files/xKoVyP3TX2Rof6dDLfOjctCf\" filename=\"Spectral_condition_for_feature_learning_concept_map_chat.pdf\"/></document>",
            "links": [
                "https://mermaid.live/edit"
            ],
            "attachments": [],
            "created_at": "2025-11-04T15:15:24.914928+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7252362,
            "author": "Ben Yu",
            "project_title": "Special Participation B: ChatGPT 5 on HW 3",
            "post_body": "What I Did:\n\nI tried to use GPT-5 to help me understand the muP coding lab (q_mup_coding.ipynb) -- especially parts (c)\u2013(d), which involved per-layer learning-rate scaling and graph scaling factors. The conversation grew extremely long because I was confused about the correct scaling factor (\u221an_in, n_in, 1/\u221an_in, 1/n_in, \u221a(n_out/n_in), etc.) and how these relate across RMS, spectral, and induced norms. Later the released solution notebook turned out to have an incorrect scaling line (x * p_shape[1] instead of x / p_shape[1]), which made it harder to verify correctness before the announcement came out.\n\nWorkflow\n\n1. Ask questions: I pasted the exact sub-questions: \u201cWhat is part d\u2019s scaling factor?\u201d, \u201cShould I divide by \u221a or n_in?\u201d, \u201cWhere in code step() handles bias?\u201d, etc.\n\n2. Compare & Probe: Each time, I compared the AI\u2019s answer to lecture notes. and when results disagreed, I asked for the missing algebra or an intuition (\u201cWhy does 1/fan-in appear?\u201d).\n\n3. Tighten understanding: I repeatedly asked it to rewrite arguments in plain words and to clarify the difference between the RMS\u2192RMS induced norm and the spectral norm.\n\nExample of confusion:\n\nI kept alternating between x /= sqrt(layer.in_features) [fan-in normalization] and x *= sqrt(layer.out_features / layer.in_features) [\u03bcP graph scaling]. Both produced different plots, and ChatGPT changed answers multiple times and still did not converge to the correct forward scaling (divide by n_in).\n\nObservations \n\nKey insight: \n\nThese factors solve different problems. For example, \u221a(1/fan-in) keeps activations numerically stable; 1/fan-in (\u03bcP) keeps feature updates width-invariant under Adam.\n\nPositives (What Worked Well):\n\nDebugging guidance: Helped me trace the coding part, where step() applies to both weight and bias tensors and where the forward multiplier belongs.\n\nNegatives (Limitations / Effort Required):\n\nConcept collision: It mixed up \u201cfan-in \u221a scaling\u201d and \u201c\u03bcP 1/n scaling\u201d several times, making things more confused.\n\nVerification cost: Because the official solution had an error, I needed to cross-check the explanations to confirm the correct direction, which is hard since AI uses different wordings that I sometimes find hard to connect between two explanations\n\nPrompting Strategies That Helped\n\n\u201cElaborate / Why this?\u201d \u2192 forced the model to expand the chain-rule or Jacobian steps.\n\n\u201cIs my thought correct? If not, why?\u201d \u2192 helped surface subtle contradictions.\n\n\u201cMake a table.\u201d \u2192 compressed multiple conflicting explanations into a clear comparative view.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What I Did:</bold></paragraph><paragraph>I tried to use GPT-5 to help me understand the muP coding lab (q_mup_coding.ipynb) -- especially parts (c)\u2013(d), which involved per-layer learning-rate scaling and graph scaling factors. The conversation grew extremely long because I was confused about the correct scaling factor (\u221an_in, n_in, 1/\u221an_in, 1/n_in, \u221a(n_out/n_in), etc.) and how these relate across RMS, spectral, and induced norms. Later the released solution notebook turned out to have an incorrect scaling line (x * p_shape[1] instead of x / p_shape[1]), which made it harder to verify correctness before the announcement came out.</paragraph><paragraph><bold>Workflow</bold></paragraph><paragraph>1. <bold>Ask questions:</bold> I pasted the exact sub-questions: \u201cWhat is part d\u2019s scaling factor?\u201d, \u201cShould I divide by \u221a or n_in?\u201d, \u201cWhere in code step() handles bias?\u201d, etc.</paragraph><paragraph>2. <bold>Compare &amp; Probe:</bold> Each time, I compared the AI\u2019s answer to lecture notes. and when results disagreed, I asked for the missing algebra or an intuition (\u201cWhy does 1/fan-in appear?\u201d).</paragraph><paragraph>3. <bold>Tighten understanding:</bold> I repeatedly asked it to rewrite arguments in plain words and to clarify the difference between the RMS\u2192RMS induced norm and the spectral norm.</paragraph><paragraph><bold>Example of confusion:</bold></paragraph><paragraph>I kept alternating between x /= sqrt(layer.in_features) [<bold>fan-in normalization]</bold> and x *= sqrt(layer.out_features / layer.in_features) [<bold>\u03bcP graph scaling]</bold>. Both produced different plots, and ChatGPT changed answers multiple times and still did not converge to the correct forward scaling (divide by n_in).</paragraph><paragraph><bold>Observations</bold> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YAxayyC6nsSyu4PzNcQbb4hx\" width=\"658\" height=\"381.21086956521737\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/pbXpDX480JPfih3JYAeia3sE\" width=\"658\" height=\"185.73493975903617\"/></figure><paragraph><bold>Key insight:</bold> </paragraph><paragraph>These factors solve different problems. For example, \u221a(1/fan-in) keeps activations numerically stable; 1/fan-in (\u03bcP) keeps feature updates width-invariant under Adam.</paragraph><paragraph><bold>Positives (What Worked Well):</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Debugging guidance</bold>: Helped me trace the coding part, where step() applies to both weight and bias tensors and where the forward multiplier belongs.</paragraph></list-item></list><paragraph><bold>Negatives (Limitations / Effort Required):</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Concept collision:</bold> It mixed up \u201cfan-in \u221a scaling\u201d and \u201c\u03bcP 1/n scaling\u201d several times, making things more confused.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Verification cost:</bold> Because the official solution had an error, I needed to cross-check the explanations to confirm the correct direction, which is hard since AI uses different wordings that I sometimes find hard to connect between two explanations</paragraph></list-item></list><paragraph><bold>Prompting Strategies That Helped</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>\u201cElaborate / Why this?\u201d</bold> \u2192 forced the model to expand the chain-rule or Jacobian steps.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>\u201cIs my thought correct? If not, why?\u201d</bold> \u2192 helped surface subtle contradictions.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>\u201cMake a table.\u201d</bold> \u2192 compressed multiple conflicting explanations into a clear comparative view.</paragraph></list-item></list><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-04T14:20:15.828791+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7250623,
            "author": "Ender Ji",
            "project_title": "Special Participation A: HW7 with Grok",
            "post_body": "I used Grok to complete the written part of HW7. I begin by clearly stating Grok\u2019s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the problem.\n\nHW7 consists of proofs, multiple choice questions, and open-ended free response problems. Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt. For the open-ended free response problems, Grok provides reasonable points with clear explanations. However, on the proof problems, Grok typically does not match the official solution on the first attempt and requires additional hints and guidance, for example, specifying which variables or equations to use.\n\n\n\nOverall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/sGI2dcFvYnUuoEMGTIY6vp7y\" filename=\"grok_hw7.pdf\"/><paragraph>I used Grok to complete the written part of HW7. I begin by clearly stating Grok\u2019s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the problem.</paragraph><paragraph>HW7 consists of proofs, multiple choice questions, and open-ended free response problems. Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt. For the open-ended free response problems, Grok provides reasonable points with clear explanations. However, on the proof problems, Grok typically does not match the official solution on the first attempt and requires additional hints and guidance, for example, specifying which variables or equations to use.</paragraph><paragraph/><paragraph>Overall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-04T09:47:19.742032+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7250482,
            "author": "Heidy Hernandez Juan",
            "project_title": "Special Participation A: Mistral on HW6",
            "post_body": "Link: https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28\n\nAnnotated Log: https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing\n\nExecutive Summary:\n\nI observed that the model fails to extract information from diagrams like tables, images, and graphs. Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information.\n\nAlso, it didn't list the explanation for its reasoning behind its own analysis of the graphs in one of the problems. I suggest being careful with problems involving visualization.\n\nIt did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation. There were times when the model outputted answers that I, myself, hadn't seen, though they were mathematically equivalent. The model considers other options when solving problems, allowing for a broader understanding of mathematics.\n\nMistral also did a good job of allowing for an open interaction with the user by making sure to end with, \u2018Ready for any further questions or clarification!\u2019\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Link</bold>: <link href=\"https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28\">https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28</link></paragraph><paragraph><bold>Annotated Log:</bold> <link href=\"https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing\">https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing</link></paragraph><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>I observed that the model fails to extract information from diagrams like tables, images, and graphs. Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information.</paragraph><paragraph>Also, it didn't list the explanation for its reasoning behind its own analysis of the graphs in one of the problems. I suggest being careful with problems involving visualization.</paragraph><paragraph>It did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation. There were times when the model outputted answers that I, myself, hadn't seen, though they were mathematically equivalent. The model considers other options when solving problems, allowing for a broader understanding of mathematics.</paragraph><paragraph>Mistral also did a good job of allowing for an open interaction with the user by making sure to end with, \u2018Ready for any further questions or clarification!\u2019</paragraph><paragraph/></document>",
            "links": [
                "https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28",
                "https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-11-04T09:28:36.310964+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7250444,
            "author": "Ben Yu",
            "project_title": "Special Participation A: Gemini Pro on HW 3",
            "post_body": "What I did:\n\nI ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0\n\nI used Gemini to sanity-check my derivations, clarify notation, and cross-validate with my own solutions\n\nWorkflow:\n\n1) Try solo first (\u226430 min). If blocked, paste the exact sub-question into Gemini.\n\n2) Compare & probe. Check Gemini\u2019s output against my derivation; ask for clarifications or for the missing step.\n\n3) Tighten. Clarify equation statements, symbol mismatches, and ask for the key identity or rules explicitly.\n\nSpecific Example (Analogy + Clarification):\n\nTopic: Interpreting p\u03b8\u200b:X\u2192\u0394(X) in Q4.\n\nMy Ask: \u201cWhat does the arrow to \u0394(X) mean in p\u03b8\u200b:X\u2192\u0394(X)?\u201d\n\nGemini\u2019s Analogy (helpful): Sound equalizer.\n\np\u03b8\u200b = the equalizer; \u03b8 = slider positions;\n\nX = the set of audible frequencies;\n\n\u0394(X) = all possible sound profiles (distributions over X).\n Changing \u03b8 selects a particular distribution in \u0394(X).\n\nMy Follow-up (to make it precise): \u201cPlease restate without analogy: define X, \u0394(X), and what an \u2018element\u2019 of \u0394(X) is.\u201d\n\nGemini (clarified): X is the sample space; \u0394(X) is the set of all probability measures on X; for each \u03b8, p\u03b8\u200b(\u22c5)\u2208\u0394(X).\n\nPattern used: take the first pass (analogy/intuition), then demand the formal mapping with symbols.\n\nObservations:\n\nUses analogies to simplify (e.g., equalizer for p\u03b8\u200b:X\u2192\u0394(X)). Helpful as a first pass.\n\nInconsistent symbols at times, likely from generic training patterns; needs nudging to match the homework question\u2019s notation.\n\nPositives (What Worked Well):\n\nOne-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questions\n\nRight methodology even when imperfect: When wrong/incomplete, it still pointed in the correct direction\n\nNegatives (Limitations and Effort Required):\n\nNotation slippage: Misread or drifted symbols\n\nOver/under-explanation: Wrapped easy steps in prose but skimmed tough steps unless I asked \u201celaborate on this step.\u201d\n\nPrompting Strategies That Helped:\n\n\u201cDerive further.\u201d Ask for the exact algebraic substitution (e.g., show \u2207p=p\u2207logp).\n\n\u201cElaborate / Why this?\u201d Request the chain-rule Jacobian or the gradient wrt input vs. parameters explicitly.\n\n\u201cIs my thought correct? If not, why?\u201d Good for catching subtle mistakes and forcing a counterexample or fix.\n\n\u201cShow this equation more clearly\u201d Keeps it concise and reduces misunderstanding\n\n\u201cMatch the HW notation.\u201d Prevents variable chaos.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What I did:</bold></paragraph><paragraph>I ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: <link href=\"https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0\">https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0</link></paragraph><paragraph>I used Gemini to sanity-check my derivations, clarify notation, and cross-validate with my own solutions</paragraph><paragraph><bold>Workflow:</bold></paragraph><paragraph><bold>1) Try solo first (\u226430 min).</bold> If blocked, paste the exact sub-question into Gemini.</paragraph><paragraph><bold>2) Compare &amp; probe.</bold> Check Gemini\u2019s output against my derivation; ask for clarifications or for the missing step.</paragraph><paragraph><bold>3) Tighten.</bold> Clarify equation statements, symbol mismatches, and ask for the key identity or rules explicitly.</paragraph><paragraph><bold>Specific Example (Analogy + Clarification):</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Topic:</bold> Interpreting p\u03b8\u200b:X\u2192\u0394(X) in Q4.</paragraph></list-item></list><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph><bold>My Ask:</bold> \u201cWhat does the arrow to \u0394(X) mean in p\u03b8\u200b:X\u2192\u0394(X)?\u201d</paragraph></list-item><list-item><paragraph><bold>Gemini\u2019s Analogy (helpful):</bold> <italic>Sound equalizer.</italic></paragraph><list style=\"unordered\"><list-item><paragraph>p\u03b8\u200b = the equalizer; \u03b8 = slider positions;</paragraph></list-item><list-item><paragraph>X = the set of audible frequencies;</paragraph></list-item><list-item><paragraph>\u0394(X) = all possible sound profiles (distributions over X).<break/> Changing \u03b8 selects a particular distribution in \u0394(X).</paragraph></list-item></list></list-item><list-item><paragraph><bold>My Follow-up (to make it precise):</bold> \u201cPlease restate without analogy: define X, \u0394(X), and what an \u2018element\u2019 of \u0394(X) is.\u201d</paragraph></list-item><list-item><paragraph><bold>Gemini (clarified):</bold> X is the sample space; \u0394(X) is the set of all probability measures on X; for each \u03b8, p\u03b8\u200b(\u22c5)\u2208\u0394(X).</paragraph></list-item><list-item><paragraph><bold>Pattern used:</bold> take the first pass (analogy/intuition), then demand the formal mapping with symbols.</paragraph></list-item></list></list-item></list><paragraph><bold>Observations:</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Uses analogies</bold> to simplify (e.g., equalizer for p\u03b8\u200b:X\u2192\u0394(X)). Helpful as a first pass.</paragraph></list-item><list-item><paragraph><bold>Inconsistent symbols</bold> at times, likely from generic training patterns; needs nudging to match the homework question\u2019s notation.</paragraph></list-item></list><paragraph><bold>Positives (What Worked Well):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>One-shot on standard rewrites:</bold> Correctly gave correct answers straight from around 30% of the questions</paragraph></list-item><list-item><paragraph><bold>Right methodology even when imperfect:</bold> When wrong/incomplete, it still pointed in the correct direction</paragraph></list-item></list><paragraph><bold>Negatives (Limitations and Effort Required):</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Notation slippage:</bold> Misread or drifted symbols</paragraph></list-item><list-item><paragraph><bold>Over/under-explanation:</bold> Wrapped easy steps in prose but skimmed tough steps unless I asked \u201celaborate on this step.\u201d</paragraph></list-item></list><paragraph><bold>Prompting Strategies That Helped:</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>\u201cDerive further.\u201d</bold> Ask for the exact algebraic substitution (e.g., show \u2207p=p\u2207logp).</paragraph></list-item><list-item><paragraph><bold>\u201cElaborate / Why this?\u201d</bold> Request the chain-rule Jacobian or the gradient wrt input vs. parameters explicitly.</paragraph></list-item><list-item><paragraph><bold>\u201cIs my thought correct? If not, why?\u201d</bold> Good for catching subtle mistakes and forcing a counterexample or fix.</paragraph></list-item><list-item><paragraph><bold>\u201cShow this equation more clearly\u201d</bold> Keeps it concise and reduces misunderstanding</paragraph></list-item><list-item><paragraph><bold>\u201cMatch the HW notation.\u201d</bold> Prevents variable chaos.</paragraph></list-item></list></document>",
            "links": [
                "https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0"
            ],
            "attachments": [],
            "created_at": "2025-11-04T09:23:05.224039+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7249718,
            "author": "Ben Yu",
            "project_title": "Special Participation E: Using GPT to Deepen Lecture Understanding",
            "post_body": "What I built:\n\nDuring lectures, I used ChatGPT as an interactive study partner rather than a passive note tool.\nWhenever a concept felt unclear, I would pause the lecture, take a screenshot, and ask a precise question in my EECS 182 project space. Each numbered thread (Week #) became a mini-lab where I clarified equations, verified derivations, and then reinterpreted AI\u2019s explanation directly into my written lecture notes:\n\nFor example (image below), in Week 9's lecture, I used GPT as my study partner to clarify the concept of Fast Fourier Transform on State Space Models, which was barely mentioned on during lecture. This way I can freely choose to dive deeper into specific concepts while learning the broad picture from the lecture.\n\n\n\nExample Conversation \u2014 Lecture 9: Hidden-State Independence\n\nTrace Link: https://chatgpt.com/g/g-p-68b1f76ecfb8819191ec5b17c4fdd059-cs-182/shared/c/68feaa70-8378-8333-a027-5935fdf14461?owner_user_id=user-AdVlGKTV9Sn7SdTqllGbrt2G\n\nOne of My Question: \u201cSo the point (of use block diagonal matrix B in state space calculation) is to make hidden states independent?\u201d\n\nAI-Guided Insights: Correlated hidden states introduce redundancy and unstable gradients, because each state begins to carry overlapping information about the sequence. By using block diagonal matrices, each hidden unit evolves as a unique information channel, making the system more stable and interpretable by decoupling the updates of different state components, because independent states yield better gradient flow, improved generalization, and a cleaner mapping between model structure and learned function.\n\nVerification:\nI implemented diag_unrolled_ssm_forward and diag_conv_ssm_forward to confirm the theory, and their outputs matched within 1e-6 and reproduced the predicted runtime behavior -- linear in T for recurrence, nearly constant on GPU for convolution.\n\nAfter each exchange, I try to reexplain the idea in my handwritten notes, converting the long AI reasoning into a concise conceptual summary that I could review quickly before exams.\n\nComments:\n\nPositives (What Worked Well):\n\nImmediate conceptual clarification: AI interaction let me elaborate on lecture ideas in real time rather than waiting for Ed Forum or office-hour responses.\n\nReduced TA load: Routine clarifications that might have required staff input were handled independently, keeping my questions tightly scoped to lecture material.\n\nDeeper integration: Reading the AI\u2019s full reasoning line by line forced me to engage with the derivations more carefully than a static answer key would have.\n\nNegatives (Limitations and Effort Required):\n\nScope uncertainty: The AI occasionally referenced material outside the intended EECS 182 syllabus, requiring me to filter which concepts were truly in scope.\n\nPotential over-explanation: Responses sometimes included extra theoretical context that, while interesting, risked obscuring the specific idea being tested.\n\nFuture Improvements:\n\nStructured project workflow: As outlined in my companion submission \u201cSpecial Participation E: AI-Enhanced Learning with ChatGPT Project Mode,\u201d we can formalize a workflow where each thread explicitly records lecture context, question scope, and verified outcomes.\n\nThis process exemplifies how AI can enhance learning when guided by course scope, topical context, and consistent self-reflection.",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What I built:</bold></paragraph><paragraph>During lectures, I used ChatGPT as an <bold>interactive study partner</bold> rather than a passive note tool.<break/>Whenever a concept felt unclear, I would <bold>pause the lecture</bold>, <bold>take a screenshot</bold>, and <bold>ask a precise question</bold> in my EECS 182 project space. Each numbered thread (Week #) became a mini-lab where I clarified equations, verified derivations, and then <bold>reinterpreted AI\u2019s explanation directly into my written lecture notes:</bold></paragraph><paragraph>For example (image below), in <bold>Week 9's</bold> lecture, I used GPT as my study partner to clarify the concept of <bold>Fast Fourier Transform</bold> on <bold>State Space Models</bold>, which was barely mentioned on during lecture. This way I can freely choose to dive deeper into specific concepts while learning the broad picture from the lecture.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MOPsugkwHHt4DRZL9dIWgPNZ\" width=\"658\" height=\"201.03782735208534\"/></figure><paragraph/><paragraph><bold>Example Conversation \u2014 Lecture 9: Hidden-State Independence</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Trace Link:</bold> <link href=\"https://chatgpt.com/g/g-p-68b1f76ecfb8819191ec5b17c4fdd059-cs-182/shared/c/68feaa70-8378-8333-a027-5935fdf14461?owner_user_id=user-AdVlGKTV9Sn7SdTqllGbrt2G\">https://chatgpt.com/g/g-p-68b1f76ecfb8819191ec5b17c4fdd059-cs-182/shared/c/68feaa70-8378-8333-a027-5935fdf14461?owner_user_id=user-AdVlGKTV9Sn7SdTqllGbrt2G</link></paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>One of My Question:</bold> \u201cSo the point (of use block diagonal matrix B in state space calculation) is to make hidden states independent?\u201d</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>AI-Guided Insights:</bold> Correlated hidden states introduce redundancy and unstable gradients, because each state begins to carry overlapping information about the sequence. By using block diagonal matrices, each hidden unit evolves as a unique information channel, making the system more stable and interpretable by decoupling the updates of different state components, because independent states yield better gradient flow, improved generalization, and a cleaner mapping between model structure and learned function.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Verification:</bold><break/>I implemented <code>diag_unrolled_ssm_forward</code> and <code>diag_conv_ssm_forward</code> to confirm the theory, and their outputs matched within 1e-6 and reproduced the predicted runtime behavior -- linear in T for recurrence, nearly constant on GPU for convolution.</paragraph></list-item></list><paragraph>After each exchange, I try to reexplain the idea in my handwritten notes, converting the long AI reasoning into a <bold>concise conceptual summary</bold> that I could review quickly before exams.</paragraph><paragraph><bold>Comments:</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Positives (What Worked Well):</bold></paragraph></list-item></list><list style=\"bullet\"><list-item><list style=\"bullet\"><list-item><paragraph><bold>Immediate conceptual clarification:</bold> AI interaction let me elaborate on lecture ideas in real time rather than waiting for Ed Forum or office-hour responses.</paragraph></list-item><list-item><paragraph><bold>Reduced TA load:</bold> Routine clarifications that might have required staff input were handled independently, keeping my questions tightly scoped to lecture material.</paragraph></list-item><list-item><paragraph><bold>Deeper integration:</bold> Reading the AI\u2019s full reasoning line by line forced me to engage with the derivations more carefully than a static answer key would have.</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Negatives (Limitations and Effort Required):</bold></paragraph></list-item></list><list style=\"bullet\"><list-item><list style=\"bullet\"><list-item><paragraph><bold>Scope uncertainty:</bold> The AI occasionally referenced material outside the intended EECS 182 syllabus, requiring me to filter which concepts were truly in scope.</paragraph></list-item><list-item><paragraph><bold>Potential over-explanation:</bold> Responses sometimes included extra theoretical context that, while interesting, risked obscuring the specific idea being tested.</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Future Improvements:</bold></paragraph></list-item></list><list style=\"bullet\"><list-item><list style=\"bullet\"><list-item><paragraph><bold>Structured project workflow:</bold> As outlined in my companion submission <italic>\u201cSpecial Participation E: AI-Enhanced Learning with ChatGPT Project Mode,\u201d</italic> we can formalize a workflow where each thread explicitly records lecture context, question scope, and verified outcomes.</paragraph></list-item></list></list-item></list><paragraph>This process exemplifies how AI can enhance learning when guided by course scope, topical context, and consistent self-reflection.</paragraph></document>",
            "links": [
                "https://chatgpt.com/g/g-p-68b1f76ecfb8819191ec5b17c4fdd059-cs-182/shared/c/68feaa70-8378-8333-a027-5935fdf14461?owner_user_id=user-AdVlGKTV9Sn7SdTqllGbrt2G"
            ],
            "attachments": [],
            "created_at": "2025-11-04T07:54:01.158912+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7249252,
            "author": "Ben Yu",
            "project_title": "Special Participation E: AI-Enhanced Learning with ChatGPT Project Mode",
            "post_body": "What I built:\nA repeatable workflow that uses ChatGPT project mode as a study copilot across lecture, discussion, and homework -- it summarizes past questions/answers from my chat history into chronological tables and acts like a \u201ccache layer\u201d I can backtrack during reviews.\n\nProject Setup:\n\nPrompt:\n\nYou are my personal TA for UC Berkeley\u2019s EECS 182/282A: Deep Neural Networks, Fall 2025.\nYour job is to help me learn, review, and succeed in this course.\n\nStick to course scope: topics include optimization, convnets, ResNets, GNNs, RNNs, state-space models, transformers, prompting, transfer/meta-learning, generative & diffusion models.\n\nHelp style:\nKeep explanations simple, direct, and intuitive before going deep in math/code.\nUse examples (math, code, visual intuition) tied to the syllabus sequence.\n\nYour primary goals:\nHelp me understand concepts deeply (intuition + formalism).\nHelp me practice effectively (HW, projects, exam prep).\n\n\nProject Files:\n\npdf of course syllabus from https://berkeley-cs182.github.io/fa25/index.html\n\npdf of course textbook\n\nExample of backtracking and knowledge retrieval:\n\nTrace Link: https://chatgpt.com/share/6908ff92-7774-8000-a975-b454a614f6ce\n\n\nTogether with the project prompt and chat prompt, ChatGPT project mode retrieves my earlier conversation threads (e.g., HW 8 questions like \u201cin unstructured W, why is recurrence faster on CPU?\u201d and \u201cdiagW = torch.diag(W) \u2014 what does it do?\u201d) and composes a factual summary table.\n\nHow classmates can reproduce (bullet list):\n\nCreate a ChatGPT project for EECS 182/282A and paste the prompt above.\n\nTag threads as Hw #, Dis #, Week #; keep each question atomic.\n\nAfter sessions, ask for a chronological project summary with the rule \u201cno invention\u2014mark not recorded when missing.\u201d\n\nWhy it helps:\n\nTurns scattered chats into structured, dated rows for HW/Dis/Week.\n\nMakes past findings searchable (e.g., what you asked 3 weeks ago when you were watching that week's lecture).\n\nReduces review time during finals by providing a durable \u201ccache layer\u201d that complements handwritten notes.\n\nComments:\n\nPositives (What Worked Well):\n\nProvides a systematic and personal way to track conceptual and coding progress across lectures, discussions, and homework.\n\nEnables quick summarization of recurring pain points \u2014 e.g., topics repeatedly causing confusion (like SSM recurrence vs convolution).\n\nWorks as a universal workflow, not just for EECS 182, but adaptable to other classes, research projects, or long-term commitments.\n\nNegatives (Limitations and Effort Required):\n\nRequires manual setup and consistent thread naming (e.g., \u201cHw 1\u201d, \u201cDis 3\u201d), which takes discipline and time.\n\nIf unrelated conversations are accidentally mixed into the project, they can poison the context, causing noisy or misleading summaries.\n\nFuture Improvements:\n\nExplore context condensation techniques \u2014 controlling the length and quality of how AI responds even during normal chat sessions so later reviews are cleaner.\n\nOrganize from the start: define thread templates (e.g., \u201cConcept | Question | Key Takeaway\u201d) to ensure consistent retrievability.\n\nPossibly integrate automated tagging or scripts to rename threads and prevent context contamination.\n\n\n\nOver time, this could evolve into a lightweight personal learning management system --  one that adapts as the AI learns your workflow.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>What I built:</bold><break/>A repeatable workflow that uses ChatGPT project mode as a study copilot across lecture, discussion, and homework -- it summarizes past questions/answers from my chat history into chronological tables and acts like a \u201ccache layer\u201d I can backtrack during reviews.</paragraph><paragraph><bold>Project Setup:</bold></paragraph><list style=\"bullet\"><list-item><paragraph>Prompt:</paragraph></list-item></list><pre>You are my personal TA for UC Berkeley\u2019s EECS 182/282A: Deep Neural Networks, Fall 2025.\nYour job is to help me learn, review, and succeed in this course.\n\nStick to course scope: topics include optimization, convnets, ResNets, GNNs, RNNs, state-space models, transformers, prompting, transfer/meta-learning, generative &amp; diffusion models.\n\nHelp style:\nKeep explanations simple, direct, and intuitive before going deep in math/code.\nUse examples (math, code, visual intuition) tied to the syllabus sequence.\n\nYour primary goals:\nHelp me understand concepts deeply (intuition + formalism).\nHelp me practice effectively (HW, projects, exam prep).\n</pre><list style=\"bullet\"><list-item><paragraph>Project Files:</paragraph></list-item></list><list style=\"number\"><list-item><paragraph>pdf of course syllabus from <link href=\"https://berkeley-cs182.github.io/fa25/index.html\">https://berkeley-cs182.github.io/fa25/index.html</link></paragraph></list-item><list-item><paragraph>pdf of course textbook</paragraph></list-item></list><paragraph><bold>Example of backtracking and knowledge retrieval:</bold></paragraph><paragraph>Trace Link: <link href=\"https://chatgpt.com/share/6908ff92-7774-8000-a975-b454a614f6ce\">https://chatgpt.com/share/6908ff92-7774-8000-a975-b454a614f6ce</link></paragraph><paragraph><break/>Together with the project prompt and chat prompt, ChatGPT project mode retrieves my earlier conversation threads (e.g., HW 8 questions like \u201cin unstructured W, why is recurrence faster on CPU?\u201d and \u201cdiagW = torch.diag(W) \u2014 what does it do?\u201d) and composes a factual summary table.</paragraph><paragraph><bold>How classmates can reproduce (bullet list):</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Create a ChatGPT <bold>project</bold> for EECS 182/282A and paste the prompt above.</paragraph></list-item><list-item><paragraph>Tag threads as <bold>Hw #</bold>, <bold>Dis #</bold>, <bold>Week #</bold>; keep each question atomic.</paragraph></list-item><list-item><paragraph>After sessions, ask for a <bold>chronological project summary</bold> with the rule \u201cno invention\u2014mark not recorded when missing.\u201d</paragraph></list-item></list><paragraph><bold>Why it helps:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Turns scattered chats into structured, dated rows for HW/Dis/Week.</paragraph></list-item><list-item><paragraph>Makes past findings searchable (e.g., what you asked 3 weeks ago when you were watching that week's lecture).</paragraph></list-item><list-item><paragraph>Reduces review time during finals by providing a durable \u201ccache layer\u201d that complements handwritten notes.</paragraph></list-item></list><paragraph><bold>Comments:</bold></paragraph><list style=\"bullet\"><list-item><paragraph><bold>Positives (What Worked Well):</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Provides a <bold>systematic and personal</bold> way to track conceptual and coding progress across lectures, discussions, and homework.</paragraph></list-item><list-item><paragraph>Enables <bold>quick summarization</bold> of recurring pain points \u2014 e.g., topics repeatedly causing confusion (like SSM recurrence vs convolution).</paragraph></list-item><list-item><paragraph>Works as a <bold>universal workflow</bold>, not just for EECS 182, but adaptable to other classes, research projects, or long-term commitments.</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Negatives (Limitations and Effort Required):</bold></paragraph></list-item></list><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph>Requires <bold>manual setup and consistent thread naming</bold> (e.g., \u201cHw 1\u201d, \u201cDis 3\u201d), which takes discipline and time.</paragraph></list-item><list-item><paragraph>If unrelated conversations are accidentally mixed into the project, they can <bold>poison the context</bold>, causing noisy or misleading summaries.</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph><bold>Future Improvements:</bold></paragraph></list-item></list><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph>Explore <bold>context condensation techniques</bold> \u2014 controlling the length and quality of how AI responds even during normal chat sessions so later reviews are cleaner.</paragraph></list-item><list-item><paragraph><bold>Organize from the start</bold>: define thread templates (e.g., \u201cConcept | Question | Key Takeaway\u201d) to ensure consistent retrievability.</paragraph></list-item><list-item><paragraph>Possibly integrate <bold>automated tagging</bold> or scripts to rename threads and prevent context contamination.</paragraph></list-item></list></list-item></list><paragraph/><paragraph>Over time, this could evolve into a <bold>lightweight personal learning management system</bold> --  one that adapts as the AI learns your workflow.</paragraph><paragraph/></document>",
            "links": [
                "https://berkeley-cs182.github.io/fa25/index.html",
                "https://chatgpt.com/share/6908ff92-7774-8000-a975-b454a614f6ce"
            ],
            "attachments": [],
            "created_at": "2025-11-04T06:54:39.812496+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7246940,
            "author": "Faiaz Khan",
            "project_title": "Special Participation B: ChatGPT on HW7",
            "post_body": "\nEvaluated ChatGPT\u20115\u2019s code for four HW7 coding tasks: (1) RNN & Gradients, (2) Last\u2011Name Classifier, (3) Autoencoders (vanilla/denoising/masked + viz + linear probe), (4) Graph\u2011Clustering (spectral).\n\nBottom line: Very strong overall. Everything in RNN and Autoencoder sections matches staff intent; the Last\u2011Name Classifier is clean and vectorized. The only material issues are in Graph\u2011Clustering:\n\nAdjacency sign error. Used Aij\u200b=exp(+\u03b3\u2225xi\u200b\u2212xj\u200b\u22252) instead of the standard exp(\u2212\u03b3\u2225xi\u200b\u2212xj\u200b\u22252) with \u03b3>0, inverting similarity and undermining spectral clustering. \n\nDegree\u2011matrix contract. A function named get_degree_matrix returned D\u22121/2 (inverse\u2011sqrt) rather than D; later steps still formed M=D\u22121/2AD\u22121/2 correctly, but the naming conflicts with the prompt/staff text and can confuse follow\u2011ups.\n\nPer\u2011task highlights:\n\nRNN & Gradients.\nRNNLayer uses two linears with a single bias on the input map, explicit unroll, and correct shapes. RecurrentRegressionModel applies a shared readout per timestep. Loss uses MSELoss and supports last_timestep_only. The gradient visualizer scales parameters and inspects the recurrent matrix Whh\u200b (right choice for exploding/vanishing diagnostics).\n\nLast\u2011Name Classifier.\nClean pipeline: Embedding \u2192 RNN/LSTM (batch_first) \u2192 gather last_pos (vectorized) \u2192 Dropout \u2192 Linear logits. Hyperparameters (2\u00d7256 LSTM, dropout 0.30, Adam 3e\u22123, grad\u2011clip 5) are plausible for the \u226580% @20 epochs target. Ethical\u2011use answer is thoughtful (proxy discrimination, surveillance, privacy, overconfidence) and aligned with staff emphasis on responsible deployment.\n\nAutoencoders.\nDecoder is symmetric (no final activation); forward/MSELoss match the objective. Denoising AE adds Gaussian noise and reconstructs the clean input; Masked AE computes masked MSE on unmasked positions only. The evaluation helper encodes features, applies a linear probe, and computes accuracy with no_grad(). The plotting helper produces mean\u2011with\u2011markers + min\u2013max band per epoch as specified. \n\nGraph\u2011Clustering.\nThe spectral pipeline (SVD on M, use U[:,:3]\u200b, row\u2011normalize, then KMeans) is correct and the commentary identifies standard pitfalls (\u03b3 scaling, need for row\u2011norm). However, the RBF sign must be negative and the degree matrix function should either return D (per prompt) or be clearly renamed if returning D\u22121/2. Fixing these yields behavior that matches the staff solution.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Ce4wVU5MIdQ148mA0fxjScdJ\" filename=\"hw7 code report.pdf\"/><paragraph><break/>Evaluated ChatGPT\u20115\u2019s code for four HW7 coding tasks: (1) <bold>RNN &amp; Gradients</bold>, (2) <bold>Last\u2011Name Classifier</bold>, (3) <bold>Autoencoders</bold> (vanilla/denoising/masked + viz + linear probe), (4) <bold>Graph\u2011Clustering</bold> (spectral).<break/><break/><bold>Bottom line:</bold> <italic>Very strong overall.</italic> Everything in <bold>RNN</bold> and <bold>Autoencoder</bold> sections matches staff intent; the <bold>Last\u2011Name Classifier</bold> is clean and vectorized. The only material issues are in <bold>Graph\u2011Clustering</bold>:</paragraph><paragraph><bold>Adjacency sign error.</bold> Used Aij\u200b=exp(+\u03b3\u2225xi\u200b\u2212xj\u200b\u22252) instead of the standard <bold>exp(\u2212\u03b3\u2225xi\u200b\u2212xj\u200b\u22252)</bold> with \u03b3&gt;0, inverting similarity and undermining spectral clustering. </paragraph><paragraph><bold>Degree\u2011matrix contract.</bold> A function named <code>get_degree_matrix</code> returned D\u22121/2 (inverse\u2011sqrt) rather than D; later steps still formed M=D\u22121/2AD\u22121/2 correctly, but the naming conflicts with the prompt/staff text and can confuse follow\u2011ups.</paragraph><paragraph><bold>Per\u2011task highlights:</bold></paragraph><paragraph><bold>RNN &amp; Gradients.</bold><break/><italic>RNNLayer</italic> uses two linears with a single bias on the input map, explicit unroll, and correct shapes. <italic>RecurrentRegressionModel</italic> applies a shared readout per timestep. Loss uses <code>MSELoss</code> and supports <code>last_timestep_only</code>. The gradient visualizer scales parameters and inspects the <bold>recurrent</bold> matrix Whh\u200b (right choice for exploding/vanishing diagnostics).</paragraph><paragraph><bold>Last\u2011Name Classifier.</bold><break/>Clean pipeline: <code>Embedding \u2192 RNN/LSTM (batch_first) \u2192 gather last_pos (vectorized) \u2192 Dropout \u2192 Linear logits</code>. Hyperparameters (2\u00d7256 LSTM, dropout 0.30, Adam 3e\u22123, grad\u2011clip 5) are plausible for the <bold>\u226580% @20 epochs</bold> target. Ethical\u2011use answer is thoughtful (proxy discrimination, surveillance, privacy, overconfidence) and aligned with staff emphasis on responsible deployment.</paragraph><paragraph><bold>Autoencoders.</bold><break/>Decoder is symmetric (no final activation); forward/<code>MSELoss</code> match the objective. Denoising AE adds Gaussian noise and reconstructs the clean input; Masked AE computes <bold>masked</bold> MSE on unmasked positions only. The evaluation helper encodes features, applies a linear probe, and computes accuracy with <code>no_grad()</code>. The plotting helper produces <bold>mean\u2011with\u2011markers + min\u2013max band</bold> per epoch as specified. </paragraph><paragraph><bold>Graph\u2011Clustering.</bold><break/>The spectral pipeline (SVD on M, use U[:,:3]\u200b, <bold>row\u2011normalize</bold>, then KMeans) is correct and the commentary identifies standard pitfalls (\u03b3 scaling, need for row\u2011norm). However, the <bold>RBF sign</bold> must be <bold>negative</bold> and the <bold>degree matrix</bold> function should either return D (per prompt) or be clearly renamed if returning D\u22121/2. Fixing these yields behavior that matches the staff solution.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T19:51:09.010159+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7246769,
            "author": "Faiaz Khan",
            "project_title": "Special Participation A: ChatGPT on HW7",
            "post_body": "I evaluated ChatGPT\u20115's capabilities for HW7 non\u2011coding parts: 3(b), 4, 7, 8, using the hw7 questions and staff\u2011solutions as ground truth.\n\nBottom line. ChatGPT\u20115\u2019s answers are correct. \n\nPer\u2011problem highlights.\n\n3(b) PCA & linear autoencoders. Derived the first\u2011order conditions exactly as in the key\u2014\u2207W2\u200b\u200bL=2(W2\u200bW1\u200b\u2212I)XX\u22a4W1\u22a4\u200b, \u2207W1\u200b\u200bL=2W2\u22a4\u200b(W2\u200bW1\u200b\u2212I)XX\u22a4\u2014and correctly verified that W2\u200b=Uk\u200b,W1\u200b=Uk\u22a4\u200b satisfies them. Verdict: correct. \n\n4 \u201cHow to train your ResNet.\u201d Reported 341 s to 94% baseline and 26 s to \u224894.1% final; reflections (b, c) are on\u2011point (data\u2011pipeline bottlenecks, batch size/LR scaling, BN precision, \u201ccatastrophic forgetting\u201d vs curvature). Verdict: correct (with extra but accurate context). \n\n7 Machine translation. (a) Correctly explains why \u201cvertical stacking\u201d is flawed (breaks variable\u2011length handling and global conditioning). (b) Teacher forcing tokens enumerated explicitly: <SOS>, I, see, a, dog. (c) Evaluation: <SOS>, then model\u2019s outputs (I, saw, a, dog). Verdict: correct. \n\n8 Self\u2011supervised linear AEs. (a) Exactly matches the objective: two Linear layers (encoder/decoder), MSELoss, SGD + weight decay; no Dropout/LN/BN. (b) Clear SVD argument that the \u03bb\u2011regularized optimum favors orthonormal columns in W2\u200b (minimizing \u03c32+1/\u03c32 at \u03c3=1). Verdict: correct. \n\nOverall assessment. The model\u2019s responses align with the staff solutions in substance and notation, with small stylistic differences only.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/xqH8GOFhYN8jRo0XPr3JoAjU\" filename=\"hw7 report.pdf\"/><paragraph>I evaluated ChatGPT\u20115's capabilities for HW7 <bold>non\u2011coding</bold> parts: <bold>3(b), 4, 7, 8</bold>, using the hw7 questions and staff\u2011solutions as ground truth.</paragraph><paragraph><bold>Bottom line.</bold> ChatGPT\u20115\u2019s answers are <bold>correct</bold>. </paragraph><paragraph><bold>Per\u2011problem highlights.</bold></paragraph><paragraph><bold>3(b) PCA &amp; linear autoencoders.</bold> Derived the <bold>first\u2011order conditions</bold> exactly as in the key\u2014\u2207W2\u200b\u200bL=2(W2\u200bW1\u200b\u2212I)XX\u22a4W1\u22a4\u200b, \u2207W1\u200b\u200bL=2W2\u22a4\u200b(W2\u200bW1\u200b\u2212I)XX\u22a4\u2014and correctly verified that W2\u200b=Uk\u200b,W1\u200b=Uk\u22a4\u200b satisfies them. <bold>Verdict: correct.</bold> </paragraph><paragraph><bold>4 \u201cHow to train your ResNet.\u201d</bold> Reported <bold>341 s to 94%</bold> baseline and <bold>26 s to \u224894.1%</bold> final; reflections (b, c) are on\u2011point (data\u2011pipeline bottlenecks, batch size/LR scaling, BN precision, \u201ccatastrophic forgetting\u201d vs curvature). <bold>Verdict: correct (with extra but accurate context).</bold> </paragraph><paragraph><bold>7 Machine translation.</bold> (a) Correctly explains why \u201cvertical stacking\u201d is flawed (breaks variable\u2011length handling and global conditioning). (b) <bold>Teacher forcing</bold> tokens enumerated explicitly: &lt;SOS&gt;, I, see, a, dog. (c) <bold>Evaluation</bold>: &lt;SOS&gt;, then model\u2019s outputs (I, saw, a, dog). <bold>Verdict: correct.</bold> </paragraph><paragraph><bold>8 Self\u2011supervised linear AEs.</bold> (a) Exactly matches the objective: <bold>two Linear</bold> layers (encoder/decoder), <bold>MSELoss</bold>, <bold>SGD + weight decay</bold>; no Dropout/LN/BN. (b) Clear SVD argument that the \u03bb\u2011regularized optimum favors <bold>orthonormal columns</bold> in W2\u200b (minimizing \u03c32+1/\u03c32 at \u03c3=1). <bold>Verdict: correct.</bold> </paragraph><paragraph><bold>Overall assessment.</bold> The model\u2019s responses align with the staff solutions in substance and notation, with small stylistic differences only.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T18:16:17.084933+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7244375,
            "author": "Ruizhe Song",
            "project_title": "Special Participation A: Gemini 2.5 Flash on HW2",
            "post_body": "I interactively engaged Gemini 2.5 Flash on the non-coding parts of Homework 2. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed.\n\nStrategies: I first clarified the main role that Gemini was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:\n\nStep 1: Provide Gemini with one homework problem.\n\nStep 2: Collect its initial (one-shot) answer.\n\nStep 3: If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.\n\nStep 4: Repeat Steps 1\u20133 for all homework problems.\n\nNotable observations and flaws. Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning process is flawless. In some cases, the model relied on specific examples provided in the prompt, which resulted in partially correct answers. Additionally, Gemini sometimes produced wrong-formed LaTeX code, making its output less readable. This issue may have been partly influenced by the formatting of the input prompts.\n\nHere's the full interaction log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I interactively engaged <bold>Gemini 2.5 Flash</bold> on the non-coding parts of <bold>Homework 2</bold>. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed.</paragraph><paragraph><bold>Strategies:</bold> I first clarified the main role that Gemini was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:</paragraph><list style=\"ordered\"><list-item><paragraph><bold>Step 1:</bold> Provide Gemini with one homework problem.</paragraph></list-item><list-item><paragraph><bold>Step 2:</bold> Collect its initial (one-shot) answer.</paragraph></list-item><list-item><paragraph><bold>Step 3:</bold> If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.</paragraph></list-item><list-item><paragraph><bold>Step 4:</bold> Repeat Steps 1\u20133 for all homework problems.</paragraph></list-item></list><paragraph><bold>Notable observations and flaws.</bold> Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning process is flawless. In some cases, the model relied on specific examples provided in the prompt, which resulted in partially correct answers. Additionally, Gemini sometimes produced wrong-formed LaTeX code, making its output less readable. This issue may have been partly influenced by the formatting of the input prompts.</paragraph><paragraph>Here's the full interaction log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:<break/><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/CQUsmoky6zcUUGfK2S2a9SwE\" filename=\"Special_Participation_A.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T10:24:52.673823+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7243310,
            "author": "Yuxiang Liu",
            "project_title": "Special Participation A: HW5 With the Help of Claude AI",
            "post_body": "Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarely see any hallucinations or misconceptions. Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward. However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point. That is to say, for long-horizon tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarely see any hallucinations or misconceptions. Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward. However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point. That is to say, for long-horizon tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal. </paragraph><file url=\"https://static.us.edusercontent.com/files/ZhphxxyAj2pRBLq6a5p1tqPg\" filename=\"HW5_walkthrough.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T07:26:53.861759+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7242208,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q6: Kernelized Linear Attention (Part 1)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/lJAtk3sNnUUMGxUXHxP4Leng\" width=\"476\" height=\"1358\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:32:33.815475+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7242203,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q5: Coding Question: Visualizing Attention",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/21D0Lcba60QCOKU4hwxA3DWg\" width=\"658\" height=\"899.4827586206897\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:31:37.446507+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7242199,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q4: Transformer Decoding Optimization",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/IIfmcOWBAncmGoGcnKO9oRxW\" width=\"462\" height=\"1547\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:30:27.91135+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7242190,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q3: Ordinary Softmax Multihead Attention Implementation",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/F8JmNchwPy28TvZzI15BUKYv\" width=\"570\" height=\"1342\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:29:07.493885+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7242181,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q2: Argmax Attention",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/dPK7K1vJyIXLHcddS06twgDI\" width=\"658\" height=\"762.9150779896013\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:27:33.285393+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7242178,
            "author": "Sultan Daniels",
            "project_title": "HW9 Q1: Justifying Scaled Dot Product Attention",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/Wg8b4HOhTLKKsWb7vgcLeThm\" width=\"658\" height=\"195.4238042269188\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-03T03:26:52.480594+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7240163,
            "author": "Andy Zhang",
            "project_title": "Special Participation E: Quiz and Flash Cards for Adam / SGD",
            "post_body": "Special Participation E: Quiz and Flash Cards for Adam / SGD\n\nFor background, Gemini claims that they can create quizzes, flash cards & study guides according to https://support.google.com/gemini/answer/16275879?hl=en&co=GENIE.Platform%3DAndroid\n\nEspecially since Gemini Pro is free for Berkeley students, I first explored and confirmed that Gemini could indeed create those; it created specific and specialized artifacts for only Quizzes and Flash Cards (and you need to be careful about language) which made it more convenient than text.\n\nWhile the initial the quiz and flash cards were reasonable, they are generally conceptual (e.g. the purpose of the first and second moments of Adam, the purpose of hyperparameters etc.) so I wanted to explore whether the quizzes and flash cards could be more mathematical and cover more rigor similar to our course and see whether latex could properly render in the quiz and flash cards.\n\nFrom there I was able to develop an improved prompt to have more mathematics to be more aligned with our course, formatted properly in latex. And depending on one\u2019s presences, the model can be steered to act accordingly. \n\nMy recommendation is that these could be helpful to students to review concepts such as Adam and SGD especially after several weeks have passed before the final. My recommended order is to go through the flash cards for a memory refresher and then take the quiz to ensure the concepts are cemented.\n\n\n\nTraces with detailed comments: https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing\n\n\n\nIndividual traces:\n\nUndetailed Prompt Trace Flash Cards:\n\nhttps://gemini.google.com/share/ed66de185158\n\nUndetailed Prompt Trace Quiz:\n\nhttps://gemini.google.com/share/598e4b5cfbcc\n\nUndetailed Flash Cards:\n\nhttps://gemini.google.com/share/cfd44ed946d6\n\nUndetailed Quiz:\n\nhttps://gemini.google.com/share/6fab8238136e\n\n\n\nFlash cards / quizzes:\n\nDetailed Prompt Trace for Flash Cards with Math:\n\nhttps://gemini.google.com/share/f7957da6c0d1\n\nDetailed Prompt Trace for Quiz:\n\nhttps://gemini.google.com/share/ec87b322ca7e\n\nDetailed Flash Cards: https://gemini.google.com/share/6afed4acff7b\n\nDetailed Quiz: https://gemini.google.com/share/01910a14266a",
            "content_xml": "<document version=\"2.0\"><paragraph>Special Participation E: Quiz and Flash Cards for Adam / SGD</paragraph><paragraph>For background, Gemini claims that they can create quizzes, flash cards &amp; study guides according to <link href=\"https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid\">https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid</link></paragraph><paragraph>Especially since Gemini Pro is free for Berkeley students, I first explored and confirmed that Gemini could indeed create those; it created specific and specialized artifacts for only Quizzes and Flash Cards (and you need to be careful about language) which made it more convenient than text.</paragraph><paragraph>While the initial the quiz and flash cards were reasonable, they are generally conceptual (e.g. the purpose of the first and second moments of Adam, the purpose of hyperparameters etc.) so I wanted to explore whether the quizzes and flash cards could be more mathematical and cover more rigor similar to our course and see whether latex could properly render in the quiz and flash cards.</paragraph><paragraph>From there I was able to develop an improved prompt to have more mathematics to be more aligned with our course, formatted properly in latex. And depending on one\u2019s presences, the model can be steered to act accordingly. </paragraph><paragraph>My recommendation is that these could be helpful to students to review concepts such as Adam and SGD especially after several weeks have passed before the final. My recommended order is to go through the flash cards for a memory refresher and then take the quiz to ensure the concepts are cemented.</paragraph><paragraph/><paragraph>Traces with detailed comments: https://drive.google.com/file/d/1X6M9JPHW4GRSUKRFckDdFsxYcfWyB-97/view?usp=sharing</paragraph><paragraph/><paragraph>Individual traces:</paragraph><paragraph>Undetailed Prompt Trace Flash Cards:</paragraph><paragraph><link href=\"https://gemini.google.com/share/ed66de185158\">https://gemini.google.com/share/ed66de185158</link></paragraph><paragraph>Undetailed Prompt Trace Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/598e4b5cfbcc\">https://gemini.google.com/share/598e4b5cfbcc</link></paragraph><paragraph>Undetailed Flash Cards:</paragraph><paragraph><link href=\"https://gemini.google.com/share/cfd44ed946d6\">https://gemini.google.com/share/cfd44ed946d6</link></paragraph><paragraph>Undetailed Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/6fab8238136e\">https://gemini.google.com/share/6fab8238136e</link></paragraph><paragraph/><paragraph>Flash cards / quizzes:</paragraph><paragraph>Detailed Prompt Trace for Flash Cards with Math:</paragraph><paragraph><link href=\"https://gemini.google.com/share/f7957da6c0d1\">https://gemini.google.com/share/f7957da6c0d1</link></paragraph><paragraph>Detailed Prompt Trace for Quiz:</paragraph><paragraph><link href=\"https://gemini.google.com/share/ec87b322ca7e\">https://gemini.google.com/share/ec87b322ca7e</link></paragraph><paragraph>Detailed Flash Cards: <link href=\"https://gemini.google.com/share/6afed4acff7b\">https://gemini.google.com/share/6afed4acff7b</link></paragraph><paragraph>Detailed Quiz: <link href=\"https://gemini.google.com/share/01910a14266a\">https://gemini.google.com/share/01910a14266a</link></paragraph></document>",
            "links": [
                "https://support.google.com/gemini/answer/16275879?hl=en&amp;co=GENIE.Platform%3DAndroid",
                "https://gemini.google.com/share/ed66de185158",
                "https://gemini.google.com/share/598e4b5cfbcc",
                "https://gemini.google.com/share/cfd44ed946d6",
                "https://gemini.google.com/share/6fab8238136e",
                "https://gemini.google.com/share/f7957da6c0d1",
                "https://gemini.google.com/share/ec87b322ca7e",
                "https://gemini.google.com/share/6afed4acff7b",
                "https://gemini.google.com/share/01910a14266a"
            ],
            "attachments": [],
            "created_at": "2025-11-02T10:39:47.878068+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7236085,
            "author": "Ender Ji",
            "project_title": "Special Participation B: HW0 with Grok",
            "post_body": "I used Grok to complete the coding portion of HW0, the first assignment of the semester. I begin by clearly stating Grok\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them.\n\nFor the coding section, I consistently supply the empty (starter) script to Grok for an initial pass, then move to a specific function and ask it to implement a solution. I repeat this pipeline for all coding problems.\n\nGrok performed very well on the coding tasks; it typically produces a correct solution on the first attempt and provides clear explanations of the variables and ideas it uses. I believe one reason is that the implemented functions in HW0 are relatively straightforward and standard compared to other assignments. However, for the final part of the notebook, where I ask it to propose a valid weight_scale and learning_rate, because it lacks access to data, Grok can only suggest reasonable parameters to try, with no guarantee of solving the problem on the first attempt.\n\nOverall, Grok demonstrated strong performance on the coding portion of HW0.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/fXXi8cPu0HhKrfQjZMf6OTyo\" filename=\"grok_hw0.pdf\"/><paragraph>I used Grok to complete the coding portion of HW0, the first assignment of the semester. I begin by clearly stating Grok\u2019s role and the assistance I require, then provide all relevant files and ask it to review and understand them.</paragraph><paragraph>For the coding section, I consistently supply the empty (starter) script to Grok for an initial pass, then move to a specific function and ask it to implement a solution. I repeat this pipeline for all coding problems.</paragraph><paragraph>Grok performed very well on the coding tasks; it typically produces a correct solution on the first attempt and provides clear explanations of the variables and ideas it uses. I believe one reason is that the implemented functions in HW0 are relatively straightforward and standard compared to other assignments. However, for the final part of the notebook, where I ask it to propose a valid weight_scale and learning_rate, because it lacks access to data, Grok can only suggest reasonable parameters to try, with no guarantee of solving the problem on the first attempt.</paragraph><paragraph>Overall, Grok demonstrated strong performance on the coding portion of HW0.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-01T08:29:46.961354+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7235411,
            "author": "Anant Sahai",
            "project_title": "Emergency pre-proposal office hours 2-3:30pm today",
            "post_body": "Hello,\n\nIt has come to our attention that a few groups were unable to find slots this week because they tried looking after many slots had already passed. So we are adding some emergency office hours today from 2-3:30pm on a drop in basis in my office: 267 Cory. (Come in through 253 Cory)",
            "content_xml": "<document version=\"1.0\"><paragraph>Hello,</paragraph><paragraph>It has come to our attention that a few groups were unable to find slots this week because they tried looking after many slots had already passed. So we are adding some emergency office hours today from 2-3:30pm on a drop in basis in my office: 267 Cory. (Come in through 253 Cory)</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-11-01T06:39:06.173706+11:00",
            "category": "Project"
        },
        {
            "guid": 7231275,
            "author": "Aryan Panda",
            "project_title": "\ud83d\udce2 Looking for 1 More Project (thread 3) Partner (Mechanistic Interpretability + Robotics!)",
            "post_body": "Hey everyone!\n\nWe\u2019re currently a team of 2 EECS undergrads + 1 Master\u2019s student working on a project in mechanistic interpretability \u2014 exploring how mechanical decisions emerge and can be analyzed within robotic automation.\n\nWe\u2019re excited to dive deeper into the intersection of Mechanistic Interpretability (proj. 3)\n\nWe\u2019re looking for one more teammate who\u2019s excited about interpretability, VPAs, or just understanding how AI \u201cthinks.\u201d \n\nIf that sounds like you, message me \u2014 we\u2019d love to have you join our team!",
            "content_xml": "<document version=\"2.0\"><paragraph>Hey everyone!</paragraph><paragraph>We\u2019re currently a team of 2 <bold>EECS undergrads + 1 Master\u2019s student</bold> working on a project in <bold>mechanistic interpretability</bold> \u2014 exploring how mechanical decisions emerge and can be analyzed within robotic automation.</paragraph><paragraph>We\u2019re excited to dive deeper into the intersection of Mechanistic <bold>Interpretability (proj. 3)</bold></paragraph><paragraph>We\u2019re looking for <bold>one more teammate</bold> who\u2019s excited about interpretability, VPAs, or just understanding how AI \u201cthinks.\u201d <break/><break/>If that sounds like you, message me \u2014 we\u2019d love to have you join our team!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-31T09:52:20.600793+11:00",
            "category": "Project"
        },
        {
            "guid": 7227705,
            "author": "Tianhao Qian",
            "project_title": "Special Participation C: Refactoring of HW2 task 3",
            "post_body": "Summary:\n\nThrough this refactoring, the project evolved from a single-use notebook into a maintainable machine learning codebase.\n\n\n\n| Dimension | Before Refactoring | After Refactoring | \n\n| --------------- | ------------------ | ---------------------- | \n\n| Structure | Flat Jupyter cells | Modular Python modules | \n\n| Reproducibility | Manual seeds | Centralized config | \n\n| Logging | `print()` | Structured logger | \n\n| Experimentation | Manual | Configurable sweeps | \n\n| Maintainability | Low | High |\n\n\n\nThis structure now supports both educational transparency and engineering rigor, serving as a clean baseline for future deep learning experiments.\n\nGithub repo: \n\nhysteri1a/-EECS182-Refactoring-Report-HW3\n\nReport:\n\nDone by Lawrence Qian(3041996584)",
            "content_xml": "<document version=\"2.0\"><paragraph>Summary:</paragraph><paragraph>Through this refactoring, the project evolved from a single-use notebook into a maintainable machine learning codebase.</paragraph><paragraph/><paragraph>| Dimension | Before Refactoring | After Refactoring | </paragraph><paragraph>| --------------- | ------------------ | ---------------------- | </paragraph><paragraph>| Structure | Flat Jupyter cells | Modular Python modules | </paragraph><paragraph>| Reproducibility | Manual seeds | Centralized config | </paragraph><paragraph>| Logging | `print()` | Structured logger | </paragraph><paragraph>| Experimentation | Manual | Configurable sweeps | </paragraph><paragraph>| Maintainability | Low | High |</paragraph><paragraph/><paragraph>This structure now supports both educational transparency and engineering rigor, serving as a clean baseline for future deep learning experiments.</paragraph><paragraph>Github repo: </paragraph><paragraph><link href=\"https://github.com/hysteri1a/-EECS182-Refactoring-Report-HW3\">hysteri1a/-EECS182-Refactoring-Report-HW3</link><break/><break/>Report:</paragraph><file url=\"https://static.us.edusercontent.com/files/wPlDrJN5jdVy6foFZlRs2rC8\" filename=\"EECS_participation_C.pdf\"/><paragraph>Done by Lawrence Qian(3041996584)</paragraph></document>",
            "links": [
                "https://github.com/hysteri1a/-EECS182-Refactoring-Report-HW3"
            ],
            "attachments": [],
            "created_at": "2025-10-30T18:23:28.716223+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7227387,
            "author": "Zhuangzhe Wu",
            "project_title": "Special Participation A: Deepseek Chat on HW3",
            "post_body": "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear algebra & calculus & probability , providing both solutions and clear, step-by-step explanations. Both the calculation and proof problems were well done. It can retrieve and understand the formulas from papers and analyze them well.\n\nEffective Information Retrieval and Synthesis: A key strength is its ability to process academic papers, accurately identify core arguments and results, and summarize them concisely, demonstrating strong comprehension and distillation skills.\n\nDeepSeek's integration of these capabilities makes it a remarkably efficient for navigating the challenges of advanced math & algorithm & deep learning studies.\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/yKIpKIoETd80ZmgZoR0qAm1j\" filename=\"deepseek_hw3_log.pdf\"/><paragraph><bold>Conclusion:</bold></paragraph><paragraph>The evaluation of DeepSeek's capabilities for homework 3 has demonstrated :</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Strong Mathematical Problem-Solving:</bold> DeepSeek reliably handles the mathematical problems, including linear algebra &amp; calculus &amp; probability , providing both solutions and clear, step-by-step explanations. Both the calculation and proof problems were well done. It can retrieve and understand the formulas from papers and analyze them well.</paragraph></list-item><list-item><paragraph><bold>Effective Information Retrieval and Synthesis:</bold> A key strength is its ability to process academic papers, accurately identify core arguments and results, and summarize them concisely, demonstrating strong comprehension and distillation skills.</paragraph></list-item></list><paragraph>DeepSeek's integration of these capabilities makes it a remarkably efficient for navigating the challenges of advanced math &amp; algorithm &amp; deep learning studies.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-30T16:12:13.694906+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7227336,
            "author": "Anders Vestrum",
            "project_title": "Special Participation E: How I used Google NotebookLM to understand SSM",
            "post_body": "Special Participation E (Google NotebookLM) - RNNs and SSMs\n\nI used Google NotebookLM as a guided tutor to study the relationship between State-Space Models (SSMs) and Recurrent Neural Networks (RNNs), using EECS 182 materials and the S4 paper. Through its Learning Guide mode, the system\u2019s questions helped me understand how SSMs convert sequential recurrence into FFT-based convolution and how diagonalization combined with the Woodbury Matrix Identity makes the S4 model both efficient and stable. I noted one factual correction: the chatbot listed the FFT cost as O(N\u00b7L log L), but standard FFT convolution parallelizes across time, giving O(L log L) instead. The session also introduced Ridge-Attention, illustrating how it reframes self-attention from a probabilistic softmax view to a linear-algebraic one.\n\nNotebookLM link (video, mind map, quiz, and flashcards in this link: https://notebooklm.google.com/notebook/1da24f39-d172-4725-a38c-5f6366e8ac95",
            "content_xml": "<document version=\"2.0\"><paragraph>Special Participation E (Google NotebookLM) - RNNs and SSMs</paragraph><paragraph>I used Google NotebookLM as a guided tutor to study the relationship between State-Space Models (SSMs) and Recurrent Neural Networks (RNNs), using EECS 182 materials and the S4 paper. Through its Learning Guide mode, the system\u2019s questions helped me understand how SSMs convert sequential recurrence into FFT-based convolution and how diagonalization combined with the Woodbury Matrix Identity makes the S4 model both efficient and stable. I noted one factual correction: the chatbot listed the FFT cost as O(N\u00b7L log L), but standard FFT convolution parallelizes across time, giving O(L log L) instead. The session also introduced Ridge-Attention, illustrating how it reframes self-attention from a probabilistic softmax view to a linear-algebraic one.</paragraph><file url=\"https://static.us.edusercontent.com/files/bDTJCfOluen1y8BEHkKwa2Ee\" filename=\"spE_notebookLM_RNN_SSM.pdf\"/><paragraph>NotebookLM link (video, mind map, quiz, and flashcards in this link: https://notebooklm.google.com/notebook/1da24f39-d172-4725-a38c-5f6366e8ac95</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-30T15:55:36.861101+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7226711,
            "author": "Sultan Daniels",
            "project_title": "HW07 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/8IeWVQUwFLfY7Dx0Wdz5pl0M\" filename=\"hw07codesolution.zip\"/><file url=\"https://static.us.edusercontent.com/files/2aSoZhCoU0diMNeuWGGEfnrt\" filename=\"hw07_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/N7V2aaEfvlufyXlt8pXkhJSc\" filename=\"hw07_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-30T13:50:22.967662+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7226661,
            "author": "Noah Lund Syrdal",
            "project_title": "Special participation E: Create a custom version of chatGPT,  \u03bcP & Modern Optimizers Coach",
            "post_body": "Link: \u03bcP & Modern Optimizers Coach\n\nPdf: \n\nExtensive summary:\n\nThis custom GPT acts as an interactive study companion for \u03bcP scaling, RMSNorm, and modern optimizers.\nIt\u2019s a great way to ask questions about concepts you don\u2019t fully understand. I\u2019ve stress-tested it on common misconceptions about \u03bcP invariance, and it consistently provided nuanced, well-grounded answers (tried to get a wrong answer out of it, didn\u2019t manage).\n\nYou can use my version directly, or follow the steps in the PDF to create your own tailored model on a different concept.\nIt works well both as a pre-lecture active reading tool and a post-lecture review partner; you can even talk with it out loud while learning.\n\nI would strongly recommend this approach for anyone in the course. I\u2019ll be creating similar custom GPTs for other core concepts in 182/282 and eventually one that covers the entire course, to explore how well it can guide learning across all modules.",
            "content_xml": "<document version=\"2.0\"><paragraph>Link<bold>:</bold> <link href=\"https://chatgpt.com/g/g-68f2b5f669148191915a03ef03fc69d6-mp-modern-optimizers-coach-eecs182-282\"><underline>\u03bcP &amp; Modern Optimizers Coach</underline></link></paragraph><paragraph>Pdf: </paragraph><file url=\"https://static.us.edusercontent.com/files/AB1TpLpivYeHk0yo44YDkb3m\" filename=\"Special_Participation_E-3.pdf\"/><paragraph>Extensive summary:</paragraph><paragraph>This custom GPT acts as an interactive study companion for \u03bcP scaling, RMSNorm, and modern optimizers.<break/>It\u2019s a great way to ask questions about concepts you don\u2019t fully understand. I\u2019ve stress-tested it on common misconceptions about \u03bcP invariance, and it consistently provided nuanced, well-grounded answers (tried to get a wrong answer out of it, didn\u2019t manage).</paragraph><paragraph>You can use my version directly, or follow the steps in the PDF to create your own tailored model on a different concept.<break/>It works well both as a pre-lecture active reading tool and a post-lecture review partner; you can even talk with it out loud while learning.</paragraph><paragraph>I would strongly recommend this approach for anyone in the course. I\u2019ll be creating similar custom GPTs for other core concepts in 182/282 and eventually one that covers the entire course, to explore how well it can guide learning across all modules.</paragraph></document>",
            "links": [
                "https://chatgpt.com/g/g-68f2b5f669148191915a03ef03fc69d6-mp-modern-optimizers-coach-eecs182-282"
            ],
            "attachments": [],
            "created_at": "2025-10-30T13:40:12.723427+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7220991,
            "author": "Junya Tsuneishi",
            "project_title": "Special Participation B: ChatGPT on HW1",
            "post_body": "I used ChatGPT on HW1 cording parts(Special Participation B). \nI posted the results, my findings about them, and my summary on the attached pdf. This is summary from the pdf.\n\nOne-Shot Capability: ChatGPT's one-shot performance was mixed.\n\nTODO 1: ChatGPT successfully implemented the logic for the exponential moving average of the gradient in one shot. Its implementation was a valid, standard implementation, although it differed from the class style. This is a valid ambiguity, as both are correct definitions, just with beta and 1-beta flipped.\n\nTODO2 : ChatGPT failed to solve this task on its first attempt. ChatGPT exhibited two distinct types of errors on the second TODO, requiring two rounds of interactive correction.\n\nSemantic Misinterpretation: The notebook prompt asks to \"further accelerate\" the faster method (GDM). ChatGPT misinterpreted this and instead modified the slower method (GD) to see if it could \"catch up.\" This was a fundamental failure to understand the problem's intent.\n\nContextual Blindness: After the first hint, ChatGPT correctly identified which algorithm to modify (GDM) but introduced new variable names (grads_m_fast, losses_m_fast). The code block is logically correct in isolation, but it fails to consider the notebook's state, as subsequent plotting cells relied on the original variable names (grads_m, losses_m), which would have caused the notebook to crash or plot incorrectly.\n\nConclusion: ChatGPT was effective at writing a self-contained algorithm block (TODO 1). However, it struggled significantly with a task that required contextual understanding of the problem's intent and the notebook's state (TODO 2). It required a human to act as a \"debugger,\" providing specific, iterative hints to correct both its logical and contextual errors.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/jUdl7UZKBDsMtebjyUpP8yel\" filename=\"Special Participation B ChatGPT on HW1.pdf\"/><paragraph>I used ChatGPT on HW1 cording parts(Special Participation B). <break/>I posted the results, my findings about them, and my summary on the attached pdf. This is summary from the pdf.</paragraph><list style=\"bullet\"><list-item><paragraph>One-Shot Capability: ChatGPT's one-shot performance was mixed.</paragraph></list-item></list><list style=\"bullet\"><list-item><list style=\"unordered\"><list-item><paragraph>TODO 1: ChatGPT successfully implemented the logic for the exponential moving average of the gradient in one shot. Its implementation was a valid, standard implementation, although it differed from the class style. This is a valid ambiguity, as both are correct definitions, just with beta and 1-beta flipped.</paragraph></list-item><list-item><paragraph>TODO2 : ChatGPT failed to solve this task on its first attempt. ChatGPT exhibited two distinct types of errors on the second TODO, requiring two rounds of interactive correction.</paragraph></list-item></list></list-item></list><list style=\"ordered\"><list-item><list style=\"ordered\"><list-item><list style=\"ordered\"><list-item><paragraph>Semantic Misinterpretation: The notebook prompt asks to \"further accelerate\" the faster method (GDM). ChatGPT misinterpreted this and instead modified the slower method (GD) to see if it could \"catch up.\" This was a fundamental failure to understand the problem's intent.</paragraph></list-item><list-item><paragraph>Contextual Blindness: After the first hint, ChatGPT correctly identified which algorithm to modify (GDM) but introduced new variable names (grads_m_fast, losses_m_fast). The code block is logically correct in isolation, but it fails to consider the notebook's state, as subsequent plotting cells relied on the original variable names (grads_m, losses_m), which would have caused the notebook to crash or plot incorrectly.</paragraph></list-item></list></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph>Conclusion: ChatGPT was effective at writing a self-contained algorithm block (TODO 1). However, it struggled significantly with a task that required contextual understanding of the problem's intent and the notebook's state (TODO 2). It required a human to act as a \"debugger,\" providing specific, iterative hints to correct both its logical and contextual errors.</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-29T15:01:20.304451+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7219478,
            "author": "Junya Tsuneishi",
            "project_title": "Special Participation A: ChatGPT on HW1",
            "post_body": "I used ChatGPT on HW1 no-cording parts(Special Participation A).\nI posted the results, my findings about them, and my summary on the attached pdf.\n\nThis is summary from the pdf.\nOverall, ChatGPT achieved fully correct answers for all problems, demonstrating strong mathematical reasoning and consistency across sequential tasks. However, the interaction also revealed distinctive behavioral patterns in how the model approached problem solving.\n\nAccuracy and One-Shot Performance\n\nFor straightforward conceptual or definitional questions (e.g., explaining optimizer structures or interpreting results), ChatGPT often produced the correct answer immediately (\u201cone-shot\u201d).\n\nFor more complex derivations or matrix manipulations, it sometimes made conceptual simplification errors rather than arithmetic ones. Nevertheless, this resulted in only one error out of seven big problems.\n\nExample of Conceptual Oversimplification\n\nIn one problem, the model incorrectly dropped the matrix when simplifying to .\n\nThis error propagated through subsequent parts, showing that ChatGPT solved sequentially, building upon its previous reasoning.\n\nOnce provided with a hint, it immediately corrected the mistake\u2014illustrating that the model is particularly strong at conditional reasoning with scaffolding.\n\nStyle of Reasoning\n\nChatGPT consistently produced mathematically formal and symbol-heavy responses, sometimes more rigorous than the problem required.\n\nEspecially for \u201cexplanation\u201d questions, it tended to over-mathematize rather than summarize intuitively. This pattern was observed across multiple sub-questions.\n\nResponse Latency and Thoughtfulness\n\nThe model typically took 2\u20135 minutes for moderately complex derivations, suggesting that its \u201cThinking\u201d mode encouraged stepwise symbolic reasoning rather than rapid generation.\n\nNotation and Presentation\n\nAll responses were logically consistent with the official solutions, though small notational differences (e.g., transposition order or matrix symbols) occasionally appeared.\n\nWhen a matrix identity was mentioned but not proven in the problem statement, ChatGPT sometimes derived it explicitly, reflecting uncertainty about whether it could assume the result.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Qlsh9EEsaZjmCN1abwyvsGlb\" filename=\"Special Participation A ChatGPT on HW1.pdf\"/><paragraph>I used ChatGPT on HW1 no-cording parts(Special Participation A).<break/>I posted the results, my findings about them, and my summary on the attached pdf.<break/><break/>This is summary from the pdf.<break/>Overall, ChatGPT achieved fully correct answers for all problems, demonstrating strong mathematical reasoning and consistency across sequential tasks. However, the interaction also revealed distinctive behavioral patterns in how the model approached problem solving.</paragraph><list style=\"ordered\"><list-item><paragraph>Accuracy and One-Shot Performance</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>For straightforward conceptual or definitional questions (e.g., explaining optimizer structures or interpreting results), ChatGPT often produced the correct answer immediately (\u201cone-shot\u201d).</paragraph></list-item><list-item><paragraph>For more complex derivations or matrix manipulations, it sometimes made conceptual simplification errors rather than arithmetic ones. Nevertheless, this resulted in only one error out of seven big problems.</paragraph></list-item></list></list-item><list-item><paragraph>Example of Conceptual Oversimplification</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>In one problem, the model incorrectly dropped the matrix when simplifying to .</paragraph></list-item><list-item><paragraph>This error propagated through subsequent parts, showing that ChatGPT solved sequentially, building upon its previous reasoning.</paragraph></list-item><list-item><paragraph>Once provided with a hint, it immediately corrected the mistake\u2014illustrating that the model is particularly strong at conditional reasoning with scaffolding.</paragraph></list-item></list></list-item><list-item><paragraph>Style of Reasoning</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>ChatGPT consistently produced mathematically formal and symbol-heavy responses, sometimes more rigorous than the problem required.</paragraph></list-item><list-item><paragraph>Especially for \u201cexplanation\u201d questions, it tended to over-mathematize rather than summarize intuitively. This pattern was observed across multiple sub-questions.</paragraph></list-item></list></list-item><list-item><paragraph>Response Latency and Thoughtfulness</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>The model typically took 2\u20135 minutes for moderately complex derivations, suggesting that its \u201cThinking\u201d mode encouraged stepwise symbolic reasoning rather than rapid generation.</paragraph></list-item></list></list-item><list-item><paragraph>Notation and Presentation</paragraph></list-item><list-item><list style=\"unordered\"><list-item><paragraph>All responses were logically consistent with the official solutions, though small notational differences (e.g., transposition order or matrix symbols) occasionally appeared.</paragraph></list-item><list-item><paragraph>When a matrix identity was mentioned but not proven in the problem statement, ChatGPT sometimes derived it explicitly, reflecting uncertainty about whether it could assume the result.</paragraph></list-item></list></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-29T11:02:12.9722+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7219165,
            "author": "Nyx Iskandar",
            "project_title": "Special Participation B: GPT-5 HW4",
            "post_body": "Generally, GPT-5 is able to one-shot all coding questions. I didn't try to ask it the observation questions, which is to say that it only completes the strictly code-writing ones. I'm quite surprised that it is able to accurately complete the coding questions, since my experience with GPT-5 for coding has not been the best, and Claude definitely edges out GPT-5 for coding-related tasks. Perhaps the implementations are relatively simple, hence the high accuracy :)",
            "content_xml": "<document version=\"2.0\"><paragraph>Generally, GPT-5 is able to one-shot all coding questions. I didn't try to ask it the observation questions, which is to say that it only completes the strictly code-writing ones. I'm quite surprised that it is able to accurately complete the coding questions, since my experience with GPT-5 for coding has not been the best, and Claude definitely edges out GPT-5 for coding-related tasks. Perhaps the implementations are relatively simple, hence the high accuracy :)</paragraph><file url=\"https://static.us.edusercontent.com/files/QbrVZ0vTgiZNfVZEPnIHPxgU\" filename=\"EECS_182_HW_4_GPT_5_Trace.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-29T10:10:12.536043+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7215058,
            "author": "Bruno Vieira",
            "project_title": "Special Participation E: Claude Study \"Buddy\"",
            "post_body": "I used Claude's study feature to use it as a study \"buddy\". I pretended to be studying for an exam, trying to review content, and study for a specific topic via pre-lecture notes all using Claude to guide my studies.\n\nI started out with this map to see if it could connect all the topics from this class into a graph and assess the relationships between them. Below is the trace with all my comments! Hope this is helpful and happy studying.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>I used Claude's study feature to use it as a study \"buddy\". I pretended to be studying for an exam, trying to review content, and study for a specific topic via pre-lecture notes all using Claude to guide my studies.</paragraph><paragraph>I started out with this map to see if it could connect all the topics from this class into a graph and assess the relationships between them. Below is the trace with all my comments! Hope this is helpful and happy studying.</paragraph><file url=\"https://static.us.edusercontent.com/files/kFuAKFCnfFtJk8o3u8FVRZpS\" filename=\"Claude Trace Study Buddy.pdf\"/><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/Eudu0uIW4L6ppMkzDd9hcPgz\" width=\"558\" height=\"323.14029850746266\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-28T17:20:38.187029+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7212394,
            "author": "Jeffrey Cheng",
            "project_title": "Special Participation B: Mistral AI's Le Chat on HW3 Q2(coding)",
            "post_body": "Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\n\nHere is the annotated log: \n\nExecutive Summary:\n\nFor part b), Le Chat was able to arrive at the correct norm implementation on one shot. I am especially surprised by its capability for understanding the RMS-norm without any prompt engineering. \n\nFor part c), Le Chat seemed to firstly stumble on the error of \"using local variable for global variable\". Then, it started to create its own implementation instead of conforming to the implementation on the Jupyter notebook. I had to manually force it to stick to the original implementation by doing some prompt engineering.  After this step, it arrived at the correct solution on one shot. \n\nFor part d), Le Chat seemed to have the same problem as part c). In particular, it created its own local variables and \"hallucinates\" on the same functionality as the original implementation. Again, some prompt engineering helped it to arrive at the correct solution.\n\nFor part e), Le Chat was able to arrive at the correct norm implementation on one shot. I conjecture that it's probably because this question contains less code implementation but more analytical components. \n\nFor part f), Le Chat seemed to have the same problem as part c)d). Again, some prompt engineering helped it to arrive at the correct implementation.",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is the online link: <link href=\"https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\">https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d</link></paragraph><paragraph>Here is the annotated log: </paragraph><file url=\"https://static.us.edusercontent.com/files/lOUNW0UbEzrSEvZjRhPaVhPw\" filename=\"Annotated log of conversation with Mistral AI coding.pdf\"/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>For part b), Le Chat was able to arrive at the correct norm implementation on one shot. I am especially surprised by its capability for understanding the RMS-norm without any prompt engineering. </paragraph><paragraph>For part c), Le Chat seemed to firstly stumble on the error of \"using local variable for global variable\". Then, it started to create its own implementation instead of conforming to the implementation on the Jupyter notebook. I had to manually force it to stick to the original implementation by doing some prompt engineering.  After this step, it arrived at the correct solution on one shot. </paragraph><paragraph>For part d), Le Chat seemed to have the same problem as part c). In particular, it created its own local variables and \"hallucinates\" on the same functionality as the original implementation. Again, some prompt engineering helped it to arrive at the correct solution.</paragraph><paragraph>For part e), Le Chat was able to arrive at the correct norm implementation on one shot. I conjecture that it's probably because this question contains less code implementation but more analytical components. </paragraph><paragraph>For part f), Le Chat seemed to have the same problem as part c)d). Again, some prompt engineering helped it to arrive at the correct implementation.</paragraph></document>",
            "links": [
                "https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d"
            ],
            "attachments": [],
            "created_at": "2025-10-28T09:53:28.472879+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7212198,
            "author": "Mehul Jaiswal",
            "project_title": "Special Participation A HW5: ChatGPT 5",
            "post_body": "Link to the discussion: https://chatgpt.com/share/68ffefde-4c64-800f-9d99-5417df7eb2b8\n\nAcross this project, GPT accuracy was strongest when my prompts were concrete about goals, constraints, and style, and I consistently drove that clarity. I asked questions in a structured way\u2014first \u201cunderstand \u2192 summarize \u2192 derive \u2192 implement,\u201d then \u201ccomplete the code\u201d with strict guardrails (no new helpers, inverted dropout, 0.5 L2 factor, BN modes, minimal diffs, NumPy-only where required). GPT answered by laying out the full math (OLS, BN, dropout expected-risk, conv/transpose-conv) and then mirroring it in code that matched the exected methods in different files. When GPT drifted\u2014e.g., offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (\u201ccomplete the notebook/file,\u201d \u201cset bn_param['mode'],\u201d \u201cuse inverted dropout and scale by 1/(1-p),\u201d \u201cdon\u2019t regularize gamma/beta\u201d), which corrected the approach and aligned outputs the expected result. That feedback loop precise nudges on the few inaccuracies and my immediate revisions in different branches of the same chat , tightened both the math explanations and the implementations until they matched my expected results and coding style.",
            "content_xml": "<document version=\"2.0\"><paragraph>Link to the discussion: https://chatgpt.com/share/68ffefde-4c64-800f-9d99-5417df7eb2b8</paragraph><list style=\"unordered\"/><paragraph>Across this project, GPT accuracy was strongest when my prompts were concrete about goals, constraints, and style, and I consistently drove that clarity. I asked questions in a structured way\u2014first \u201cunderstand \u2192 summarize \u2192 derive \u2192 implement,\u201d then \u201ccomplete the code\u201d with strict guardrails (no new helpers, inverted dropout, 0.5 L2 factor, BN modes, minimal diffs, NumPy-only where required). GPT answered by laying out the full math (OLS, BN, dropout expected-risk, conv/transpose-conv) and then mirroring it in code that matched the exected methods in different files. When GPT drifted\u2014e.g., offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (\u201ccomplete the notebook/file,\u201d \u201cset <code>bn_param['mode']</code>,\u201d \u201cuse inverted dropout and scale by 1/(1-p),\u201d \u201cdon\u2019t regularize gamma/beta\u201d), which corrected the approach and aligned outputs the expected result. That feedback loop precise nudges on the few inaccuracies and my immediate revisions in different branches of the same chat , tightened both the math explanations and the implementations until they matched my expected results and coding style.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-28T09:29:12.904783+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7212131,
            "author": "Jeffrey Cheng",
            "project_title": "Special Participation A: Mistral AI's Le Chat on HW3",
            "post_body": "Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\n\nHere is the annotated log:\n\nExecutive Summary:\n\nFrom my observation, Le Chat was able to answer most written questions correctly on one shot. However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking. In particular, it could reference a table on a different page or a formula in a different section. Doing some prompt engineering helps the model to reference the correct table/figure.\n\nIn addition, for questions that involve numerical counting, it could mistake the computation by a small margin, even after engineering the prompt. For instance, it could count something twice and mess up with the calculation. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is the online link: <link href=\"https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\">https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d</link></paragraph><paragraph>Here is the annotated log:</paragraph><file url=\"https://static.us.edusercontent.com/files/UjxEQxQUHDdCDMAgY62e9YSr\" filename=\"Annotated log of conversation with Mistral AI.pdf\"/><paragraph><bold>Executive Summary:</bold></paragraph><paragraph>From my observation, Le Chat was able to answer most written questions correctly on one shot. However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking. In particular, it could reference a table on a different page or a formula in a different section. Doing some prompt engineering helps the model to reference the correct table/figure.</paragraph><paragraph>In addition, for questions that involve numerical counting, it could mistake the computation by a small margin, even after engineering the prompt. For instance, it could count something twice and mess up with the calculation. </paragraph></document>",
            "links": [
                "https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d"
            ],
            "attachments": [],
            "created_at": "2025-10-28T09:17:55.632059+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7202422,
            "author": "Jerry Xiao",
            "project_title": "Special Participation A: Deepseek on HW5",
            "post_body": "For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files. The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems. There are some traits of the Deepseek AI:\n\n1. Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer.\n2. Ambiguous question prompts will lead to longer thinking time.\n3. Deepseek is better at explanation than real calculation. My recommendations for using Deepseek as a learning tool to make it help with conceptual understanding.\n\nThis is observed from some of the facts:\n\n1. Of the 11 questions, Deepseek gets 9 of them correct first shot, while one of the question relies on vision input and the other is a calculation problem. It gets the calculation problem correct after another prompt of asking itself to self examine the calculation process. \n2. I try to ask Deepseek examines its own mistake after I finish all the problems and it still can directly and precisely locate where might go wrong, which demonstrate its capable long context understanding capability.\n3. For problem 4, Deepseek not only provides very clear demonstration on how to derive the answer correctly and can show the direct relation between batchnorm and dropout by directly deriving the scaling factor. \n\n\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/r7sceYqPygJpgizFNzKZ3MLp\" filename=\"report.pdf\"/><paragraph>For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files. The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems. There are some traits of the Deepseek AI:<break/><break/>1. Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer.<break/>2. Ambiguous question prompts will lead to longer thinking time.<break/>3. Deepseek is better at explanation than real calculation. My recommendations for using Deepseek as a learning tool to make it help with conceptual understanding.</paragraph><paragraph>This is observed from some of the facts:<break/><break/>1. Of the 11 questions, Deepseek gets 9 of them correct first shot, while one of the question relies on vision input and the other is a calculation problem. It gets the calculation problem correct after another prompt of asking itself to self examine the calculation process. <break/>2. I try to ask Deepseek examines its own mistake after I finish all the problems and it still can directly and precisely locate where might go wrong, which demonstrate its capable long context understanding capability.<break/>3. For problem 4, Deepseek not only provides very clear demonstration on how to derive the answer correctly and can show the direct relation between batchnorm and dropout by directly deriving the scaling factor. </paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-26T16:13:16.614618+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7199845,
            "author": "Hong Joey",
            "project_title": "HW8 Q4: Ridge Attention",
            "post_body": "Problem Context: This is a former exam problem. It is meant to setup transformers by connecting attention to classical learning approaches, specifically ridge regression. The problem is a conceptual bridge between recurrent models and transformers. Though the problem references transformers, you should be able to solve it with what you have learned thus far. This should both help further demystify why transformers are able to do in-context learning as well as help you understand why recurrent models can have the same capability. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is a former exam problem. It is meant to setup transformers by connecting attention to classical learning approaches, specifically ridge regression. The problem is a conceptual bridge between recurrent models and transformers. Though the problem references transformers, you should be able to solve it with what you have learned thus far. This should both help further demystify why transformers are able to do in-context learning as well as help you understand why recurrent models can have the same capability. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/WCaQfIM7e4U4uEHcdB7oVvTU\" width=\"658\" height=\"753.2659932659932\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/VadJylV2GkEEUd7fIEdIbcAs\" width=\"658\" height=\"788.7564102564103\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-26T05:51:46.466398+11:00",
            "category": "Admin"
        },
        {
            "guid": 7199813,
            "author": "Hong Joey",
            "project_title": "HW8 Q3: Self-Supervised Linear Purification",
            "post_body": "Problem Context: This is an old exam problem showing another potential use case for autoencoders. In this problem, even when the representations are not lower-dimension, they can still be useful for feature purification.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This is an old exam problem showing another potential use case for autoencoders. In this problem, even when the representations are not lower-dimension, they can still be useful for feature purification.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/t4GyT37SGdYVf1tYpdXiYOaC\" width=\"658\" height=\"277.54502369668245\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/elKKyojDFSEiBOR2RMgIX9Zu\" width=\"658\" height=\"891.0416666666667\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/bjOdbSTBl9XleAtbslx9abI6\" width=\"658\" height=\"118.1619718309859\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-26T05:45:06.241656+11:00",
            "category": "Admin"
        },
        {
            "guid": 7199801,
            "author": "Hong Joey",
            "project_title": "HW8 Q2: Coding SSM Forward",
            "post_body": "Problem Context: The goal of this problem is to gain practical experience with how modern SSMs are implemented. You will implement both recurrence and convolution-based forward passes to analyze runtime efficiency. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: The goal of this problem is to gain practical experience with how modern SSMs are implemented. You will implement both recurrence and convolution-based forward passes to analyze runtime efficiency. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/m7eODymzQHbyITRzufcYNRKl\" width=\"658.0000000000001\" height=\"570.059748427673\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-26T05:42:04.950311+11:00",
            "category": "Admin"
        },
        {
            "guid": 7199791,
            "author": "Hong Joey",
            "project_title": "HW8 Q1: SSM Convolution Kernel",
            "post_body": "Problem Context: This problem explores the theoretical foundation of State-Space Models (SSMs) by connecting their recurrent update form to an equivalent convolution kernel representation. By doing so, you should be able to see why SSMs are efficient to parallelize.  Parts of this problem will also be engaged with in discussion.",
            "content_xml": "<document version=\"2.0\"><paragraph>Problem Context: This problem explores the theoretical foundation of State-Space Models (SSMs) by connecting their recurrent update form to an equivalent convolution kernel representation. By doing so, you should be able to see why SSMs are efficient to parallelize.  Parts of this problem will also be engaged with in discussion.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kqnS41i47QQAMebBQIdcZdKW\" width=\"658\" height=\"726.7769110764431\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-26T05:40:30.510897+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7196339,
            "author": "Hong Joey",
            "project_title": "Discussion 8 Solutions",
            "post_body": "Attached are questions and solutions to Discussion 8:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are questions and solutions to Discussion 8:</paragraph><file url=\"https://static.us.edusercontent.com/files/ZehcayfyArHXC5ZAdqkAXFAb\" filename=\"dis08_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/NGbehdH0YcruNpq3VWmx57te\" filename=\"dis08_solution.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-25T08:26:51.079494+11:00",
            "category": "Sections"
        },
        {
            "guid": 7191116,
            "author": "Noah Lund Syrdal",
            "project_title": "Special Participation C: Refactoring of HW3 task 2",
            "post_body": "Executive summary:\nRefactored the original notebook into a clean, modular codebase. Split into models.py, optimizers.py, and training.py, each with docstrings, validation, and reproducibility features. Preserved all \u03bcP and optimizer experiments while improving readability, maintainability, and adherence to PEP 8/257 and PyTorch best practices.\n\nGithub repo: https://github.com/NoahLundSyrdal/SpecialParticipationC \n\nReport\n\nDone by Noah Lund Syrdal(3041928386), Anders Vestrum(3041972833) and Srikar Babu (3041813297)",
            "content_xml": "<document version=\"2.0\"><paragraph>Executive summary:<break/>Refactored the original notebook into a clean, modular codebase. Split into <code>models.py</code>, <code>optimizers.py</code>, and <code>training.py</code>, each with docstrings, validation, and reproducibility features. Preserved all \u03bcP and optimizer experiments while improving readability, maintainability, and adherence to PEP 8/257 and PyTorch best practices.</paragraph><paragraph>Github repo: <link href=\"https://github.com/NoahLundSyrdal/SpecialParticipationC\">https://github.com/NoahLundSyrdal/SpecialParticipationC</link> </paragraph><paragraph>Report</paragraph><file url=\"https://static.us.edusercontent.com/files/8WiMHRBTeP9yhAjWQjbBrMea\" filename=\"Special_Participation_C__HW3___2-5.pdf\"/><paragraph>Done by Noah Lund Syrdal(3041928386), Anders Vestrum(3041972833) and Srikar Babu (3041813297)</paragraph></document>",
            "links": [
                "https://github.com/NoahLundSyrdal/SpecialParticipationC"
            ],
            "attachments": [],
            "created_at": "2025-10-24T09:51:14.765194+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7180201,
            "author": "Anant Sahai",
            "project_title": "Lecture 15: Autoencoder and Self-Supervision Part",
            "post_body": "This thread is to discuss the lecture part on self-supervision. To keep things clear, the second set of writing building towards state-space models will be kept distinct. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/qI8y8TXguhynXBv9IjNhmuS3\" filename=\"Lecture 15Sahai.pdf\"/><paragraph>This thread is to discuss the lecture part on self-supervision. To keep things clear, the second set of writing building towards state-space models will be kept distinct. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-22T17:30:26.938884+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7178115,
            "author": "Leon Kornfeld",
            "project_title": "Participation B: Grok HW 6",
            "post_body": "I used Grok to solve problems 5 and 6 of HW6 (coding).\n\nNote: Because problem 1 wasn\u2019t implementation-based and was about running cells and reading off values generated from GPU data, I figured it was unnecessary \u2014 or odd \u2014 to solve it with Grok.\n\nProblem 5:\n\nReflection:\nTo have Grok solve the problems associated with this notebook, I first gave it an overall prompt:\n\n\u201cAs I provide you sequential code blocks, solve all the TODOs without changing the code skeleton.\u201d\n\nAfter that, I started providing context for each question (the markdown cells in the notebook detailing the instructions) and then the actual skeleton code where it needed to fill things in. Grok then output the completed code, which I copied into the notebook and ran. I repeated this process throughout the notebook. With each response, Grok provided its reasoning, which can be seen in the transcript above.\n\nFast-forwarding, the first bug I encountered was when trying to instantiate the full gnn_model. The variable hidden_dim was passed in as a list, while Grok had coded it as an integer. This naturally led to a type error, since the code Grok wrote treated the list as an integer.\n\nMy next prompt to Grok was incorrect: I suggested just changing the list to an integer to avoid modifying the skeleton code. Grok acknowledged that while this was possible, it wasn\u2019t the best solution because it went against the intended design. Instead, it suggested that a better approach was to fix the skeleton code to treat hidden_dim as a list. After one \u201cindex out of range\u201d error and one more prompt to Grok, the code ran successfully.\n\nThe remainder of the problem went smoothly. For Q9 \u2014 the conceptual question on accuracy \u2014 Grok provided a multitude of possible explanations for the discrepancy. Because I hadn\u2019t provided the graph in the prompt, all of Grok\u2019s reasoning was speculative based on the limited context. My prompt said:\n\n\u201cAfter training the model, we get 100 percent accuracy on the test data. However, we see two samples that are misclassified.\u201d\n\nThis didn\u2019t specify that we were looking at all the samples (including the training data) or analyzing a graph. Without that information, Grok didn\u2019t get the correct answer. When I clarified that we were looking at the entire dataset, Grok correctly deduced that the misclassifications came from the training set.\n\nProblem 6:\n\nReflection:\n\nI used the exact same prompting approach that I used for question 5.\n\nGrok originally assumed that the X matrix in the Newton\u2013Schulz calculation was square. Therefore, the original code it produced used (X @ X @ X) instead of (X @ X.T @ X.T). When later code failed due to dimension errors, I had to prompt Grok that X was not necessarily square. It then gave a more complex solution using a Gram matrix. I further prompted it to just use X, and it responded with the correct answer.\n\nOnce Grok finished the coding questions, I started prompting it with the conceptual ones. I asked,\n\n\u201cWhich optimizer performed best between Muon, SGD, and AdamW?\u201d\n\nI also provided a screenshot of the graphs. It gave the correct answer, though I doubted whether it actually analyzed the graph. I tested this by asking graph-specific questions to see if it could interpret the image. Grok correctly identified that the training ran for 5 epochs but was slightly off on the lowest loss (saying 0.8 instead of 0.6).\n\nAfter a few more partially accurate answers, I decided to provide the numerical training data instead of a screenshot. With that, Grok did a much better job answering the conceptual questions.\n\nInterestingly, all the questions except the last one required little reasoning. The final conceptual question \u2014 whether orthogonalization or muP mattered more \u2014 took Grok about two minutes of processing, during which it visited around 15 webpages. It eventually answered the question correctly.\n\nConclusion:\n\nGrok does a pretty good job at zero-shotting the coding problems. It struggles when the user prompt lacks specific information (e.g. matrix shape, what image/graph we are analyzing) and thus makes generalizations that could lead to the wrong answer/code bugs. However, once more context was provided, Grok was able to correct itself and arrive at the right answer.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>I used Grok to solve problems 5 and 6 of HW6 (coding).</bold></paragraph><paragraph><bold>Note:</bold> Because problem 1 wasn\u2019t implementation-based and was about running cells and reading off values generated from GPU data, I figured it was unnecessary \u2014 or odd \u2014 to solve it with Grok.</paragraph><paragraph>Problem 5:</paragraph><file url=\"https://static.us.edusercontent.com/files/hZ5veIUZxIlXBfMh8bTtzoo3\" filename=\"GCN Karate Club Node Classification - Grok.pdf\"/><paragraph><bold>Reflection:<break/></bold>To have Grok solve the problems associated with this notebook, I first gave it an overall prompt:</paragraph><blockquote>\u201cAs I provide you sequential code blocks, solve all the TODOs without changing the code skeleton.\u201d</blockquote><paragraph>After that, I started providing context for each question (the markdown cells in the notebook detailing the instructions) and then the actual skeleton code where it needed to fill things in. Grok then output the completed code, which I copied into the notebook and ran. I repeated this process throughout the notebook. With each response, Grok provided its reasoning, which can be seen in the transcript above.</paragraph><paragraph>Fast-forwarding, the first bug I encountered was when trying to instantiate the full <code>gnn_model</code>. The variable <code>hidden_dim</code> was passed in as a list, while Grok had coded it as an integer. This naturally led to a type error, since the code Grok wrote treated the list as an integer.</paragraph><paragraph>My next prompt to Grok was incorrect: I suggested just changing the list to an integer to avoid modifying the skeleton code. Grok acknowledged that while this was possible, it wasn\u2019t the best solution because it went against the intended design. Instead, it suggested that a better approach was to fix the skeleton code to treat <code>hidden_dim</code> as a list. After one \u201cindex out of range\u201d error and one more prompt to Grok, the code ran successfully.</paragraph><paragraph>The remainder of the problem went smoothly. For Q9 \u2014 the conceptual question on accuracy \u2014 Grok provided a multitude of possible explanations for the discrepancy. Because I hadn\u2019t provided the graph in the prompt, all of Grok\u2019s reasoning was speculative based on the limited context. My prompt said:</paragraph><blockquote>\u201cAfter training the model, we get 100 percent accuracy on the test data. However, we see two samples that are misclassified.\u201d</blockquote><paragraph>This didn\u2019t specify that we were looking at all the samples (including the training data) or analyzing a graph. Without that information, Grok didn\u2019t get the correct answer. When I clarified that we were looking at the entire dataset, Grok correctly deduced that the misclassifications came from the training set.</paragraph><paragraph><bold>Problem 6:</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/27hqn25r2cBL3J1BQcqdxLzo\" filename=\"Newton-Schulz Matrix Orthogonalization Implementation - Grok.pdf\"/><paragraph><bold>Reflection:</bold></paragraph><paragraph>I used the exact same prompting approach that I used for question 5.</paragraph><paragraph>Grok originally assumed that the <code>X</code> matrix in the Newton\u2013Schulz calculation was square. Therefore, the original code it produced used <code>(X @ X @ X)</code> instead of <code>(X @ X.T @ X.T)</code>. When later code failed due to dimension errors, I had to prompt Grok that <code>X</code> was not necessarily square. It then gave a more complex solution using a Gram matrix. I further prompted it to just use <code>X</code>, and it responded with the correct answer.</paragraph><paragraph>Once Grok finished the coding questions, I started prompting it with the conceptual ones. I asked,</paragraph><blockquote>\u201cWhich optimizer performed best between Muon, SGD, and AdamW?\u201d</blockquote><paragraph>I also provided a screenshot of the graphs. It gave the correct answer, though I doubted whether it actually analyzed the graph. I tested this by asking graph-specific questions to see if it could interpret the image. Grok correctly identified that the training ran for 5 epochs but was slightly off on the lowest loss (saying 0.8 instead of 0.6).</paragraph><paragraph>After a few more partially accurate answers, I decided to provide the numerical training data instead of a screenshot. With that, Grok did a much better job answering the conceptual questions.</paragraph><paragraph>Interestingly, all the questions except the last one required little reasoning. The final conceptual question \u2014 whether orthogonalization or muP mattered more \u2014 took Grok about two minutes of processing, during which it visited around 15 webpages. It eventually answered the question correctly.</paragraph><paragraph><bold>Conclusion:</bold></paragraph><paragraph>Grok does a pretty good job at zero-shotting the coding problems. It struggles when the user prompt lacks specific information (e.g. matrix shape, what image/graph we are analyzing) and thus makes generalizations that could lead to the wrong answer/code bugs. However, once more context was provided, Grok was able to correct itself and arrive at the right answer.</paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-22T11:12:47.760104+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7177111,
            "author": "Sultan Daniels",
            "project_title": "Homework 6 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/H7QiXdPgh1jg1s4QevBTLFsQ\" filename=\"hw06codesolution.zip\"/><file url=\"https://static.us.edusercontent.com/files/J681dJ4KORFWAUMSucpC79sY\" filename=\"hw06_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/LMNEmlsr4cX21i0HdZsVObQf\" filename=\"hw06_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-22T08:59:39.280008+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7166398,
            "author": "Lance Mathias",
            "project_title": "HW7 Q1: Implementing RNNs (Coding)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/EgY4PmGz84l65IdfaPjzvYzW\" width=\"568\" height=\"513\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/8Eca13PVHvdrNRSbuWLCY86m\" width=\"566\" height=\"751.4137931034484\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/Mq0a2O2tGg3iHKBbr5RwiIaE\" width=\"566\" height=\"521.8387516254877\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-21T05:49:19.852257+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7166397,
            "author": "Lance Mathias",
            "project_title": "HW7 Q2: RNNs for Last Name Classification (Coding)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/xbvkhtcq0tX7Quftev6rbnUt\" width=\"581\" height=\"137.90272373540856\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-21T05:49:17.633312+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7166396,
            "author": "Lance Mathias",
            "project_title": "HW7 Q3: Auto-encoder: Learning without Labels",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/c5EdUpX5ccLSEAbJ4aqmjitc\" width=\"581\" height=\"83.42564102564103\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/ytx501ettcfSxxxEgmSW3YCF\" width=\"581\" height=\"781.1308411214953\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/5U0l0jqjbDQQtmQCQN3uOIWL\" width=\"586\" height=\"92\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-21T05:49:13.820227+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7166394,
            "author": "Lance Mathias",
            "project_title": "HW7 Q4: Read a Blog Post: How to train your Resnet",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/LMokhUCcwp7zsTLE2RhzUUI9\" width=\"581\" height=\"288.63063063063066\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-21T05:48:59.655356+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7166393,
            "author": "Lance Mathias",
            "project_title": "HW7 Q5: The power of the graph perspective in clustering (Coding)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/6F36JK9GiQ3h4ygTys7srlXa\" width=\"581\" height=\"357.8245838668374\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/b79ePjGSRpBR9fh0oBs0Hf1M\" width=\"566\" height=\"525.2539267015707\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-21T05:48:57.069018+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7162279,
            "author": "Tianyu Gu",
            "project_title": "Special Participation A: Grok on HW0",
            "post_body": "For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2\u20135). The performance of Grok really impressed me, almost all questions are one-shot correct except for 5(b)(iii).\n\nSummary: Grok demonstrated reliable analytical reasoning for most problems, with clear and accurate derivations for problems 2, 3, and 4. The answer of them are all fully correct. However, the error on problem 5(b)(iii) suggests its sensitivity to complex or ambiguous problem structures. \n\nHere is the link https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737 ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/hNHXrBnPiBe37nccrn8vAbZ4\" filename=\"Special participation A.pdf\"/><paragraph>For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2\u20135). The performance of Grok really impressed me, almost all questions are one-shot correct except for 5(b)(iii).</paragraph><paragraph><bold>Summary:</bold> Grok demonstrated reliable analytical reasoning for most problems, with clear and accurate derivations for problems 2, 3, and 4. The answer of them are all fully correct. However, the error on problem 5(b)(iii) suggests its sensitivity to complex or ambiguous problem structures. </paragraph><paragraph>Here is the link <link href=\"https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737\">https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737</link> </paragraph></document>",
            "links": [
                "https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737"
            ],
            "attachments": [],
            "created_at": "2025-10-20T18:33:07.202349+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7159164,
            "author": "Yuxiang Liu",
            "project_title": "Special Participation E: A Comprehensive Tutorial for Literature Review",
            "post_body": "Before we get started with the literature review of the final project, I think it would be worthwhile to spend 5 minutes studying about some sensible ways of applying LLMs for the purpose of literature searching. The link below is one of my personal blogs which concludes a recipe of useful tricks that will help you coorporate with LLMs more smoothly and efficiently when doing deep research :D This blog includes advice from some experienced researchers as well as my personal suggestions. If you have any other valuable insights, feel free to leave a comment here!\n\nhttps://xiang-foothill.github.io/personal-website/blog/make-ai-your-best-assistant.html",
            "content_xml": "<document version=\"2.0\"><paragraph>Before we get started with the literature review of the final project, I think it would be worthwhile to spend 5 minutes studying about some sensible ways of applying LLMs for the purpose of literature searching. The link below is one of my personal blogs which concludes a recipe of useful tricks that will help you coorporate with LLMs more smoothly and efficiently when doing deep research :D This blog includes advice from some experienced researchers as well as my personal suggestions. If you have any other valuable insights, feel free to leave a comment here!</paragraph><paragraph>https://xiang-foothill.github.io/personal-website/blog/make-ai-your-best-assistant.html</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-20T08:36:06.64917+11:00",
            "category": "Admin"
        },
        {
            "guid": 7151370,
            "author": "Noah Lund Syrdal",
            "project_title": "Special Participation A: GPT-Oss on HW5",
            "post_body": "\n\nFor this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1\u2013Q4).\nThe model was tested on symbolic derivations and conceptual reasoning without code execution.\n\nAccuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)\n\nMain errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusion\n\nNo hallucinations: all final answers matched the official HW5 solution key\n\nTakeaway: once prompts were clearly formatted in LaTeX, the model produced correct and interpretable derivations, showing reliable analytical reasoning but high sensitivity to input structure.\n\nBit slow(90-180 seconds)\n\n(Logs and annotations are attached in the full PDF report.)",
            "content_xml": "<document version=\"2.0\"><paragraph/><file url=\"https://static.us.edusercontent.com/files/36Ibqekm0cq7clyfZo5xJMtP\" filename=\"SpecialParticipationA-GPTOssHW5.pdf\"/><paragraph>For this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1\u2013Q4).<break/>The model was tested on symbolic derivations and conceptual reasoning without code execution.</paragraph><list style=\"unordered\"><list-item><paragraph>Accuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)</paragraph></list-item><list-item><paragraph>Main errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusion</paragraph></list-item><list-item><paragraph>No hallucinations: all final answers matched the official HW5 solution key</paragraph></list-item><list-item><paragraph>Takeaway: once prompts were clearly formatted in LaTeX, the model produced correct and interpretable derivations, showing reliable analytical reasoning but high sensitivity to input structure.</paragraph></list-item><list-item><paragraph>Bit slow(90-180 seconds)</paragraph></list-item></list><paragraph>(Logs and annotations are attached in the full PDF report.)</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-18T11:31:49.638758+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7148413,
            "author": "Anders Vestrum",
            "project_title": "Special Participation A: Grok on HW5",
            "post_body": "This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional networks, batch normalization, dropout, depthwise separable convolutions, and regularization. The report includes:\n\nAn overview of Grok\u2019s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve.\n\nA reflection on Grok\u2019s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps could have been emphasized more clearly.\n\nMy recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding.\n\nFull transcripts of my Q&A exchanges with Grok, including its solutions, reasoning, and my reflections after each subproblem. \n\nOverall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness.",
            "content_xml": "<document version=\"2.0\"><paragraph>This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional networks, batch normalization, dropout, depthwise separable convolutions, and regularization. The report includes:</paragraph><list style=\"unordered\"><list-item><paragraph>An overview of Grok\u2019s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve.</paragraph></list-item><list-item><paragraph>A reflection on Grok\u2019s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps could have been emphasized more clearly.</paragraph></list-item><list-item><paragraph>My recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding.</paragraph></list-item><list-item><paragraph>Full transcripts of my Q&amp;A exchanges with Grok, including its solutions, reasoning, and my reflections after each subproblem. </paragraph></list-item></list><paragraph>Overall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness.</paragraph><file url=\"https://static.us.edusercontent.com/files/Zag0csJASJ5J7AvKA2n8mUmb\" filename=\"special_participation_A.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-18T02:48:56.469848+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7147361,
            "author": "Anant Sahai",
            "project_title": "Lecture 14",
            "post_body": "This is for discussing basic RNNs and the start of self-supervision. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/xNGOdEwaHMuPn90WywUlLrJI\" filename=\"Lecture 14.pdf\"/><paragraph>This is for discussing basic RNNs and the start of self-supervision. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-17T17:06:38.45759+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7146912,
            "author": "Noah Lund Syrdal",
            "project_title": "Special Participation B: Deepseek on problem 2 HW3",
            "post_body": "Deepseek conv: https://chat.deepseek.com/share/b9xx6hqmk42jzod0uu \n\nTranscript with comments: https://drive.google.com/file/d/1MWOQsiybYJ3Tgw99TATzzucOYQ1j57MV/view?usp=sharing \n\nFor this special participation, I used DeepSeek to solve the Maximal Update Parameterization (\u03bcP) problem in Homework 3, evaluating its ability to reason about \u03bcP\u2019s theory and implement its optimizer and network-scaling components correctly.\n\nDeepSeek performed strongly across Parts B\u2013E, showing solid mathematical reasoning and clean, well-structured code, though occasionally verbose.\n\nIn Part B, it identified the RMS\u2192RMS induced norm as the key to explaining activation deltas and derived the correct relation\n||W||_(RMS\u2192RMS) = \u03c3_max(W) * sqrt(n/m).\nIts explanation of why Frobenius and spectral norms fail to capture functional effects was clear and technically precise\u2014one of its best moments.\n\nIn Part C, DeepSeek implemented the SimpleAdamMuP optimizer with proper per-layer scaling (\u221a(n_out / n_in)) and justified input/output layer \u201cfudge\u201d factors, accurately linking them to \u03bcP\u2019s goal of uniform activation updates.\n\nPart D was the highlight, but also where its main weakness appeared. DeepSeek initially applied the wrong scaling (\u221a(n_in / n_out)) and only realized the mistake after I asked why it had chosen that direction. It first tried to justify the incorrect reasoning before re-deriving and correcting itself to \u221a(n_out / n_in). While it eventually reached the right conclusion and explained it well, this showed that it didn\u2019t truly verify its own logic until prompted. Interestingly, I also noticed that it sometimes produced very smart intermediate reasoning during its \u201cthinking\u201d process but then left those insights out of its final answer.\n\nIn Part E, it correctly demonstrated \u03bcP\u2019s hyperparameter-transfer property\u2014that a single global learning rate (\u22481\u20133) generalizes across widths 4\u2013256\u2014capturing \u03bcP\u2019s essence of \u201ctune once, scale everywhere.\u201d\n\nOverall, DeepSeek showed strong theoretical grasp, clean implementation, and genuine reflective reasoning. However, I found ChatGPT generally does a better job at maintaining coherence, verifying its logic, and integrating its best reasoning directly into the final answers. DeepSeek was technically solid but less self-consistent, making it feel more like an intelligent assistant that sometimes \u201cknows\u201d the right thing but doesn\u2019t always say it.",
            "content_xml": "<document version=\"2.0\"><paragraph>Deepseek conv: <link href=\"https://chat.deepseek.com/share/b9xx6hqmk42jzod0uu\"><underline>https://chat.deepseek.com/share/b9xx6hqmk42jzod0uu</underline></link> </paragraph><paragraph>Transcript with comments: <link href=\"https://drive.google.com/file/d/1MWOQsiybYJ3Tgw99TATzzucOYQ1j57MV/view?usp=sharing\"><underline>https://drive.google.com/file/d/1MWOQsiybYJ3Tgw99TATzzucOYQ1j57MV/view?usp=sharing</underline></link> </paragraph><paragraph>For this special participation, I used DeepSeek to solve the Maximal Update Parameterization (\u03bcP) problem in Homework 3, evaluating its ability to reason about \u03bcP\u2019s theory and implement its optimizer and network-scaling components correctly.</paragraph><paragraph>DeepSeek performed strongly across Parts B\u2013E, showing solid mathematical reasoning and clean, well-structured code, though occasionally verbose.</paragraph><paragraph>In Part B, it identified the RMS\u2192RMS induced norm as the key to explaining activation deltas and derived the correct relation<break/>||W||_(RMS\u2192RMS) = \u03c3_max(W) * sqrt(n/m).<break/>Its explanation of why Frobenius and spectral norms fail to capture functional effects was clear and technically precise\u2014one of its best moments.</paragraph><paragraph>In Part C, DeepSeek implemented the SimpleAdamMuP optimizer with proper per-layer scaling (\u221a(n_out / n_in)) and justified input/output layer \u201cfudge\u201d factors, accurately linking them to \u03bcP\u2019s goal of uniform activation updates.</paragraph><paragraph>Part D was the highlight, but also where its main weakness appeared. DeepSeek initially applied the wrong scaling (\u221a(n_in / n_out)) and only realized the mistake after I asked why it had chosen that direction. It first tried to justify the incorrect reasoning before re-deriving and correcting itself to \u221a(n_out / n_in). While it eventually reached the right conclusion and explained it well, this showed that it didn\u2019t truly verify its own logic until prompted. Interestingly, I also noticed that it sometimes produced very smart intermediate reasoning during its \u201cthinking\u201d process but then left those insights out of its final answer.</paragraph><paragraph>In Part E, it correctly demonstrated \u03bcP\u2019s hyperparameter-transfer property\u2014that a single global learning rate (\u22481\u20133) generalizes across widths 4\u2013256\u2014capturing \u03bcP\u2019s essence of \u201ctune once, scale everywhere.\u201d</paragraph><paragraph>Overall, DeepSeek showed strong theoretical grasp, clean implementation, and genuine reflective reasoning. However, I found ChatGPT generally does a better job at maintaining coherence, verifying its logic, and integrating its best reasoning directly into the final answers. DeepSeek was technically solid but less self-consistent, making it feel more like an intelligent assistant that sometimes \u201cknows\u201d the right thing but doesn\u2019t always say it.</paragraph></document>",
            "links": [
                "https://chat.deepseek.com/share/b9xx6hqmk42jzod0uu",
                "https://drive.google.com/file/d/1MWOQsiybYJ3Tgw99TATzzucOYQ1j57MV/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-10-17T15:02:58.680275+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7144449,
            "author": "Hong Joey",
            "project_title": "HW5 Solutions",
            "post_body": "Attached are solutions to HW5. Feel free to ask questions about them in this thread:",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are solutions to HW5. Feel free to ask questions about them in this thread:</paragraph><file url=\"https://static.us.edusercontent.com/files/xHf1THUKAJkdneDCDIsqFDML\" filename=\"hw05_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/GJGCf5elX1QPeUPHDGUO9zoO\" filename=\"q_coding_dropout_sol.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/thE7EzWQsa6FruyPKMT5Lvqz\" filename=\"q_coding_bn_drop_cnn_sol.zip\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-17T07:37:54.495293+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7144277,
            "author": "Sultan Daniels",
            "project_title": "Discussion 7 Solutions",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/X7AtMPpLORIcQpeKpet5Ewnc\" filename=\"dis07_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/LqaBjFudCYIwb23cQ09P8Ajx\" filename=\"dis07_question.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-17T07:14:15.947513+11:00",
            "category": "Sections"
        },
        {
            "guid": 7139209,
            "author": "Anant Sahai",
            "project_title": "Lecture 13 Thread",
            "post_body": "Use this to ask questions about DiffPool and the general design principles we used to figure out how to do pooling in a mostly generic way for graphs. The PDF above has been slightly annotated to reflect discussions in office hours. \n\nWe also started on RNNs, but will be continuing that on Thursday's lecture. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/nZvjX9QR0fCSiCYGFPxLZdWQ\" filename=\"Lecture 13.pdf\"/><paragraph>Use this to ask questions about DiffPool and the general design principles we used to figure out how to do pooling in a mostly generic way for graphs. The PDF above has been slightly annotated to reflect discussions in office hours. </paragraph><paragraph>We also started on RNNs, but will be continuing that on Thursday's lecture. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-16T10:43:45.603854+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7134659,
            "author": "Bruno Vieira",
            "project_title": "Participation B Gemini HW5",
            "post_body": "The chat log with commentary is attached above with all the code Gemini suggested.\n\nReflection:\n\nFor Q5, Gemini performed surprisingly well \u2014 the problem itself was fairly straightforward, with most of the code already scaffolded in the notebook, so it didn\u2019t have to handle much structural uncertainty. Because I used Gemini inside Colab, it had full access to the notebook\u2019s context and previous cells, which definitely helped it \u201cone-shot\u201d nearly all the subparts correctly. The conceptual depth was solid. It provided well-reasoned explanations of dropout and different variants of gradient descent, and it even interpreted the loss graphs in part D accurately. Where Gemini did particularly well was in connecting the observed training behavior to the theoretical effects of dropout like reducing over-reliance on specific features like the \u201ccheating features\u201d, improving generalization, and stabilizing updates. However, its answers sometimes felt overly verbose and repetitive, often rephrasing the same idea multiple times without adding much substance. It\u2019s clear Gemini does well with low-complexity, well-defined prompts that rely on standard neural network concepts, but it still struggles to be concise or to offer creative insights beyond textbook explanations.\n\nQ6 was more complex, and Gemini\u2019s performance was mixed, especially since I had to switch from Colab (with contextual code cells) to the browser version, meaning it didn\u2019t have the same continuity since a lot of the coding was done on .py files instead on a notebook like Q5. For the batch normalization implementation, though, it was genuinely impressive where Gemini inferred missing details like using batch statistics for the forward pass without explicit instruction, and it correctly executed the full forward and backward propagation steps. It\u2019s unclear whether it derived the math itself or simply recalled a learned pattern, but either way, it achieved the correct result. However, when asked to modify the fully connected network to include dropout, it completely failed where it essentially reprinted the same model without incorporating dropout at all, as if it didn\u2019t register the change request. This suggests Gemini\u2019s weakness in applying structural modifications to existing code, especially when they require integrating a new mechanism into a preexisting design. It seems more comfortable regenerating code from scratch than adapting it thoughtfully.\n\nFor the spatial batch normalization section, Gemini\u2019s inline import was technically correct but unnecessary where a habit that made the code less clean and showed an overreliance on re-importing modules. The implementation itself worked, but the approach was not the smoothest and not very well thought before it gave me the code in the chat. Similarly, in the CNN design question, the model it proposed was overly shallow and minimalistic for CIFAR-scale data. While Gemini was very receptive to feedback and quickly acknowledged that a deeper network would be better, it didn\u2019t independently propose parameter or structural improvements, indicating limited architectural creativity. I suggested it use another convolution layer to pick up more information and it followed instructions well but didn\u2019t demonstrate the capacity to think beyond them.\n\nOverall, Gemini was accurate, stable, and efficient for standard or moderately complex neural network tasks, especially those that map directly to known architectures or formulas. It showed good conceptual understanding and a strong ability to produce syntactically correct code on the first try. However, it lacked flexibility and creative reasoning when the task demanded architectural innovation or modification of existing code. It\u2019s great at execution and explanation, but not yet capable of adapting in nuanced, design-heavy tasks where intuition and experimentation matter most.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/UkfG27zoh384GEFS32UqNoB4\" filename=\"CS_182_Participation_B_HW5_Gemini.pdf\"/><paragraph>The chat log with commentary is attached above with all the code Gemini suggested.</paragraph><paragraph><bold>Reflection:</bold></paragraph><paragraph>For Q5, Gemini performed surprisingly well \u2014 the problem itself was fairly straightforward, with most of the code already scaffolded in the notebook, so it didn\u2019t have to handle much structural uncertainty. Because I used Gemini inside Colab, it had full access to the notebook\u2019s context and previous cells, which definitely helped it \u201cone-shot\u201d nearly all the subparts correctly. The conceptual depth was solid. It provided well-reasoned explanations of dropout and different variants of gradient descent, and it even interpreted the loss graphs in part D accurately. Where Gemini did particularly well was in connecting the observed training behavior to the theoretical effects of dropout like reducing over-reliance on specific features like the \u201ccheating features\u201d, improving generalization, and stabilizing updates. However, its answers sometimes felt overly verbose and repetitive, often rephrasing the same idea multiple times without adding much substance. It\u2019s clear Gemini does well with low-complexity, well-defined prompts that rely on standard neural network concepts, but it still struggles to be concise or to offer creative insights beyond textbook explanations.</paragraph><paragraph>Q6 was more complex, and Gemini\u2019s performance was mixed, especially since I had to switch from Colab (with contextual code cells) to the browser version, meaning it didn\u2019t have the same continuity since a lot of the coding was done on .py files instead on a notebook like Q5. For the batch normalization implementation, though, it was genuinely impressive where Gemini inferred missing details like using batch statistics for the forward pass without explicit instruction, and it correctly executed the full forward and backward propagation steps. It\u2019s unclear whether it derived the math itself or simply recalled a learned pattern, but either way, it achieved the correct result. However, when asked to modify the fully connected network to include dropout, it completely failed where it essentially reprinted the same model without incorporating dropout at all, as if it didn\u2019t register the change request. This suggests Gemini\u2019s weakness in applying structural modifications to existing code, especially when they require integrating a new mechanism into a preexisting design. It seems more comfortable regenerating code from scratch than adapting it thoughtfully.</paragraph><paragraph>For the spatial batch normalization section, Gemini\u2019s inline import was technically correct but unnecessary where a habit that made the code less clean and showed an overreliance on re-importing modules. The implementation itself worked, but the approach was not the smoothest and not very well thought before it gave me the code in the chat. Similarly, in the CNN design question, the model it proposed was overly shallow and minimalistic for CIFAR-scale data. While Gemini was very receptive to feedback and quickly acknowledged that a deeper network would be better, it didn\u2019t independently propose parameter or structural improvements, indicating limited architectural creativity. I suggested it use another convolution layer to pick up more information and it followed instructions well but didn\u2019t demonstrate the capacity to think beyond them.</paragraph><paragraph>Overall, Gemini was accurate, stable, and efficient for standard or moderately complex neural network tasks, especially those that map directly to known architectures or formulas. It showed good conceptual understanding and a strong ability to produce syntactically correct code on the first try. However, it lacked flexibility and creative reasoning when the task demanded architectural innovation or modification of existing code. It\u2019s great at execution and explanation, but not yet capable of adapting in nuanced, design-heavy tasks where intuition and experimentation matter most.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-15T16:42:20.35258+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7132324,
            "author": "Srikar Babu Gadipudi",
            "project_title": "Special Participation A: HW 4 using DeepSeek",
            "post_body": "Problem Context\n\nThere are 5 non-coding questions in this homework. Two questions on optimization (specifically Newton-Schultz iteration and MuP scaling) and three questions on CNNs. I took a special interest in how DeepSeek performs when solving these computationally heavy problems when prompted in different formats: purely text, purely images, a PDF and a hybrid of text and images. I also tested DeepSeek's ability to understand and retain multi-part instructions in between these different modalities. The idea was to evaluate DeepSeek's overall problem-solving performance while also attempting to understand which prompt type yielded the best results. \n\nIn this experiment:\n\nDeepSeek solved four out of five questions correctly in a single attempt.\n\nQuestion 2 was the only one it got wrong, though it could solve it with further prompting.\n\nQuestion 3 required a brief nudge from me to complete the final computation, likely because the context limit was reached mid-session.\n\nExecutive Summary\n\nFrom testing out different formats of the prompts, I observed the following:\n\nWhen only text was provided (Question 2), DeepSeek tended to lose context more easily. it sometimes failed to connect earlier parts of the questions or instructions.\n\nWith images or PDFs, it demonstrated significant better context retention and reasoning continuity. It appeared to process the questions more holistically, possibly due to the visual cues.\n\nA hybrid approach (text +images) also performed better. It was able to mix information from images and text efficiently to provide the correct solution (see Question 4).\n\nIn essence, DeepSeek performs impressively well on conceptual and numerical reasoning when the sufficient structured context is provided. Its performance, however, degraded slightly in purely textual prompts, indicating that context formatting plays a role in achieving accurate results.\n\nNote: This analysis is based on a limited sample of five questions, and is therefore not exhaustive or conclusive.\n\nHere is a link to the conversation I had with DeepSeek: https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b\n\nHere is the file containing the conversation, with question-wise comments (annotated in the PDF): ",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Problem Context</bold></paragraph><paragraph>There are 5 non-coding questions in this homework. Two questions on optimization (specifically Newton-Schultz iteration and MuP scaling) and three questions on CNNs. I took a special interest in how DeepSeek performs when solving these computationally heavy problems when <bold>prompted in different formats:</bold> purely text, purely images, a PDF and a hybrid of text and images. I also tested DeepSeek's ability to understand and retain multi-part instructions in between these different modalities. The idea was to evaluate DeepSeek's overall problem-solving performance while also attempting to understand which prompt type yielded the best results. </paragraph><paragraph>In this experiment:</paragraph><list style=\"bullet\"><list-item><paragraph>DeepSeek solved four out of five questions correctly in a single attempt.</paragraph></list-item><list-item><paragraph>Question 2 was the only one it got wrong, though it could solve it with further prompting.</paragraph></list-item><list-item><paragraph>Question 3 required a brief nudge from me to complete the final computation, likely because the context limit was reached mid-session.</paragraph></list-item></list><paragraph><bold>Executive Summary</bold></paragraph><paragraph>From testing out different formats of the prompts, I observed the following:</paragraph><list style=\"bullet\"><list-item><paragraph>When only text was provided (Question 2), DeepSeek tended to lose context more easily. it sometimes failed to connect earlier parts of the questions or instructions.</paragraph></list-item><list-item><paragraph>With images or PDFs, it demonstrated significant better context retention and reasoning continuity. It appeared to process the questions more holistically, possibly due to the visual cues.</paragraph></list-item><list-item><paragraph>A hybrid approach (text +images) also performed better. It was able to mix information from images and text efficiently to provide the correct solution (see Question 4).</paragraph></list-item></list><paragraph>In essence, DeepSeek performs impressively well on conceptual and numerical reasoning when the sufficient structured context is provided. Its performance, however, degraded slightly in purely textual prompts, indicating that context formatting plays a role in achieving accurate results.</paragraph><paragraph><bold>Note:</bold> This analysis is based on a limited sample of five questions, and is therefore not exhaustive or conclusive.</paragraph><paragraph>Here is a link to the conversation I had with DeepSeek: <link href=\"https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b\">https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b</link></paragraph><paragraph>Here is the file containing the conversation, with question-wise comments (annotated in the PDF): </paragraph><file url=\"https://static.us.edusercontent.com/files/o3Zt9CHGvmcfvWwfdgVQKVN1\" filename=\"participationA_HW4_DeepSeek.pdf\"/></document>",
            "links": [
                "https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b"
            ],
            "attachments": [],
            "created_at": "2025-10-15T10:18:16.802913+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7124383,
            "author": "Anders Vestrum",
            "project_title": "Special Participation B: Mistral on HW4",
            "post_body": "This report summarizes my work with Mistral on HW4 coding problems. Mistral initially struggled with strict coding constraints (Problem 5) but quickly adapted and produced correct minimal code. It handled dataset setup, training, and loader tasks effectively, and excelled in hyperparameter tuning through iterative feedback.\n\nIts written explanations showed strong conceptual understanding of CNNs, covering topics like inductive bias, domain shift, and model behavior on permuted images.\n\nKey Points\n\nMinor issues in Problem 5 (strict coding constraints), mostly smooth completion elsewhere.\n\nHyperparameter tuning and conceptual explanations were particularly strong.",
            "content_xml": "<document version=\"2.0\"><paragraph>This report summarizes my work with Mistral on HW4 coding problems. Mistral initially struggled with strict coding constraints (Problem 5) but quickly adapted and produced correct minimal code. It handled dataset setup, training, and loader tasks effectively, and excelled in hyperparameter tuning through iterative feedback.</paragraph><paragraph>Its written explanations showed strong conceptual understanding of CNNs, covering topics like inductive bias, domain shift, and model behavior on permuted images.</paragraph><paragraph>Key Points</paragraph><list style=\"bullet\"><list-item><paragraph>Minor issues in Problem 5 (strict coding constraints), mostly smooth completion elsewhere.</paragraph></list-item></list><list style=\"bullet\"><list-item><paragraph>Hyperparameter tuning and conceptual explanations were particularly strong.</paragraph></list-item></list><file url=\"https://static.us.edusercontent.com/files/0ovt32aAieyxJcE42yryMUSQ\" filename=\"special_participation_B.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-14T07:29:06.012043+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7116236,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q6: Implementing Muon (Coding Question)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/UqlocaiKiDfgeXKx3CZsWH57\" width=\"658\" height=\"776.9741697416974\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:50:19.135949+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7116232,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q5: Zachary\u2019s Karate Club (Coding)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/eRDymoeUpXy4ePS3bFyEp4Zq\" width=\"658.0000000000001\" height=\"697.803402646503\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/3J9NAknDQQUfI5P3cVVFHTaT\" width=\"643\" height=\"446.63473053892216\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:48:50.64531+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7116227,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q4: Exploring Deep Learning Tooling",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/0YU5ifNvyCbS6SlfjObvZEWX\" width=\"658\" height=\"414.527514231499\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:46:37.20113+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7116225,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q3: Graph Neural Networks",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/UknmM7SI2gQB2rpCvfQm5fbE\" width=\"658\" height=\"488.19354838709677\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/MMkQKlWVRF4AxpzU7GQlZJrE\" width=\"643\" height=\"774.1465346534653\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:45:28.027302+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7116221,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q2: Graph Dynamics and GNN Concepts",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/dN9xe6HyhNbq3WcavfLtnwyR\" width=\"658\" height=\"204.48615384615385\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/cuMWMvIPL3xbsxhigEEcsf2P\" width=\"658\" height=\"844.7297297297298\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/XziMeA2oRF6TUwDvU3429sgs\" width=\"643\" height=\"803.75\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/9d8WRXvJ1etOXEMM4g68fWJt\" width=\"643\" height=\"393.171974522293\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:43:42.238937+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7116214,
            "author": "Sultan Daniels",
            "project_title": "HW6 Q1: Memory considerations when using GPUs (Coding Question)",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/7zOoaaD9tOYQLwmnoRVVWCkf\" width=\"658\" height=\"494.042904290429\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-12T17:39:15.115165+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7111658,
            "author": "Iana Lin",
            "project_title": "Special Participation A - HW 3 ChatGPT 5",
            "post_body": "Executive Summary\n\nI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learning material, I was still impressed with its performance. The majority of questions were one-shotted correctly.\n\n1: Maximal Update Parameterization: \n\nSuccessfully one shotted, reasoning good.\n\n3: Maximal Update Parameterization Research:\n\nWhen interpreting figure 1, ChatGPT 5's explanation of the visual figure was actually more informative and explanatory than the official solutions were. It critically highlighted that \"each layer\u2019s updates remain of the same order regardless of width\"\n\nRather than using the RMS-to-RMS matrix norm that we covered in lecture (scaled version of spectral norm), it hallucinated the definition of the RMS matrix norm. It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \n\n$$\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}$$\n\nAs a result, it used this relation (that it claimed applies to random matrices)\n\n\n$$\u2225W_l\u2225_2\\approx\\frac{\u2225W_l\u2225_F}{\\sqrt{\\left\\{\\min(n_l,n_{l-1})\\right\\}}}$$\n\nto derive the incorrect order for the weight matrix and update's RMS norms.\n\nWhile it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for its derivation). This is not that surprising given that I fed in the \"A Spectral Condition for Feature Learning\" pdf at the beginning of the chat and ChatGPT produced a very similar derivation. Additionally, it was still able to correctly identified the key assumption that allowed for the derivation of the lower bound.\n\nAdditionally, after providing the definition of the induced norm, it was able to correctly derive the  order for the weight matrix and update's RMS norms.\n\n4: Policy Gradient and the Reparameterization Gradient Estimator:\n\n Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutions. \n\n5: Tensor Rematerialization:\nChatGPT 5 at first guessed 2 loadmems instead of 10 (thinking once a layer was loaded, it would stay in memory for the next layer). After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10.\n\nOther than that, it was able to provide sound reasoning and the correct answers for the rest of the problem\n\nOverall:\nChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an unnecessary amount), and seemed to be able to course-correct with a little bit of feedback. It seemed to falter more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>I used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learning material, I was still impressed with its performance. The majority of questions were one-shotted correctly.</paragraph><paragraph><underline>1: Maximal Update Parameterization</underline>: </paragraph><paragraph>Successfully one shotted, reasoning good.</paragraph><paragraph><underline>3: Maximal Update Parameterization Research</underline>:</paragraph><paragraph>When interpreting figure 1, ChatGPT 5's explanation of the visual figure was actually more informative and explanatory than the official solutions were. It critically highlighted that \"each layer\u2019s updates remain of the same order regardless of width\"</paragraph><paragraph>Rather than using the RMS-to-RMS matrix norm that we covered in lecture (scaled version of spectral norm), it hallucinated the definition of the RMS matrix norm. It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, </paragraph><math>\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}</math><paragraph>As a result, it used this relation (that it claimed applies to random matrices)<break/></paragraph><math>\u2225W_l\u2225_2\\approx\\frac{\u2225W_l\u2225_F}{\\sqrt{\\left\\{\\min(n_l,n_{l-1})\\right\\}}}</math><paragraph>to derive the incorrect order for the weight matrix and update's RMS norms.<break/><break/>While it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for its derivation). This is not that surprising given that I fed in the \"A Spectral Condition for Feature Learning\" pdf at the beginning of the chat and ChatGPT produced a very similar derivation. Additionally, it was still able to correctly identified the key assumption that allowed for the derivation of the lower bound.<break/><break/>Additionally, after providing the definition of the induced norm, it was able to correctly derive the  order for the weight matrix and update's RMS norms.</paragraph><paragraph><underline>4: Policy Gradient and the Reparameterization Gradient Estimator</underline>:</paragraph><paragraph> Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutions. </paragraph><paragraph><underline>5: Tensor Rematerialization</underline>:<break/>ChatGPT 5 at first guessed 2 loadmems instead of 10 (thinking once a layer was loaded, it would stay in memory for the next layer). After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10.<break/><break/>Other than that, it was able to provide sound reasoning and the correct answers for the rest of the problem</paragraph><paragraph><bold>Overall</bold>:<break/>ChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an unnecessary amount), and seemed to be able to course-correct with a little bit of feedback. It seemed to falter more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)</paragraph><file url=\"https://static.us.edusercontent.com/files/6R4M2iy4LV8vlsb0kHIiaROd\" filename=\"ParticipationA-HW3.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-11T14:07:05.580704+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7110289,
            "author": "Hong Joey",
            "project_title": "Discussion 6 Solutions",
            "post_body": "Attached are the questions and solutions for discussion 6:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are the questions and solutions for discussion 6:</paragraph><file url=\"https://static.us.edusercontent.com/files/zKSePwcxBLJgltcRBreTHHtv\" filename=\"dis06_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/muZvuV13Yt1iIGyNE3ObxedA\" filename=\"dis06_solution.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-11T08:52:16.473292+11:00",
            "category": "Sections"
        },
        {
            "guid": 7107719,
            "author": "Jordan Argel",
            "project_title": "Special Participation B: HW 4",
            "post_body": "Executive Summary\n\nI worked with Deepseek on coding questions 5 and 6 of HW 4. For the most part, Deepseek was able to tackle the coding questions with no problems, nearly one-shotting all of them. However, due to its lack of vision beyond texts, Deepseek is not really good at making visual observations. This gave Deepseek a little bit of trouble answering a couple of the written questions, as it had to rely on general knowledge it had on CNNs and MLPs. As for parameter tuning problems, DeepSeek did a good job getting the user started, but the users will most of the time have to tune the parameters themselves and use trial-and-error until the specs are met. On the other hand, DeepSeek does a good job at explaining key concepts, simplifying them, and explaining how a function runs line by line. This shows that DeepSeek is a good tool for users to use in debugging, but also that it needs a lot of context in its prompts to ensure it outputs accurate results. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>I worked with Deepseek on coding questions 5 and 6 of HW 4. For the most part, Deepseek was able to tackle the coding questions with no problems, nearly one-shotting all of them. However, due to its lack of vision beyond texts, Deepseek is not really good at making visual observations. This gave Deepseek a little bit of trouble answering a couple of the written questions, as it had to rely on general knowledge it had on CNNs and MLPs. As for parameter tuning problems, DeepSeek did a good job getting the user started, but the users will most of the time have to tune the parameters themselves and use trial-and-error until the specs are met. On the other hand, DeepSeek does a good job at explaining key concepts, simplifying them, and explaining how a function runs line by line. This shows that DeepSeek is a good tool for users to use in debugging, but also that it needs a lot of context in its prompts to ensure it outputs accurate results. </paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/FmLaweUH2PTr7huFaPeTupEZ\" filename=\"cs182_hw4_q6_deepseek.pdf\"/><file url=\"https://static.us.edusercontent.com/files/ED1XnYsZ9aK1awfafhSSknlh\" filename=\"cs182_hw4_q5_deepseek.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-11T01:15:09.787674+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7105095,
            "author": "Hong Joey",
            "project_title": "Make-up Office Hour for Friday 3-4pm in 531 Cory",
            "post_body": "Hi all,\n\nApologies to those who came this Tuesday for OH from 6-7pm. I did not plan my commute well and arrived very late, and it looks like I missed quite a few people because of it. To make up for it, I will be hosting office hours this Friday (10/10) from 3-4pm in 531 Cory right after the newly added one. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>Apologies to those who came this Tuesday for OH from 6-7pm. I did not plan my commute well and arrived very late, and it looks like I missed quite a few people because of it. To make up for it, I will be hosting office hours this Friday (10/10) from 3-4pm in 531 Cory right after the newly added one. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-10T10:25:18.703499+11:00",
            "category": "Admin"
        },
        {
            "guid": 7104957,
            "author": "Tianhao Qian",
            "project_title": "Special Participation B: Qwen3-Max on HW2",
            "post_body": "Qwen3-MAX's answer:\n\nDialogue:\n\nIntro:\n\nI'm using Qwen3-MAX to solve the coding part of HW2, including three IPython notebooks. Notably, we can't use deep thinking mode for Qwen3-MAX because it's unavailable for this model.\n\nMy prompts:\n\n1. Please help me deal with these problems about deep neural networks. Think it step by step.\n\n<answer>(Which problem or section would you like to tackle first?)\n\n2. All of the problems. Plz, start from one PDF then others\n\n<answer> (Given the solution of notebooks 1)\n\n3. Yes. Go ahead.\n\n<answer> (Given the solution of notebooks 2)\n\n4. Yes.\n\n<answer>(Given the solution of notebooks 3)\n\nMy observation:\n\n1.  Qwen3-MAX doesn't show consistency between the code tip and the actual variable name. The following figures are the code and LLM's answer, respectively.\n\n\n\nWe can see that the code tip uses the correct word 'principal' instead of 'principle', while the variable name 'principle_feature' is given mistakenly. However, the code given by Qwen3-MAX uses the word 'principal' instead of 'principle', indicating that it is unable to clarify what really matters in coding.\n\n2. Qwen3-MAX tends to generate the solution in its own way, ignoring the code tip. As is often the case, generated code ignores the parameters of the function and even modifies the code, which is not allowed. An example is given in the following figures: \n\nPrompt: Plz, use delta in the function q_grad_step\n\nThe generated code ignores the parameter delta.\n\n\nPrompt: At the same time, maintain tx and ty.\n\nThen, although I told it that beside delta, it should maintain tx and ty. But the result showed that it just changed to another way of realizing this function instead of including delta, dx, and dy inside the function. \n\n\nPrompt: Please use tx, ty, and delta.\n\nOnly when I explicitly told it to use tx, ty and delta, it gave the correct code. \n\nFrom my perspective, Qwen3-MAX just used a standard template to give the classic/simple solution, the mode that was pre-trained beforehand. Therefore, even though I can't illustrate the ignoring behavior(ignoring the code tip),  Qwen3-MAX tends to use its own coding style.\n\n\n3.  Qwen3-MAX only answers coding questions, but not conceptual questions inside PDF. Remember that the prompts ask the model to deal with questions, but not coding questions. \n\nPerhaps it's due to model training bias: The model was trained more on code-completion tasks and therefore prefers giving code-like solutions.\n\nBut I'm curious whether it's just because, when reading the PDF, the model recognized that the task leans toward code generation and therefore pre-determined that it only needed to solve the code-generation part. \n\n\n\n4. There is another strange observation: Even if I don't give the \u2018\u2019model.py\u2018\u2019, for example, if I ask the model to realize a certain part in \"model.py\", it always gives one solution instead of requesting the original files of \"model.py\", which is significantly important for the model to know what they need and what they don't need so that the result given by LLMs is plausible and reliable. In this way, Qwen3-MAX lacks uncertainty awareness.\n\n\n\nI think, if we aim to utilize Qwen3-MAX to help with coding problems, we should be careful with these 4 aspects, which can be solved by detailed prompts.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Qwen3-MAX's answer:</paragraph><file url=\"https://static.us.edusercontent.com/files/B7K1jJBzxWs8Vlj0O38UYoA0\" filename=\"Qwen4HW2.pdf\"/><paragraph>Dialogue:</paragraph><file url=\"https://static.us.edusercontent.com/files/hJmjn7jcEaBp4kKA8hO8t30w\" filename=\"Qwen3-MAX.pdf\"/><paragraph><bold>Intro:</bold><break/><break/>I'm using Qwen3-MAX to solve the coding part of HW2, including three IPython notebooks. Notably, we can't use deep thinking mode for Qwen3-MAX because it's unavailable for this model.</paragraph><paragraph><bold>My prompts:</bold></paragraph><paragraph>1. Please help me deal with these problems about deep neural networks. Think it step by step.</paragraph><paragraph>&lt;answer&gt;(Which problem or section would you like to tackle first?)</paragraph><paragraph>2. All of the problems. Plz, start from one PDF then others</paragraph><paragraph>&lt;answer&gt; (Given the solution of notebooks 1)</paragraph><paragraph>3. Yes. Go ahead.</paragraph><paragraph>&lt;answer&gt; (Given the solution of notebooks 2)</paragraph><paragraph>4. Yes.</paragraph><paragraph>&lt;answer&gt;(Given the solution of notebooks 3)</paragraph><paragraph><bold>My observation:</bold></paragraph><paragraph>1.  Qwen3-MAX doesn't show consistency between the code tip and the actual variable name. The following figures are the code and LLM's answer, respectively.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/iqxVDWoT8FSmcP1cZ6OP58E9\" width=\"307\" height=\"113.53550295857988\"/></figure><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/PHZnKYOhUnk5E13SKTZO0kBQ\" width=\"323\" height=\"88\"/></figure><paragraph>We can see that the code tip uses the correct word 'principal' instead of 'principle', while the variable name 'principle_feature' is given mistakenly. However, the code given by Qwen3-MAX uses the word 'principal' instead of 'principle', indicating that it is <bold>unable to clarify</bold> <bold>what really matters in coding</bold>.</paragraph><paragraph>2. Qwen3-MAX tends to generate the solution in its own way, ignoring the code tip. As is often the case, generated code ignores the parameters of the function and even modifies the code, which is not allowed. An example is given in the following figures: </paragraph><paragraph>Prompt: Plz, use delta in the function q_grad_step</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Jz1IRrt63zSrL7XbsRBbQBce\" width=\"307\" height=\"114.86394557823128\"/></figure><paragraph>The generated code ignores the parameter <bold>delta</bold>.</paragraph><paragraph><break/>Prompt: At the same time, maintain tx and ty.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/5IjlQrZ1OfMbxxdW0TopKaia\" width=\"307\" height=\"142.95419847328245\"/></figure><paragraph>Then, although I told it that beside <bold>delta</bold>, it should maintain <bold>tx</bold> and <bold>ty</bold>. But the result showed that it just changed to another way of realizing this function instead of including <bold>delta</bold>, <bold>dx,</bold> and <bold>dy</bold> inside the function. </paragraph><paragraph><break/>Prompt: Please use tx, ty, and delta.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XQiOj4rdmOTkSbSq5uLFWvTz\" width=\"307\" height=\"198.3204134366925\"/></figure><paragraph>Only when I explicitly told it to use <bold>tx</bold>, <bold>ty</bold> and <bold>delta</bold>, it gave the correct code. </paragraph><paragraph>From my perspective, Qwen3-MAX just used a standard template to give the classic/simple solution, the mode that was pre-trained beforehand. Therefore, even though I can't illustrate the ignoring behavior(ignoring the code tip),  Qwen3-MAX tends to <bold>use its own coding style.</bold></paragraph><paragraph><break/>3.  Qwen3-MAX only answers coding questions, but not conceptual questions inside PDF. Remember that the prompts ask the model to deal with questions, but not coding questions. </paragraph><paragraph>Perhaps it's due to <bold>model training bias</bold>: The model was trained more on code-completion tasks and therefore prefers giving code-like solutions.</paragraph><paragraph>But I'm curious whether it's just because, when reading the PDF, the model recognized that the task <bold>leans toward code generation</bold> and therefore pre-determined that it only needed to solve the code-generation part. </paragraph><paragraph/><paragraph>4. There is another strange observation: Even if I don't give the \u2018\u2019model.py\u2018\u2019, for example, if I ask the model to realize a certain part in \"model.py\", it always gives one solution instead of requesting the original files of \"model.py\", which is significantly important for the model to know what they need and what they don't need so that the result given by LLMs is <bold>plausible and reliable.</bold> In this way, Qwen3-MAX lacks <bold>uncertainty awareness</bold>.</paragraph><paragraph/><paragraph>I think, if we aim to utilize Qwen3-MAX to help with coding problems, we should be careful with these 4 aspects, which can be solved by detailed prompts.</paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-10T10:02:56.629461+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7104948,
            "author": "Anant Sahai",
            "project_title": "Lecture 12 Thread",
            "post_body": "Use this thread to ask questions about GNNs. \n\nNote: You will observe that there is a difference between the coverage here and the coverage in Chapter 13 of the Prince textbook. However, you should be able to understand the textbook's coverage with respect to most of the things there that were not in lecture. The most important omission from lecture (from a practical standpoint as well as conceptual interest) is 13.7.1 where some strategies for dealing graphs too big to hold in memory are discussed. The second most important is 13.9 where thinking about edges is discussed with a simple approach. \n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/kFri7534RRAJaFIT9Xe9nobV\" filename=\"Lecture 12.pdf\"/><paragraph>Use this thread to ask questions about GNNs. </paragraph><paragraph>Note: You will observe that there is a difference between the coverage here and the coverage in Chapter 13 of the Prince textbook. However, you should be able to understand the textbook's coverage with respect to most of the things there that were not in lecture. The most important omission from lecture (from a practical standpoint as well as conceptual interest) is 13.7.1 where some strategies for dealing graphs too big to hold in memory are discussed. The second most important is 13.9 where thinking about edges is discussed with a simple approach. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-10T10:02:07.936619+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7097283,
            "author": "Anant Sahai",
            "project_title": "Friday 2-3pm GSI Office Hour Added in 531 Cory",
            "post_body": "As announced in Lecture, the last Tuesday GSI Office Hour was cancelled due to low attendance and has been moved to Friday 2-3pm in response to student requests for Friday office hours.\n\nYou are encouraged to attend with HW or discussion-related questions, or to just find others to work alongside. You don't have to have a question to attend office hours.",
            "content_xml": "<document version=\"2.0\"><paragraph>As announced in Lecture, the last Tuesday GSI Office Hour was cancelled due to low attendance and has been moved to Friday 2-3pm in response to student requests for Friday office hours.</paragraph><paragraph>You are encouraged to attend with HW or discussion-related questions, or to just find others to work alongside. You don't have to have a question to attend office hours.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-09T07:09:25.97416+11:00",
            "category": "Admin"
        },
        {
            "guid": 7095749,
            "author": "Tianhao Qian",
            "project_title": "Special Participation A: - Deepseek on HW1",
            "post_body": "Intro:\nI'm using Deepseek to solve HW1, including 7 problems.\n\nMy prompts:\n\n1. Please help me deal with these problems about deep neural networks. Think it step by step.\n\n<answer> (Given the solution of Problem 1)\n\n2. You have done a good job! What about the remaining problems? Think it step by step.\n\n<answer> (Given the solution of Problem 2)\n\n3.  Yes, proceed with Problem 3 about momentum.\n\n<answer>(Given the solution of Problem 3)\n\n4. Plz, continue.\n<answer>(Given the solution of Problem 4-7)\n\nMy observation:\n\n1. When the thinking and answer content was too long, DeepSeek only answered a part of the problems, probably due to the chunking technique and LLM's inclination to output shorter contents. So that's why there're 4 prompts for HW1.\n\n2. The accuracy is very high, especially for the computational tasks. However, If format of the question is not ordinary, it will be ignored sometimes, such as 4(a).\n\nI don't know whether it can be called \"hallucination\", but actually in the thinking content, it indeed notices this task but considers 4(a) internally without giving the solution: \n\n3. A serious problem occurs that at times, Deepseek will unnoticeably make an assumption, even if there is no information about it. This problem fails the COT(Chain of Thought) if the assumption is not expected, leading to the wrong answer. More seriously, if the users don't look through the process of thinking, they may heartily accept the answer, but the conflict is that the thinking content is too long to read.\n\n4. Deepseek fails to perfectly answer ambiguous questions, such as 2(c).  It seems that it tends to give a conceptual, verbal description, if possible. It can be seen as a kind of laziness from my perspective.\n\nLet's first compare the standard answer and Deepseek's answer:\n\nStandard\uff1a \n\nDeepseek:\n\nAlthough the two solutions points to the same conclusion, Deepseek's answer is quite short. I guessed that Deepseek mixed up the identity of reader. Therefore, I asked it to rewrite the solution with different identity prompting:\n\n--------------\n\nPrompt1:\n\nYou are a student doing the assignment. Please rewrite Question 2(c).\n\nPrompt2:\n\nThe professor will read your solution! Please rewrite Question 2(c).\n\n--------------\n\nBut the result shows that Deepseek still gives conceptual answers, only adding more description. Only when I mentioned 'calculation' externally, It does work. \n\n\n\nRecommendation:\n\nFor Observation 1: \n\nIf there's no question number, the chunking will be unpredictable. Therefore , I recommend managing long problems with explicit chunking by yourself.\n\nFor Observation 2:\n\nI don't know how to solve this problem. I think reducing the use of table may help.\n\nFor Observation 3:\n\nRequire DeepSeek to state assumptions explicitly. But sometimes, the reasoning chain will be too long so that perhaps looking through it by yourself is also another good choice. \n\nFor Observation 4:\n\nHandle ambiguity with targeted instruction.\n\n\n\n\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/9b87UXPWxoGpSeJ24LygVVjq\" filename=\"Deepseek4HW1.pdf\"/><paragraph><bold>Intro:</bold><break/>I'm using Deepseek to solve HW1, including 7 problems.</paragraph><paragraph><bold>My prompts:</bold></paragraph><paragraph>1. Please help me deal with these problems about deep neural networks. Think it step by step.</paragraph><paragraph>&lt;answer&gt; (Given the solution of Problem 1)</paragraph><paragraph>2. You have done a good job! What about the remaining problems? Think it step by step.</paragraph><paragraph>&lt;answer&gt; (Given the solution of Problem 2)</paragraph><paragraph>3.  Yes, proceed with Problem 3 about momentum.</paragraph><paragraph>&lt;answer&gt;(Given the solution of Problem 3)</paragraph><paragraph>4. Plz, continue.<break/>&lt;answer&gt;(Given the solution of Problem 4-7)</paragraph><paragraph><bold>My observation:</bold></paragraph><paragraph>1. When the thinking and answer content was too long, DeepSeek only answered a part of the problems, probably due to the chunking technique and LLM's inclination to output shorter contents. So that's why there're 4 prompts for HW1.</paragraph><paragraph>2. The accuracy is very high, especially for the computational tasks. However, If format of the question is not ordinary, it will be ignored sometimes, such as 4(a).</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/4gaMWC7UD8nXyOnM3NdmjUgz\" width=\"591\" height=\"300.5084745762712\"/></figure><paragraph>I don't know whether it can be called \"hallucination\", but actually in the thinking content, it indeed notices this task but considers 4(a) internally without giving the solution: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/iCaLmR7icGptJdGhkqtigVWv\" width=\"590.9999999999999\" height=\"425.21298701298696\"/></figure><paragraph>3. A serious problem occurs that at times, Deepseek will unnoticeably make an assumption, even if there is no information about it. This problem fails the COT(Chain of Thought) if the assumption is not expected, leading to the wrong answer. More seriously, if the users don't look through the process of thinking, they may heartily accept the answer, but the conflict is that the thinking content is too long to read.</paragraph><paragraph>4. Deepseek fails to perfectly answer ambiguous questions, such as 2(c).  It seems that it tends to give a conceptual, verbal description, if possible. It can be seen as a kind of laziness from my perspective.</paragraph><paragraph>Let's first compare the standard answer and Deepseek's answer:</paragraph><paragraph><bold>Standard\uff1a</bold> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/mguN0hFf7dt6fzH44ncKs96l\" width=\"591\" height=\"638.4187866927593\"/></figure><paragraph><bold>Deepseek:</bold></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kDsSDD3Auy9y8bcXcfYsudll\" width=\"591\" height=\"107.60982658959537\"/></figure><paragraph>Although the two solutions points to the same conclusion, Deepseek's answer is quite short. I guessed that Deepseek mixed up the identity of reader. Therefore, I asked it to rewrite the solution with different identity prompting:</paragraph><paragraph>--------------</paragraph><paragraph><bold>Prompt1:</bold></paragraph><paragraph>You are a student doing the assignment. Please rewrite Question 2(c).</paragraph><paragraph><bold>Prompt2:</bold></paragraph><paragraph>The professor will read your solution! Please rewrite Question 2(c).</paragraph><paragraph>--------------</paragraph><paragraph>But the result shows that Deepseek still gives conceptual answers, only adding more description. Only when I mentioned 'calculation' externally, It does work. </paragraph><paragraph/><paragraph><bold>Recommendation:</bold></paragraph><paragraph>For Observation 1: </paragraph><paragraph>If there's no question number, the chunking will be unpredictable. Therefore , I recommend managing long problems with explicit chunking by yourself.</paragraph><paragraph>For Observation 2:</paragraph><paragraph>I don't know how to solve this problem. I think reducing the use of table may help.</paragraph><paragraph>For Observation 3:</paragraph><paragraph>Require DeepSeek to <bold>state assumptions explicitly</bold>. But sometimes, the reasoning chain will be too long so that perhaps looking through it by yourself is also another good choice. </paragraph><paragraph>For Observation 4:</paragraph><paragraph>Handle ambiguity with targeted instruction.</paragraph><paragraph/><paragraph/><paragraph/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-09T03:33:14.844297+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7094320,
            "author": "Anant Sahai",
            "project_title": "Lecture 11 Thread",
            "post_body": "This is for clarifying questions on Lecture 11 --- which finished up the conv-net material.\n\nThere was a very good question in office hours the answer to which has been added to the written pdf above but does not appear in the lecture video. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/7A0TMPr1qJOcgKwRz4NjxsNa\" filename=\"Lecture 11.pdf\"/><paragraph>This is for clarifying questions on Lecture 11 --- which finished up the conv-net material.</paragraph><paragraph>There was a very good question in office hours the answer to which has been added to the written pdf above but does not appear in the lecture video. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-08T16:55:02.320093+11:00",
            "category": "Lectures"
        },
        {
            "guid": 7092540,
            "author": "Andy Zhang",
            "project_title": "Special Participation E: Learning Mode as an Effective Step By Step Tutor",
            "post_body": "Special Participation E: Learning Mode as an Effective Step By Step Tutor\n\nGemini Learning Mode: https://g.co/gemini/share/4e1faf6d804c\n\nGemini Non-Learning Mode: https://g.co/gemini/share/c6ce8062f1d6\n\nChatGPT Learning Mode: https://chatgpt.com/share/68e6a057-981c-800b-b06d-2b65a8332d36\n\nChatGPT Non-Learning Mode: https://chatgpt.com/share/68e6bd6d-f24c-800b-a8ba-96be1e554c78\n\nRecommendation:\n\nI would recommend fellow students to leverage Gemini / ChatGPT learning mode to go through problems in piece meal step by step fashion with a guided tutor. This may be helpful after completing a homework, noticing a discrepancy with the solutions, and deciding to go through the homework again with some guidance to build a stronger understanding. ChatGPT is more helpful if you\u2019re concerned about reward hacking, but Berkeley students have free Gemini Pro which may favor that since it\u2019s easy to hit usage limits with ChatGPT.\n\nThe learning modes are effective tools for breaking down problems into piecemeal parts, as opposed to generating the entire solution in one go.\n\nExecutive summary:\n\nI was interested in investigating the extent to which Guided Learning modes could help. I experimented with Gemini and ChatGPT which are the two models with learning modes available for free (in contrast, Claude\u2019s learning mode is not accessible). I tested two different problems on two different homework assignments for Gemini and ChatGPT, and experimented with learning and non-learning modes.\n\nFor Gemini Learning Mode, I was initially curious (1) to what extent it could help guide learning (2) whether it would break things down piece meal (3) how well it would handle latex formatting (4) whether it could produce new insights and (5) whether there would be reward hacking.\n\nI applied it to HW0 Vector Calculus Review a-e.\n\nFor (1-3) it performed admirably, though it did fail to format one part of one response. (4) was more of an issue; I prompted the model to \u201chelp me view the problem from the different lenses of vector calculus, and walk through both elementwise, rowwise, columnwise, and examples\u201d and it failed to do so.\n\n(5) is a significant concern. If you ask for the answer, it will provide it. In fact, simply asking \u201cFormatting messed up\u201d led to \u201cYou correctly calculated the three partial derivatives! That was the toughest part.\u201d\n\nMy observation is that guided learning is trained on successful learning traces and does not handle incorrect/out of distribution inputs well (and infers it as correct).\n\nAlso interestingly it converted the questions from a \u201cShow X = Y\u201d to \u201cWhat is derivative of X\u201d, which could be a plus or minus depending on your use case (in a way, not knowing the answer could be helpful).\n\nOne nice note is that after the problems were complete, it suggested additional questions, which may be useful for additional practice.\n\nIn contrast, Gemini directly answered all the parts when not in learning mode which could feel overwhelming.\n\nFor ChatGPT learning mode, it performed similarly to Gemini, though had the advantage of not simply giving the answer / getting reward hacked. However, Berkeley students have free Gemini Pro which may favor that since it\u2019s easy to hit usage limits with ChatGPT.\n\nSimilarly without learning mode, it answered all parts which could be overwhelming.",
            "content_xml": "<document version=\"2.0\"><paragraph>Special Participation E: Learning Mode as an Effective Step By Step Tutor</paragraph><paragraph>Gemini Learning Mode: <link href=\"https://g.co/gemini/share/4e1faf6d804c\">https://g.co/gemini/share/4e1faf6d804c</link></paragraph><paragraph>Gemini Non-Learning Mode: <link href=\"https://g.co/gemini/share/c6ce8062f1d6\">https://g.co/gemini/share/c6ce8062f1d6</link></paragraph><paragraph>ChatGPT Learning Mode: <link href=\"https://chatgpt.com/share/68e6a057-981c-800b-b06d-2b65a8332d36\">https://chatgpt.com/share/68e6a057-981c-800b-b06d-2b65a8332d36</link></paragraph><paragraph>ChatGPT Non-Learning Mode: <link href=\"https://chatgpt.com/share/68e6bd6d-f24c-800b-a8ba-96be1e554c78\">https://chatgpt.com/share/68e6bd6d-f24c-800b-a8ba-96be1e554c78</link></paragraph><paragraph>Recommendation:</paragraph><paragraph>I would recommend fellow students to leverage Gemini / ChatGPT learning mode to go through problems in piece meal step by step fashion with a guided tutor. This may be helpful after completing a homework, noticing a discrepancy with the solutions, and deciding to go through the homework again with some guidance to build a stronger understanding. ChatGPT is more helpful if you\u2019re concerned about reward hacking, but Berkeley students have free Gemini Pro which may favor that since it\u2019s easy to hit usage limits with ChatGPT.</paragraph><paragraph>The learning modes are effective tools for breaking down problems into piecemeal parts, as opposed to generating the entire solution in one go.</paragraph><paragraph>Executive summary:</paragraph><paragraph>I was interested in investigating the extent to which Guided Learning modes could help. I experimented with Gemini and ChatGPT which are the two models with learning modes available for free (in contrast, Claude\u2019s learning mode is not accessible). I tested two different problems on two different homework assignments for Gemini and ChatGPT, and experimented with learning and non-learning modes.</paragraph><paragraph>For Gemini Learning Mode, I was initially curious (1) to what extent it could help guide learning (2) whether it would break things down piece meal (3) how well it would handle latex formatting (4) whether it could produce new insights and (5) whether there would be reward hacking.</paragraph><paragraph>I applied it to HW0 Vector Calculus Review a-e.</paragraph><paragraph>For (1-3) it performed admirably, though it did fail to format one part of one response. (4) was more of an issue; I prompted the model to \u201chelp me view the problem from the different lenses of vector calculus, and walk through both elementwise, rowwise, columnwise, and examples\u201d and it failed to do so.</paragraph><paragraph>(5) is a significant concern. If you ask for the answer, it will provide it. In fact, simply asking \u201cFormatting messed up\u201d led to \u201cYou correctly calculated the three partial derivatives! That was the toughest part.\u201d</paragraph><paragraph>My observation is that guided learning is trained on successful learning traces and does not handle incorrect/out of distribution inputs well (and infers it as correct).</paragraph><paragraph>Also interestingly it converted the questions from a \u201cShow X = Y\u201d to \u201cWhat is derivative of X\u201d, which could be a plus or minus depending on your use case (in a way, not knowing the answer could be helpful).</paragraph><paragraph>One nice note is that after the problems were complete, it suggested additional questions, which may be useful for additional practice.</paragraph><paragraph>In contrast, Gemini directly answered all the parts when not in learning mode which could feel overwhelming.</paragraph><paragraph>For ChatGPT learning mode, it performed similarly to Gemini, though had the advantage of not simply giving the answer / getting reward hacked. However, Berkeley students have free Gemini Pro which may favor that since it\u2019s easy to hit usage limits with ChatGPT.</paragraph><paragraph>Similarly without learning mode, it answered all parts which could be overwhelming.</paragraph></document>",
            "links": [
                "https://g.co/gemini/share/4e1faf6d804c",
                "https://g.co/gemini/share/c6ce8062f1d6",
                "https://chatgpt.com/share/68e6a057-981c-800b-b06d-2b65a8332d36",
                "https://chatgpt.com/share/68e6bd6d-f24c-800b-a8ba-96be1e554c78"
            ],
            "attachments": [],
            "created_at": "2025-10-08T11:57:35.487299+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7088853,
            "author": "Andy Zhang",
            "project_title": "Special Participation B: Qwen3-Max on HW1",
            "post_body": "Here is the online link: https://chat.qwen.ai/s/68228653-4e58-46ba-b8bc-1e11f7a10f6c?fev=0.0.222 Annotated log: https://drive.google.com/file/d/1SspCCWNN5ekf5kutndGvgZsZcgzkQzUU/view?usp=sharing Executive Summary: The most challenging part was passing context, since Qwen3-Max Qwen3-Max does not support ipynb inputs and does not directly interface as an IDE. I initially attempted copy and pasting parts of the assignment, but selecting the right context can be tricky. I ended up converting the ipynb to a py file and pasting that in.\n\nQwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. It took several prompts to get it to answer the right TODO.\n\nFinally, it also noted a potential issue with nomenclature how the assignment leveraged EMEA rather than classical momentum, though if you stuck to the TODO then it would be clear.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is the online link: https://chat.qwen.ai/s/68228653-4e58-46ba-b8bc-1e11f7a10f6c?fev=0.0.222 Annotated log: https://drive.google.com/file/d/1SspCCWNN5ekf5kutndGvgZsZcgzkQzUU/view?usp=sharing Executive Summary: The most challenging part was passing context, since Qwen3-Max Qwen3-Max does not support ipynb inputs and does not directly interface as an IDE. I initially attempted copy and pasting parts of the assignment, but selecting the right context can be tricky. I ended up converting the ipynb to a py file and pasting that in.</paragraph><paragraph>Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. It took several prompts to get it to answer the right TODO.</paragraph><paragraph>Finally, it also noted a potential issue with nomenclature how the assignment leveraged EMEA rather than classical momentum, though if you stuck to the TODO then it would be clear.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-08T03:40:00.499107+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7086331,
            "author": "Hong Joey",
            "project_title": "HW4 Solutions",
            "post_body": "Below are solutions to HW4. Ask any questions about the solutions in this thread.",
            "content_xml": "<document version=\"2.0\"><paragraph>Below are solutions to HW4. Ask any questions about the solutions in this thread.</paragraph><file url=\"https://static.us.edusercontent.com/files/hv1tbxZ1KN1mLgX8vsUCtVwN\" filename=\"hw04_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/s6C6Lph0jaBFSI4bwgVc1Y3F\" filename=\"HandDesignFilters_Sol.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/OMutqNvcywVFdOkjW1Ba5ctg\" filename=\"edge_detection_sol.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T13:30:16.556929+11:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7085536,
            "author": "Zhangzhi 'csrookie' Xiong",
            "project_title": "Participation Section B\uff1a HW3 - Grok 3",
            "post_body": "Brief Intro\n\nI'm using Grok Auto mode (Grok 3). The coding HW the LLM will work on is HW3 coding.\n\nPrompt: I'm a student learning deep learning. Please read this coding assignment .ipynb file, and complete the code and elaborate on your code.\n\nThinking:\n\nAnalyzing assignment\n\nThis coding assignment explores maximal update parameterization (muP) for training MLPs with varying layer widths.\n\nThe notebook includes sections on examining norms, implementing muP, and exploring related concepts like Shampoo updates.\n\nFor part a, the MLP layers are input 784, then hidden sizes [8,16,64,64,64,256,256,1024], and output 10.\n\nThe task involves completing TODOs and answering questions about layer dimensions and norm effects.\n\nLet's check how layer dimensions affect RMS norms of activation deltas in part a.\n\nCompleting part b code\n\nThe code needs to compute Frobenius, spectral, and RMS-RMS induced norms for weight updates.\n\nFor each weight matrix, calculate Frobenius norm using torch.norm with 'fro'.\n\nCompute spectral norm using the maximum singular value from SVD.\n\nThe RMS-RMS induced norm likely correlates most with activation deltas RMS due to update effects.\n\nFor implementation, detect weights by shape, compute norms, and plot for comparison.\n\nImplementing muP scaling\n\nFor biases, scale learning rate by 0.01 as a fudge factor.\n\nFor input layer, multiply learning rate by 10 to adjust for initial scale.\n\nFor hidden layers, adjust learning rate by dividing by fan_in to mimic muP behavior.\n\nFor output layer, use a fixed learning rate of 0.003 instead of muP scaling.\n\nTo make RMS activation deltas uniform, larger layers need adjusted learning rates to balance update sizes.\n\nRefining muP reasoning\n\nTo have constant RMS activation deltas, ensure update norms are independent of layer widths.\n\nThe Frobenius norm of gradients scales with sqrt(in * out / batch), affecting update sizes.\n\nThe induced RMS norm of weight updates equals spectral norm times sqrt(fan_in / fan_out).\n\nSpectral norm of weight matrices at initialization is approximately sigma * (sqrt(fan_in) + sqrt(fan_out)), with sigma based on variance.\n\nFor random matrices, spectral norm scales with sqrt(max(fan_in, fan_out)) times entry variance.\n\nExploring gradient scaling\n\nGradients to pre-activations scale with sqrt(next layer width / current layer width) due to backpropagation.\n\nThe induced RMS norm of weight updates depends on spectral norm and layer dimensions.\n\nIn homogeneous layers, RMS of gradients to pre-activations is approximately 1/sqrt(width).\n\nmuP allows using a single global learning rate across different layer widths for consistent training.\n\nFor larger hidden layers, RMS activation deltas are smaller in vanilla, needing larger learning rates to balance.\n\nAdjusting learning rates\n\nTo increase delta act for large fan_out, increase learning rate for that layer's weights.\n\nInduced norm of weight updates scales with learning rate and layer dimensions, affecting activation deltas.\n\nFor each layer, RMS of gradients to pre-activations is approximately 1/sqrt(fan_out), impacting update scaling.\n\nTo make activation deltas constant, scale learning rate by sqrt(fan_out / fan_in) for each layer.\n\nSearching for implementation details\n\nCurrently looking for \"maximal update parameterization muP implementation pytorch\" to clarify scaling.\n\nSearched for \"maximal update parameterization muP implementation pytorch\"\n\ngithub.com\n\ncerebras.ai\n\nblog.speechmatics.com\n\nblog.eleuther.ai\n\nmichal.io\n\narxiv.org\n\npureai.com\n\ntraining-api.cerebras.ai\n\nhowtoscalenn.github.io\n\nFound resources like GitHub for muP implementation in PyTorch.\n\nBrowsed https://github.com/microsoft/mup\n\nReviewing muP implementation\n\nThe mup package scales learning rates by dividing globalLR by width multiplier for larger layers.\n\nUsers set the same learning rate for different widths, and the package adjusts parameterization for consistent updates.\n\nExtracting scaling logic\n\nThe code shows learning rate scaling by dividing globalLR by a width multiplier for each parameter.\n\nBrowsed https://github.com/microsoft/mup/blob/main/src/mup/optim.py\n\nFor wider layers, effective learning rate decreases, aligning with standard optimal behavior.\n\nBrowsed https://raw.githubusercontent.com/microsoft/mup/main/mup/optim.py\n\nTroubleshooting tool access\n\nThe tool for extracting code from optim.py returned empty, possibly due to an incorrect path.\n\nSearched for \"mup optim.py github\"\n\ngithub.com\n\npypi.org\n\ncerebras.ai\n\nLocating optim.py file\n\nNow checking GitHub to list files and find optim.py for lr scaling details.\n\nBrowsd https://github.com/microsoft/mup/tree/main/mup\n\nExploring alternative resources\n\nNow searching for simpler implementations like ezmup on GitHub to understand muP lr scaling.\n\nBrowsed https://github.com/cloneofsimo/ezmup\n\nInvestigating ezmup scaling\n\nIn ezmup, learning rate scaling uses a dictionary based on parameter widths.\n\nBrowsed https://raw.githubusercontent.com/cloneofsimo/ezmup/main/ezmup/core.py\n\nLet's explore Eleuther AI blog for muP implementation details on lr scaling.\n\nBrowsed https://blog.eleuther.ai/mutransfer/\n\nApplying muP insights\n\nFor hidden layers, muP scales learning rate by dividing by layer width to maintain update consistency.\n\nThe input layer is treated as input, hidden as hidden, and output as output in the MLP structure.\n\nFinalizing muP implementation\n\nFor biases, learning rate is scaled by multiplying by 0.01 as a fudge factor.\n\nFor output bias, since it's a vector, scale learning rate by 0.01, keeping changes small.\n\nScaling the output of a layer by a constant doesn't directly adjust Adam's learning rate due to normalization.\n\nEvaluating layer scaling effects\n\nScaling a layer's output by a constant affects gradient propagation to previous layers.\n\nFor SignGD, scaling output by a constant keeps updates unchanged due to sign invariance.\n\nConsidering per-weight multipliers\n\nScaling layer output by a factor like sqrt(fan_in) might help make activation deltas uniform.\n\nFor part d, adjusting the network graph by scaling outputs can mimic muP effects with standard Adam.\n\nAnalyzing output scaling\n\nScaling a layer's output by a constant scales gradients, but Adam's normalization keeps updates unchanged.\n\nAdding a multiplier after a layer doesn't change activation deltas in Adam due to scale invariance.\n\nClarifying multiplier effects\n\nFor Adam, multiplying layer output by a constant doesn't alter updates due to normalization.\n\nScaling output by a constant affects next layer's input, potentially impacting update consistency.\n\nFinalizing multiplier strategy\n\nTo make activation deltas constant, scale layer output by sqrt(fan_out / fan_in).\n\nMultiplying layer output by a constant adjusts effective learning rate for the previous layer in SGD, explaining part d equivalence.\n\nRevisiting update behavior\n\nFor the first step, SimpleAdam acts like SignGD, with updates based on sign of gradients.\n\nAssessing graph adjustments\n\nScaling layer output by a constant doesn't change Adam updates due to normalization.\n\nFor SignGD, scaling output by a constant keeps updates unchanged, as sign is invariant.\n\nReevaluating multiplier impact\n\nAdding a constant multiplier after a layer doesn't alter activation deltas in Adam due to scale invariance.\n\nScaling output affects forward signal, altering loss and gradients for later layers.\n\nRefining scaling approach\n\nScaling layer output by sqrt(fan_in / fan_out) may reduce pre-activation RMS for larger layers, aiding sigmoid linearity.\n\nScaling logits by large factors can make softmax peaked, reducing gradient magnitude for correct predictions.\n\nExamining softmax behavior\n\nSoftmax is not scale invariant; for large scaling, it approaches onehot for the maximum logit.\n\nScaling hidden activations affects logit scale, altering gradient magnitudes through backpropagation.\n\nAdjusting forward scaling\n\nScaling pre-activation by a factor affects sigmoid derivative, impacting gradient flow.\n\nScaling post-sigmoid output by a constant directly scales activation deltas, making them adjustable for uniformity.\n\nFinalizing scaling strategy\n\nScaling pre-activation by sqrt(fan_out / fan_in) helps balance activation deltas across layers.\n\nFor part d, implement scaling in forward pass to ensure uniform activation deltas with standard Adam.\n\nCompleting part a answers\n\nThe dimensions of each layer are 784 input, then hidden [8,16,64,64,64,256,256,1024], and output 10.\n\nLarger layer dimensions typically reduce RMS norms of activation deltas due to scaling effects.\n\nResponse:\n\nI've read the provided Jupyter notebook file (q_mup_coding.ipynb) and completed the TODO sections based on the requirements of each part. I'll structure my response by section, providing the completed code for each TODO, the answers to the questions (as they would appear in the markdown cells), and elaborations on the code and reasoning. My goal was to make the code functional, follow the safety instructions, and ensure the implementations align with the concepts of muP, norms, and optimizers as described.\n\nThe notebook uses a simple MLP on MNIST, with focus on activation deltas, weight updates, and hyperparameter stability across widths.\n\na. Examining the norms of a heterogenous MLP.\n\nCompleted Code for TODO:\n\nThe TODO is to call some plotting code with different widths to observe changes.\n\n# TODO: Call some plotting code here.\ntrain_one_step(optimizer=SimpleAdam, hiddens=[128, 64, 32, 16, 8])  # Reversed widths to see if RMS changes as expected\n\n\nAnswers to Questions (as in markdown cell):\n\nAnswer: The dimensions of each layer in the neural network are 784 (input) -> 8 -> 16 -> 64 -> 64 -> 64 -> 256 -> 256 -> 1024 (hidden layers) -> 10 (output classes).\n\nAnswer: The dimensionality (hidden size) affects the RMS norm of the activation deltas by making it decrease as the layer width increases. Larger layers tend to have smaller activation deltas because the gradients are diluted across more dimensions in vanilla parameterization, leading to smaller updates.\n\nAnswer: Yes, changing the widths (e.g., to [128, 64, 32, 16, 8]) reverses the trend\u2014the RMS values increase as layer sizes decrease, confirming that wider layers have smaller RMS deltas in vanilla setup.\n\nElaboration:\n\nThe train_one_step function runs two forward passes and a gradient step, computing the RMS of activation changes between passes. The plot shows bar heights decreasing with larger hidden sizes in the default call. By reversing the widths, the plot should show increasing RMS, validating the inverse relationship with dimensionality. This is due to standard Kaiming initialization and gradient flow, where wider layers average out updates more.\n\nb. Examining the norms of the updates to the weights.\n\nCompleted Code for TODO:\n\nfor p in delta_params:\n    if len(p.shape) == 2:\n        ### TODO: Log the respective norms.\n        frob_norms.append(torch.norm(p, 'fro').item())\n        spectral_norms.append(torch.linalg.norm(p, ord=2).item())  # spectral norm\n        induced_norms.append(torch.linalg.norm(p, ord=2).item() * np.sqrt(p.shape[1] / p.shape[0]))  # RMS-RMS induced norm\n        p_shapes.append(f\"{p.shape[1]} -> {p.shape[0]}\")\n        ### \n\n\nAnswers to Questions (as in markdown cell):\n\nAnswer: The RMS-RMS induced norm correlates the most with the RMS norms of the activation deltas, as it accounts for the input/output dimensions and matches the scale of how updates propagate through activations.\n\nElaboration:\n\nFrobenius norm: Measures the total magnitude of the matrix update as \u221a(sum of squares). It tends to increase with layer size because larger matrices have more elements.\n\nSpectral norm: The largest singular value, measuring the maximum stretch. It often stays relatively constant or decreases slightly with width.\n\nRMS-RMS induced norm: Calculated as spectral_norm * \u221a(fan_in / fan_out). This represents the induced norm under RMS vector norms, which aligns with how activation changes are propagated (\u03b4_act \u2248 induced_norm(\u03b4W) * rms(prev_act)).\n\nThe code clones parameters before and after the step, computes deltas, and plots only for 2D weights (ignoring biases). The correlation with induced norm comes from the propagation: activation changes depend on how the weight update \"induces\" changes in the output space from the input space.\n\nThe plot will show Frobenius and spectral varying with width, but induced more stable or matching the activation RMS pattern from part a.\n\nc. Implementing muP\n\nCompleted Code for TODO:\n\n############################\n### Todo: Adjust the per-layer learning rate scaling factor so per-layer RMS activation deltas are constant.\n### Hint for part e: The following tricks will help you retain performance when using muP scaling.\n###  - Treat biases as a hidden layer with size (d_out, 1). You will need to use a fudge-factor of around 0.01 -- we want to keep the change in bias terms low.\n###  - For the input layer, a fudge factor of 10 appears to help.\n###  - For the output layer, we find it is best to ignore the muP scaling, and instead use a fixed learning rate (e.g. 0.003).\n############################\nlr = group['lr']\nif len(p.shape) == 2:  # Weight\n    fan_in = p.shape[1]\n    fan_out = p.shape[0]\n    if fan_out == 10:  # Output layer\n        lr = 0.003\n    else:\n        lr /= fan_in\n        if fan_in == 784:  # Input layer\n            lr *= 10\nelse:  # Bias\n    fan_in = 1\n    fan_out = p.shape[0]\n    lr /= fan_in\n    lr *= 0.01  # Fudge for biases\n############################\n############################\n\n\nAnswers to Questions (as in markdown cell):\n\nNote: The note asks why the first 2-3 activation-deltas may have a lower norm.\n\nAnswer: The first 2-3 activation-deltas may have lower norms because the early layers have smaller widths in the default hiddens list, leading to larger relative gradients but smaller updates after scaling. Additionally, chain effects from backpropagation and initial random activations may cause lower effective updates in early layers.\n\nElaboration:\n\nThe scaling follows muP principles for Adam: lr scaled by 1/fan_in for hidden layers to make updates independent of width. The fudges are as per the hint to stabilize training\u2014larger for input (to boost update), small for biases (to keep changes low), fixed for output (to avoid instability in logits).\n\nThe code applies this per parameter in the step function. The plots for the two hiddens lists should show roughly uniform bar heights for RMS deltas, unlike vanilla Adam.\n\nd. Per-Weight Multipliers\n\nCompleted Code for TODO:\n\nclass ScaledMLP(nn.Module):\n    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n        super(ScaledMLP, self).__init__()\n        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n        self.layers = nn.ModuleList()\n        for i in range(len(all_hidden_sizes)-1):\n            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        activations = []\n        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            ## TODO\n            x *= np.sqrt(layer.out_features / layer.in_features)  # Scale preactivation by sqrt(fan_out / fan_in) to counter vanilla scaling\n            ##\n            x = self.sigmoid(x)\n            activations.append(x)\n        x = self.layers[-1](x)\n        activations = activations[1:]\n        return x, [a.detach() for a in activations]\n\n\nAnswers to Questions (as in markdown cell):\n\nAnswer: Multiplying the output of a layer by a constant is equivalent to adjusting the learning rate of that layer because it scales the gradients during backpropagation, effectively changing the update magnitude. For Adam, it's approximately equivalent due to normalization (though invariant in theory, the first-step behavior is SignGD-like, where scaling affects effective step size through chain effects). For SignGD, it scales the effective step in propagation.\n\nElaboration:\n\nBy scaling the preactivation by \u221a(fan_out / fan_in), we counter the vanilla trend where deltas ~ \u221a(fan_in / fan_out), making deltas uniform. This adjusts the graph to mimic muP without optimizer changes. The plot with SimpleAdam should now show uniform RMS deltas. The equivalence holds more for SGD-like optimizers, but works here due to the problem's SimpleAdam being SignGD-like in early steps and chain effects in the network.\n\ne. Hyperparameter Transfer\n\nNo TODO in code, but the answers:\n\nAnswers to Questions (as in markdown cell):\n\nAnswer: In Adam, the optimal learning rate decreases as the network increases in size (e.g., from 0.01 for small widths to 0.001 for large), because larger models have higher curvature and require smaller steps for stability.\n\nAnswer: With muP, there is a single global learning rate (e.g., around 1.0-3.0) that works for a majority of widths, as the per-layer scaling makes updates independent of width. The 256-width network achieves ~0.5 loss, comparable to Adam.\n\nElaboration:\n\nThe sweep code is provided, and with SimpleAdamMuP, the loss curves align across widths, showing hyperparameter transfer. This is muP's key benefit\u2014tune on small, transfer to large.\n\ne. Shampoo and Orthogonalization (second e)\n\nCompleted Code for TODO:\n\n############ TODO\nif len(m.shape) == 1:\n    u = m  # Ignore biases for this question, it's not important.\nelse:\n    U, S, Vh = torch.linalg.svd(m, full_matrices=False)\n    u = U @ Vh.T\n#############\n\n\nAnswers to Questions (as in markdown cell):\n\nAnswer: For Adam, the Frobenius norm is much larger than the spectral norm for large layers (frob ~ sqrt(rank) * spectral, where rank is high). For Shampoo, the spectral norm is constant (~1), and frob = sqrt(rank), showing orthogonalization makes the update \"unit-spectral\" but frob grows with dimension.\n\nAnswer: (Bonus) Scale u by 1 / sqrt(in / out) to make the induced RMS norm constant (e.g., 1).\n\nCompleted bonus code:\n\nu = U @ Vh.T\nu *= 1 / np.sqrt(p.shape[1] / p.shape[0])  # Scale to make induced norm constant\n\n\nElaboration:\n\nShampoo orthogonalizes the momentum by taking the SVD and dropping the singular values, keeping only the directions (U V^T). This makes the update rank-preserving but with unit spectral norm. The plot shows Adam's norms varying, Shampoo's spectral constant. The bonus scales to constant induced norm, ensuring uniform propagation effects.\n\nThis completes the assignment. If you run the notebook with these changes, the plots and training should behave as described. Let me know if you need the full JSON notebook or further clarifications!\n\nComment\n\nThe codes are basically correct, except for the last part. The implementations are clean, efficient, and demonstrate a strong grasp of PyTorch mechanics, norm calculations, and hyperparameter scaling. AIs like Grok are trained on massive datasets including GitHub repositories, research papers, and educational notebooks on deep learning. They can pattern-match common experimentation techniques from similar assignments or tutorials! The AI likely \"understands\" from training data that reversing parameters is a standard way to validate scaling laws, drawing from sources like the NTK literature or muP discussions.\n\nFor more recently advanced Shampoo model, from my perspective, Shampoo details from KFAC/Shampoo papers are in training data. AIs compute matrix ops accurately, avoiding full_matrices=True for efficiency.\n\nBut as for the most recently developed model muon, Grok provided the wrong code:\n\nif len(m.shape) == 1:\n    u = m  # Ignore biases for this question, it's not important.\nelse:\n    U, S, Vh = torch.linalg.svd(m, full_matrices=False)\n    u = U @ Vh.T\n\n\nAs the error of dimension will be thrown out. I reported the error to the grok and ask it to revise:\n\nPrompt:\n\nFile c:\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    117 @functools.wraps(func)\n    118 def decorate_context(*args, **kwargs):\n...\n---> 33     u = U @ Vh.T\n     36 #############\n     37 p.add_(u, alpha=-group['lr'])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (8x8 and 784x8)\nPlease read the error log and debug about your corresponding code\n\n\nResponse:\n\nThank you for providing the error log. I've analyzed it step by step based on the code context (from the Shampoo implementation in the notebook) and reproduced the issue using a tool to confirm matrix shapes. This is a matrix multiplication shape mismatch in the SVD reconstruction within the SimpleShampoo optimizer's step method. Below, I'll explain the root cause, why it occurs, and how to fix it. I'll also provide the corrected code snippet.\nStep-by-Step Diagnosis\n\nError Context:\n\nThe error occurs on line u = U @ Vh.T inside the Shampoo optimizer.\nIt's a RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x8 and 784x8).\nThis happens during matrix multiplication (@ operator) where the dimensions don't align: the number of columns in the left matrix (8) must equal the number of rows in the right matrix, but here the right matrix has 784 rows.\n\n\nUnderstanding the SVD Output:\n\nIn torch.linalg.svd(m, full_matrices=False), for a momentum matrix m of shape (out_features, in_features) (e.g., (8, 784) for the first layer: 784 input to 8 hidden).\n\nU: Shape (out, min(out, in)) \u2192 (8, 8).\nS: Shape (min(out, in)) \u2192 (8).\nVh: Shape (min(out, in), in) \u2192 (8, 784). Note: Vh is already the conjugate transpose (for real matrices, just transpose) of the right singular vectors.\n\n\nThe full SVD reconstruction (with singular values) would be U @ torch.diag(S) @ Vh, which correctly yields shape (8, 784).\nIn the simplified Shampoo, we're dropping S to \"orthogonalize\" the update (keeping directions but normalizing magnitudes implicitly).\n\n\nWhy the Error Happens:\n\nThe code computes U @ Vh.T:\n\nVh.T transposes Vh from (8, 784) to (784, 8).\nNow: U (8x8) @ Vh.T (784x8) \u2192 Columns of left (8) != rows of right (784) \u2192 Mismatch!\n\n\nThis fails specifically for non-square matrices (e.g., when out_features != in_features, like in early layers: 784\u21928).\nIn square layers (e.g., 64\u219264), it might coincidentally work if dimensions align, but it's still incorrect.\nRoot cause: Unnecessary .T on Vh. PyTorch's Vh is already in the correct orientation for reconstruction (U @ ... @ Vh).\n\n\ncode:\n\nU, S, Vh = torch.linalg.svd(m, full_matrices=False)\nu = U @ Vh  # Correct: No .T needed; Vh is already transposed in PyTorch convention\n\n\nNow the code is correct! The dimension problem, typically, can be resolved by AI if error log is well provided. In summary, the coding power of Grok is tremendous. Especially for assignment codes, Grok, and as for many AI agent I believe, are good at it, since the instruction text and partial implemented code themselves can provide enough information for Grok to reason. Not even to mention that Grok has been trained on tons of code on public website, so it even may has seen the similar code assignment before!\n\n",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\">Brief Intro</heading><paragraph>I'm using Grok Auto mode (Grok 3). The coding HW the LLM will work on is HW3 coding.</paragraph><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this coding assignment .ipynb file, and complete the code and elaborate on your code.</bold></italic></paragraph><paragraph><italic><bold>Thinking:</bold></italic></paragraph><paragraph>Analyzing assignment</paragraph><list style=\"unordered\"><list-item><paragraph>This coding assignment explores maximal update parameterization (muP) for training MLPs with varying layer widths.</paragraph></list-item><list-item><paragraph>The notebook includes sections on examining norms, implementing muP, and exploring related concepts like Shampoo updates.</paragraph></list-item><list-item><paragraph>For part a, the MLP layers are input 784, then hidden sizes [8,16,64,64,64,256,256,1024], and output 10.</paragraph></list-item><list-item><paragraph>The task involves completing TODOs and answering questions about layer dimensions and norm effects.</paragraph></list-item><list-item><paragraph>Let's check how layer dimensions affect RMS norms of activation deltas in part a.</paragraph></list-item></list><paragraph>Completing part b code</paragraph><list style=\"unordered\"><list-item><paragraph>The code needs to compute Frobenius, spectral, and RMS-RMS induced norms for weight updates.</paragraph></list-item><list-item><paragraph>For each weight matrix, calculate Frobenius norm using torch.norm with 'fro'.</paragraph></list-item><list-item><paragraph>Compute spectral norm using the maximum singular value from SVD.</paragraph></list-item><list-item><paragraph>The RMS-RMS induced norm likely correlates most with activation deltas RMS due to update effects.</paragraph></list-item><list-item><paragraph>For implementation, detect weights by shape, compute norms, and plot for comparison.</paragraph></list-item></list><paragraph>Implementing muP scaling</paragraph><list style=\"unordered\"><list-item><paragraph>For biases, scale learning rate by 0.01 as a fudge factor.</paragraph></list-item><list-item><paragraph>For input layer, multiply learning rate by 10 to adjust for initial scale.</paragraph></list-item><list-item><paragraph>For hidden layers, adjust learning rate by dividing by fan_in to mimic muP behavior.</paragraph></list-item><list-item><paragraph>For output layer, use a fixed learning rate of 0.003 instead of muP scaling.</paragraph></list-item><list-item><paragraph>To make RMS activation deltas uniform, larger layers need adjusted learning rates to balance update sizes.</paragraph></list-item></list><paragraph>Refining muP reasoning</paragraph><list style=\"unordered\"><list-item><paragraph>To have constant RMS activation deltas, ensure update norms are independent of layer widths.</paragraph></list-item><list-item><paragraph>The Frobenius norm of gradients scales with sqrt(in * out / batch), affecting update sizes.</paragraph></list-item><list-item><paragraph>The induced RMS norm of weight updates equals spectral norm times sqrt(fan_in / fan_out).</paragraph></list-item><list-item><paragraph>Spectral norm of weight matrices at initialization is approximately sigma * (sqrt(fan_in) + sqrt(fan_out)), with sigma based on variance.</paragraph></list-item><list-item><paragraph>For random matrices, spectral norm scales with sqrt(max(fan_in, fan_out)) times entry variance.</paragraph></list-item></list><paragraph>Exploring gradient scaling</paragraph><list style=\"unordered\"><list-item><paragraph>Gradients to pre-activations scale with sqrt(next layer width / current layer width) due to backpropagation.</paragraph></list-item><list-item><paragraph>The induced RMS norm of weight updates depends on spectral norm and layer dimensions.</paragraph></list-item><list-item><paragraph>In homogeneous layers, RMS of gradients to pre-activations is approximately 1/sqrt(width).</paragraph></list-item><list-item><paragraph>muP allows using a single global learning rate across different layer widths for consistent training.</paragraph></list-item><list-item><paragraph>For larger hidden layers, RMS activation deltas are smaller in vanilla, needing larger learning rates to balance.</paragraph></list-item></list><paragraph>Adjusting learning rates</paragraph><list style=\"unordered\"><list-item><paragraph>To increase delta act for large fan_out, increase learning rate for that layer's weights.</paragraph></list-item><list-item><paragraph>Induced norm of weight updates scales with learning rate and layer dimensions, affecting activation deltas.</paragraph></list-item><list-item><paragraph>For each layer, RMS of gradients to pre-activations is approximately 1/sqrt(fan_out), impacting update scaling.</paragraph></list-item><list-item><paragraph>To make activation deltas constant, scale learning rate by sqrt(fan_out / fan_in) for each layer.</paragraph></list-item></list><paragraph>Searching for implementation details</paragraph><list style=\"unordered\"><list-item><paragraph>Currently looking for \"maximal update parameterization muP implementation pytorch\" to clarify scaling.</paragraph></list-item></list><paragraph>Searched for \"maximal update parameterization muP implementation pytorch\"</paragraph><paragraph><link href=\"https://github.com/microsoft/mup\">github.com</link></paragraph><paragraph><link href=\"https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization\">cerebras.ai</link></paragraph><paragraph><link href=\"https://blog.speechmatics.com/mup\">blog.speechmatics.com</link></paragraph><paragraph><link href=\"https://blog.eleuther.ai/mutransfer/\">blog.eleuther.ai</link></paragraph><paragraph><link href=\"https://michal.io/wiki/Maximal-Update-Parametrization-%28%25CE%25BCP%29\">michal.io</link></paragraph><paragraph><link href=\"https://arxiv.org/html/2407.17465v1\">arxiv.org</link></paragraph><paragraph><link href=\"https://pureai.com/articles/2022/04/04/neural-net-training.aspx\">pureai.com</link></paragraph><paragraph><link href=\"https://training-api.cerebras.ai/en/latest/wsc/Model-zoo/tutorials/mup/mup_docs.html\">training-api.cerebras.ai</link></paragraph><paragraph><link href=\"https://howtoscalenn.github.io/\">howtoscalenn.github.io</link></paragraph><list style=\"unordered\"><list-item><paragraph>Found resources like <link href=\"https://github.com/microsoft/mup\">GitHub</link> for muP implementation in PyTorch.</paragraph></list-item></list><paragraph>Browsed https://github.com/microsoft/mup</paragraph><paragraph>Reviewing muP implementation</paragraph><list style=\"unordered\"><list-item><paragraph>The mup package scales learning rates by dividing globalLR by width multiplier for larger layers.</paragraph></list-item><list-item><paragraph>Users set the same learning rate for different widths, and the package adjusts parameterization for consistent updates.</paragraph></list-item></list><paragraph>Extracting scaling logic</paragraph><list style=\"unordered\"><list-item><paragraph>The code shows learning rate scaling by dividing globalLR by a width multiplier for each parameter.</paragraph></list-item></list><paragraph>Browsed https://github.com/microsoft/mup/blob/main/src/mup/optim.py</paragraph><list style=\"unordered\"><list-item><paragraph>For wider layers, effective learning rate decreases, aligning with standard optimal behavior.</paragraph></list-item></list><paragraph>Browsed https://raw.githubusercontent.com/microsoft/mup/main/mup/optim.py</paragraph><paragraph>Troubleshooting tool access</paragraph><list style=\"unordered\"><list-item><paragraph>The tool for extracting code from optim.py returned empty, possibly due to an incorrect path.</paragraph></list-item></list><paragraph>Searched for \"mup optim.py github\"</paragraph><paragraph><link href=\"https://github.com/microsoft/mup\">github.com</link></paragraph><paragraph><link href=\"https://pypi.org/project/mup/0.0.0/\">pypi.org</link></paragraph><paragraph><link href=\"https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization\">cerebras.ai</link></paragraph><paragraph>Locating optim.py file</paragraph><list style=\"unordered\"><list-item><paragraph>Now checking <link href=\"https://github.com/microsoft/mup/tree/main/mup\">GitHub</link> to list files and find optim.py for lr scaling details.</paragraph></list-item></list><paragraph>Browsd https://github.com/microsoft/mup/tree/main/mup</paragraph><paragraph>Exploring alternative resources</paragraph><list style=\"unordered\"><list-item><paragraph>Now searching for simpler implementations like <link href=\"https://github.com/cloneofsimo/ezmup\">ezmup on GitHub</link> to understand muP lr scaling.</paragraph></list-item></list><paragraph>Browsed https://github.com/cloneofsimo/ezmup</paragraph><paragraph>Investigating ezmup scaling</paragraph><list style=\"unordered\"><list-item><paragraph>In ezmup, learning rate scaling uses a dictionary based on parameter widths.</paragraph></list-item></list><paragraph>Browsed https://raw.githubusercontent.com/cloneofsimo/ezmup/main/ezmup/core.py</paragraph><list style=\"unordered\"><list-item><paragraph>Let's explore <link href=\"https://blog.eleuther.ai/mutransfer/\">Eleuther AI blog</link> for muP implementation details on lr scaling.</paragraph></list-item></list><paragraph>Browsed https://blog.eleuther.ai/mutransfer/</paragraph><paragraph>Applying muP insights</paragraph><list style=\"unordered\"><list-item><paragraph>For hidden layers, muP scales learning rate by dividing by layer width to maintain update consistency.</paragraph></list-item><list-item><paragraph>The input layer is treated as input, hidden as hidden, and output as output in the MLP structure.</paragraph></list-item></list><paragraph>Finalizing muP implementation</paragraph><list style=\"unordered\"><list-item><paragraph>For biases, learning rate is scaled by multiplying by 0.01 as a fudge factor.</paragraph></list-item><list-item><paragraph>For output bias, since it's a vector, scale learning rate by 0.01, keeping changes small.</paragraph></list-item><list-item><paragraph>Scaling the output of a layer by a constant doesn't directly adjust Adam's learning rate due to normalization.</paragraph></list-item></list><paragraph>Evaluating layer scaling effects</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling a layer's output by a constant affects gradient propagation to previous layers.</paragraph></list-item><list-item><paragraph>For SignGD, scaling output by a constant keeps updates unchanged due to sign invariance.</paragraph></list-item></list><paragraph>Considering per-weight multipliers</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling layer output by a factor like sqrt(fan_in) might help make activation deltas uniform.</paragraph></list-item><list-item><paragraph>For part d, adjusting the network graph by scaling outputs can mimic muP effects with standard Adam.</paragraph></list-item></list><paragraph>Analyzing output scaling</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling a layer's output by a constant scales gradients, but Adam's normalization keeps updates unchanged.</paragraph></list-item><list-item><paragraph>Adding a multiplier after a layer doesn't change activation deltas in Adam due to scale invariance.</paragraph></list-item></list><paragraph>Clarifying multiplier effects</paragraph><list style=\"unordered\"><list-item><paragraph>For Adam, multiplying layer output by a constant doesn't alter updates due to normalization.</paragraph></list-item><list-item><paragraph>Scaling output by a constant affects next layer's input, potentially impacting update consistency.</paragraph></list-item></list><paragraph>Finalizing multiplier strategy</paragraph><list style=\"unordered\"><list-item><paragraph>To make activation deltas constant, scale layer output by sqrt(fan_out / fan_in).</paragraph></list-item><list-item><paragraph>Multiplying layer output by a constant adjusts effective learning rate for the previous layer in SGD, explaining part d equivalence.</paragraph></list-item></list><paragraph>Revisiting update behavior</paragraph><list style=\"unordered\"><list-item><paragraph>For the first step, SimpleAdam acts like SignGD, with updates based on sign of gradients.</paragraph></list-item></list><paragraph>Assessing graph adjustments</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling layer output by a constant doesn't change Adam updates due to normalization.</paragraph></list-item><list-item><paragraph>For SignGD, scaling output by a constant keeps updates unchanged, as sign is invariant.</paragraph></list-item></list><paragraph>Reevaluating multiplier impact</paragraph><list style=\"unordered\"><list-item><paragraph>Adding a constant multiplier after a layer doesn't alter activation deltas in Adam due to scale invariance.</paragraph></list-item><list-item><paragraph>Scaling output affects forward signal, altering loss and gradients for later layers.</paragraph></list-item></list><paragraph>Refining scaling approach</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling layer output by sqrt(fan_in / fan_out) may reduce pre-activation RMS for larger layers, aiding sigmoid linearity.</paragraph></list-item><list-item><paragraph>Scaling logits by large factors can make softmax peaked, reducing gradient magnitude for correct predictions.</paragraph></list-item></list><paragraph>Examining softmax behavior</paragraph><list style=\"unordered\"><list-item><paragraph>Softmax is not scale invariant; for large scaling, it approaches onehot for the maximum logit.</paragraph></list-item><list-item><paragraph>Scaling hidden activations affects logit scale, altering gradient magnitudes through backpropagation.</paragraph></list-item></list><paragraph>Adjusting forward scaling</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling pre-activation by a factor affects sigmoid derivative, impacting gradient flow.</paragraph></list-item><list-item><paragraph>Scaling post-sigmoid output by a constant directly scales activation deltas, making them adjustable for uniformity.</paragraph></list-item></list><paragraph>Finalizing scaling strategy</paragraph><list style=\"unordered\"><list-item><paragraph>Scaling pre-activation by sqrt(fan_out / fan_in) helps balance activation deltas across layers.</paragraph></list-item><list-item><paragraph>For part d, implement scaling in forward pass to ensure uniform activation deltas with standard Adam.</paragraph></list-item></list><paragraph>Completing part a answers</paragraph><list style=\"unordered\"><list-item><paragraph>The dimensions of each layer are 784 input, then hidden [8,16,64,64,64,256,256,1024], and output 10.</paragraph></list-item><list-item><paragraph>Larger layer dimensions typically reduce RMS norms of activation deltas due to scaling effects.</paragraph></list-item></list><paragraph><italic><bold>Response:</bold></italic></paragraph><paragraph>I've read the provided Jupyter notebook file (q_mup_coding.ipynb) and completed the TODO sections based on the requirements of each part. I'll structure my response by section, providing the completed code for each TODO, the answers to the questions (as they would appear in the markdown cells), and elaborations on the code and reasoning. My goal was to make the code functional, follow the safety instructions, and ensure the implementations align with the concepts of muP, norms, and optimizers as described.</paragraph><paragraph>The notebook uses a simple MLP on MNIST, with focus on activation deltas, weight updates, and hyperparameter stability across widths.</paragraph><heading level=\"2\">a. Examining the norms of a heterogenous MLP.</heading><paragraph><bold>Completed Code for TODO:</bold></paragraph><paragraph>The TODO is to call some plotting code with different widths to observe changes.</paragraph><pre># TODO: Call some plotting code here.\ntrain_one_step(optimizer=SimpleAdam, hiddens=[128, 64, 32, 16, 8])  # Reversed widths to see if RMS changes as expected\n</pre><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph><italic>Answer:</italic> The dimensions of each layer in the neural network are 784 (input) -&gt; 8 -&gt; 16 -&gt; 64 -&gt; 64 -&gt; 64 -&gt; 256 -&gt; 256 -&gt; 1024 (hidden layers) -&gt; 10 (output classes).</paragraph><paragraph><italic>Answer:</italic> The dimensionality (hidden size) affects the RMS norm of the activation deltas by making it decrease as the layer width increases. Larger layers tend to have smaller activation deltas because the gradients are diluted across more dimensions in vanilla parameterization, leading to smaller updates.</paragraph><paragraph><italic>Answer:</italic> Yes, changing the widths (e.g., to [128, 64, 32, 16, 8]) reverses the trend\u2014the RMS values increase as layer sizes decrease, confirming that wider layers have smaller RMS deltas in vanilla setup.</paragraph><paragraph><bold>Elaboration:</bold></paragraph><paragraph>The train_one_step function runs two forward passes and a gradient step, computing the RMS of activation changes between passes. The plot shows bar heights decreasing with larger hidden sizes in the default call. By reversing the widths, the plot should show increasing RMS, validating the inverse relationship with dimensionality. This is due to standard Kaiming initialization and gradient flow, where wider layers average out updates more.</paragraph><heading level=\"2\">b. Examining the norms of the updates to the weights.</heading><paragraph><bold>Completed Code for TODO:</bold></paragraph><pre>for p in delta_params:\n    if len(p.shape) == 2:\n        ### TODO: Log the respective norms.\n        frob_norms.append(torch.norm(p, 'fro').item())\n        spectral_norms.append(torch.linalg.norm(p, ord=2).item())  # spectral norm\n        induced_norms.append(torch.linalg.norm(p, ord=2).item() * np.sqrt(p.shape[1] / p.shape[0]))  # RMS-RMS induced norm\n        p_shapes.append(f\"{p.shape[1]} -&gt; {p.shape[0]}\")\n        ### \n</pre><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph><italic>Answer:</italic> The RMS-RMS induced norm correlates the most with the RMS norms of the activation deltas, as it accounts for the input/output dimensions and matches the scale of how updates propagate through activations.</paragraph><paragraph><bold>Elaboration:</bold></paragraph><list style=\"unordered\"><list-item><paragraph><bold>Frobenius norm</bold>: Measures the total magnitude of the matrix update as \u221a(sum of squares). It tends to increase with layer size because larger matrices have more elements.</paragraph></list-item><list-item><paragraph><bold>Spectral norm</bold>: The largest singular value, measuring the maximum stretch. It often stays relatively constant or decreases slightly with width.</paragraph></list-item><list-item><paragraph><bold>RMS-RMS induced norm</bold>: Calculated as spectral_norm * \u221a(fan_in / fan_out). This represents the induced norm under RMS vector norms, which aligns with how activation changes are propagated (\u03b4_act \u2248 induced_norm(\u03b4W) * rms(prev_act)).</paragraph></list-item></list><paragraph>The code clones parameters before and after the step, computes deltas, and plots only for 2D weights (ignoring biases). The correlation with induced norm comes from the propagation: activation changes depend on how the weight update \"induces\" changes in the output space from the input space.</paragraph><paragraph>The plot will show Frobenius and spectral varying with width, but induced more stable or matching the activation RMS pattern from part a.</paragraph><heading level=\"2\">c. Implementing muP</heading><paragraph><bold>Completed Code for TODO:</bold></paragraph><pre>############################\n### Todo: Adjust the per-layer learning rate scaling factor so per-layer RMS activation deltas are constant.\n### Hint for part e: The following tricks will help you retain performance when using muP scaling.\n###  - Treat biases as a hidden layer with size (d_out, 1). You will need to use a fudge-factor of around 0.01 -- we want to keep the change in bias terms low.\n###  - For the input layer, a fudge factor of 10 appears to help.\n###  - For the output layer, we find it is best to ignore the muP scaling, and instead use a fixed learning rate (e.g. 0.003).\n############################\nlr = group['lr']\nif len(p.shape) == 2:  # Weight\n    fan_in = p.shape[1]\n    fan_out = p.shape[0]\n    if fan_out == 10:  # Output layer\n        lr = 0.003\n    else:\n        lr /= fan_in\n        if fan_in == 784:  # Input layer\n            lr *= 10\nelse:  # Bias\n    fan_in = 1\n    fan_out = p.shape[0]\n    lr /= fan_in\n    lr *= 0.01  # Fudge for biases\n############################\n############################\n</pre><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph>Note: The note asks why the first 2-3 activation-deltas may have a lower norm.</paragraph><paragraph><italic>Answer:</italic> The first 2-3 activation-deltas may have lower norms because the early layers have smaller widths in the default hiddens list, leading to larger relative gradients but smaller updates after scaling. Additionally, chain effects from backpropagation and initial random activations may cause lower effective updates in early layers.</paragraph><paragraph><bold>Elaboration:</bold></paragraph><paragraph>The scaling follows muP principles for Adam: lr scaled by 1/fan_in for hidden layers to make updates independent of width. The fudges are as per the hint to stabilize training\u2014larger for input (to boost update), small for biases (to keep changes low), fixed for output (to avoid instability in logits).</paragraph><paragraph>The code applies this per parameter in the step function. The plots for the two hiddens lists should show roughly uniform bar heights for RMS deltas, unlike vanilla Adam.</paragraph><heading level=\"2\">d. Per-Weight Multipliers</heading><paragraph><bold>Completed Code for TODO:</bold></paragraph><pre>class ScaledMLP(nn.Module):\n    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n        super(ScaledMLP, self).__init__()\n        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n        self.layers = nn.ModuleList()\n        for i in range(len(all_hidden_sizes)-1):\n            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        activations = []\n        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            ## TODO\n            x *= np.sqrt(layer.out_features / layer.in_features)  # Scale preactivation by sqrt(fan_out / fan_in) to counter vanilla scaling\n            ##\n            x = self.sigmoid(x)\n            activations.append(x)\n        x = self.layers[-1](x)\n        activations = activations[1:]\n        return x, [a.detach() for a in activations]\n</pre><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph><italic>Answer:</italic> Multiplying the output of a layer by a constant is equivalent to adjusting the learning rate of that layer because it scales the gradients during backpropagation, effectively changing the update magnitude. For Adam, it's approximately equivalent due to normalization (though invariant in theory, the first-step behavior is SignGD-like, where scaling affects effective step size through chain effects). For SignGD, it scales the effective step in propagation.</paragraph><paragraph><bold>Elaboration:</bold></paragraph><paragraph>By scaling the preactivation by \u221a(fan_out / fan_in), we counter the vanilla trend where deltas ~ \u221a(fan_in / fan_out), making deltas uniform. This adjusts the graph to mimic muP without optimizer changes. The plot with SimpleAdam should now show uniform RMS deltas. The equivalence holds more for SGD-like optimizers, but works here due to the problem's SimpleAdam being SignGD-like in early steps and chain effects in the network.</paragraph><heading level=\"2\">e. Hyperparameter Transfer</heading><paragraph>No TODO in code, but the answers:</paragraph><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph><italic>Answer:</italic> In Adam, the optimal learning rate decreases as the network increases in size (e.g., from 0.01 for small widths to 0.001 for large), because larger models have higher curvature and require smaller steps for stability.</paragraph><paragraph><italic>Answer:</italic> With muP, there is a single global learning rate (e.g., around 1.0-3.0) that works for a majority of widths, as the per-layer scaling makes updates independent of width. The 256-width network achieves ~0.5 loss, comparable to Adam.</paragraph><paragraph><bold>Elaboration:</bold></paragraph><paragraph>The sweep code is provided, and with SimpleAdamMuP, the loss curves align across widths, showing hyperparameter transfer. This is muP's key benefit\u2014tune on small, transfer to large.</paragraph><heading level=\"2\">e. Shampoo and Orthogonalization (second e)</heading><paragraph><bold>Completed Code for TODO:</bold></paragraph><pre>############ TODO\nif len(m.shape) == 1:\n    u = m  # Ignore biases for this question, it's not important.\nelse:\n    U, S, Vh = torch.linalg.svd(m, full_matrices=False)\n    u = U @ Vh.T\n#############\n</pre><paragraph><bold>Answers to Questions (as in markdown cell):</bold></paragraph><paragraph><italic>Answer:</italic> For Adam, the Frobenius norm is much larger than the spectral norm for large layers (frob ~ sqrt(rank) * spectral, where rank is high). For Shampoo, the spectral norm is constant (~1), and frob = sqrt(rank), showing orthogonalization makes the update \"unit-spectral\" but frob grows with dimension.</paragraph><paragraph><italic>Answer:</italic> (Bonus) Scale u by 1 / sqrt(in / out) to make the induced RMS norm constant (e.g., 1).</paragraph><paragraph>Completed bonus code:</paragraph><pre>u = U @ Vh.T\nu *= 1 / np.sqrt(p.shape[1] / p.shape[0])  # Scale to make induced norm constant\n</pre><paragraph><bold>Elaboration:</bold></paragraph><paragraph>Shampoo orthogonalizes the momentum by taking the SVD and dropping the singular values, keeping only the directions (U V^T). This makes the update rank-preserving but with unit spectral norm. The plot shows Adam's norms varying, Shampoo's spectral constant. The bonus scales to constant induced norm, ensuring uniform propagation effects.</paragraph><paragraph>This completes the assignment. If you run the notebook with these changes, the plots and training should behave as described. Let me know if you need the full JSON notebook or further clarifications!</paragraph><heading level=\"2\">Comment</heading><paragraph>The codes are basically correct, <italic><bold>except for the last part</bold></italic>. The implementations are clean, efficient, and demonstrate a strong grasp of PyTorch mechanics, norm calculations, and hyperparameter scaling. AIs like Grok are trained on massive datasets including GitHub repositories, research papers, and educational notebooks on deep learning. They can pattern-match common experimentation techniques from similar assignments or tutorials! The AI likely \"understands\" from training data that reversing parameters is a standard way to validate scaling laws, drawing from sources like the NTK literature or muP discussions.</paragraph><paragraph>For more recently advanced Shampoo model, from my perspective, Shampoo details from KFAC/Shampoo papers are in training data. AIs compute matrix ops accurately, avoiding full_matrices=True for efficiency.</paragraph><paragraph>But as for the most recently developed model muon, Grok provided the wrong code:</paragraph><pre>if len(m.shape) == 1:\n    u = m  # Ignore biases for this question, it's not important.\nelse:\n    U, S, Vh = torch.linalg.svd(m, full_matrices=False)\n    u = U @ Vh.T\n</pre><paragraph>As the error of dimension will be thrown out. I reported the error to the grok and ask it to revise:</paragraph><paragraph><italic><bold>Prompt:</bold></italic></paragraph><pre>File c:\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n    117 @functools.wraps(func)\n    118 def decorate_context(*args, **kwargs):\n...\n---&gt; 33     u = U @ Vh.T\n     36 #############\n     37 p.add_(u, alpha=-group['lr'])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (8x8 and 784x8)\nPlease read the error log and debug about your corresponding code\n</pre><paragraph><italic><bold>Response:</bold></italic></paragraph><pre>Thank you for providing the error log. I've analyzed it step by step based on the code context (from the Shampoo implementation in the notebook) and reproduced the issue using a tool to confirm matrix shapes. This is a matrix multiplication shape mismatch in the SVD reconstruction within the SimpleShampoo optimizer's step method. Below, I'll explain the root cause, why it occurs, and how to fix it. I'll also provide the corrected code snippet.\nStep-by-Step Diagnosis\n\nError Context:\n\nThe error occurs on line u = U @ Vh.T inside the Shampoo optimizer.\nIt's a RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x8 and 784x8).\nThis happens during matrix multiplication (@ operator) where the dimensions don't align: the number of columns in the left matrix (8) must equal the number of rows in the right matrix, but here the right matrix has 784 rows.\n\n\nUnderstanding the SVD Output:\n\nIn torch.linalg.svd(m, full_matrices=False), for a momentum matrix m of shape (out_features, in_features) (e.g., (8, 784) for the first layer: 784 input to 8 hidden).\n\nU: Shape (out, min(out, in)) \u2192 (8, 8).\nS: Shape (min(out, in)) \u2192 (8).\nVh: Shape (min(out, in), in) \u2192 (8, 784). Note: Vh is already the conjugate transpose (for real matrices, just transpose) of the right singular vectors.\n\n\nThe full SVD reconstruction (with singular values) would be U @ torch.diag(S) @ Vh, which correctly yields shape (8, 784).\nIn the simplified Shampoo, we're dropping S to \"orthogonalize\" the update (keeping directions but normalizing magnitudes implicitly).\n\n\nWhy the Error Happens:\n\nThe code computes U @ Vh.T:\n\nVh.T transposes Vh from (8, 784) to (784, 8).\nNow: U (8x8) @ Vh.T (784x8) \u2192 Columns of left (8) != rows of right (784) \u2192 Mismatch!\n\n\nThis fails specifically for non-square matrices (e.g., when out_features != in_features, like in early layers: 784\u21928).\nIn square layers (e.g., 64\u219264), it might coincidentally work if dimensions align, but it's still incorrect.\nRoot cause: Unnecessary .T on Vh. PyTorch's Vh is already in the correct orientation for reconstruction (U @ ... @ Vh).\n</pre><paragraph><italic><bold>code:</bold></italic></paragraph><pre>U, S, Vh = torch.linalg.svd(m, full_matrices=False)\nu = U @ Vh  # Correct: No .T needed; Vh is already transposed in PyTorch convention\n</pre><paragraph>Now the code is correct! The dimension problem, typically, can be resolved by AI if error log is well provided. In summary, the coding power of Grok is tremendous. Especially for assignment codes, Grok, and as for many AI agent I believe, are good at it, since the instruction text and partial implemented code themselves can provide enough information for Grok to reason. Not even to mention that Grok has been trained on tons of code on public website, so it even may has seen the similar code assignment before!</paragraph><paragraph/></document>",
            "links": [
                "https://github.com/microsoft/mup",
                "https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization",
                "https://blog.speechmatics.com/mup",
                "https://blog.eleuther.ai/mutransfer/",
                "https://michal.io/wiki/Maximal-Update-Parametrization-%28%25CE%25BCP%29",
                "https://arxiv.org/html/2407.17465v1",
                "https://pureai.com/articles/2022/04/04/neural-net-training.aspx",
                "https://training-api.cerebras.ai/en/latest/wsc/Model-zoo/tutorials/mup/mup_docs.html",
                "https://howtoscalenn.github.io/",
                "https://github.com/microsoft/mup",
                "https://github.com/microsoft/mup",
                "https://pypi.org/project/mup/0.0.0/",
                "https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization",
                "https://github.com/microsoft/mup/tree/main/mup",
                "https://github.com/cloneofsimo/ezmup",
                "https://blog.eleuther.ai/mutransfer/"
            ],
            "attachments": [],
            "created_at": "2025-10-07T11:35:38.68132+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7085474,
            "author": "Zhangzhi 'csrookie' Xiong",
            "project_title": "Participation Section A: HW0 - Gemini 2.5 Pro",
            "post_body": "Participation Section A: Homework with LLM\n\nBrief Intro\n\nI'm using Gemini 2.5 Pro provided by Berkeley. The homework I use LLM to deal with is HW0 written part. The problem 1, 6 and 7 are neglected, since these problems are not targeting deep learning knowledge.\n\nInteraction on Problem 2\n\nPrompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them.\n\nAttachment: The figure of problem 2.\n\nResponse: \n\nComment: The Gemini 2.5 Pro can grasp the problem correctly from the picture and extract the core of each subproblem. It get comfortable with proving the derivative in element-wise perspective. I'm confident that it can answer the question correctly. \n\nPrompt: Please give your formal and detailed answer to this question\n\nAttachment: The figure of the problem\n\nResponse:\n\n\n\nComment: The proof of LLM is correct and rigorous! I can't agree with it more. I guess the reason why AI can prove this easily is because similar proofs are well included in the training corpus. \n\nInteraction on Problem 3\n\nPrompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them briefly.\n\nResponse: \n\nComment: For problem a, AI failed to use the derivative of MSE and choose to solve the problem directly. This is a little bit suprising for me! And for the following subproblems, AI follow its thought on a. Maybe I should hint it to consider the derivative. \n\nPrompt: From problem (a), you should consider the derivative of MSE and get the optimal answer. Please follow this instruction and give me the formal and brief answer of these problems.\n\nResponse: \n\n\n\nComment: Under my hint, now AI can solve all these subproblems! Few haluccination is presented from its answer. I suppose these problems should be quite classic, and should be easy for the advanced model Gemini 2.5 Pro. \n\nInteraction on Problem 4\n\nPrompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them briefly.\n\nAttachment: The figures of the problem. \n\nResponse: \n\n\n\nComment: Gemini successfully extracted the essences of these subproblems, and provide detailed extension for these distinct perspectives about ridge regression. For some of the problem, it seems that AI has already know the answer and tell me the conclusion of these perspectives.\n\nPrompt: Your ideas are correct. Please provide your formal answer in an accurate and brief manner.\n\nResponse: \n\n\n\nComment: All the answers to the subproblems are correct! Most impressively, Gemini handle the dimension of these complicated formulas accurately! I doubt that Gemini probably have seen these problems or topics from its corpus. \n\nInteraction on Problem 5\n\nPrompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them briefly.\n\nResponse: \n\n\n\nComment: The AI seems to haluccinate with the last problem......I will reopen a chat box, and ask it to answer the problem directly.\n\nPrompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, and give me your accurate and brief answer.\n\nComment: The problem of a, b and c are answered correctly, but d problem is not correct. Gemini fail to analyse the problem based on multiple conditions and possibility. I think this is because subproblem d is never designed before, and Gemini has to do deduction and reasoning itself, and it failed to answer the question comprehensively. \n\nSummary\n\nTotal score: 96 / 100\n\nThe accuracy of Gemini is amazing, especially for these classic problems that are highly likely to have appeared on the web forum or documents. For unseen questions, Gemini can provide inspiring insights, but still struggle to ace them all. \n\nIf input figures multiple times in one single chat, the AI may haluccinate across these figures. So for every single problem, it is suggestive to open a new box and communicate with LLM there. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/sKolFHeZPoY6qRxffjkdqwUj\" filename=\"log.pdf\"/><heading level=\"1\">Participation Section A: Homework with LLM</heading><heading level=\"2\">Brief Intro</heading><paragraph>I'm using Gemini 2.5 Pro provided by Berkeley. The homework I use LLM to deal with is HW0 written part. The problem 1, 6 and 7 are neglected, since these problems are not targeting deep learning knowledge.</paragraph><heading level=\"2\">Interaction on Problem 2</heading><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them.</bold></italic></paragraph><paragraph><italic><bold>Attachment: The figure of problem 2.</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DyVSIZuechHzIo5QcE8QevHh\" width=\"645\" height=\"791.7762326169405\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/FqueWtdwOVjITu58mf7Z3qaf\" width=\"645\" height=\"765.8867924528303\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/DbWSkICC5dYWfqU1Pnj9Z5UE\" width=\"645\" height=\"765.4764930114358\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/6YNVOG0XR9pOCQFaF9OU9EeF\" width=\"645\" height=\"360.2480916030534\"/></figure><paragraph><italic><bold>Comment: The Gemini 2.5 Pro can grasp the problem correctly from the picture and extract the core of each subproblem. It get comfortable with proving the derivative in element-wise perspective. I'm confident that it can answer the question correctly.</bold></italic> </paragraph><paragraph><italic><bold>Prompt: Please give your formal and detailed answer to this question</bold></italic></paragraph><paragraph><italic><bold>Attachment: The figure of the problem</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/vURxvLitdmSealCZFlgmKtXI\" width=\"645\" height=\"813.831003811944\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/MXdWVFvVv4RsD0oao8gkziP6\" width=\"645\" height=\"806.0445859872611\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/EBM00gSK1hw2tUbCXeBJTlKa\" width=\"645\" height=\"775.3196930946292\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/IiDrnf6t7jLeYeYSe6l4OfEe\" width=\"645\" height=\"805.41015625\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/lluCfIlznX6qxquYOJ0BHlsK\" width=\"645\" height=\"126.34020618556701\"/></figure><paragraph/><paragraph><italic><bold>Comment: The proof of LLM is correct and rigorous! I can't agree with it more. I guess the reason why AI can prove this easily is because similar proofs are well included in the training corpus.</bold></italic> </paragraph><heading level=\"2\">Interaction on Problem 3</heading><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them <underline>briefly</underline>.</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/qmEXTIZbVDI6NBejE87dPLkk\" width=\"645\" height=\"765.4764930114358\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/OOsC8N84UhuDivVCoaswQgJ9\" width=\"645\" height=\"823.6658195679796\"/></figure><paragraph><italic><bold>Comment: For problem a, AI failed to use the derivative of MSE and choose to solve the problem directly. This is a little bit suprising for me! And for the following subproblems, AI follow its thought on a. Maybe I should hint it to consider the derivative.</bold></italic> </paragraph><paragraph><italic><bold>Prompt: From problem (a), you should consider the derivative of MSE and get the optimal answer. Please follow this instruction and give me the formal and brief answer of these problems.</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2u1mccIYzZVFg7BlXts6Vgef\" width=\"645\" height=\"790.4887218045113\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/CIkZnadV6ghLjEMiiiYILMKK\" width=\"645\" height=\"834.7058823529412\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/86RKdJSG8vNeUp3P0ssiKEOw\" width=\"645\" height=\"730.4522292993631\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/TncAWVsf1H1f6zM8w31h6pF9\" width=\"645\" height=\"738.5882352941177\"/></figure><paragraph/><paragraph><italic><bold>Comment: Under my hint, now AI can solve all these subproblems! Few haluccination is presented from its answer. I suppose these problems should be quite classic, and should be easy for the advanced model Gemini 2.5 Pro.</bold></italic> </paragraph><heading level=\"2\">Interaction on Problem 4</heading><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them <underline>briefly</underline>.</bold></italic></paragraph><paragraph><italic><bold>Attachment: The figures of the problem.</bold></italic> </paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/57RtJSenVoKA1hs2kzCiu7Ep\" width=\"645\" height=\"758.3438685208597\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/jZl1kwlc3wGmhqA7JcWYS3oL\" width=\"645\" height=\"822.84625158831\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/22HJGZyEruV33tq2dLBTB7xj\" width=\"645\" height=\"822.84625158831\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/DaXN4aL2eHlJ8rdWnVWJhE9F\" width=\"645\" height=\"355.919689119171\"/></figure><paragraph/><paragraph><italic><bold>Comment: Gemini successfully extracted the essences of these subproblems, and provide detailed extension for these distinct perspectives about ridge regression. For some of the problem, it seems that AI has already know the answer and tell me the conclusion of these perspectives.</bold></italic></paragraph><paragraph><italic><bold>Prompt: Your ideas are correct. Please provide your formal answer in an accurate and brief manner.</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/FnQjdOiVRfLr1BWY4mh5PE6t\" width=\"645\" height=\"844.7863818424566\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/t0cjgG5eX0ZSYHBez0rFz7wE\" width=\"645\" height=\"719.4230769230769\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/hma5m7VtR4N7CiyWSiaBT2cB\" width=\"645\" height=\"819.8593350383632\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/fLdzUgPiDaJaoLIxnDbELAna\" width=\"645\" height=\"575.3836150845253\"/></figure><paragraph/><paragraph><italic><bold>Comment: All the answers to the subproblems are correct! Most impressively, Gemini handle the dimension of these complicated formulas accurately! I doubt that Gemini probably have seen these problems or topics from its corpus.</bold></italic> </paragraph><heading level=\"2\">Interaction on Problem 5</heading><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, interpret them, and then tell me idea about them <underline>briefly</underline>.</bold></italic></paragraph><paragraph><italic><bold>Response:</bold></italic> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/zCSsjdjQsSMfbM44j2Hn1Sar\" width=\"645\" height=\"800.71791613723\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/C6uQa2Us9vzkGhGrTmnKPfjx\" width=\"645\" height=\"810.7405063291139\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/zgAX14lHhQS6TcQrxFO1ek9T\" width=\"645\" height=\"840.4043645699614\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/rGa5rda7UzZURRbcwPmJFVB7\" width=\"645\" height=\"331.3598901098901\"/></figure><paragraph/><paragraph><italic><bold>Comment: The AI seems to haluccinate with the last problem......I will reopen a chat box, and ask it to answer the problem directly.</bold></italic></paragraph><paragraph><italic><bold>Prompt: I'm a student learning deep learning. Please read this picture and extract the problem. Read the problem carefully, and give me your accurate and brief answer.</bold></italic></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/aeS9Xc9SyvYl4CBubbCmQftZ\" width=\"645\" height=\"814.7368421052631\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/wILCI1GE6lE2mURteb8RD0v8\" width=\"645\" height=\"805.6353240152478\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/WHebTRNssQgJDfkdy6SE2Rsd\" width=\"645\" height=\"786.9811320754717\"/></figure><paragraph><italic><bold>Comment: The problem of a, b and c are answered correctly, but d problem is not correct. Gemini fail to analyse the problem based on multiple conditions and possibility. I think this is because subproblem d is never designed before, and Gemini has to do deduction and reasoning itself, and it failed to answer the question comprehensively.</bold></italic> </paragraph><heading level=\"2\">Summary</heading><blockquote>Total score: 96 / 100</blockquote><paragraph>The accuracy of Gemini is amazing, especially for these classic problems that are highly likely to have appeared on the web forum or documents. For unseen questions, Gemini can provide inspiring insights, but still struggle to ace them all. </paragraph><paragraph>If input figures multiple times in one single chat, the AI may haluccinate across these figures. So for every single problem, it is suggestive to open a new box and communicate with LLM there. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T11:26:34.38945+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7083805,
            "author": "Andy Zhang",
            "project_title": "Special Participation A: Qwen3-Max on HW0",
            "post_body": "Here is the online link: https://chat.qwen.ai/s/6240e96b-585c-4943-870d-3af47859ec5f?fev=0.0.222 Here is annotated log: https://drive.google.com/file/d/1vVCY_yCtDPNHagaoSDGR6BSnjG78wVpI/view?usp=sharing Executive Summary: Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed steps. This was especially the case for \u201c4. The 5 Interpretations of Ridge Regression\u201d where you want to show how the concepts tie together. Prompting it repeatedly to show more detailed steps helped.\n\nIt\u2019s also important to be careful about pasting in from the PDF as the copy paste can lead the model to parse text incorrectly, where I needed to correct the model to consider \u201c||w||^2 not ||w||_2\u201d.\n\nFinally, it struggled greatly with \u201c5. ReLU Elbow Update under SGD\u201d. In particular, it attempted to draw and label using ascii art, but the plots were messed up and even after repeated prompting, they are still not presentable.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is the online link: https://chat.qwen.ai/s/6240e96b-585c-4943-870d-3af47859ec5f?fev=0.0.222 Here is annotated log: https://drive.google.com/file/d/1vVCY_yCtDPNHagaoSDGR6BSnjG78wVpI/view?usp=sharing Executive Summary: Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed steps. This was especially the case for \u201c4. The 5 Interpretations of Ridge Regression\u201d where you want to show how the concepts tie together. Prompting it repeatedly to show more detailed steps helped.</paragraph><paragraph>It\u2019s also important to be careful about pasting in from the PDF as the copy paste can lead the model to parse text incorrectly, where I needed to correct the model to consider \u201c||w||^2 not ||w||_2\u201d.</paragraph><paragraph>Finally, it struggled greatly with \u201c5. ReLU Elbow Update under SGD\u201d. In particular, it attempted to draw and label using ascii art, but the plots were messed up and even after repeated prompting, they are still not presentable.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T08:03:27.722243+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7082498,
            "author": "Lance Mathias",
            "project_title": "Discussion 5 Solutions",
            "post_body": "Discussion 5 and solutions:",
            "content_xml": "<document version=\"2.0\"><paragraph>Discussion 5 and solutions:</paragraph><file url=\"https://static.us.edusercontent.com/files/5WmgHtlfbJFd5GQ3f0CXXMcF\" filename=\"dis05_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/V2rHn2gOW286fbRrh1sr7U35\" filename=\"dis05_solution.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T05:20:59.608758+11:00",
            "category": "Sections"
        },
        {
            "guid": 7082392,
            "author": "Hong Joey",
            "project_title": "HW5 Old Exam Problem 2",
            "post_body": "Context: This old exam problem is meant to give you some concrete experience with what batch normalization does.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This old exam problem is meant to give you some concrete experience with what batch normalization does.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/VpwArf1ZrbSRSxogIKKUBnxV\" width=\"658\" height=\"291.44292237442926\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/KfDZPCvKDHDYYROHaSOZMuFr\" width=\"658\" height=\"769.870170015456\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T05:05:18.935207+11:00",
            "category": "Admin"
        },
        {
            "guid": 7082367,
            "author": "Hong Joey",
            "project_title": "HW5 Old Exam Problem 1",
            "post_body": "We are adding threads for the old exam problems at the end of the homework. These are simply for your reference, but feel free to ask questions in this thread:\n\nContext: This old exam problem studies an alternative approach to \"augmentation\" at intermediate layers.",
            "content_xml": "<document version=\"2.0\"><paragraph>We are adding threads for the old exam problems at the end of the homework. These are simply for your reference, but feel free to ask questions in this thread:</paragraph><paragraph>Context: This old exam problem studies an alternative approach to \"augmentation\" at intermediate layers.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/gKcO29GALUvWNAtMneBWpuag\" width=\"658\" height=\"521.6661870503597\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-07T05:02:59.641145+11:00",
            "category": "Admin"
        },
        {
            "guid": 7079059,
            "author": "Deena Sun",
            "project_title": "Participation B: HW 4 - Cursor",
            "post_body": "\n\nExecutive Summary\n\nI worked with Cursor on coding questions 5 and 6 of homework 4. Cursor tackled the TODOs in this homework very effectively\u2014it even caught an error where it used a `transforms` variable when instantiated an `EdgeDetectionDataset` before `transforms` was defined, and fixed it in the same turn. Also, Cursor\u2019s vision capabilities are very strong. It could easily interpret visualizations I included in the prompts, which was helpful for questions in this homework that asked to evaluate kernel visualizations or compare training/validation performance of different architectures. One of the most helpful things Cursor included in its responses was outside information on the trade-offs of different architecture/hyperparameter choices, especially during questions on hyperparameter tuning. I think this shows an example of how using AI coding tools effectively can not just help one improve their machine learning code for one situation, but help them learn about training paradigms/best practices to grow as a researcher/engineer. The AI coding partner equivalent of teaching someone to fish and feeding them for a lifetime.\n\n\n\nAnnotated Logs\n\nQuestion 5: Designing 2D Filters\n\nQuestion 6: Inductive Bias of CNNs",
            "content_xml": "<document version=\"2.0\"><paragraph/><paragraph><bold>Executive Summary</bold></paragraph><paragraph>I worked with Cursor on coding questions 5 and 6 of homework 4. Cursor tackled the TODOs in this homework very effectively\u2014it even caught an error where it used a `transforms` variable when instantiated an `EdgeDetectionDataset` before `transforms` was defined, and fixed it in the same turn. Also, Cursor\u2019s vision capabilities are very strong. It could easily interpret visualizations I included in the prompts, which was helpful for questions in this homework that asked to evaluate kernel visualizations or compare training/validation performance of different architectures. One of the most helpful things Cursor included in its responses was outside information on the trade-offs of different architecture/hyperparameter choices, especially during questions on hyperparameter tuning. I think this shows an example of how using AI coding tools effectively can not just help one improve their machine learning code for one situation, but help them learn about training paradigms/best practices to grow as a researcher/engineer. The AI coding partner equivalent of teaching someone to fish and feeding them for a lifetime.</paragraph><paragraph/><paragraph><bold>Annotated Logs</bold></paragraph><paragraph>Question 5: Designing 2D Filters</paragraph><file url=\"https://static.us.edusercontent.com/files/64PmMm4kmJ2VQmWOhR9xXp0R\" filename=\"cs182_hw4_q5_cursor.pdf\"/><paragraph>Question 6: Inductive Bias of CNNs</paragraph><file url=\"https://static.us.edusercontent.com/files/1RWjIrsDfmneY8uQFN4oR0nx\" filename=\"cs182_hw4_q6_cursor.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-06T13:42:07.59673+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7077280,
            "author": "Wesley Kai Zheng",
            "project_title": "Special Participation B: Grok with Fast Mode on HW1",
            "post_body": "Here\u2019s the online link to our conversation: https://grok.com/share/c2hhcmQtNA%3D%3D_022c3b42-efd7-401d-b403-f216932c1c91\n\nHere\u2019s the annotated log (PDF): https://drive.google.com/file/d/1oKPBzCc9KNtUOqok7Xixqgccg2V6VWAV/view?usp=sharing \n\nHere\u2019s the Python file generated by Grok (unnecessary for it to do so, since it could have simply provided code snippets. In fact, the snippets themselves are a bit buggy, but I\u2019m including the file here for transparency): https://drive.google.com/file/d/1mH2D93vh20FGKXsGpTb_HLfnXEA5kC3x/view?usp=sharing\n\nExecutive Summary\n\nFor Grok, the model generally does well on simple coding questions. I can imagine many people have already asked it similar questions for solving deep learning class problems (e.g., Stanford or elsewhere). The methods here\u2014GD and GD with momentum\u2014aren\u2019t new concepts, so the LLMs were likely trained on them. Grok one-shotted most of the answers and got the main ideas correct.\n\nFor Question 2, it successfully generated code to make GD with momentum converge faster. However, the issue is that it doesn\u2019t behave like a human would. A human would likely just copy the code from the previous part, tweak the parameters, and test whether the model converges faster. Instead, Grok wrote entirely new variables for this purpose. Both approaches work, but because Grok introduced new variables, the later graph-comparison code failed\u2014since it reused the original inefficient parameters instead of the updated ones.\n\nThis likely happened because I didn\u2019t include the later code. From a human perspective, we would only read what comes before the question, not anything beyond it yet.\n\nOn the positive side, there were no hallucinations.",
            "content_xml": "<document version=\"2.0\"><paragraph>Here\u2019s the online link to our conversation: <link href=\"https://grok.com/share/c2hhcmQtNA%3D%3D_022c3b42-efd7-401d-b403-f216932c1c91\">https://grok.com/share/c2hhcmQtNA%3D%3D_022c3b42-efd7-401d-b403-f216932c1c91</link></paragraph><paragraph>Here\u2019s the annotated log (PDF): <link href=\"https://drive.google.com/file/d/1oKPBzCc9KNtUOqok7Xixqgccg2V6VWAV/view?usp=sharing\">https://drive.google.com/file/d/1oKPBzCc9KNtUOqok7Xixqgccg2V6VWAV/view?usp=sharing</link> </paragraph><paragraph>Here\u2019s the Python file generated by Grok (unnecessary for it to do so, since it could have simply provided code snippets. In fact, the snippets themselves are a bit buggy, but I\u2019m including the file here for transparency): <link href=\"https://drive.google.com/file/d/1mH2D93vh20FGKXsGpTb_HLfnXEA5kC3x/view?usp=sharing\">https://drive.google.com/file/d/1mH2D93vh20FGKXsGpTb_HLfnXEA5kC3x/view?usp=sharing</link></paragraph><paragraph><bold>Executive Summary</bold></paragraph><paragraph>For Grok, the model generally does well on simple coding questions. I can imagine many people have already asked it similar questions for solving deep learning class problems (e.g., Stanford or elsewhere). The methods here\u2014GD and GD with momentum\u2014aren\u2019t new concepts, so the LLMs were likely trained on them. Grok one-shotted most of the answers and got the main ideas correct.</paragraph><paragraph>For Question 2, it successfully generated code to make GD with momentum converge faster. However, the issue is that it doesn\u2019t behave like a human would. A human would likely just copy the code from the previous part, tweak the parameters, and test whether the model converges faster. Instead, Grok wrote entirely new variables for this purpose. Both approaches work, but because Grok introduced new variables, the later graph-comparison code failed\u2014since it reused the original inefficient parameters instead of the updated ones.</paragraph><paragraph>This likely happened because I didn\u2019t include the later code. From a human perspective, we would only read what comes before the question, not anything beyond it yet.</paragraph><paragraph>On the positive side, there were no hallucinations.</paragraph></document>",
            "links": [
                "https://grok.com/share/c2hhcmQtNA%3D%3D_022c3b42-efd7-401d-b403-f216932c1c91",
                "https://drive.google.com/file/d/1oKPBzCc9KNtUOqok7Xixqgccg2V6VWAV/view?usp=sharing",
                "https://drive.google.com/file/d/1mH2D93vh20FGKXsGpTb_HLfnXEA5kC3x/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-10-06T09:13:57.875689+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7077134,
            "author": "Deena Sun",
            "project_title": "Participation A: HW 3 - Kimi",
            "post_body": "Executive Summary\n\nI tackled homework 3 with Kimi 2. This was my first time trying Kimi\u2019s family of LLMs and overall, I was quite impressed with Kimi\u2019s derivation capabilities. For the most part, I successfully one-shotted question 1: Maximal Update Parameterization with Kimi 2.\n\nFor question 3: Maximal Update Parameterization Research, Kimi often hallucinated with visualizations and technical details. When interpreting figure 1, Kimi 2 hallucinated and guessed what the two loss curves represented. Kimi 1.5 (with vision capabilities) did a better job actually explaining that the right panel was proof of \u03bcTransfer\u2019s improvement over standard practice, but incorrectly asserted that even with \u03bcTransfer, wider models couldn\u2019t achieve a lower loss. Kimi 1.5 exhibited persistent issues trying to correctly interpret the different colors in table 3 and how they corresponded to standard practice versus \u03bcTransfer. I also noticed that throughout this assignment, Kimi interpreted the induced RMS matrix norm as simply a reformulation of the vector RMS norm that treated the matrix as a flattened vector. While it got the main idea of how the 2 desiderata in the paper ensured that the l2-norms of each hidden layer/hidden layer update would scale proportionally to \u0398(\u221a n\u2113), it didn\u2019t use the RMS-to-RMS matrix norm we covered in lecture.\n\nWhen approaching question 4: Policy Gradient and the Reparameterization Gradient Estimator, I used Kimi a lot to clarify additional questions I had about taking derivatives with expectations and score functions. I felt like Kimi had more of a tendency to plug formulae/identities in its derivations here without providing the reasoning or the \u201cwhy\u201d behind how they fit into the solution. Whereas I would have liked more explicit explanations so that I could extrapolate these concepts to other problems.\n\nLastly, Kimi\u2019s responses for question 5: Tensor Rematerialization were pretty clear to follow. Kimi did have one hiccup where it kept claiming that we would only need 8 loadmems instead of 10, and unlike in question 3, it was much more assertive. It persisted this mistake even in subpart 5(c).\n\nAll in all, I think Kimi demonstrated strong mathematical derivation reasoning, but has clear areas of improvement for visual understanding and adapting to feedback. As a side note, Kimi also seems to suffer from random switch-ups in markdown formatting and style that I\u2019ve noticed in other LLMs.\n\nAnnotated logs for each question: to make things more organized, I'll make separate comments for each question!\n\nQuestion 1: https://edstem.org/us/courses/84647/discussion/7077134?comment=16480696\n\nQuestion 3: https://edstem.org/us/courses/84647/discussion/7077134?comment=16480725\n\nQuestion 4: https://edstem.org/us/courses/84647/discussion/7077134?comment=16480735\n\nQuestion 5: https://edstem.org/us/courses/84647/discussion/7077134?comment=16480774\n\nFull PDF",
            "content_xml": "<document version=\"2.0\"><paragraph><bold>Executive Summary</bold></paragraph><paragraph>I tackled homework 3 with Kimi 2. This was my first time trying Kimi\u2019s family of LLMs and overall, I was quite impressed with Kimi\u2019s derivation capabilities. For the most part, I successfully one-shotted question 1: Maximal Update Parameterization with Kimi 2.</paragraph><paragraph>For question 3: Maximal Update Parameterization Research, Kimi often hallucinated with visualizations and technical details. When interpreting figure 1, Kimi 2 hallucinated and guessed what the two loss curves represented. Kimi 1.5 (with vision capabilities) did a better job actually explaining that the right panel was proof of \u03bcTransfer\u2019s improvement over standard practice, but incorrectly asserted that even with \u03bcTransfer, wider models couldn\u2019t achieve a lower loss. Kimi 1.5 exhibited persistent issues trying to correctly interpret the different colors in table 3 and how they corresponded to standard practice versus \u03bcTransfer. I also noticed that throughout this assignment, Kimi interpreted the induced RMS matrix norm as simply a reformulation of the vector RMS norm that treated the matrix as a flattened vector. While it got the main idea of how the 2 desiderata in the paper ensured that the l2-norms of each hidden layer/hidden layer update would scale proportionally to \u0398(\u221a n\u2113), it didn\u2019t use the RMS-to-RMS matrix norm we covered in lecture.</paragraph><paragraph>When approaching question 4: Policy Gradient and the Reparameterization Gradient Estimator, I used Kimi a lot to clarify additional questions I had about taking derivatives with expectations and score functions. I felt like Kimi had more of a tendency to plug formulae/identities in its derivations here without providing the reasoning or the \u201cwhy\u201d behind how they fit into the solution. Whereas I would have liked more explicit explanations so that I could extrapolate these concepts to other problems.</paragraph><paragraph>Lastly, Kimi\u2019s responses for question 5: Tensor Rematerialization were pretty clear to follow. Kimi did have one hiccup where it kept claiming that we would only need 8 loadmems instead of 10, and unlike in question 3, it was much more assertive. It persisted this mistake even in subpart 5(c).</paragraph><paragraph>All in all, I think Kimi demonstrated strong mathematical derivation reasoning, but has clear areas of improvement for visual understanding and adapting to feedback. As a side note, Kimi also seems to suffer from random switch-ups in markdown formatting and style that I\u2019ve noticed in other LLMs.</paragraph><paragraph><bold>Annotated logs for each question</bold>: to make things more organized, I'll make separate comments for each question!</paragraph><list style=\"bullet\"><list-item><paragraph>Question 1: <link href=\"https://edstem.org/us/courses/84647/discussion/7077134?comment=16480696\">https://edstem.org/us/courses/84647/discussion/7077134?comment=16480696</link></paragraph></list-item><list-item><paragraph>Question 3: <link href=\"https://edstem.org/us/courses/84647/discussion/7077134?comment=16480725\">https://edstem.org/us/courses/84647/discussion/7077134?comment=16480725</link></paragraph></list-item><list-item><paragraph>Question 4: <link href=\"https://edstem.org/us/courses/84647/discussion/7077134?comment=16480735\">https://edstem.org/us/courses/84647/discussion/7077134?comment=16480735</link></paragraph></list-item><list-item><paragraph>Question 5: <link href=\"https://edstem.org/us/courses/84647/discussion/7077134?comment=16480774\">https://edstem.org/us/courses/84647/discussion/7077134?comment=16480774</link></paragraph></list-item></list><paragraph><bold>Full PDF</bold></paragraph><file url=\"https://static.us.edusercontent.com/files/oB8VrbeptXvw2k7i2tCDa5Ue\" filename=\"deenasun_cs182_participation_a.pdf\"/></document>",
            "links": [
                "https://edstem.org/us/courses/84647/discussion/7077134?comment=16480696",
                "https://edstem.org/us/courses/84647/discussion/7077134?comment=16480725",
                "https://edstem.org/us/courses/84647/discussion/7077134?comment=16480735",
                "https://edstem.org/us/courses/84647/discussion/7077134?comment=16480774"
            ],
            "attachments": [],
            "created_at": "2025-10-06T08:50:54.954978+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7074543,
            "author": "Wesley Kai Zheng",
            "project_title": "Special Participation A: Deepseek with Deep Thinking on HW0",
            "post_body": "Here is the online link: https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0\n\nHere is my annotated version of the log: https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing\n\nExecutive Summary:\nFrom my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning. However, even after some targeted prompt engineering, it often fails to provide a detailed explanation of its reasoning and solution in the actual response to the user. At times, it even skips crucial steps in deriving mathematical expressions. For example, when asked for the minimum norm solution of ridge regression, it outputs the correct expression but provides little explanation of how it was derived.\n\nMost of the time, the model answers in a single step rather than engaging in a back-and-forth process to verify whether its solution is fully correct. This one-shot approach can be acceptable for straightforward problems, but it does cause errors in more complex cases. This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question.\n\nOn the positive side, I did not notice any hallucinations so far.",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is the online link: <link href=\"https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0\">https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0</link></paragraph><paragraph>Here is my annotated version of the log: <link href=\"https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing\">https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing</link></paragraph><paragraph><bold>Executive Summary:</bold><break/>From my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning. However, even after some targeted prompt engineering, it often fails to provide a detailed explanation of its reasoning and solution in the actual response to the user. At times, it even skips crucial steps in deriving mathematical expressions. For example, when asked for the minimum norm solution of ridge regression, it outputs the correct expression but provides little explanation of how it was derived.</paragraph><paragraph>Most of the time, the model answers in a single step rather than engaging in a back-and-forth process to verify whether its solution is fully correct. This one-shot approach can be acceptable for straightforward problems, but it does cause errors in more complex cases. This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question.</paragraph><paragraph>On the positive side, I did not notice any hallucinations so far.</paragraph></document>",
            "links": [
                "https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0",
                "https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing"
            ],
            "attachments": [],
            "created_at": "2025-10-05T19:56:43.327041+11:00",
            "category": "Curiosity"
        },
        {
            "guid": 7069997,
            "author": "Hong Joey",
            "project_title": "HW5 Q6",
            "post_body": "Context: This is an involved coding problem. It has many components to it and is designed to get you to implement certain key operations by hand as a way to help you understand them better. The final part will require GPUs to train in feasible time (up to a few hours), so using the GPU runtime via colab is recommended.  ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This is an involved coding problem. It has many components to it and is designed to get you to implement certain key operations by hand as a way to help you understand them better. The final part will require GPUs to train in feasible time (up to a few hours), so using the GPU runtime via colab is recommended.  </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ZKTDuLNhNuXABcawceNG8cMO\" width=\"657.9999999999999\" height=\"647.5721077654516\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T17:08:40.573642+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7069993,
            "author": "Hong Joey",
            "project_title": "HW5 Q5",
            "post_body": "Context: This coding question asks you to empirically analyze the effect of dropout in a simplified linear regression setting.",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This coding question asks you to empirically analyze the effect of dropout in a simplified linear regression setting.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/EIMqglXiSqmxbm9HMtbyJgK5\" width=\"657.9999999999999\" height=\"559.701219512195\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T17:06:25.833413+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7069988,
            "author": "Hong Joey",
            "project_title": "HW5 Q4",
            "post_body": "Context: This was an old exam problem. It makes you see the inductive bias of dropout in a manner where the math is a little heavier, but you can connect to things that you understand well, namely ridge regression. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This was an old exam problem. It makes you see the inductive bias of dropout in a manner where the math is a little heavier, but you can connect to things that you understand well, namely ridge regression. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/NIdeLd7wgQcDrp2jq3D5UAmU\" width=\"658\" height=\"550.3455657492354\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/hJ7sdgAb8iIdwZeWKaiLgL8d\" width=\"658\" height=\"309.16846986089644\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T17:05:11.913711+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7069984,
            "author": "Hong Joey",
            "project_title": "HW5 Q3",
            "post_body": "Context: This is an old exam problem that asks you to understand the basics of depth-wise convolutions. You will need to understand the structure of these convolutions, and how they can result in a reduction in learnable parameters compared to traditional convolutions.  \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This is an old exam problem that asks you to understand the basics of depth-wise convolutions. You will need to understand the structure of these convolutions, and how they can result in a reduction in learnable parameters compared to traditional convolutions.  </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/nlOzha6xhxpDwYGbd6g50agu\" width=\"658\" height=\"561.3562499999999\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/MtCJSL4dEc97DtQCTegbpalY\" width=\"658\" height=\"329.5022900763359\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T17:03:14.439853+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7069975,
            "author": "Hong Joey",
            "project_title": "HW5 Q2",
            "post_body": "Context: First, this problem asks you to understand the key difference between batch normalization and layer normalization, namely in the dimension over which statistics are computed. Then, you will consider a simplified batch normalization layer and understand how back-propagation is affected. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: First, this problem asks you to understand the key difference between batch normalization and layer normalization, namely in the dimension over which statistics are computed. Then, you will consider a simplified batch normalization layer and understand how back-propagation is affected. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/VXRN2TXvryioAxfe5oUnoAKX\" width=\"658\" height=\"293\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/TtUCA0Hp1w7lQpAk99cYp36P\" width=\"658\" height=\"310\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T17:00:04.978819+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7069967,
            "author": "Hong Joey",
            "project_title": "HW5 Q1",
            "post_body": "Context: This is an old exam problem that asks you to think about convolutional layers from a different angle. First, you will be asked to derive the filter itself from output and input. Second, you will need to understand how the idea of convolutional filters can be used for upsampling  (this is called \"transpose convolutions\") instead of the downsampling. This is important to understand as we may not get much coverage of transpose convolutions in this course.\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This is an old exam problem that asks you to think about convolutional layers from a different angle. First, you will be asked to derive the filter itself from output and input. Second, you will need to understand how the idea of convolutional filters can be used for upsampling  (this is called \"transpose convolutions\") instead of the downsampling. This is important to understand as we may not get much coverage of transpose convolutions in this course.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/0cInOmWVdW7iqrFul90b3lCU\" width=\"658\" height=\"429.538644470868\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-04T16:55:41.313748+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7062286,
            "author": "Anant Sahai",
            "project_title": "Lectures 8 and 9",
            "post_body": "Use this thread to ask questions about conv-net basics introduced in these two lectures. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/GGl7Yolh9Xa3rcj6Vtl6dYR2\" filename=\"Lecture 8.pdf\"/><file url=\"https://static.us.edusercontent.com/files/2gZ9ghp1Q4bzsTyAlYQqP1Kw\" filename=\"Lecture 9.pdf\"/><paragraph>Use this thread to ask questions about conv-net basics introduced in these two lectures. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-03T09:10:10.954458+10:00",
            "category": "Lectures"
        },
        {
            "guid": 7062276,
            "author": "Anant Sahai",
            "project_title": "Lecture 10 and link",
            "post_body": "https://www.youtube.com/watch?v=T5yhCKwGoyk\n\nSorry for not being able to be there in person today...\n\nUse this thread to ask lecture-specific questions.",
            "content_xml": "<document version=\"2.0\"><paragraph><link href=\"https://www.youtube.com/watch?v=T5yhCKwGoyk\">https://www.youtube.com/watch?v=T5yhCKwGoyk</link></paragraph><paragraph>Sorry for not being able to be there in person today...</paragraph><file url=\"https://static.us.edusercontent.com/files/qQy6iPBiN1OHAh21vEwG50ph\" filename=\"Lecture 10.pdf\"/><paragraph>Use this thread to ask lecture-specific questions.</paragraph></document>",
            "links": [
                "https://www.youtube.com/watch?v=T5yhCKwGoyk"
            ],
            "attachments": [],
            "created_at": "2025-10-03T09:09:00.874153+10:00",
            "category": "Lectures"
        },
        {
            "guid": 7061441,
            "author": "Sammie Smith",
            "project_title": "Participation E: Automatic Glossary of Keywords Introduced in Lecture",
            "post_body": "Hi everyone,\n\nI put together a small website that automatically builds a glossary from our lecture transcripts using (free!!!) API calls from Gemini-2.5-flash. The goal is to make it easier to keep track of technical terms, see plain-English definitions, and get beginner-friendly explanations of how those terms show up in machine learning and deep learning.\n\n\ud83d\udc49 Website link: https://sammiesmith.github.io/DL_glossaries_Smith.github.io/ \n\nHow to use the site\n\nThe Glossary page lists all the terms extracted from Lecture 1 (so far).\n\nYou can search using the bar in the top-right (search is limited to the \u201cTerm\u201d column).\n\nEach entry shows:\n\nthe plain-English definition,\n\nthe prerequisite material reference (when available),\n\nand a description of how the term is used in ML/DL.\n\nThe Methods page explains how the glossary is generated (pipeline, prompts, limitations, and ideas for improvement).\n\nWhy I built this\n\nSometimes the hardest part of lecture is just keeping all the new vocabulary straight. My hope is this tool can give quick, reliable definitions and context, and eventually we could add features like quiz-me buttons for studying.\n\nFeel free to explore it and let me know if you run into bugs or have ideas for features that would make it more useful!\n\n\u2014 Sammie",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>I put together a small website that automatically builds a <bold>glossary from our lecture transcripts</bold> using (free!!!) API calls from Gemini-2.5-flash. The goal is to make it easier to keep track of technical terms, see plain-English definitions, and get beginner-friendly explanations of how those terms show up in machine learning and deep learning.</paragraph><paragraph>\ud83d\udc49 <bold>Website link:</bold> <link href=\"https://sammiesmith.github.io/DL_glossaries_Smith.github.io/\">https://sammiesmith.github.io/DL_glossaries_Smith.github.io/</link> </paragraph><heading level=\"3\">How to use the site</heading><list style=\"unordered\"><list-item><paragraph>The <bold>Glossary page</bold> lists all the terms extracted from Lecture 1 (so far).</paragraph><list style=\"unordered\"><list-item><paragraph>You can <bold>search</bold> using the bar in the top-right (search is limited to the \u201cTerm\u201d column).</paragraph></list-item><list-item><paragraph>Each entry shows:</paragraph><list style=\"unordered\"><list-item><paragraph>the plain-English definition,</paragraph></list-item><list-item><paragraph>the prerequisite material reference (when available),</paragraph></list-item><list-item><paragraph>and a description of how the term is used in ML/DL.</paragraph></list-item></list></list-item></list></list-item><list-item><paragraph>The <bold>Methods page</bold> explains how the glossary is generated (pipeline, prompts, limitations, and ideas for improvement).</paragraph></list-item></list><heading level=\"3\">Why I built this</heading><paragraph>Sometimes the hardest part of lecture is just keeping all the new vocabulary straight. My hope is this tool can give quick, reliable definitions and context, and eventually we could add features like quiz-me buttons for studying.</paragraph><paragraph>Feel free to explore it and let me know if you run into bugs or have ideas for features that would make it more useful!</paragraph><paragraph>\u2014 Sammie</paragraph></document>",
            "links": [
                "https://sammiesmith.github.io/DL_glossaries_Smith.github.io/"
            ],
            "attachments": [],
            "created_at": "2025-10-03T06:58:36.91518+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7049136,
            "author": "Bruno Vieira",
            "project_title": "Special Participation A - Grok HW3",
            "post_body": "REFLECTION\n\n\u200b\u200bIn completing the non-coding parts of the homework with Grok, I found that it could one-shot questions about 70\u201380% of the time. Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D. This showed that while hints can significantly improve accuracy, the model is capable of reasoning independently in some cases. I also noticed that the model tended to give very long and verbose responses, often providing explanations two to three pages long, even when a short answer would have sufficed. This happened especially when I asked follow-up questions, and Grok would completely lose focus.\n\nOne particularly striking behavior was how Grok interacted over the course of the chat. When I gave feedback\u2014acknowledging correct answers or asking for more details\u2014it seemed to \u201clearn\u201d from the previous interaction, adjusting its responses accordingly. However, this sometimes caused it to overextend or go beyond the original scope of a question. For instance, in question five, Grok spent over seven minutes thinking about an answer before I had even formally asked the question, showing both its ambition to solve problems and its tendency to act preemptively. Similarly, when I asked multiple follow-up questions, it occasionally lost focus on the original prompt, producing responses that were less relevant or harder to parse.\n\nOverall, my experience with Grok highlighted both its strengths and its quirks. It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively. At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation. Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an interactive problem-solving setting.",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/7F0bdQDEuCdMckpfhLnGvc2Z\" filename=\"CS_182_Participation_A_HW3_Grok.pdf\"/><paragraph><bold>REFLECTION</bold></paragraph><paragraph>\u200b\u200bIn completing the non-coding parts of the homework with Grok, I found that it could one-shot questions about 70\u201380% of the time. Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D. This showed that while hints can significantly improve accuracy, the model is capable of reasoning independently in some cases. I also noticed that the model tended to give very long and verbose responses, often providing explanations two to three pages long, even when a short answer would have sufficed. This happened especially when I asked follow-up questions, and Grok would completely lose focus.</paragraph><paragraph>One particularly striking behavior was how Grok interacted over the course of the chat. When I gave feedback\u2014acknowledging correct answers or asking for more details\u2014it seemed to \u201clearn\u201d from the previous interaction, adjusting its responses accordingly. However, this sometimes caused it to overextend or go beyond the original scope of a question. For instance, in question five, Grok spent over seven minutes thinking about an answer before I had even formally asked the question, showing both its ambition to solve problems and its tendency to act preemptively. Similarly, when I asked multiple follow-up questions, it occasionally lost focus on the original prompt, producing responses that were less relevant or harder to parse.</paragraph><paragraph>Overall, my experience with Grok highlighted both its strengths and its quirks. It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively. At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation. Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an interactive problem-solving setting.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-10-01T09:44:26.995833+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7043667,
            "author": "Jason Guo",
            "project_title": "Special Participation B",
            "post_body": "Claude conversation transcript: https://claude.ai/share/c79a2051-2b23-492a-8883-9325b0b7237e\n\nAnnotated transcript: https://drive.google.com/file/d/1lj_yW-rTQtqUbtlWBrcVS5_mHA2UtZKt/view?usp=sharing\n\nFor this special participation, I used Claude to solve the muP problem (question 2) of homework 3. I already did the assignment myself, so I began by just telling Claude that I was trying to evaluate its capability to solve the problems, and asked it to go through the problems one by one. For the most part, it did good, and pretty much one shot all parts except a and d. For part a, it struggled a bit with the RMS to RMS norm, but I think that\u2019s just because it was confused with the definition of the RMS to RMS norm. After I clarified, Claude was able to solve it. \n\nFor part d, it first gave a scaling factor of 1/sqrt(d_in), and doubted itself on its correctness. When I prompted it to investigate what the step size would be scaled by, it pointed out that in adam, the constant would actually cancel out in the update step. At this point, I got kind of confused because when I did the problem, I was thinking about it in terms of regular SGD, so I just divided by 1/(d_in), but from what I can see Claude is actually correct that the constant cancels. I tried leading it to the 1/(d_in) answer, but I realized there was a mistake in my reasoning too. After a while, I couldn\u2019t really see why the solution scales by (d_in), or why 1/(d_in) apparently works in actually making the graph more uniform, and Claude said it couldn\u2019t either, so I gave up trying to get it to help figure out the scaling factor. \nOverall, I think Claude was really helpful in this problem. It solved most parts without needing help, and gave good explanations for how it got the answers. In addition, it questioned itself a lot, which I think I don\u2019t see a lot in other LLMs like ChatGPT, and this questioning was even able to make me realize a mistake in my reasoning.",
            "content_xml": "<document version=\"2.0\"><paragraph>Claude conversation transcript: <link href=\"https://claude.ai/share/c79a2051-2b23-492a-8883-9325b0b7237e\"><underline>https://claude.ai/share/c79a2051-2b23-492a-8883-9325b0b7237e</underline></link><break/><break/>Annotated transcript: https://drive.google.com/file/d/1lj_yW-rTQtqUbtlWBrcVS5_mHA2UtZKt/view?usp=sharing</paragraph><paragraph>For this special participation, I used Claude to solve the muP problem (question 2) of homework 3. I already did the assignment myself, so I began by just telling Claude that I was trying to evaluate its capability to solve the problems, and asked it to go through the problems one by one. For the most part, it did good, and pretty much one shot all parts except a and d. For part a, it struggled a bit with the RMS to RMS norm, but I think that\u2019s just because it was confused with the definition of the RMS to RMS norm. After I clarified, Claude was able to solve it. </paragraph><paragraph>For part d, it first gave a scaling factor of 1/sqrt(d_in), and doubted itself on its correctness. When I prompted it to investigate what the step size would be scaled by, it pointed out that in adam, the constant would actually cancel out in the update step. At this point, I got kind of confused because when I did the problem, I was thinking about it in terms of regular SGD, so I just divided by 1/(d_in), but from what I can see Claude is actually correct that the constant cancels. I tried leading it to the 1/(d_in) answer, but I realized there was a mistake in my reasoning too. After a while, I couldn\u2019t really see why the solution scales by (d_in), or why 1/(d_in) apparently works in actually making the graph more uniform, and Claude said it couldn\u2019t either, so I gave up trying to get it to help figure out the scaling factor. <break/>Overall, I think Claude was really helpful in this problem. It solved most parts without needing help, and gave good explanations for how it got the answers. In addition, it questioned itself a lot, which I think I don\u2019t see a lot in other LLMs like ChatGPT, and this questioning was even able to make me realize a mistake in my reasoning.</paragraph></document>",
            "links": [
                "https://claude.ai/share/c79a2051-2b23-492a-8883-9325b0b7237e"
            ],
            "attachments": [],
            "created_at": "2025-09-30T12:42:27.09515+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7037485,
            "author": "Lance Mathias",
            "project_title": "HW3 Solutions",
            "post_body": "Hi all,\n\nPlease ask any questions related to HW3 solutions here.\n\nEDIT (10/16): The coding solution has been updated to reflect the correction mentioned in #149",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>Please ask any questions related to HW3 solutions here.</paragraph><file url=\"https://static.us.edusercontent.com/files/T54IADqbY7BgXVGdG4fJuDHy\" filename=\"hw03_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/rtcd9l9G8lvvqyOB42pw9eKt\" filename=\"q_mup_coding_sol.ipynb\"/><paragraph>EDIT (10/16): The coding solution has been updated to reflect the correction mentioned in #149</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-29T19:40:16.057496+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7036991,
            "author": "Joe Berry",
            "project_title": "Participation B Post",
            "post_body": "This is my participation B Post\n\nJoseph Berry\n\nCS182 HW1 Google Colab With Gemini\n\n Google Gemini was able to easily complete this assignment. It wrote code that was slightly more rudimentary than the staff solution, it was missing an if (i == 0) statement.\n\n\n\nMe:\n\nCan you please read through my code and fill in the section of runGDM() that needs to be implemented?\n\nGemini:\n\nSure, I can help you with that! I'll add the code to implement gradient descent with momentum in the runGDM function.\n\nCode Gemini Added:\n smoothed_grad = beta * smoothed_grad + (1 - beta) * grad\n\nMe:\n\nWhat if i == 0? Can you add a check for that? Only modify in between the ### lines\n\nGemini:\n\nYou are correct. I will add an explicit check for i == 0 within the runGDM function.\n\nCode Gemini Added:\n if i == 0:\n\n smoothed_grad = grad\n\n else:\n\n smoothed_grad = beta * smoothed_grad + (1 - beta) * grad\n\nMe:\n\nNow can you complete the code section under Question 2: text section? Fill in that code with one of the methods used in this notebook and give it a different learning rate.\n\nGemini:\n\nI will add code to the cell under Question 2 to run gradient descent with momentum using a different learning rate and compare it with the results from plain gradient descent.\n\nCode Gemini Added:\n stepsize_new = 2e-4 # Increase the learning rate slightly\n\ngrads_m_new, ws_m_new, losses_m_new = runGDM(maxiter, stepsize_new, beta)\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>This is my participation B Post<break/><break/>Joseph Berry</paragraph><paragraph>CS182 HW1 Google Colab With Gemini<break/><break/> Google Gemini was able to easily complete this assignment. It wrote code that was slightly more rudimentary than the staff solution, it was missing an if (i == 0) statement.</paragraph><paragraph/><paragraph><bold>Me:</bold></paragraph><paragraph><bold>Can you please read through my code and fill in the section of runGDM() that needs to be implemented?</bold></paragraph><paragraph>Gemini:</paragraph><paragraph>Sure, I can help you with that! I'll add the code to implement gradient descent with momentum in the runGDM function.</paragraph><paragraph>Code Gemini Added:<break/> smoothed_grad = beta * smoothed_grad + (1 - beta) * grad</paragraph><paragraph><bold>Me:</bold></paragraph><paragraph><bold>What if i == 0? Can you add a check for that? Only modify in between the ### lines</bold></paragraph><paragraph>Gemini:</paragraph><paragraph>You are correct. I will add an explicit check for i == 0 within the runGDM function.</paragraph><paragraph>Code Gemini Added:<break/> if i == 0:</paragraph><paragraph> smoothed_grad = grad</paragraph><paragraph> else:</paragraph><paragraph> smoothed_grad = beta * smoothed_grad + (1 - beta) * grad</paragraph><paragraph><bold>Me:</bold></paragraph><paragraph><bold>Now can you complete the code section under Question 2: text section? Fill in that code with one of the methods used in this notebook and give it a different learning rate.</bold></paragraph><paragraph>Gemini:</paragraph><paragraph>I will add code to the cell under Question 2 to run gradient descent with momentum using a different learning rate and compare it with the results from plain gradient descent.</paragraph><paragraph>Code Gemini Added:<break/> stepsize_new = 2e-4 # Increase the learning rate slightly</paragraph><paragraph>grads_m_new, ws_m_new, losses_m_new = runGDM(maxiter, stepsize_new, beta)</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/8NFg95iQVrFOCdww3VHUqDMh\" filename=\"q_sgd_momentum_analysis_Google_Colab_w_Gemini.ipynb\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-29T15:12:58.434367+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7034106,
            "author": "Joe Berry",
            "project_title": "Participation A Post",
            "post_body": "This is my participation A post. I have an executive summary and then the full logs attached. Please let me know if there is anything wrong with my format and I will repost. \n\nJoseph Berry\n CS182 HW 1 \n Claude Answer Analysis\n\nI used Claude Model Opus 4.1, with the Research button activated allowing for \u201cAgentic mode\u201d or web searches while answering a question. \n\nThis model was able to one shot most of the questions. It failed on questions 2 and 3 and was unable to compute the eigenvalues or discriminant of part 3 without help. Even when presented with the correct answer. \n\nProblem 1. Why Learning Rates Cannot be Too Big\n Part a.\n\n Answered the question completely.\nPart b. \n Solved the question, then proceeded to give me a second answer it thought was equivalent and an example with numerical value. \n\nPart c.\n\n Solved the question but arrived at a different solution of 2/\u03c3^2 instead of 1/\u03c3^2. Also defined loss L(W) as 1/2|| y \u2013 \u03a3w ||^2 instead of || y \u2013 \u03a3w ||^2. It claimed the \u00bd was a \u201cconvenience factor\u201d making the gradient slightly easier to compute, and that it would lead to a different effective learning rate, which it didn\u2019t believe was an issue. \n\nPart d.\n\n Solved the question and then continued explaining which \u03c3 value, large or small, would converge faster.\n\nPart e.\n\nGave a slightly incorrect answer:\n \u03b7= 1/(\u03c3\u2113 2 + \u03c3s 2 ) = 1/(\u03c3\u2113 2) instead of the course solution 1/(\u03c3\u2113 2 + \u03c3s 2 )\n\nPart f.\n\n Answered the question mathematically instead of using definitions. \n\nProblem 2. Stochastic Gradient Descent (when it is possible to interpolate)\n\nPart a.\n\n Full Solution.\n\nPart b.\n\n Full Solution.\n\nPart c.\n\n Found the correct answer then continued creating coordinate changes, and tried to create a formula for the coordinate changes. \n\nPart d.\n\n Full Solution. \n\nPart e.\n\n Claude wasn\u2019t able to produce the correct solution, but provided a similar solution (missing expectation of loss, just had loss) and then proceeded to try and show how the problem scaled with just \u03c3 and \u03b5. It was able to produce an answer that loss goes to 0 as t goes to infinity. \n\nPart f.\n\n Full Solution. \n\nPart g.\n\n Full Solution. \n\nPart h.\n\n Gave the correct answer but left out the learning rate term from c1. When asked why c1 did not include the learning rate it stated that the learning rate \u201cBy keeping \u03b7\\eta \u03b7 separate, we can later analyze how to choose \u03b7\\eta \u03b7 optimally\u201d\n\nPart i.\n\n Gave a solution without including the learning rate like in Part h. But also left out the Rho value, or any equivalent value, for the largest norm of the rows of coordinate shifted X. Then when asked if it could include a value representing this, it gave out the answer without including sigma max but now including Rho. \n When asked again it stated that sigma max and rho would be the same value. Stating that both solutions, including rho and excluding rho were correct. \n\nPart j.\n\n Full Solution. \n\nPart k. \n\n This question required reading a jupyter notebook. After inputting Claude explained a small amount about standard ridge regression and feature augmentation. \n\n Problem 3. Accelerating Gradient Descent with Momentum\n\nPart a. \n\n Full Solution.\n\nPart b. \n\n Full Solution.\n\nPart c. \n\n At this problem Claude took over 3 minutes \u201ccompiling source\u201d from the internet to provide a solution. I interrupted this and then repeated the question. Claude then gave me an incorrect answer for the discriminant and answers for eigenvalues that didn\u2019t include the square root term. \n\nPart d. \n\n Incorrect answer, due to an incorrect solution for part a. I then supplied it with the correct solution for part a. After giving it the correct discriminant and eigenvalues it still produced an incorrect answer. \n\nPart e. \n\n Incorrect solution.\n\nPart f. \n\n Full Solution\n\nPart g. \n\n Gave a close answer for Ordinary Gradient Descent (342 where true answer was 346) And a wildly incorrect answer for the momentum version (919 where true answer was 132)\n\nPart h.\n\n I skipped this question, it required a jupyter notebook\n\nPart i. \n\n  I skipped this question, it required a jupyter notebook\n\n4. Optimizers\n\n\n Part a. \n\n Full Solution.\n\nPart b. \n\nFull Solution.\n\n5. Regularization and Instance Noise\n\nPart a. \n\n Full Solution. \n\nPart b. \n\n Failed to produce the correct answer, was given hints on how to find the correct answer answer. On third prompt I gave it the correct answer and asked it to work backwards. It claimed that it needed to know that this was for the scalar case in order to give a correct answer. \n\nPart c. \n\n Full Solution\n\nPart d. \n\n Produced an equivalent correct answer\n\n6. General Case Tikhonov Regularization\n\nPart a. \n\n Full Solution\n\nPart b.\n\n Full Solution\n\nPart c.\n\n Full Solution\n\n\n\n7. An Alternate MAP Interpretation of Ridge Regression\n\nFull Solution.\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>This is my participation A post. I have an executive summary and then the full logs attached. Please let me know if there is anything wrong with my format and I will repost. <break/><break/>Joseph Berry<break/> CS182 HW 1 <break/> Claude Answer Analysis</paragraph><paragraph>I used Claude Model Opus 4.1, with the Research button activated allowing for \u201cAgentic mode\u201d or web searches while answering a question. </paragraph><paragraph>This model was able to one shot most of the questions. It failed on questions 2 and 3 and was unable to compute the eigenvalues or discriminant of part 3 without help. Even when presented with the correct answer. </paragraph><paragraph><bold>Problem 1. Why Learning Rates Cannot be Too Big<break/> Part a.</bold></paragraph><paragraph> Answered the question completely.<break/><bold>Part b.</bold> <break/> Solved the question, then proceeded to give me a second answer it thought was equivalent and an example with numerical value. </paragraph><paragraph><bold>Part c.</bold></paragraph><paragraph> Solved the question but arrived at a different solution of 2/\u03c3^2 instead of 1/\u03c3^2. Also defined loss L(W) as 1/2|| y \u2013 \u03a3w ||^2 instead of || y \u2013 \u03a3w ||^2. It claimed the \u00bd was a \u201cconvenience factor\u201d making the gradient slightly easier to compute, and that it would lead to a different effective learning rate, which it didn\u2019t believe was an issue. <break/><break/><bold>Part d.</bold></paragraph><paragraph> Solved the question and then continued explaining which \u03c3 value, large or small, would converge faster.<break/><break/><bold>Part e.</bold></paragraph><paragraph>Gave a slightly incorrect answer:<break/> \u03b7= 1/(\u03c3\u2113 2 + \u03c3s 2 ) = 1/(\u03c3\u2113 2) instead of the course solution 1/(\u03c3\u2113 2 + \u03c3s 2 )</paragraph><paragraph><bold>Part f.</bold></paragraph><paragraph> Answered the question mathematically instead of using definitions. </paragraph><paragraph><bold>Problem 2. Stochastic Gradient Descent (when it is possible to interpolate)<break/><break/>Part a.</bold></paragraph><paragraph> Full Solution.</paragraph><paragraph><bold>Part b.</bold></paragraph><paragraph> Full Solution.</paragraph><paragraph><bold>Part c.</bold></paragraph><paragraph> Found the correct answer then continued creating coordinate changes, and tried to create a formula for the coordinate changes. </paragraph><paragraph><bold>Part d.</bold></paragraph><paragraph> Full Solution. </paragraph><paragraph><bold>Part e.</bold></paragraph><paragraph> Claude wasn\u2019t able to produce the correct solution, but provided a similar solution (missing expectation of loss, just had loss) and then proceeded to try and show how the problem scaled with just \u03c3 and \u03b5. It was able to produce an answer that loss goes to 0 as t goes to infinity. </paragraph><paragraph><bold>Part f.</bold></paragraph><paragraph> Full Solution. </paragraph><paragraph><bold>Part g.</bold></paragraph><paragraph> Full Solution. </paragraph><paragraph><bold>Part h.</bold></paragraph><paragraph> Gave the correct answer but left out the learning rate term from c1. When asked why c1 did not include the learning rate it stated that the learning rate \u201cBy keeping \u03b7\\eta \u03b7 separate, we can later analyze how to choose \u03b7\\eta \u03b7 optimally\u201d</paragraph><paragraph><bold>Part i.</bold></paragraph><paragraph> Gave a solution without including the learning rate like in Part h. But also left out the Rho value, or any equivalent value, for the largest norm of the rows of coordinate shifted X. Then when asked if it could include a value representing this, it gave out the answer without including sigma max but now including Rho. <break/> When asked again it stated that sigma max and rho would be the same value. Stating that both solutions, including rho and excluding rho were correct. </paragraph><paragraph><bold>Part j.</bold></paragraph><paragraph> Full Solution. </paragraph><paragraph><bold>Part k.</bold> </paragraph><paragraph> This question required reading a jupyter notebook. After inputting Claude explained a small amount about standard ridge regression and feature augmentation. </paragraph><paragraph> <bold>Problem 3. Accelerating Gradient Descent with Momentum</bold></paragraph><paragraph><bold>Part a.</bold> </paragraph><paragraph> Full Solution.</paragraph><paragraph><bold>Part b.</bold> </paragraph><paragraph> Full Solution.</paragraph><paragraph><bold>Part c.</bold> </paragraph><paragraph> At this problem Claude took over 3 minutes \u201ccompiling source\u201d from the internet to provide a solution. I interrupted this and then repeated the question. Claude then gave me an incorrect answer for the discriminant and answers for eigenvalues that didn\u2019t include the square root term. </paragraph><paragraph><bold>Part d.</bold> </paragraph><paragraph> Incorrect answer, due to an incorrect solution for part a. I then supplied it with the correct solution for part a. After giving it the correct discriminant and eigenvalues it still produced an incorrect answer. </paragraph><paragraph><bold>Part e.</bold> </paragraph><paragraph> Incorrect solution.</paragraph><paragraph><bold>Part f.</bold> </paragraph><paragraph> Full Solution</paragraph><paragraph><bold>Part g.</bold> </paragraph><paragraph> Gave a close answer for Ordinary Gradient Descent (342 where true answer was 346) And a wildly incorrect answer for the momentum version (919 where true answer was 132)</paragraph><paragraph><bold>Part h.</bold></paragraph><paragraph> I skipped this question, it required a jupyter notebook</paragraph><paragraph><bold>Part i.</bold> </paragraph><paragraph>  I skipped this question, it required a jupyter notebook</paragraph><paragraph><bold>4. Optimizers</bold></paragraph><paragraph><bold><break/> Part a.</bold> </paragraph><paragraph> Full Solution.</paragraph><paragraph><bold>Part b.</bold> </paragraph><paragraph>Full Solution.</paragraph><paragraph><bold>5. Regularization and Instance Noise</bold></paragraph><paragraph><bold>Part a.</bold> </paragraph><paragraph> Full Solution. </paragraph><paragraph><bold>Part b.</bold> </paragraph><paragraph> Failed to produce the correct answer, was given hints on how to find the correct answer answer. On third prompt I gave it the correct answer and asked it to work backwards. It claimed that it needed to know that this was for the scalar case in order to give a correct answer. </paragraph><paragraph><bold>Part c.</bold> </paragraph><paragraph> Full Solution</paragraph><paragraph><bold>Part d.</bold> </paragraph><paragraph> Produced an equivalent correct answer</paragraph><paragraph><bold>6. General Case Tikhonov Regularization</bold></paragraph><paragraph><bold>Part a.</bold> </paragraph><paragraph> Full Solution</paragraph><paragraph><bold>Part b.</bold></paragraph><paragraph> Full Solution</paragraph><paragraph><bold>Part c.</bold></paragraph><paragraph> Full Solution</paragraph><paragraph/><paragraph><bold>7. An Alternate MAP Interpretation of Ridge Regression</bold></paragraph><paragraph>Full Solution.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/D7bfXOAWeivPhFWd0nmuYzdb\" filename=\"HW1_Claude_Notes.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-29T07:02:28.705216+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7030840,
            "author": "Nyx Iskandar",
            "project_title": "Special Participation E: ChatGPT as Quiz Tutor",
            "post_body": "Trace: https://chatgpt.com/share/68d891e8-3f3c-8010-8f9f-edd2aff1297b\n\nPrompted ChatGPT to teach me the CNN architecture step-by-step and quiz me after each concept. Pretty fun and useful as a lecture prep or consolidation! Can also extend this to uploading lecture notes and quizzing based on the notes.",
            "content_xml": "<document version=\"2.0\"><paragraph>Trace: <link href=\"https://chatgpt.com/share/68d891e8-3f3c-8010-8f9f-edd2aff1297b\">https://chatgpt.com/share/68d891e8-3f3c-8010-8f9f-edd2aff1297b</link></paragraph><paragraph>Prompted ChatGPT to teach me the CNN architecture step-by-step and quiz me after each concept. Pretty fun and useful as a lecture prep or consolidation! Can also extend this to uploading lecture notes and quizzing based on the notes.</paragraph></document>",
            "links": [
                "https://chatgpt.com/share/68d891e8-3f3c-8010-8f9f-edd2aff1297b"
            ],
            "attachments": [],
            "created_at": "2025-09-28T11:42:41.568091+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7027566,
            "author": "Hong Joey",
            "project_title": "HW4 Q7",
            "post_body": "Context: This problem analyzes the behavior of weights and gradients of a convolutional layer in detail. You will be asked to:\n\nDerive the gradient of a weight matrix as a weighted average of image patches\n\nAnalyze the mean and std of the gradient\n\nShow the effect of pooling on the gradient and in particular, how max pooling routes gradients. It is important to understand this here because the story in transformers is related. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This problem analyzes the behavior of weights and gradients of a convolutional layer in detail. You will be asked to:</paragraph><list style=\"bullet\"><list-item><paragraph>Derive the gradient of a weight matrix as a weighted average of image patches</paragraph></list-item><list-item><paragraph>Analyze the mean and std of the gradient</paragraph></list-item><list-item><paragraph>Show the effect of pooling on the gradient and in particular, how max pooling routes gradients. It is important to understand this here because the story in transformers is related. </paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/N6mLfVf8darcIYm9rcsqcKUn\" width=\"658\" height=\"788.9049295774647\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/b7JrGMiqSxkfLqC3dwoaSH3Y\" width=\"658\" height=\"879.7304189435337\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/bN5ZMzWSnshRvQUfnbXrhjTz\" width=\"658\" height=\"221.63001745200697\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T17:02:18.20626+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027562,
            "author": "Hong Joey",
            "project_title": "HW4 Q6",
            "post_body": "Context: This coding problem explores the important concept of inductive bias in CNNs. If you are unfamiliar with the term, inductive bias refers to assumptions about the input data that when satisfied, enable CNNs to work more effectively, i.e. locality (pixels closer to eachother are more correlated) and translation invariance. In the problem, you will investigate patterns learned by filters and how they evolve during training in different scenarios.",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This coding problem explores the important concept of inductive bias in CNNs. If you are unfamiliar with the term, inductive bias refers to assumptions about the input data that when satisfied, enable CNNs to work more effectively, i.e. locality (pixels closer to eachother are more correlated) and translation invariance. In the problem, you will investigate patterns learned by filters and how they evolve during training in different scenarios.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NzbrJZuHYqeuxA67ICIaiMP2\" width=\"658\" height=\"646.4965034965036\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/ajTW13n2TVzfpERtceFcuf9c\" width=\"658\" height=\"69.325\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:58:01.608037+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027553,
            "author": "Hong Joey",
            "project_title": "HW4 Q5",
            "post_body": "Context: This short coding question is designed to build an intuitive understanding of how convolutional filters work. In the question, you will manually design filters, which are essentially small matrices of weights, for blurring and edge detection. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This short coding question is designed to build an intuitive understanding of how convolutional filters work. In the question, you will manually design filters, which are essentially small matrices of weights, for blurring and edge detection. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/SZwliD4rUcJKvwW4eKC3xyCY\" width=\"658\" height=\"48.230366492146594\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/MO07NtKxkCJysRjCXwVYMrvC\" width=\"657.9999999999999\" height=\"219.71280276816606\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:51:39.495072+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027545,
            "author": "Hong Joey",
            "project_title": "HW4 Q4",
            "post_body": "Context: This problem  (the latter parts of which are from an old exam) serves as a practical exercise in determining the size of feature maps and number of parameters in CNN layers in a general convolutional layer. You will also compute the effect of operations such as max-pooling and downsampling on output feature shape and number of parameters in a specific CNN architecture.  ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This problem  (the latter parts of which are from an old exam) serves as a practical exercise in determining the size of feature maps and number of parameters in CNN layers in a general convolutional layer. You will also compute the effect of operations such as max-pooling and downsampling on output feature shape and number of parameters in a specific CNN architecture.  </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/5FhJ3bJqRT2BMWckKZS24mbI\" width=\"658\" height=\"71.22680412371133\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/sgFuM70KUeamPglhe3qGxiR4\" width=\"658\" height=\"822.2262895174708\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/gIDyHeRMdFoKA4VFTrnSVe6g\" width=\"658\" height=\"796.348717948718\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:48:43.390493+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027532,
            "author": "Hong Joey",
            "project_title": "HW4 Q3",
            "post_body": "Context: This problem establishes a link between convolutional operations and discrete-time signal processing. The goal is to solidify understanding of the convolution operation and its properties (such as translational invariance). ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This problem establishes a link between convolutional operations and discrete-time signal processing. The goal is to solidify understanding of the convolution operation and its properties (such as translational invariance). </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YNfxQngQ376W15GWR7jg13Ht\" width=\"658\" height=\"382.37758112094394\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/Imc8WZFokafngtljYyOcPGoS\" width=\"658\" height=\"779.5659824046921\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:42:02.921189+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027521,
            "author": "Hong Joey",
            "project_title": "HW4 Q2",
            "post_body": "Context: This problem is aimed at cementing your understanding of muP. You will need to grasp the following points well in order to solve this problem\n\nImpact of weight initialization on scaling the forward pass\n\nHow to scale the weight updates under different scenarios\n\nEffect on the backward pass when scaling is applied",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This problem is aimed at cementing your understanding of muP. You will need to grasp the following points well in order to solve this problem</paragraph><list style=\"bullet\"><list-item><paragraph>Impact of weight initialization on scaling the forward pass</paragraph></list-item><list-item><paragraph>How to scale the weight updates under different scenarios</paragraph></list-item><list-item><paragraph>Effect on the backward pass when scaling is applied</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/0h6fi8QfAsjCpMbSCuIXAOtw\" width=\"658\" height=\"478.54545454545456\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/C4HoEeeddP8nGEz4bLPm3CGm\" width=\"658\" height=\"427.2089552238806\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:38:33.426508+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027477,
            "author": "Hong Joey",
            "project_title": "HW4 Q1",
            "post_body": "Context: This problem asks you to analyze the runtime of a single iteration of Newton-Schulz. In most implementations, you may see that the matrix is transposed so that the row dimension is always smaller than the column dimension. This problem also will help you understand why that is done. ",
            "content_xml": "<document version=\"2.0\"><paragraph>Context: This problem asks you to analyze the runtime of a single iteration of Newton-Schulz. In most implementations, you may see that the matrix is transposed so that the row dimension is always smaller than the column dimension. This problem also will help you understand why that is done. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/sV5M6UHbDCvhMDJpQ0Yh7Nxo\" width=\"658\" height=\"260.6749435665914\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T16:09:55.573783+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 7027450,
            "author": "Hong Joey",
            "project_title": "Discussion 3 Solutions",
            "post_body": "It has come to our attention that Discussion 3 solutions were never posted. Apologies for the delay, but attached are questions and solutions for Discussion 3:\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>It has come to our attention that Discussion 3 solutions were never posted. Apologies for the delay, but attached are questions and solutions for Discussion 3:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/wt9GRta2emjV4f3gKz20Dr3J\" filename=\"dis03_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/ebx1ThCmISNc8qEZv4onIPhE\" filename=\"dis03_solution.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T15:55:48.359596+10:00",
            "category": "Admin"
        },
        {
            "guid": 7027430,
            "author": "Nyx Iskandar",
            "project_title": "Special Participation E: FlashcardGPT",
            "post_body": "Use FlashcardGPT to generate flashcards out of lecture notes etc. :)\n\nURL: https://github.com/xyntechx/flashcard-gpt\n\nFull disclosure:\n\nI made this app in about 3 hours + I'm not the best designer\n\nYou have to run this locally (follow the instructions in the README)\n\nYou need to use your own OpenAI API keys (sorry haha)\n\nWhen I was testing this, I used a pre-generated set of flashcards so that it won't take too long, and to save API credits, which is why the demo is \ud83d\udd25 blazingly fast \ud83d\udd25. The flashcards were indeed generated by GPT-5.\n\nOpen to feature requests / issues / PRs!\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Use FlashcardGPT to generate flashcards out of lecture notes etc. :)</paragraph><paragraph>URL: https://github.com/xyntechx/flashcard-gpt</paragraph><paragraph>Full disclosure:</paragraph><list style=\"bullet\"><list-item><paragraph>I made this app in about 3 hours + I'm not the best designer</paragraph></list-item><list-item><paragraph>You have to run this locally (follow the instructions in the README)</paragraph></list-item><list-item><paragraph>You need to use your own OpenAI API keys (sorry haha)</paragraph></list-item><list-item><paragraph>When I was testing this, I used a pre-generated set of flashcards so that it won't take too long, and to save API credits, which is why the demo is \ud83d\udd25 blazingly fast \ud83d\udd25. The flashcards were indeed generated by GPT-5.</paragraph></list-item></list><paragraph>Open to feature requests / issues / PRs!</paragraph><file url=\"https://static.us.edusercontent.com/files/2GNHHP8zSUbCiuZhzkvmJvoo\" filename=\"Screen Recording 2025-09-26 at 22.34.08.mov\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T15:47:18.640074+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 7027253,
            "author": "Hong Joey",
            "project_title": "Discussion 4 Solutions",
            "post_body": "Attached are the questions and solutions to Discussion 4 from this week:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Attached are the questions and solutions to Discussion 4 from this week:</paragraph><file url=\"https://static.us.edusercontent.com/files/fFhRXCuQme0LuA9NUsJ4mXPc\" filename=\"dis04_question.pdf\"/><paragraph/><file url=\"https://static.us.edusercontent.com/files/sXsjGXm55z57aPDnxAWPKvhY\" filename=\"dis04_solution.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-27T14:25:47.508519+10:00",
            "category": "Admin"
        },
        {
            "guid": 7007009,
            "author": "Gireeja Ranade",
            "project_title": "Lecture 6 and 7 slides and questions",
            "post_body": "Muon/MuP questions here!",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/5e2fumbMGggxcOkOZ6SBPyQH\" filename=\"Lecture 7.pdf\"/><file url=\"https://static.us.edusercontent.com/files/fJefvrPiaf5ZRgf4YSjzGNTJ\" filename=\"Lecture 6.pdf\"/><paragraph>Muon/MuP questions here!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-24T07:31:47.091163+10:00",
            "category": "Lectures"
        },
        {
            "guid": 7002423,
            "author": "Kevin Frans",
            "project_title": "HW2 Solutions",
            "post_body": "Analytic Solutions\n\nCoding Solutions",
            "content_xml": "<document version=\"2.0\"><paragraph>Analytic Solutions</paragraph><file url=\"https://static.us.edusercontent.com/files/Fl5ImcrPMJFalJvj2o7JhfIE\" filename=\"hw02_solution.pdf\"/><paragraph>Coding Solutions</paragraph><file url=\"https://static.us.edusercontent.com/files/S2JurSV6DcKgGcSAN3XWEjqe\" filename=\"hw02_solutions_coding.zip\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-23T11:07:53.485499+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6989296,
            "author": "Sultan Daniels",
            "project_title": "HW3 Q5",
            "post_body": "Problem context: This is another basic problem that was a former exam problem and is there to help you appreciate more about the \"systems\" dimension of deep learning. How to tradeoff computation and memory... Again, this helps you understand that you can understand ideas even beyond what is explicitly taught in lecture or discussion.\n\n",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/DI87dKkPpXbdzkCIfIazobes\" width=\"658\" height=\"1095.065693430657\"/></figure><paragraph>Problem context: This is another basic problem that was a former exam problem and is there to help you appreciate more about the \"systems\" dimension of deep learning. How to tradeoff computation and memory... Again, this helps you understand that you can understand ideas even beyond what is explicitly taught in lecture or discussion.</paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-20T15:12:57.287527+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6989292,
            "author": "Sultan Daniels",
            "project_title": "HW3 Q4",
            "post_body": "Problem context: This problem builds on the problem from last week to gently expose you to core ideas related to reinforcement learning. The goal continues to be to both empower you and help you see your own empowerment: we didn't touch on this stuff at all during lecture or discussion. But you can still understand it. \n\nWhereas the earlier problem had you explore and understand via code and experimentation, this helps you look at a particular piece from a mathematical lens. The use of the standard normal is designed to help you see how helpful the normal distribution is when getting started vis-a-vis insight, while the later parts helps you see how the pattern extends beyond normals. (Although notice: the normal case is where it is easiest to discover what the pattern actually is.)\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/aR60H0l3DbBJyR8jZIA5IPwa\" width=\"658\" height=\"836.1541501976285\"/></figure><paragraph>Problem context: This problem builds on the problem from last week to gently expose you to core ideas related to reinforcement learning. The goal continues to be to both empower you and help you see your own empowerment: we didn't touch on this stuff at all during lecture or discussion. But you can still understand it. </paragraph><paragraph>Whereas the earlier problem had you explore and understand via code and experimentation, this helps you look at a particular piece from a mathematical lens. The use of the standard normal is designed to help you see how helpful the normal distribution is when getting started vis-a-vis insight, while the later parts helps you see how the pattern extends beyond normals. (Although notice: the normal case is where it is easiest to discover what the pattern actually is.)</paragraph><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-20T15:11:26.238996+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6989291,
            "author": "Sultan Daniels",
            "project_title": "HW3 Q3",
            "post_body": "Problem context: We have tried to make this extremely important material accessible to you, and so we would not necessarily expect you to be able to understand these papers without what we have taught you. For Tensor Programs V, we wouldn't expect that even after our guidance. But one important skill for Deep Learning is to be able to extract some understanding and insight from papers even when you can't understand everything. This problem is designed to help guide you to do that. After all, you are going to have to learn on your own once you are done with this class. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/TNIh5ciAhAWGRbbrydqrZkno\" width=\"658\" height=\"611.4747474747475\"/></figure><paragraph>Problem context: We have tried to make this extremely important material accessible to you, and so we would not necessarily expect you to be able to understand these papers without what we have taught you. For Tensor Programs V, we wouldn't expect that even after our guidance. But one important skill for Deep Learning is to be able to extract some understanding and insight from papers even when you can't understand everything. This problem is designed to help guide you to do that. After all, you are going to have to learn on your own once you are done with this class. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-20T15:10:50.686214+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6989287,
            "author": "Sultan Daniels",
            "project_title": "HW3 Q2",
            "post_body": "\n\nProblem context: This is a critically important homework problem as it has you complement the analytically driven understanding you were given in lecture with seeing what happens in an actual toy neural net. This problem touches on many things that you have seen in the previous few lectures. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/BjfQ713wMpOYRSamgNKdUfDr\" width=\"658\" height=\"99.11297071129707\"/></figure><paragraph/><paragraph>Problem context: This is a critically important homework problem as it has you complement the analytically driven understanding you were given in lecture with seeing what happens in an actual toy neural net. This problem touches on many things that you have seen in the previous few lectures. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-20T15:09:59.1106+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6989285,
            "author": "Sultan Daniels",
            "project_title": "HW3 Q1",
            "post_body": "\n\nProblem context: this problem builds directly on discussion and lecture. It has you recapitulate things that you've seen in lecture, but they're important and you should internalize them. Consequently, we recommend you try to do this without constantly looking at the lecture. This needs to be in your head. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/3XC5gmvt3iwLFCKA0Jvx5qpn\" width=\"658\" height=\"480.5420560747663\"/></figure><paragraph/><paragraph>Problem context: this problem builds directly on discussion and lecture. It has you recapitulate things that you've seen in lecture, but they're important and you should internalize them. Consequently, we recommend you try to do this without constantly looking at the lecture. This needs to be in your head. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-20T15:09:17.716872+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6979789,
            "author": "Leon Kornfeld",
            "project_title": "Participation A (HW2): Grok",
            "post_body": "Q1. \n\nPart a of this problem was straight forward and was one-shotted by Grok. \n\nPart b was tougher. Because I directly copy and pasted from the PDF, the formatting was a bit weird and Grok understood the infinity norm to be squared. Regardless, it was still able to reason its way that this was a form of sign SGD. \n\nMy next prompt to it was \"You misread part b. the infinity norm is not squared.\" In response, it tried again and once again reached an incorrect answer (but it was on the right track). It said that term of u, corresponding to the max value of delta_theta, is\u2212\u03b1\u22c5sign(gk\u200b) and all other u terms are zero. This is not correct as all terms should be -a*sign(g). \n\nI decided to steer Grok in the right direction by saying \"Wouldn't it make sense to make all u equal to the negative max size of the step size times the sign of each element of g. This will minimize the inner product even more.\"  This resembles the process we did in class.\n\nUnfortunately, Grok then reverted back to the squared version of the infinity norm, but correctly said that this was an example of signSGD. Lastly, I affirmed that this was correct by saying \"You are right that the final answer recovers signSGD, but the original problem does not square the infinity norm. Please do not square it.\" This finally brought it to what I find to be the correct answer, yet through a different thought process that we had in class. \n\nPart b of this problem was by far the most complex one and the one Grok found most difficult to solve. Please see the individual reasoning times for each prompt. The first one took 5.5 minutes of reasoning in total.\n\n\n\nQ2. This problem was  one-shotted by Grok. I copy and pasted the description and all 3 sub-problems. Based on my review of the answers, Grok's thought process made sense and was very similar to the process I took for solving the problems. Grok did have a problem with rendering the latex originally so I had to reprompt it to render the latex properly. It took 1 minute and 40 seconds to reason the original answer but was very fast with rerendering the latex.\n\n\n\nQ5. This problem was one-shotted by Grok when I copy and pasted the entire question (including the table). I first tried saying fill in the blanks while adding a screenshot of the table. It had no context and it had a bit of trouble parsing the image. When I transcribed the table after the question context in a new chat, it was able to fill in all the blanks on the first try with the the proper explanations. The reasoning period to answer this question was little to none.\n\n\n\nHere is where Grok messed up with the screenshot of the table:\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Q1. </paragraph><paragraph>Part a of this problem was straight forward and was one-shotted by Grok. </paragraph><paragraph>Part b was tougher. Because I directly copy and pasted from the PDF, the formatting was a bit weird and Grok understood the infinity norm to be squared. Regardless, it was still able to reason its way that this was a form of sign SGD. </paragraph><paragraph>My next prompt to it was \"You misread part b. the infinity norm is not squared.\" In response, it tried again and once again reached an incorrect answer (but it was on the right track). It said that term of u, corresponding to the max value of delta_theta, is\u2212\u03b1\u22c5sign(gk\u200b) and all other u terms are zero. This is not correct as all terms should be -a*sign(g). </paragraph><paragraph>I decided to steer Grok in the right direction by saying \"Wouldn't it make sense to make all u equal to the negative max size of the step size times the sign of each element of g. This will minimize the inner product even more.\"  This resembles the process we did in class.</paragraph><paragraph>Unfortunately, Grok then reverted back to the squared version of the infinity norm, but correctly said that this was an example of signSGD. Lastly, I affirmed that this was correct by saying \"You are right that the final answer recovers signSGD, but the original problem does not square the infinity norm. Please do not square it.\" This finally brought it to what I find to be the correct answer, yet through a different thought process that we had in class. </paragraph><paragraph>Part b of this problem was by far the most complex one and the one Grok found most difficult to solve. Please see the individual reasoning times for each prompt. The first one took 5.5 minutes of reasoning in total.</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/TfJUXY00alWLzlFs14iroScv\" filename=\"Optimization_ Linear Improvement, Norm Penalties - Grok.pdf\"/><paragraph>Q2. This problem was  one-shotted by Grok. I copy and pasted the description and all 3 sub-problems. Based on my review of the answers, Grok's thought process made sense and was very similar to the process I took for solving the problems. Grok did have a problem with rendering the latex originally so I had to reprompt it to render the latex properly. It took 1 minute and 40 seconds to reason the original answer but was very fast with rerendering the latex.</paragraph><file url=\"https://static.us.edusercontent.com/files/Cc9KtSSKzC1Co9o7W0oewkVA\" filename=\"Adam Optimizer Convergence Analysis - Grok.pdf\"/><paragraph/><paragraph>Q5. This problem was one-shotted by Grok when I copy and pasted the entire question (including the table). I first tried saying fill in the blanks while adding a screenshot of the table. It had no context and it had a bit of trouble parsing the image. When I transcribed the table after the question context in a new chat, it was able to fill in all the blanks on the first try with the the proper explanations. The reasoning period to answer this question was little to none.</paragraph><file url=\"https://static.us.edusercontent.com/files/3tveS0amgHGqgztEM5DGquLf\" filename=\"Distributed Training Paradigms Analysis - Grok.pdf\"/><paragraph/><paragraph>Here is where Grok messed up with the screenshot of the table:</paragraph><file url=\"https://static.us.edusercontent.com/files/l2chaJN40dBv7VmfK2LXgVHA\" filename=\"WrongDistributed Computing Message Size Patterns - Grok.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-18T13:55:45.022993+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6972386,
            "author": "Gireeja Ranade",
            "project_title": "Lecture 4 and 5 slides and thread",
            "post_body": "Notes from lectures 4 and 5. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/kcNG4UIFsMn1fFb4vQ5j2V0o\" filename=\"Lecture 5.pdf\"/><file url=\"https://static.us.edusercontent.com/files/saT4WyfenQI3bamAntRbUa17\" filename=\"Lecture 4.pdf\"/><paragraph>Notes from lectures 4 and 5. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-17T07:32:22.152256+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6957534,
            "author": "Sultan Daniels",
            "project_title": "HW1 Solutions",
            "post_body": "A thread for the homework 1 solutions:",
            "content_xml": "<document version=\"2.0\"><paragraph>A thread for the homework 1 solutions:</paragraph><file url=\"https://static.us.edusercontent.com/files/GQjeXuC7D1PM5npnIotmftvF\" filename=\"hw01codesolution.zip\"/><file url=\"https://static.us.edusercontent.com/files/aO8NnPYdzjOpEyEphrp1j0nZ\" filename=\"hw01_solution.pdf\"/><file url=\"https://static.us.edusercontent.com/files/wfhyVdY3qm8RjcFtilajxu8o\" filename=\"hw01_question.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-14T10:28:03.327588+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954670,
            "author": "Kevin Frans",
            "project_title": "HW2 Q6",
            "post_body": "Problem context: This is a problem this semester that serves two purposes. (1) While within the overall optimization theme, it hits material that has not been covered in lecture and discussion. This is intentional. Our learning objective is to have you be able to learn stuff on your own after this course is done. So we need to practice that too. The problem is carefully curated so you can learn as you work through it. (2) It is a part of our attempt to gently expose you to core ideas related to reinforcement learning with the objective of making sure that by the end of the semester, you have the mental tools to properly understand post-training of foundation models for LLMs. We take an optimization perspective here to bring out how randomness can help smooth things in a helpful manner. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/GiJJeipIZJMTage95kVl51OW\" width=\"588\" height=\"1145\"/></figure><paragraph>Problem context: This is a problem this semester that serves two purposes. (1) While within the overall optimization theme, it hits material that has not been covered in lecture and discussion. This is intentional. Our learning objective is to have you be able to learn stuff on your own after this course is done. So we need to practice that too. The problem is carefully curated so you can learn as you work through it. (2) It is a part of our attempt to gently expose you to core ideas related to reinforcement learning with the objective of making sure that by the end of the semester, you have the mental tools to properly understand post-training of foundation models for LLMs. We take an optimization perspective here to bring out how randomness can help smooth things in a helpful manner. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-13T13:17:54.285867+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954668,
            "author": "Kevin Frans",
            "project_title": "HW2 Q5",
            "post_body": "Problem context: \n\nThis is another old (and easy) exam problem. It is there to do three things: (1) Get you to learn on your own --- this wasn't covered in lecture or discussion. But you can do this without anything else besides the problem itself and thinking on your part. (2) Act as fair warning that we can ask you to tackle truly new material on the final itself. (3) Get you light exposure to some of the practical systems-oriented side thinking of how to deal with training when you have multiple machines and decide to split the training data among them.  \n\nYou want to be in a position (at least after this course) to be able to read and understand:\nhttps://jax-ml.github.io/scaling-book/\n\nYou're not there yet. But this problem is meant to help you see that you don't need us to teach you this stuff in lecture itself. You can pick it up yourself.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/0srLafhaKfd12u2cNX5FMzaS\" width=\"607\" height=\"1110\"/></figure><paragraph>Problem context: </paragraph><paragraph>This is another old (and easy) exam problem. It is there to do three things: (1) Get you to learn on your own --- this wasn't covered in lecture or discussion. But you can do this without anything else besides the problem itself and thinking on your part. (2) Act as fair warning that we can ask you to tackle truly new material on the final itself. (3) Get you light exposure to some of the practical systems-oriented side thinking of how to deal with training when you have multiple machines and decide to split the training data among them.  <break/><break/>You want to be in a position (at least after this course) to be able to read and understand:<break/><link href=\"https://jax-ml.github.io/scaling-book/\">https://jax-ml.github.io/scaling-book/</link><break/><break/>You're not there yet. But this problem is meant to help you see that you don't need us to teach you this stuff in lecture itself. You can pick it up yourself.</paragraph></document>",
            "links": [
                "https://jax-ml.github.io/scaling-book/"
            ],
            "attachments": [],
            "created_at": "2025-09-13T13:17:34.250771+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954665,
            "author": "Kevin Frans",
            "project_title": "HW2 Q4",
            "post_body": "Problem Context:\n\nThis engages with new material (the Taylor expansion perspective taught in lecture) and builds on the visualizations you saw in the first two discussion sections as well as the analytic part of the previous discussion. This problem requires you to translate what is being asked into math yourself, and then use those concepts to visualize what is going on by modifying a given jupyter notebook.  \n\nThis is a Deep Learning course, not a math course. Of course, we leverage math. This problem has lots of conceptual moving parts, and by trying to understand what is being asked and how to answer it, you will hopefully straighten out those concepts in your mind.\n\nAlong the way, this will also likely help many of you strengthen your understanding of classic dimensionality-reduction via PCA in machine learning. In particular, what are the dimensionality-reduced features when you apply them to new data...",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/2jfaBZof4468uTUnWThv68zZ\" width=\"658\" height=\"748.8177083333334\"/></figure><paragraph>Problem Context:</paragraph><paragraph>This engages with new material (the Taylor expansion perspective taught in lecture) and builds on the visualizations you saw in the first two discussion sections as well as the analytic part of the previous discussion. This problem requires you to translate what is being asked into math yourself, and then use those concepts to visualize what is going on by modifying a given jupyter notebook.  </paragraph><paragraph>This is a Deep Learning course, not a math course. Of course, we leverage math. This problem has lots of conceptual moving parts, and by trying to understand what is being asked and how to answer it, you will hopefully straighten out those concepts in your mind.</paragraph><paragraph>Along the way, this will also likely help many of you strengthen your understanding of classic dimensionality-reduction via PCA in machine learning. In particular, what are the dimensionality-reduced features when you apply them to new data...</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-13T13:16:32.338449+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954664,
            "author": "Kevin Frans",
            "project_title": "HW2 Q3",
            "post_body": "\n\nProblem context: This problem lets you see the ideas of optimization and initialization play out in the context of real dataset: CIFAR10 using fully-connected neural nets.\n\nThe problem also lets you play with some of the standard kinds of plots that one uses in deep learning.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/kgt7epOyJftPOMaxWEJ6YtLB\" width=\"658\" height=\"213.3359375\"/></figure><paragraph/><paragraph>Problem context: This problem lets you see the ideas of optimization and initialization play out in the context of real dataset: CIFAR10 using fully-connected neural nets.</paragraph><paragraph>The problem also lets you play with some of the standard kinds of plots that one uses in deep learning.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-13T13:16:10.269664+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954663,
            "author": "Kevin Frans",
            "project_title": "HW2 Q2",
            "post_body": "\n\nProblem context: This is a part of a past final exam question. It should help you appreciate how different optimizers can converge to different solutions when multiple solutions exist. As well as get a better sense for what the inductive bias of the different optimizers is like. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/IodSUOh6yQPdwZPr4qhUyw1n\" width=\"658\" height=\"455.80208333333337\"/></figure><paragraph/><paragraph>Problem context: This is a part of a past final exam question. It should help you appreciate how different optimizers can converge to different solutions when multiple solutions exist. As well as get a better sense for what the inductive bias of the different optimizers is like. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-13T13:15:41.368866+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6954661,
            "author": "Kevin Frans",
            "project_title": "HW2 Q1",
            "post_body": "\nProblem context: You've seen in your optimization courses how to move back and forth between constrained and unconstrained optimizations. You've seen this idea for (b) done in lecture explicitly with a hard constraint. This problem is there to help make sure you do things yourself and understand the connections more clearly. Do this problem as soon as you can, because it will help you appreciate lectures and discussions even more. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/hRccz92g9tQDDMP6tVfal4xC\" width=\"658\" height=\"695.5533498759305\"/></figure><paragraph><break/>Problem context: You've seen in your optimization courses how to move back and forth between constrained and unconstrained optimizations. You've seen this idea for (b) done in lecture explicitly with a hard constraint. This problem is there to help make sure you do things yourself and understand the connections more clearly. Do this problem as soon as you can, because it will help you appreciate lectures and discussions even more. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-13T13:15:03.332079+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6944928,
            "author": "Sultan Daniels",
            "project_title": "Discussion 2 Solution Thread",
            "post_body": "Discussion 2 and solutions:\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Discussion 2 and solutions:<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/b8b5thSG1Ech1J0fEIMYvQhz\" filename=\"dis02_question.pdf\"/><file url=\"https://static.us.edusercontent.com/files/hijs7QSM33yu5J00a0t4jdoK\" filename=\"dis02_solution.pdf\"/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-11T15:42:43.561759+10:00",
            "category": "Sections"
        },
        {
            "guid": 6944706,
            "author": "Anant Sahai",
            "project_title": "Check your enrollment...",
            "post_body": "Dear students,\n\nIf you submitted HW0, you should now be in the class. \n\nIf you did not submit HW0, you should now be dropped from the class.\n\n\n\nIf the above didn't happen for you properly, please feel tree to either respond in this thread (where everyone else can see --- so you probably don't want to do that) or make a private post here in Ed. You should not send an email to us about this. \n\nHopefully, this being settled will reduce everyone's stress and you can focus on learning the material. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Dear students,</paragraph><paragraph>If you submitted HW0, you should now be in the class. </paragraph><paragraph>If you did not submit HW0, you should now be dropped from the class.</paragraph><paragraph/><paragraph>If the above didn't happen for you properly, please feel tree to either respond in this thread (where everyone else can see --- so you probably don't want to do that) or make a private post here in Ed. You should not send an email to us about this. </paragraph><paragraph>Hopefully, this being settled will reduce everyone's stress and you can focus on learning the material. </paragraph><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-11T14:29:15.973995+10:00",
            "category": "Admin"
        },
        {
            "guid": 6937658,
            "author": "Gireeja Ranade",
            "project_title": "Lecture 3 thread",
            "post_body": "Following up from the question in lecture: I only partially stated the conditions required for the Lyapunov function to converge, sorry about that. They are written out fully here, and also corrected in the attached notes. The proof is fully correct, I just misspoke. \n\n\n\nTo show that the expected loss, and therefore $z$, goes to zero, we have to show that:\n\n1. It is non-negative.\n\n2. $L(z) = 0$ implies $z = 0$.\n\n3. Expected loss decreases at every step, and that there is a constant downward pressure, so it cannot converge somewhere other than zero. For this we show:\n\n$$E[L(z_{t+1})] \\leq C L(z_t)$$, where $0< C <  1$ is a constant.\n\nEssentially this shows an exponential decay, and the decay will continue as long as $L(z_t) > 0.$Once we show this, we see that the only place the loss can converge to is zero, because converging anywhere else would mean you could still decrease the expected loss, which would be a contradiction.\n\n\n\nNotes for lecture 2 and 3 are below. \n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Following up from the question in lecture: I only partially stated the conditions required for the Lyapunov function to converge, sorry about that. They are written out fully here, and also corrected in the attached notes. The proof is fully correct, I just misspoke. </paragraph><paragraph/><paragraph>To show that the expected loss, and therefore $z$, goes to zero, we have to show that:</paragraph><paragraph>1. It is non-negative.</paragraph><paragraph>2. $L(z) = 0$ implies $z = 0$.</paragraph><paragraph>3. Expected loss decreases at every step, and that there is a constant downward pressure, so it cannot converge somewhere other than zero. For this we show:</paragraph><paragraph>$$E[L(z_{t+1})] \\leq C L(z_t)$$, where $0&lt; C &lt;  1$ is a constant.</paragraph><paragraph>Essentially this shows an exponential decay, and the decay will continue as long as $L(z_t) &gt; 0.$Once we show this, we see that the only place the loss can converge to is zero, because converging anywhere else would mean you could still decrease the expected loss, which would be a contradiction.</paragraph><paragraph/><paragraph>Notes for lecture 2 and 3 are below. </paragraph><file url=\"https://static.us.edusercontent.com/files/Er1pb2HvgYsi7GNsVhdWoKkA\" filename=\"Lecture 2 (1).pdf\"/><file url=\"https://static.us.edusercontent.com/files/NnFTZ6v9HiF05ho2UjNXAqh5\" filename=\"Lecture 3.pdf\"/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-10T09:10:53.900393+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6932715,
            "author": "Sultan Daniels",
            "project_title": "HW0 Solutions",
            "post_body": "Here is a thread for discussing the solutions to HW 0. The datasets for the code can still be found at the Github repo for the course.",
            "content_xml": "<document version=\"2.0\"><paragraph>Here is a thread for discussing the solutions to HW 0. <link href=\"https://github.com/Berkeley-CS182/cs182fa25_public/tree/main/hw00/code/deeplearning/datasets\">The datasets for the code can still be found at the Github repo for the course.</link></paragraph><file url=\"https://static.us.edusercontent.com/files/Twv5xdhqAC0poSnqiYIrEBva\" filename=\"hw0codesolution.zip\"/><file/><file url=\"https://static.us.edusercontent.com/files/5bmdhY0mpj5ldLKyB8smTyK4\" filename=\"hw00_solution.pdf\"/></document>",
            "links": [
                "https://github.com/Berkeley-CS182/cs182fa25_public/tree/main/hw00/code/deeplearning/datasets"
            ],
            "attachments": [],
            "created_at": "2025-09-09T11:18:38.338212+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6931407,
            "author": "Anant Sahai",
            "project_title": "New GSI Office Hours and Discussion Sections (updated)",
            "post_body": "Starting tomorrow (Tue Sep 9th), we will have additional office hours staffed by GSIs. The scope for these is always going to be the current homework (So HW1 for this week), the solutions to the immediately past homework (So HW0 for this week), and the previous week's discussion section. Lecture questions should be asked either in Prof office hours or on the Ed thread for that lecture. \n\nGSI Office hours:\n\nTue 2-3pm in 102 Latimer\n\nTue 6-7pm in 405 Soda\n\nTue 7-8pm in 540 Cory  [Cancelled due to student request for day diversity]\n\nNEW: Fri 2-3 in 531 Cory\n\n\n\nDiscussion Sections (starting on Wed Sep 10th)\n\nWed 11-noon in 108 Wheeler\n\nWed 5-6pm in 242 Hearst Gym\n\nWed 6-7pm in 104 Social Sciences\n\nThe Wed 3-4pm and Wed 4-5pm discussions will also continue. With five discussion sections, we should be able to fit the expanded course. (Everyone who didn't do HW0 should expect to be dropped and everyone who did do HW0 should expect to get into the course.)\n\n\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Starting tomorrow (Tue Sep 9th), we will have additional office hours staffed by GSIs. The scope for these is always going to be the current homework (So HW1 for this week), the solutions to the immediately past homework (So HW0 for this week), and the previous week's discussion section. Lecture questions should be asked either in Prof office hours or on the Ed thread for that lecture. </paragraph><paragraph>GSI Office hours:</paragraph><paragraph>Tue 2-3pm in 102 Latimer</paragraph><paragraph>Tue 6-7pm in 405 Soda</paragraph><paragraph><strike>Tue 7-8pm in 540 Cory</strike>  [Cancelled due to student request for day diversity]</paragraph><paragraph><bold>NEW:</bold> Fri 2-3 in 531 Cory</paragraph><paragraph/><paragraph>Discussion Sections (starting on Wed Sep 10th)</paragraph><paragraph>Wed 11-noon in 108 Wheeler</paragraph><paragraph>Wed 5-6pm in 242 Hearst Gym</paragraph><paragraph>Wed 6-7pm in 104 Social Sciences</paragraph><paragraph>The Wed 3-4pm and Wed 4-5pm discussions will also continue. With five discussion sections, we should be able to fit the expanded course. (Everyone who didn't do HW0 should expect to be dropped and everyone who did do HW0 should expect to get into the course.)</paragraph><paragraph/><paragraph/><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-09T07:21:36.193959+10:00",
            "category": "Admin"
        },
        {
            "guid": 6920624,
            "author": "Justin Yang",
            "project_title": "Matrix Differentiation Notes",
            "post_body": "https://atmos.washington.edu/~dennis/MatrixCalculus.pdf was helpful for me for reviewing matrix operations for anyone else still reviewing.\n\nAlso I took some notes on it if anyone's interested\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph><link href=\"https://atmos.washington.edu/~dennis/MatrixCalculus.pdf\">https://atmos.washington.edu/~dennis/MatrixCalculus.pdf</link> was helpful for me for reviewing matrix operations for anyone else still reviewing.<break/><break/>Also I took some notes on it if anyone's interested<break/><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/PDZmRfDF7HUBFqZcIHmZIPzy\" filename=\"Matrix Stuff.pdf\"/><paragraph/></document>",
            "links": [
                "https://atmos.washington.edu/~dennis/MatrixCalculus.pdf"
            ],
            "attachments": [],
            "created_at": "2025-09-06T19:48:02.041047+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 6919704,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q7",
            "post_body": "Problem context: \n\nThis can be viewed as a continuation of the problem on the last HW, where here, we are also providing you with an opportunity to review basic conditional expectations in the context of jointly normal vectors. This interpretation provides you with an intuitive anchor for the Kernel Ridge form. Although we do not ask here, it also gives you a way to get an estimate for uncertainty in the solution to ridge regression by looking at the entire posterior. (This problem should also help you review more probability.)\n\nThis question will also be helpful for the large number of you who might find yourself in need of using Gaussian-Process based Bayesian approaches for machine learning. These tend to be quite useful in a diverse set of application domains where uncertainty quantification is useful, and is a place where Deep Learning approaches are increasingly being folded in (to help metalearn good kernels for application domains).",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/lRPdaXgzmA1KcQNDE6o2VBCz\" width=\"658\" height=\"700.310421286031\"/></figure><paragraph>Problem context: </paragraph><paragraph>This can be viewed as a continuation of the problem on the last HW, where here, we are also providing you with an opportunity to review basic conditional expectations in the context of jointly normal vectors. This interpretation provides you with an intuitive anchor for the Kernel Ridge form. Although we do not ask here, it also gives you a way to get an estimate for uncertainty in the solution to ridge regression by looking at the entire posterior. (This problem should also help you review more probability.)</paragraph><paragraph>This question will also be helpful for the large number of you who might find yourself in need of using Gaussian-Process based Bayesian approaches for machine learning. These tend to be quite useful in a diverse set of application domains where uncertainty quantification is useful, and is a place where Deep Learning approaches are increasingly being folded in (to help metalearn good kernels for application domains).</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T11:00:02.670544+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919698,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q6",
            "post_body": "Problem Context:\n\nMore review for you of basic ML and optimization ideas, although some of you may not have seen this exactly in your previous ML or optimzation course. However, it also provides you with a good opportunity to review the mathematical manipulations needed to understand this.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/M4gzjb1nsAgItiJRRWhmiVdD\" width=\"658\" height=\"550.3272727272727\"/></figure><paragraph>Problem Context:</paragraph><paragraph>More review for you of basic ML and optimization ideas, although some of you may not have seen this exactly in your previous ML or optimzation course. However, it also provides you with a good opportunity to review the mathematical manipulations needed to understand this.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:59:27.811611+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919693,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q5",
            "post_body": "Problem context: This is a past midterm question that also adds another perspective on how data augmentation can provide a regularizing effect.  In the previous homework, you saw augmenting with fake data that was very different from the given data. Here, the augmentation is modifications of the existing data for use during training. \n\nData augmentation is a really important part of deep learning practice, and it is good to understand why it works in a very simple setting. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/vlTdR64ocdLWURJwcuEjVcL1\" width=\"658\" height=\"1028.580931263858\"/></figure><paragraph>Problem context: This is a past midterm question that also adds another perspective on how data augmentation can provide a regularizing effect.  In the previous homework, you saw augmenting with fake data that was very different from the given data. Here, the augmentation is modifications of the existing data for use during training. </paragraph><paragraph>Data augmentation is a really important part of deep learning practice, and it is good to understand why it works in a very simple setting. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:58:55.688746+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919689,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q4",
            "post_body": "Problem context: \n\nThis is an old exam problem that is there to make sure you understand the basics of Adam as well as the connection of weight-decay to regularization in the SGD context.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/c9GWtb9VhhwxFP8vFwbkaorl\" width=\"658\" height=\"901.7558886509636\"/></figure><paragraph>Problem context: </paragraph><paragraph>This is an old exam problem that is there to make sure you understand the basics of Adam as well as the connection of weight-decay to regularization in the SGD context.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:58:19.415944+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919686,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q3",
            "post_body": "\n\nProblem context:\n\nThis has you work out for yourself why momentum can help accelerate convergence. The earlier problem sets the crucial background for this problem since if you don't understand gradient descent without momentum terms, you aren't going to understand the point of momentum. \n\nParts of the calculation can feel a bit grungy at times. But that's just the nature of what is going on. In Deep Learning, we often just have to power through the grunge --- using computer algebra systems can be useful in this regard. ",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/toyuemf9NjplPv4HDpbKYiJF\" width=\"658\" height=\"1148.1767676767677\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/BjyPs9dFyjeXvb9MybNPlCIs\" width=\"658\" height=\"668.2545454545455\"/></figure><paragraph/><paragraph>Problem context:</paragraph><paragraph>This has you work out for yourself why momentum can help accelerate convergence. The earlier problem sets the crucial background for this problem since if you don't understand gradient descent without momentum terms, you aren't going to understand the point of momentum. </paragraph><paragraph>Parts of the calculation can feel a bit grungy at times. But that's just the nature of what is going on. In Deep Learning, we often just have to power through the grunge --- using computer algebra systems can be useful in this regard. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:57:42.672188+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919682,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q2",
            "post_body": "Problem context:\n\nThis is a problem that asks you to carefully work through why SGD works for a simple setting. It's not clear that everyone in the class has seen a proof of SGD working in any setting, and it is important to understand this stuff for at least a simple example. (Just watching lecture is not enough.) The demo part is interesting to see so you can viscerally appreciate what is going on with SGD. There is a lot to observe and soak in from the demo actually, so we really want to encourage you to play with it.\n\nThis particular proof is elementary and gets at essential ideas, but it is not commonly appreciated or known. The core ideas here can be generalized broadly beyond this particular setting, but the mathematical abstractions involved (PL conditions, etc...) don't have the required \"conceptual bang for the buck\" needed to make it into this foundational course",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/v8FvhyhlTA9eaUQjejgvloFW\" width=\"568\" height=\"1300\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/JXGh6ySdvu0hHgg9su143s9x\" width=\"510\" height=\"838\"/></figure><paragraph>Problem context:</paragraph><paragraph>This is a problem that asks you to carefully work through why SGD works for a simple setting. It's not clear that everyone in the class has seen a proof of SGD working in any setting, and it is important to understand this stuff for at least a simple example. (Just watching lecture is not enough.) The demo part is interesting to see so you can viscerally appreciate what is going on with SGD. There is a lot to observe and soak in from the demo actually, so we really want to encourage you to play with it.</paragraph><paragraph>This particular proof is elementary and gets at essential ideas, but it is not commonly appreciated or known. The core ideas here can be generalized broadly beyond this particular setting, but the mathematical abstractions involved (PL conditions, etc...) don't have the required \"conceptual bang for the buck\" needed to make it into this foundational course</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:55:29.916796+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6919674,
            "author": "Sultan Daniels",
            "project_title": "HW1 Q1",
            "post_body": "Problem context:\n\nThis is likely review for most people, but without a solid understanding of what is going on here you can't understand why we use momentum or the need/motivation for adaptive methods like Adam. That's why it is really important for everyone to do this problem. Otherwise, you don't understand the most important optimization hyperparameter of them all: the learning rate.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/26gdOEb8pz13ZBAzANzgizgj\" width=\"658\" height=\"1056.3050847457628\"/></figure><paragraph>Problem context:</paragraph><paragraph>This is likely review for most people, but without a solid understanding of what is going on here you can't understand why we use momentum or the need/motivation for adaptive methods like Adam. That's why it is really important for everyone to do this problem. Otherwise, you don't understand the most important optimization hyperparameter of them all: the learning rate.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-06T10:52:48.264081+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6914620,
            "author": "Gireeja Ranade",
            "project_title": "Lecture 2 Thread",
            "post_body": "Lecture 2 questions thread. \n\n\n\n\n\nI will complete the SGD convergence proof in next weeks class, so save questions on that.",
            "content_xml": "<document version=\"2.0\"><paragraph>Lecture 2 questions thread. </paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/mNn50bgPeQFXz5VtcQlj1oJI\" filename=\"Lecture 2.pdf\"/><paragraph/><paragraph>I will complete the SGD convergence proof in next weeks class, so save questions on that.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-05T09:04:50.447728+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6911315,
            "author": "Anant Sahai",
            "project_title": "Discussion 1 Solution Thread",
            "post_body": "This thread is to support the discussion solution. By course policy, the discussion will be lightly supported for the week of discussion. After that, it will no longer be supported by us. For students, you have to stay caught up in the course. ",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/2KCUBO9rkyadPimvpN4YagZS\" filename=\"dis01_solution.pdf\"/><paragraph>This thread is to support the discussion solution. By course policy, the discussion will be lightly supported for the week of discussion. After that, it will no longer be supported by us. For students, you have to stay caught up in the course. </paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-04T14:59:14.800545+10:00",
            "category": "Sections"
        },
        {
            "guid": 6908196,
            "author": "Sultan Daniels",
            "project_title": "Discussion 1 Thread",
            "post_body": "Discussion 1 will be held at two times:\n\nWednesday at 3-4pm in Social Sciences Building 136\n\nand then nearby at:\n\nWednesday 4-5pm at Hearst Field Annex B1.\n\nIf you would like to run the coding portion of the lab locally, download this zip file: \n\nIf you would like to run the coding portion of the lab on Google Colab, go to this link: https://tinyurl.com/cs182-dis01-code.\n\nTo save time, it would be useful to get the code set up (including any environments, etc.) on your laptop before the discussion begins. \n\nFor the non-coding part, we will have printed discussion worksheets for you. We will have extras so even if students can't find a seat in the room, we can give you a worksheet so you can get together with other students without a seat and do your own discussion.\n\nThe structure of the discussion in the room will be very interactive. The time will be spent working in small groups on the problems and discussing the concepts, with course staff members circulating among the groups to aid with conceptual understanding, etc. \n\nThere will never be mini-lectures in this course and students are expected to come to discussion wanting and willing to work with others, and fully caught up on lectures and previously due homeworks.",
            "content_xml": "<document version=\"1.0\"><paragraph>Discussion 1 will be held at two times:</paragraph><paragraph>Wednesday at 3-4pm in Social Sciences Building 136</paragraph><paragraph>and then nearby at:</paragraph><paragraph>Wednesday 4-5pm at Hearst Field Annex B1.</paragraph><paragraph>If you would like to run the coding portion of the lab locally, download this zip file: </paragraph><paragraph>If you would like to run the coding portion of the lab on Google Colab, go to this link: https://tinyurl.com/cs182-dis01-code.</paragraph><paragraph>To save time, it would be useful to get the code set up (including any environments, etc.) on your laptop before the discussion begins. </paragraph><paragraph>For the non-coding part, we will have printed discussion worksheets for you. We will have extras so even if students can't find a seat in the room, we can give you a worksheet so you can get together with other students without a seat and do your own discussion.</paragraph><paragraph>The structure of the discussion in the room will be very interactive. The time will be spent working in small groups on the problems and discussing the concepts, with course staff members circulating among the groups to aid with conceptual understanding, etc. </paragraph><paragraph>There will never be mini-lectures in this course and students are expected to come to discussion wanting and willing to work with others, and fully caught up on lectures and previously due homeworks.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-04T04:54:39.687764+10:00",
            "category": "Sections"
        },
        {
            "guid": 6904759,
            "author": "Anant Sahai",
            "project_title": "Lecture 1 Thread",
            "post_body": "This is to discuss or ask questions pertaining to the lecture",
            "content_xml": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/2occ6GXddxDkaQD7QszYJpdk\" filename=\"Lecture1Sahai.pdf\"/><file url=\"https://static.us.edusercontent.com/files/5ehAy3zJMOpi7nlgKh5bxEK5\" filename=\"Lecture1Ranade.pdf\"/><paragraph>This is to discuss or ask questions pertaining to the lecture</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-09-03T09:18:48.709917+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6894737,
            "author": "Manan Roongta",
            "project_title": "Free Colab Pro for Students",
            "post_body": "Google Colab Pro is free for students (normally $10). You get monthly compute units and they are good for 90 days, so it's a good idea to sign up now and have them for heavier hws.\n\nhttps://colab.research.google.com/signup\n\n\n\n",
            "content_xml": "<document version=\"2.0\"><paragraph>Google Colab Pro is free for students (normally $10). You get monthly compute units and they are good for 90 days, so it's a good idea to sign up now and have them for heavier hws.</paragraph><paragraph><link href=\"https://colab.research.google.com/signup\">https://colab.research.google.com/signup</link></paragraph><paragraph/><paragraph/></document>",
            "links": [
                "https://colab.research.google.com/signup"
            ],
            "attachments": [],
            "created_at": "2025-08-31T13:09:03.195661+10:00",
            "category": "Admin"
        },
        {
            "guid": 6891458,
            "author": "Micah Mok",
            "project_title": "Friends to Talk about Deep Learning With",
            "post_body": "Hi guys! My name is Micah Mok. I'm definitely newer and very hungry to learn about ML. I would love to create a community where we can talk about the ideas we're thinking about and the projects we're working on. In addition, study buddies are always nice. If you're interested in any of that, I'm making a \"Deep Learning at Berkeley\" Slack workspace. Come join!\n\nThanks,\n\nhttps://join.slack.com/t/deeplearninga-j2h9314/shared_invite/zt-3c7b6s2uv-4m~g2YaJguBoPa9dsua1Bw",
            "content_xml": "<document version=\"2.0\"><paragraph>Hi guys! My name is Micah Mok. I'm definitely newer and very hungry to learn about ML. I would love to create a community where we can talk about the ideas we're thinking about and the projects we're working on. In addition, study buddies are always nice. If you're interested in any of that, I'm making a \"Deep Learning at Berkeley\" Slack workspace. Come join!</paragraph><paragraph>Thanks,</paragraph><paragraph>https://join.slack.com/t/deeplearninga-j2h9314/shared_invite/zt-3c7b6s2uv-4m~g2YaJguBoPa9dsua1Bw</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T07:33:59.1233+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 6890959,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q7",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/XmvrGEKwH9uzVWM36WlzZCiQ\" width=\"658\" height=\"184.3736040609137\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:36:52.16716+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890956,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q6",
            "post_body": "\n\nProblem Context:\n\nWe need to make sure that everyone is comfortable with basic programming and also understands basic neural nets from their earlier ML course.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/5dDAu05etV7Ay2vmfHBjU1yE\" width=\"658\" height=\"184.3736040609137\"/></figure><paragraph/><paragraph>Problem Context:</paragraph><paragraph>We need to make sure that everyone is comfortable with basic programming and also understands basic neural nets from their earlier ML course.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:36:43.097429+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890952,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q5",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/L0oVxC9pZukNWnLjnw2ySVQc\" width=\"658\" height=\"284.5766497461929\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/7srCK4ThqovJAax0aKGEVDi1\" width=\"658\" height=\"428.2010152284264\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/5ehaEvlWdFEkW7so7MVEk0Ub\" width=\"658\" height=\"184.3736040609137\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:36:17.65215+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890943,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q4",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/gVVPsgCbOxqJqrvIKQHgh1fs\" width=\"658\" height=\"301.2771573604061\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/hzjN81Elty9OfXkAhLzSiaZP\" width=\"658\" height=\"371.4192893401015\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/y4EjpNH43JAFVf5fzAFmwmXo\" width=\"658\" height=\"213.0984771573604\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/TrSafHpmt9EskYnsNLRWfxjB\" width=\"658\" height=\"277.8964467005076\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/dkxZqi1b0HEGfloOgMBYPuXt\" width=\"658\" height=\"107.551269035533\"/></figure><paragraph/></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:34:07.828947+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890933,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q3",
            "post_body": "\nProblem Context:\n\nThis is another review problem that is just there to help you bring some classic material back into your memory. You'll need facility with this --- moving to SVD coordinates, etc. --- to understand key ideas in the course.",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/pdjFoZiuDo9e1PY7TxW16MlZ\" width=\"658\" height=\"428.2010152284264\"/></figure><paragraph><break/>Problem Context:</paragraph><paragraph>This is another review problem that is just there to help you bring some classic material back into your memory. You'll need facility with this --- moving to SVD coordinates, etc. --- to understand key ideas in the course.</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:32:17.744468+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890923,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q2",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/1XdBM8RHry8XFh80gvD2q8Ib\" width=\"658\" height=\"179.69746192893402\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/xg1HKKhUwriqEUPHx8uWsA8R\" width=\"658\" height=\"120.24365482233502\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:31:10.390007+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6890908,
            "author": "Patrick Mendoza",
            "project_title": "HW0 Q1",
            "post_body": "",
            "content_xml": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/EK7oHHcvefl8M4drKfgVB4qS\" width=\"658\" height=\"424.32394366197184\"/></figure></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-30T05:28:43.876101+10:00",
            "category": "Problem Sets"
        },
        {
            "guid": 6887897,
            "author": "Anant Sahai",
            "project_title": "Lecture 0 Thread",
            "post_body": "The lecture video is available:\n\n https://www.youtube.com/live/53sm-H51TqA?si=08DVgOuDDTgKeLj5\n\nThis is the thread for asking questions related to this lecture. In general, there will be lecture threads for every lecture given. Help your fellow students out by answering their questions. ",
            "content_xml": "<document version=\"2.0\"><paragraph>The lecture video is available:</paragraph><paragraph> <link href=\"https://www.youtube.com/live/53sm-H51TqA?si=08DVgOuDDTgKeLj5\">https://www.youtube.com/live/53sm-H51TqA?si=08DVgOuDDTgKeLj5</link></paragraph><paragraph>This is the thread for asking questions related to this lecture. In general, there will be lecture threads for every lecture given. Help your fellow students out by answering their questions. </paragraph></document>",
            "links": [
                "https://www.youtube.com/live/53sm-H51TqA?si=08DVgOuDDTgKeLj5"
            ],
            "attachments": [],
            "created_at": "2025-08-29T09:16:51.902539+10:00",
            "category": "Lectures"
        },
        {
            "guid": 6887086,
            "author": "Sammie Smith",
            "project_title": "Study Group: Women Identifying / AFAB Students",
            "post_body": "Study Group Meet Up! (opt in space for women) \n\nHey everyone, my name is Sammie! I'm a 4th year double major in Data Science and Computer Science, and I'm currently enrolled in this class. (I'm from rural Oregon, I'm on the Cal women's ultimate frisbee team, and I love to skate)\n\nI reserved Room B1M20E in Kresge Engineering & Mathematical Sciences Library (capacity 15 people) from 12pm-2pm next Wednesday, Sept 3.\n\nIf you self-identify as a woman, please show up so we can get a study group going / exchange numbers!\n\nIntroduce yourself on this thread in the meantime!\n\nName\n\nMajor, year\n\nWhere you're from/hobbies\n\nIf you're enrolled vs waitlisted in this class",
            "content_xml": "<document version=\"2.0\"><heading level=\"2\">Study Group Meet Up! (opt in space for women) </heading><paragraph>Hey everyone, my name is Sammie! I'm a 4th year double major in Data Science and Computer Science, and I'm currently enrolled in this class. (I'm from rural Oregon, I'm on the Cal women's ultimate frisbee team, and I love to skate)</paragraph><paragraph>I reserved <bold>Room B1M20E</bold> in Kresge Engineering &amp; Mathematical Sciences Library (capacity 15 people) from <bold>12pm-2pm next Wednesday, Sept 3</bold>.</paragraph><paragraph>If you self-identify as a woman, please show up so we can get a study group going / exchange numbers!</paragraph><paragraph>Introduce yourself on this thread in the meantime!</paragraph><list style=\"bullet\"><list-item><paragraph>Name</paragraph></list-item><list-item><paragraph>Major, year</paragraph></list-item><list-item><paragraph>Where you're from/hobbies</paragraph></list-item><list-item><paragraph>If you're enrolled vs waitlisted in this class</paragraph></list-item></list></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-29T06:07:25.11396+10:00",
            "category": "Curiosity"
        },
        {
            "guid": 6886970,
            "author": "Gireeja Ranade",
            "project_title": "Office Hours Now",
            "post_body": "Office Hours now in 400 Cory!",
            "content_xml": "<document version=\"2.0\"><paragraph>Office Hours now in 400 Cory!</paragraph></document>",
            "links": [],
            "attachments": [],
            "created_at": "2025-08-29T05:40:59.811182+10:00",
            "category": "Admin"
        }
    ]
}